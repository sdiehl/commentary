<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
</head>
<body>
<div id="TOC">
<ul>
<li><a href="#ghc-source-code-abbreviations">GHC Source Code Abbreviations</a></li>
<li><a href="#aging-in-the-generational-gc">Aging in the generational GC</a></li>
<li><a href="#improving-llvm-alias-analysis">Improving LLVM Alias Analysis</a><ul>
<li><a href="#llvm-alias-analysis-infrastructure">LLVM Alias Analysis Infrastructure</a></li>
<li><a href="#maxs-work">Max's Work</a></li>
<li><a href="#tbaa">TBAA</a></li>
<li><a href="#stg-cmm-alias-properties">STG / Cmm Alias Properties</a></li>
<li><a href="#how-to-track-tbaa-information">How to Track TBAA information</a></li>
<li><a href="#llvm-type-system">LLVM type system</a></li>
<li><a href="#problems-optmisations-to-solve">Problems / Optmisations to Solve</a><ul>
<li><a href="#llvm-optimisations">LLVM Optimisations</a></li>
<li><a href="#safe-loads-speculative-load">Safe Loads (speculative load)</a></li>
<li><a href="#ghc-heap-check-case-merging">GHC Heap Check (case merging)</a></li>
</ul></li>
</ul></li>
<li><a href="#ghc-commentary-the-ghc-api">GHC Commentary: The GHC API</a><ul>
<li><a href="#targets">Targets</a></li>
<li><a href="#dependency-analysis">Dependency Analysis</a></li>
<li><a href="#the-modsummary-type">The !ModSummary type</a></li>
<li><a href="#loading-compiling-the-modules">Loading (compiling) the Modules</a></li>
</ul></li>
<li><a href="#ghc-commentary-asynchronous-exceptions">GHC Commentary: Asynchronous Exceptions</a></li>
<li><a href="#ghc-commentary-backends">GHC Commentary: Backends</a></li>
<li><a href="#types-in-the-back-end-aka-the-rep-swamp">Types in the back end (aka &quot;The `Rep` swamp&quot;)</a><ul>
<li><a href="#cmmtype">`CmmType`</a></li>
<li><a href="#the-machop-type">The `MachOp` type</a></li>
<li><a href="#foreign-calls-and-hints">Foreign calls and hints</a></li>
<li><a href="#native-code-generation-and-the-size-type">Native code generation and the `Size` type</a></li>
</ul></li>
<li><a href="#the-block-allocator">The Block Allocator</a><ul>
<li><a href="#structure-of-blocks">Structure of blocks</a></li>
</ul></li>
<li><a href="#ghc-commentary-garbage-collecting-cafs">GHC Commentary: Garbage Collecting CAFs</a><ul>
<li><a href="#static-reference-tables">Static Reference Tables</a></li>
<li><a href="#evacuating-static-objects">Evacuating Static Objects</a></li>
</ul></li>
<li><a href="#calling-convention">Calling Convention</a></li>
<li><a href="#return-convention">Return Convention</a><ul>
<li><a href="#historical-page">Historical page</a></li>
</ul></li>
<li><a href="#cleanup-after-the-new-codegen-is-enabled">Cleanup after the new codegen is enabled</a><ul>
<li><a href="#independent-tasks">Independent tasks</a></li>
<li><a href="#towards-removing-codegencg">Towards removing codeGen/Cg*</a></li>
<li><a href="#towards-removing-oldcmm">Towards removing `OldCmm`</a></li>
<li><a href="#later">Later</a></li>
</ul></li>
<li><a href="#cmm-implementing-exception-handling">Cmm: Implementing Exception Handling</a><ul>
<li><a href="#note-to-reader">Note To Reader</a></li>
</ul></li>
<li><a href="#table-of-contents">Table of Contents</a></li>
<li><a href="#the-cmm-language">The Cmm language</a><ul>
<li><a href="#additions-in-cmm">Additions in Cmm</a></li>
<li><a href="#compiling-cmm-with-ghc">Compiling Cmm with GHC</a></li>
<li><a href="#basic-cmm">Basic Cmm</a><ul>
<li><a href="#code-blocks-in-cmm">Code Blocks in Cmm</a></li>
<li><a href="#variables-registers-and-types">Variables, Registers and Types</a></li>
<li><a href="#literals-and-labels">Literals and Labels</a></li>
<li><a href="#sections-and-directives">Sections and Directives</a></li>
<li><a href="#expressions">Expressions</a></li>
<li><a href="#statements-and-calls">Statements and Calls</a></li>
<li><a href="#operators-and-primitive-operations">Operators and Primitive Operations</a></li>
</ul></li>
<li><a href="#cmm-design-observations-and-areas-for-potential-improvement">Cmm Design: Observations and Areas for Potential Improvement</a></li>
</ul></li>
<li><a href="#ghc-commentary-what-the-hell-is-a-.cmm-file">GHC Commentary: What the hell is a `.cmm` file?</a><ul>
<li><a href="#reading-references">Reading references</a></li>
<li><a href="#other-information">Other information</a></li>
</ul></li>
<li><a href="#code-generator">Code Generator</a><ul>
<li><a href="#a-brief-history-of-code-generator">A brief history of code generator</a></li>
<li><a href="#overview">Overview</a></li>
<li><a href="#first-stage-stg-to-cmm-conversion">First stage: STG to Cmm conversion</a></li>
<li><a href="#second-stage-the-cmm-pipeline">Second stage: the Cmm pipeline</a></li>
<li><a href="#dumping-and-debugging-cmm">Dumping and debugging Cmm</a></li>
<li><a href="#register-allocator-code">Register Allocator Code</a><ul>
<li><a href="#the-register-allocator">The register allocator</a></li>
<li><a href="#graph-coloring">Graph coloring</a></li>
<li><a href="#miscellanea">Miscellanea</a></li>
</ul></li>
</ul></li>
<li><a href="#the-ghc-commentary---coding-style-guidelines-for-the-compiler">The GHC Commentary - Coding Style Guidelines for the compiler</a><ul>
<li><a href="#general-style">General Style</a></li>
<li><a href="#comments">Comments</a><ul>
<li><a href="#comments-on-top-level-entities">Comments on top-level entities</a></li>
<li><a href="#comments-in-the-source-code">Comments in the source code</a></li>
<li><a href="#comments-and-examples">Comments and examples</a></li>
<li><a href="#longer-comments-or-architectural-commentary">Longer comments or architectural commentary</a></li>
<li><a href="#commit-messages">Commit messages</a></li>
</ul></li>
<li><a href="#warnings">Warnings</a></li>
<li><a href="#exports-and-imports">Exports and Imports</a><ul>
<li><a href="#exports">Exports</a></li>
<li><a href="#imports">Imports</a></li>
</ul></li>
<li><a href="#compiler-versions-and-language-extensions">Compiler versions and language extensions</a><ul>
<li><a href="#section"></a></li>
<li><a href="#literate-haskell">Literate Haskell</a></li>
<li><a href="#the-c-preprocessor-cpp">The C Preprocessor (CPP)</a></li>
<li><a href="#platform-tests">Platform tests</a></li>
</ul></li>
<li><a href="#tabs-vs-spaces">Tabs vs Spaces</a></li>
</ul></li>
<li><a href="#coercions-in-ghcs-core-language">Coercions in GHC's core language</a><ul>
<li><a href="#difficulties-with-the-current-approach">Difficulties with the current approach</a></li>
<li><a href="#main-proposal">Main proposal</a></li>
</ul></li>
<li><a href="#parsing-of-command-line-arguments">Parsing of command line arguments</a><ul>
<li><a href="#static-flags">Static flags</a></li>
<li><a href="#dynamic-flags">Dynamic flags</a></li>
</ul></li>
<li><a href="#the-ghc-commentary">The GHC Commentary</a><ul>
<li><a href="#editing-the-commentary">Editing the Commentary</a></li>
<li><a href="#contents">Contents</a></li>
<li><a href="#contributed-documentation">Contributed Documentation</a></li>
</ul></li>
<li><a href="#compiler-and-runtime-system-ways-in-ghc">Compiler and runtime system ways in GHC</a><ul>
<li><a href="#available-ways-in-a-standard-ghc">Available ways in a standard GHC</a><ul>
<li><a href="#ways-for-parallel-execution-on-clusters-and-multicores">Ways for parallel execution on clusters and multicores</a></li>
</ul></li>
<li><a href="#combining-ways">Combining ways</a></li>
</ul></li>
<li><a href="#internals">Internals</a></li>
<li><a href="#ghc-commentary-the-compiler">GHC Commentary: The Compiler</a><ul>
<li><a href="#overall-structure">Overall Structure</a></li>
<li><a href="#what-problems-do-we-need-to-solve">What problems do we need to solve?</a></li>
<li><a href="#current-mechanisms">Current mechanisms</a></li>
<li><a href="#new-concepts-for-backpack">New concepts for Backpack</a></li>
<li><a href="#features">Features</a></li>
<li><a href="#constraints">Constraints</a></li>
</ul></li>
<li><a href="#rts-configurations">RTS Configurations</a><ul>
<li><a href="#combinations">Combinations</a></li>
<li><a href="#other-configuration-options">Other configuration options</a></li>
</ul></li>
<li><a href="#contracts-for-haskell">Contracts for Haskell</a><ul>
<li><a href="#involved">Involved</a></li>
<li><a href="#overview-1">Overview</a></li>
<li><a href="#the-plan">The plan</a></li>
<li><a href="#current-status">Current status</a></li>
<li><a href="#questions">Questions</a></li>
<li><a href="#references">References</a></li>
</ul></li>
<li><a href="#the-ghc-commentary-coding-style-guidelines-for-rts-c-code">The GHC Commentary: Coding Style Guidelines for RTS C code</a><ul>
<li><a href="#comments-1">Comments</a></li>
<li><a href="#references-1">References</a></li>
<li><a href="#portability-issues">Portability issues</a><ul>
<li><a href="#which-c-standard">Which C Standard?</a></li>
<li><a href="#other-portability-conventions">Other portability conventions</a></li>
</ul></li>
<li><a href="#debuggingrobustness-tricks">Debugging/robustness tricks</a></li>
<li><a href="#syntactic-details">Syntactic details</a></li>
<li><a href="#inline-functions">Inline functions</a></li>
<li><a href="#source-control-issues">Source-control issues</a></li>
</ul></li>
<li><a href="#copying-gc">Copying GC</a></li>
<li><a href="#the-type">The  type</a><ul>
<li><a href="#case-expressions">Case expressions</a></li>
<li><a href="#shadowing">Shadowing</a></li>
<li><a href="#human-readable-core-generation">Human readable Core generation</a></li>
</ul></li>
<li><a href="#cps-conversion">CPS Conversion</a><ul>
<li><a href="#overview-2">Overview</a></li>
<li><a href="#design-aspects">Design Aspects</a></li>
<li><a href="#simple-design">Simple Design</a></li>
<li><a href="#to-be-worked-out">To be worked out</a></li>
<li><a href="#pipeline">Pipeline</a></li>
<li><a href="#todo">TODO</a></li>
<li><a href="#current-pipeline">Current Pipeline</a><ul>
<li><a href="#section-1"></a></li>
</ul></li>
<li><a href="#non-cps-changes">Non-CPS Changes</a></li>
<li><a href="#notes">Notes</a></li>
<li><a href="#loopholes">Loopholes</a><ul>
<li><a href="#gc-blocks">GC Blocks</a></li>
<li><a href="#update-frames">Update Frames</a></li>
<li><a href="#user-defined-continuations">User defined continuations</a></li>
<li><a href="#branches-to-continuations">Branches to continuations</a></li>
</ul></li>
<li><a href="#not-in-scope-of-current-work">Not in Scope of Current Work</a><ul>
<li><a href="#static-reference-table-handling-srt">Static Reference Table Handling (SRT)</a></li>
<li><a href="#cmm-optimization-assumed-by-cps">Cmm Optimization assumed by CPS</a></li>
</ul></li>
<li><a href="#notes-on-future-development">Notes on future development</a><ul>
<li><a href="#handling-gc">Handling GC</a></li>
</ul></li>
</ul></li>
<li><a href="#the-ghc-commentary-data-types-and-data-constructors">The GHC Commentary: Data types and data constructors</a><ul>
<li><a href="#data-types">Data types</a></li>
</ul></li>
<li><a href="#the-life-cycle-of-a-data-type">The life cycle of a data type</a><ul>
<li><a href="#the-constructor-wrapper-functions">The constructor wrapper functions</a></li>
<li><a href="#the-constructor-worker-functions">The constructor worker functions</a></li>
<li><a href="#external-core">External Core</a></li>
<li><a href="#unboxing-strict-fields">Unboxing strict fields</a></li>
<li><a href="#labels-and-info-tables">Labels and info tables</a></li>
</ul></li>
<li><a href="#demand-analyser-in-ghc">Demand analyser in GHC</a><ul>
<li><a href="#demand-signatures">Demand signatures</a><ul>
<li><a href="#demand-descriptions">Demand descriptions</a></li>
</ul></li>
<li><a href="#worker-wrapper-split">Worker-Wrapper split</a></li>
<li><a href="#relevant-compiler-parts">Relevant compiler parts</a></li>
</ul></li>
<li><a href="#support-for-deriving-and-instances">Support for deriving , , and  instances</a><ul>
<li><a href="#example">Example</a></li>
<li><a href="#algorithm-description">Algorithm description</a><ul>
<li><a href="#section-2"></a></li>
<li><a href="#section-3"></a></li>
<li><a href="#section-4"></a></li>
<li><a href="#covariant-and-contravariant-positions">Covariant and contravariant positions</a></li>
</ul></li>
<li><a href="#requirements-for-legal-instances">Requirements for legal instances</a><ul>
<li><a href="#relaxed-universality-check-for">Relaxed universality check for </a></li>
</ul></li>
<li><a href="#alternative-strategy-for-deriving-foldable-and-traversable">Alternative strategy for deriving `Foldable` and `Traversable`</a></li>
</ul></li>
<li><a href="#llvm-back-end-design">LLVM Back-end Design</a></li>
<li><a href="#implementation">Implementation</a><ul>
<li><a href="#framework">Framework</a></li>
<li><a href="#llvm-code-generation">LLVM Code Generation</a></li>
<li><a href="#register-pinning">Register Pinning</a></li>
<li><a href="#code-generation">Code Generation</a><ul>
<li><a href="#unregisterised-vs.-registerised">Unregisterised Vs. Registerised</a></li>
</ul></li>
<li><a href="#cmmdata">!CmmData</a><ul>
<li><a href="#st-pass-generation">1st Pass : Generation</a></li>
</ul></li>
<li><a href="#cmmstaticlit">!CmmStaticLit</a><ul>
<li><a href="#nd-pass-resolution">2nd Pass : Resolution</a></li>
</ul></li>
<li><a href="#cmmproc">!CmmProc</a></li>
</ul></li>
<li><a href="#desugaring-instance-declarations">Desugaring instance declarations</a><ul>
<li><a href="#basic-stuff">Basic stuff</a></li>
<li><a href="#dictionary-functions">Dictionary functions</a></li>
<li><a href="#the-inline-strategy">The INLINE strategy</a></li>
<li><a href="#the-out-of-line-a-strategy">The out-of-line (A) strategy</a></li>
<li><a href="#the-out-of-line-b-strategy">The out-of-line (B) strategy</a></li>
<li><a href="#user-inline-pragmas-and-out-of-line-a">User INLINE pragmas and out-of-line (A)</a></li>
<li><a href="#summary">Summary</a></li>
</ul></li>
<li><a href="#bugs-other-problems">Bugs &amp; Other Problems</a></li>
<li><a href="#compiling-more-than-one-module-at-once">Compiling more than one module at once</a><ul>
<li><a href="#the-overall-driver">The overall driver</a><ul>
<li><a href="#dependency-analysis-1">Dependency analysis</a></li>
<li><a href="#recompilation-checking-and-stability">Recompilation checking and stability</a></li>
<li><a href="#compilation">Compilation</a></li>
</ul></li>
</ul></li>
<li><a href="#eager-promotion">Eager Promotion</a></li>
<li><a href="#eager-version-bumping-strategy">Eager Version Bumping Strategy</a></li>
<li><a href="#data-types-for-haskell-entities-and">Data types for Haskell entities: , , , , and </a><ul>
<li><a href="#type-variables-and-term-variables">Type variables and term variables</a></li>
<li><a href="#and-implict-ids"> and implict Ids</a></li>
</ul></li>
<li><a href="#hc-files-and-the-evil-mangler">HC files and the Evil Mangler</a></li>
<li><a href="#strictness-analysis-examples">Strictness analysis: examples</a></li>
<li><a href="#system-fc-equality-constraints-and-coercions">System FC: equality constraints and coercions</a><ul>
<li><a href="#coercions-and-coercion-kinds">Coercions and Coercion Kinds</a></li>
<li><a href="#gadts">GADTs</a></li>
<li><a href="#representation-of-coercion-assumptions">Representation of coercion assumptions</a></li>
<li><a href="#newtypes-are-coerced-types">Newtypes are coerced types</a></li>
<li><a href="#roles">Roles</a></li>
<li><a href="#simplification">Simplification</a></li>
</ul></li>
<li><a href="#ghc-commentary-runtime-aspects-of-the-ffi">GHC Commentary: Runtime aspects of the FFI</a><ul>
<li><a href="#foreign-import-wrapper">Foreign Import &quot;wrapper&quot;</a></li>
</ul></li>
<li><a href="#function-calls">Function Calls</a><ul>
<li><a href="#generic-apply">Generic apply</a></li>
</ul></li>
<li><a href="#the-garbage-collector">The Garbage Collector</a><ul>
<li><a href="#gc-overview">GC overview</a></li>
<li><a href="#gc-data-structures">GC data structures</a><ul>
<li><a href="#generation">generation</a></li>
<li><a href="#nursery">nursery</a></li>
</ul></li>
</ul></li>
<li><a href="#i-know-kung-fu-learning-stg-by-example">I know kung fu: learning STG by example</a><ul>
<li><a href="#what-is-stg-exactly">What is STG, exactly?</a></li>
<li><a href="#an-overview-of-the-stg-machine">An overview of the STG machine</a><ul>
<li><a href="#components-of-the-machine">Components of the machine</a></li>
<li><a href="#important-concepts-in-the-machine">Important concepts in the machine</a></li>
<li><a href="#overview-of-execution-model-of-the-machine">Overview of execution model of the machine</a></li>
</ul></li>
<li><a href="#saturated-application-to-known-functions">Saturated application to known functions</a><ul>
<li><a href="#example-1-function-application-with-sufficient-stack-space">Example 1: function application with sufficient stack space</a></li>
<li><a href="#example-2-function-application-that-needs-to-grow-the-stack">Example 2: function application that needs to grow the stack</a></li>
</ul></li>
<li><a href="#example-3-unsaturated-applications-to-known-functions">Example 3: Unsaturated applications to known functions</a></li>
<li><a href="#example-4-applications-to-unknown-functions">Example 4: Applications to unknown functions</a><ul>
<li><a href="#dealing-with-generic-application">Dealing with generic application</a></li>
<li><a href="#making-the-call-to-the-generic-application-code">Making the call to the generic application code</a></li>
</ul></li>
<li><a href="#example-5-oversaturated-applications-to-known-functions">Example 5: oversaturated applications to known functions</a></li>
<li><a href="#example-6-allocation-of-thunks-and-data">Example 6: allocation of thunks and data</a><ul>
<li><a href="#checking-for-sufficient-heap-space">Checking for sufficient heap space</a></li>
<li><a href="#performing-the-actual-allocation">Performing the actual allocation</a></li>
<li><a href="#returning-an-allocated-value-to-the-caller">Returning an allocated value to the caller</a></li>
</ul></li>
<li><a href="#example-7-case-expressions">Example 7: `case` expressions</a><ul>
<li><a href="#forcing-the-scrutinee-of-the-case">Forcing the scrutinee of the `case`</a></li>
<li><a href="#dealing-with-the-forced-scrutinee">Dealing with the forced scrutinee</a></li>
</ul></li>
<li><a href="#example-8-thunks-and-thunk-update">Example 8: thunks and thunk update</a><ul>
<li><a href="#thunk-entry-point">Thunk entry point</a></li>
<li><a href="#continuation-of-the-thunk">Continuation of the thunk</a></li>
</ul></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul></li>
<li><a href="#support-for-generic-programming">Support for generic programming</a><ul>
<li><a href="#status">Status</a></li>
<li><a href="#main-components">Main components</a></li>
<li><a href="#things-that-have-been-removed">Things that have been removed</a></li>
<li><a href="#what-already-works">What already works</a></li>
<li><a href="#testing">Testing</a></li>
</ul></li>
<li><a href="#kind-polymorphic-overhaul">Kind polymorphic overhaul</a><ul>
<li><a href="#generic-representation-universe">Generic representation universe</a></li>
<li><a href="#universe-interpretation">Universe interpretation</a><ul>
<li><a href="#names">Names</a></li>
</ul></li>
<li><a href="#metadata-representation">Metadata representation</a></li>
<li><a href="#conversion-between-user-datatypes-and-generic-representation">Conversion between user datatypes and generic representation</a></li>
<li><a href="#example-generic-function-fmap-kind--">Example generic function: `fmap` (kind `* -&gt; *`)</a></li>
<li><a href="#example-generic-function-show-kind-uses-metadata">Example generic function: `show` (kind `*`, uses metadata)</a></li>
<li><a href="#example-datatype-encoding-lists-derived-by-the-compiler">Example datatype encoding: lists (derived by the compiler)</a><ul>
<li><a href="#digression">Digression</a></li>
</ul></li>
<li><a href="#ghc-8.0-and-later">GHC 8.0 and later</a><ul>
<li><a href="#type-level-metadata-encoding">Type-level metadata encoding</a></li>
<li><a href="#strictness">Strictness</a></li>
</ul></li>
<li><a href="#source-tree-layout">Source Tree Layout</a></li>
<li><a href="#build-system-basics">Build System Basics</a></li>
<li><a href="#coding-style">Coding Style</a></li>
</ul></li>
<li><a href="#the-ghc-commentary-ghci">The GHC Commentary: GHCi</a><ul>
<li><a href="#debugging-the-interpreter">Debugging the interpreter</a></li>
<li><a href="#useful-stuff-to-know-about-the-interpreter">Useful stuff to know about the interpreter</a><ul>
<li><a href="#stack-management">Stack management</a></li>
<li><a href="#building-constructors">Building constructors</a></li>
<li><a href="#perspective">Perspective</a></li>
</ul></li>
<li><a href="#case-returns-between-interpreted-and-compiled-code">case returns between interpreted and compiled code</a><ul>
<li><a href="#returning-to-interpreted-code.">Returning to interpreted code.</a></li>
<li><a href="#returning-to-compiled-code.">Returning to compiled code.</a></li>
</ul></li>
<li><a href="#unboxed-tuples-a-right-royal-spanner-in-the-works">Unboxed tuples: a Right Royal Spanner In The Works</a></li>
</ul></li>
<li><a href="#porting-ghc-using-llvm-backend">Porting GHC using LLVM backend</a><ul>
<li><a href="#registerised-mode">Registerised Mode</a></li>
</ul></li>
<li><a href="#packages-in-ghc">Packages in GHC</a><ul>
<li><a href="#the-problem">The problem</a></li>
<li><a href="#assumptions">Assumptions</a></li>
<li><a href="#the-open-question">The open question</a></li>
<li><a href="#plan-a-ghcs-current-story">Plan A: GHC's current story</a></li>
<li><a href="#plan-b-package-mounting">Plan B: package mounting</a></li>
<li><a href="#plan-c-mention-the-package-in-the-import">Plan C: mention the package in the import</a></li>
</ul></li>
<li><a href="#problems">Problems</a><ul>
<li><a href="#breaking-re-installations">Breaking re-installations</a></li>
<li><a href="#type-errors-when-using-packages-together">Type errors when using packages together</a></li>
</ul></li>
<li><a href="#goals">Goals</a></li>
<li><a href="#implementation-plan">Implementation Plan</a><ul>
<li><a href="#persistent-package-store">Persistent package store</a></li>
<li><a href="#views">Views</a></li>
<li><a href="#consistent-developer-environment">Consistent developer environment</a></li>
<li><a href="#garbage-collection">Garbage collection</a></li>
<li><a href="#cabal-remove">cabal remove</a></li>
<li><a href="#cabal-upgrade">cabal upgrade</a></li>
<li><a href="#current-status-1">Current Status</a><ul>
<li><a href="#unique-install-location">Unique Install Location</a></li>
<li><a href="#ghc-pkg">ghc-pkg</a></li>
<li><a href="#adhoc-dependency-resolution">Adhoc dependency resolution</a></li>
<li><a href="#detect-whether-an-overwrite-happens-and-warn-about-it">Detect whether an overwrite happens and warn about it</a></li>
<li><a href="#communicate-the-installedpackageid-back-to-cabal-install">Communicate the `InstalledPackageId` back to cabal-install</a></li>
<li><a href="#garbage-collection-1">Garbage Collection</a></li>
<li><a href="#about-shadowing">About Shadowing</a></li>
<li><a href="#about-unique-identifier">About Unique Identifier</a></li>
</ul></li>
<li><a href="#original-plan">Original Plan</a></li>
<li><a href="#hashes-and-identifiers">Hashes and identifiers</a></li>
<li><a href="#install-location-of-installed-cabal-packages">Install location of installed Cabal packages</a><ul>
<li><a href="#hash">Hash</a></li>
<li><a href="#unique-number">Unique number</a></li>
</ul></li>
<li><a href="#ghc-pkg-1">`ghc-pkg`</a></li>
<li><a href="#simplistic-dependency-resolution">Simplistic dependency resolution</a></li>
<li><a href="#build-flavours">Build flavours</a><ul>
<li><a href="#the-cabal-hash">The Cabal hash</a></li>
<li><a href="#released-and-unreleased-packages">Released and Unreleased packages</a></li>
</ul></li>
<li><a href="#dependency-resolution-in-cabal-install">Dependency resolution in cabal-install</a></li>
<li><a href="#garbage-collection-2">Garbage Collection</a></li>
<li><a href="#currently-open-design-decisions">Currently open design decisions</a><ul>
<li><a href="#installedpackageid-and-install-path">`InstalledPackageId` and install path</a></li>
<li><a href="#handling-of-dirty-builds">Handling of dirty builds</a></li>
<li><a href="#build-flavours-1">Build flavours</a></li>
<li><a href="#installedpackageinfo-and-solver-algorithm">`InstalledPackageInfo` and solver algorithm</a></li>
<li><a href="#simplistic-dependency-resolution-1">Simplistic dependency resolution</a></li>
</ul></li>
<li><a href="#related-topics">Related topics</a><ul>
<li><a href="#separating-storage-and-selection-of-packages">Separating storage and selection of packages</a></li>
<li><a href="#first-class-environments">First class environments</a></li>
</ul></li>
<li><a href="#questions-to-remember">Questions to remember</a></li>
</ul></li>
<li><a href="#the-haskell-execution-model">The Haskell Execution Model</a></li>
<li><a href="#heap_alloced">HEAP_ALLOCED</a><ul>
<li><a href="#speeding-up-heap_alloced">Speeding up `HEAP_ALLOCED()`</a></li>
<li><a href="#eliminating-heap_alloced-completely">Eliminating `HEAP_ALLOCED` completely</a><ul>
<li><a href="#method-1-put-static-closures-in-an-aligned-section">Method 1: put static closures in an aligned section</a></li>
<li><a href="#method-2-copy-static-closures-into-a-special-area-at-startup">Method 2: copy static closures into a special area at startup</a></li>
</ul></li>
</ul></li>
<li><a href="#heap-and-stack-checks">Heap and Stack checks</a></li>
<li><a href="#ghc-commentary-the-layout-of-heap-objects">GHC Commentary: The Layout of Heap Objects</a><ul>
<li><a href="#terminology">Terminology</a></li>
<li><a href="#heap-objects">Heap Objects</a></li>
<li><a href="#info-tables">Info Tables</a><ul>
<li><a href="#section-5"></a></li>
</ul></li>
<li><a href="#types-of-payload-layout">Types of Payload Layout</a><ul>
<li><a href="#pointers-first-layout">Pointers-first layout</a></li>
<li><a href="#bitmap-layout">Bitmap layout</a></li>
</ul></li>
<li><a href="#dynamic-vs.-static-objects">Dynamic vs. Static objects</a><ul>
<li><a href="#dynamic-objects">Dynamic objects</a></li>
<li><a href="#static-objects">Static objects</a></li>
</ul></li>
<li><a href="#types-of-object">Types of object</a><ul>
<li><a href="#data-constructors">Data Constructors</a></li>
<li><a href="#function-closures">Function Closures</a></li>
<li><a href="#thunks">Thunks</a></li>
<li><a href="#selector-thunks">Selector thunks</a></li>
<li><a href="#partial-applications">Partial applications</a></li>
<li><a href="#generic-application">Generic application</a></li>
<li><a href="#stack-application">Stack application</a></li>
<li><a href="#indirections">Indirections</a></li>
<li><a href="#byte-code-objects">Byte-code objects</a></li>
<li><a href="#black-holes">Black holes</a></li>
<li><a href="#arrays">Arrays</a></li>
<li><a href="#mvars">MVars</a></li>
<li><a href="#weak-pointers">Weak pointers</a></li>
<li><a href="#stable-names">Stable Names</a></li>
<li><a href="#thread-state-objects">Thread State Objects</a></li>
<li><a href="#stm-objects">STM objects</a></li>
<li><a href="#forwarding-pointers">Forwarding Pointers</a></li>
</ul></li>
<li><a href="#how-to-add-new-heap-objects">How to add new heap objects</a></li>
<li><a href="#change-history">Change History</a></li>
<li><a href="#speculation-and-commentary">Speculation and Commentary</a></li>
<li><a href="#record-of-performance-improvements-made-to-the-hoopl-library-starting-january-2012">Record of performance improvements made to the Hoopl library starting January 2012</a></li>
</ul></li>
<li><a href="#haskell-program-coverage">Haskell Program Coverage</a><ul>
<li><a href="#binary-tick-boxes">Binary Tick Boxes</a></li>
<li><a href="#machine-generated-haskell">Machine Generated Haskell</a></li>
</ul></li>
<li><a href="#compiling-one-module-hscmain">Compiling one module: !HscMain</a></li>
<li><a href="#the-diagram">The Diagram</a></li>
<li><a href="#picture-of-the-main-compiler-pipeline">Picture of the main compiler pipeline</a></li>
<li><a href="#the-types">The  types</a><ul>
<li><a href="#decorating-hssyn-with-type-information">Decorating `HsSyn` with type information</a></li>
<li><a href="#source-locations">Source Locations</a></li>
</ul></li>
<li><a href="#interface-files">Interface files</a><ul>
<li><a href="#when-is-an-interface-file-loaded">When is an interface file loaded?</a></li>
</ul></li>
<li><a href="#immix-garbage-collector">Immix Garbage Collector</a></li>
<li><a href="#the-patches">The patches</a><ul>
<li><a href="#the-main-patch">The main patch</a></li>
<li><a href="#line-before-inscreasing-block-size">Line before inscreasing block size</a></li>
<li><a href="#allocate-in-lines-in-minor-gcs">Allocate in lines in minor GCs</a></li>
<li><a href="#remove-partial-list">Remove partial list</a></li>
</ul></li>
<li><a href="#to-do">To do</a></li>
<li><a href="#ghc-source-tree-roadmap-includes">GHC Source Tree Roadmap: includes/</a><ul>
<li><a href="#external-apis">External APIs</a></li>
<li><a href="#derived-constants">Derived Constants</a></li>
<li><a href="#used-when-compiling-via-c">Used when compiling via C</a></li>
<li><a href="#the-rts-external-apis">The RTS external APIs</a></li>
<li><a href="#included-into-c---.cmm-code">Included into C-- (`.cmm`) code</a></li>
</ul></li>
<li><a href="#installing-using-the-llvm-back-end">Installing &amp; Using the LLVM Back-end</a><ul>
<li><a href="#installing">Installing</a></li>
<li><a href="#llvm-support">LLVM Support</a></li>
<li><a href="#using">Using</a></li>
<li><a href="#supported-platforms-correctness">Supported Platforms &amp; Correctness</a></li>
<li><a href="#shared-libraries">Shared Libraries</a></li>
<li><a href="#performance">Performance</a></li>
</ul></li>
<li><a href="#ghc-commentary-librariesinteger">GHC Commentary: Libraries/Integer</a><ul>
<li><a href="#selecting-an-integer-implementation">Selecting an Integer implementation</a></li>
<li><a href="#the-integer-interface">The Integer interface</a></li>
<li><a href="#how-integer-is-handled-inside-ghc">How Integer is handled inside GHC</a></li>
</ul></li>
<li><a href="#an-integrated-code-generator-for-ghc">An Integrated Code Generator for GHC</a><ul>
<li><a href="#design-elements">Design elements</a></li>
<li><a href="#design-philosophy">Design philosophy</a></li>
<li><a href="#proposed-compilation-pipeline">Proposed compilation pipeline</a><ul>
<li><a href="#convert-from-stg-to-control-flow-graph">Convert from STG to control flow graph</a></li>
<li><a href="#instruction-selection">Instruction selection</a></li>
<li><a href="#optimisation">Optimisation</a></li>
<li><a href="#proc-point-analysis">Proc-point analysis</a></li>
<li><a href="#register-allocation">Register allocation</a></li>
<li><a href="#stack-layout">Stack layout</a></li>
<li><a href="#tidy-up">Tidy up</a></li>
</ul></li>
<li><a href="#machine-dependence">Machine-dependence</a></li>
</ul></li>
<li><a href="#ghc-commentary-the-byte-code-interpreter-and-dynamic-linker">GHC Commentary: The byte-code interpreter and dynamic linker</a><ul>
<li><a href="#linker">Linker</a></li>
<li><a href="#bytecode-interpreter">Bytecode Interpreter</a></li>
</ul></li>
<li><a href="#the-io-manager">The I/O Manager</a></li>
<li><a href="#key-data-types">Key data types</a></li>
<li><a href="#kinds">Kinds</a><ul>
<li><a href="#representing-kinds">Representing kinds</a></li>
<li><a href="#kind-subtyping">Kind subtyping</a></li>
</ul></li>
<li><a href="#linearity">Linearity</a></li>
<li><a href="#ticky">Ticky</a><ul>
<li><a href="#declarations-for-ticky-counters">Declarations for ticky counters</a></li>
</ul></li>
<li><a href="#strictness-and-let-floating">Strictness and let-floating</a></li>
<li><a href="#coercions">Coercions</a></li>
<li><a href="#warn-arity">WARN: arity /</a></li>
<li><a href="#explaining-demand-transformers">Explaining demand transformers</a></li>
<li><a href="#nofib-stuff">Nofib stuff</a></li>
<li><a href="#ghc-commentary-libraries">GHC Commentary: Libraries</a></li>
<li><a href="#building-packages-that-ghc-doesnt-depend-on">Building packages that GHC doesn't depend on</a></li>
<li><a href="#classifying-boot-packages">Classifying boot packages</a><ul>
<li><a href="#required-or-optional">Required or optional</a></li>
<li><a href="#coupling-to-ghc">Coupling to GHC</a></li>
<li><a href="#zero-boot-packages">Zero-boot packages</a></li>
<li><a href="#installation">Installation</a></li>
</ul></li>
<li><a href="#boot-packages-dependencies">Boot packages dependencies</a><ul>
<li><a href="#warning-pattern-matching-in-ghc-prim-integer-simple-and-integer-gmp">WARNING: Pattern matching in `ghc-prim`, `integer-simple`, and `integer-gmp`</a></li>
</ul></li>
<li><a href="#repositories">Repositories</a></li>
<li><a href="#the-llvm-backend">The LLVM backend</a></li>
<li><a href="#loopification">Loopification</a></li>
<li><a href="#llvm-mangler">LLVM Mangler</a><ul>
<li><a href="#tables_next_to_code-tntc">TABLES_NEXT_TO_CODE (TNTC)</a></li>
<li><a href="#stack-alignment">Stack Alignment</a></li>
<li><a href="#simd-avx">SIMD / AVX</a></li>
</ul></li>
<li><a href="#migrating-old-commentary">Migrating Old Commentary</a><ul>
<li><a href="#before-the-show-begins">Before the Show Begins</a></li>
<li><a href="#genesis">Genesis</a></li>
<li><a href="#the-beast-dissected">The Beast Dissected</a></li>
<li><a href="#rts-libraries">RTS &amp; Libraries</a></li>
<li><a href="#extensions-or-making-a-complicated-system-more-complicated">Extensions, or Making a Complicated System More Complicated</a></li>
</ul></li>
<li><a href="#the-marvellous-module-structure-of-ghc">The Marvellous Module Structure of GHC</a><ul>
<li><a href="#compilation-order-is-as-follows">Compilation order is as follows:</a></li>
<li><a href="#typechecker-stuff">Typechecker stuff</a></li>
<li><a href="#hssyn-stuff">!HsSyn stuff</a></li>
<li><a href="#library-stuff-base-package">Library stuff: base package</a></li>
<li><a href="#high-level-dependency-graph">High-level Dependency Graph</a></li>
</ul></li>
<li><a href="#module-types">Module Types</a><ul>
<li><a href="#module">Module</a></li>
<li><a href="#modiface">!ModIface</a></li>
<li><a href="#moddetails">!ModDetails</a><ul>
<li><a href="#modguts">!ModGuts</a></li>
</ul></li>
<li><a href="#modsummary">!ModSummary</a></li>
<li><a href="#homemodinfo">!HomeModInfo</a></li>
<li><a href="#homepackagetable">!HomePackageTable</a></li>
<li><a href="#externalpackagestate">!ExternalPackageState</a></li>
</ul></li>
<li><a href="#multi-instance-packages">Multi-instance packages</a><ul>
<li><a href="#todo-list">!ToDo list</a></li>
<li><a href="#next-step-dealing-with-ways">Next step: dealing with ways</a></li>
</ul></li>
<li><a href="#the-type-1">The  type</a><ul>
<li><a href="#the-of-a-name">The  of a Name</a></li>
<li><a href="#entities-and">Entities and </a></li>
</ul></li>
<li><a href="#native-code-generator-ncg">Native Code Generator (NCG)</a><ul>
<li><a href="#files-parts">Files, Parts</a></li>
<li><a href="#overview-3">Overview</a><ul>
<li><a href="#spilling">Spilling</a></li>
<li><a href="#dealing-with-common-cases-fast">Dealing with common cases fast</a></li>
</ul></li>
<li><a href="#complications-observations-and-possible-improvements">Complications, observations, and possible improvements</a><ul>
<li><a href="#real-vs-virtual-registers-in-the-instruction-selectors">Real vs virtual registers in the instruction selectors</a></li>
</ul></li>
<li><a href="#selecting-insns-for-64-bit-valuesloadsstores-on-32-bit-platforms">Selecting insns for 64-bit values/loads/stores on 32-bit platforms</a></li>
<li><a href="#shortcomings-and-inefficiencies-in-the-register-allocator">Shortcomings and inefficiencies in the register allocator</a><ul>
<li><a href="#redundant-reconstruction-of-the-control-flow-graph">Redundant reconstruction of the control flow graph</a></li>
<li><a href="#really-ridiculous-method-for-doing-spilling">Really ridiculous method for doing spilling</a></li>
<li><a href="#redundant-move-support-for-revised-instruction-selector-suggestion">Redundant-move support for revised instruction selector suggestion</a></li>
</ul></li>
<li><a href="#x86-arcana-that-you-should-know-about">x86 arcana that you should know about</a></li>
<li><a href="#generating-code-for-ccalls">Generating code for ccalls</a></li>
<li><a href="#duplicate-implementation-for-many-stg-macros">Duplicate implementation for many STG macros</a></li>
<li><a href="#how-to-debug-the-ncg-without-losing-your-sanityhaircool">How to debug the NCG without losing your sanity/hair/cool</a></li>
<li><a href="#historical-page-1">Historical page</a></li>
</ul></li>
<li><a href="#overview-of-modules-in-the-new-code-generator">Overview of modules in the new code generator</a><ul>
<li><a href="#the-new-cmm-data-type">The new Cmm data type</a></li>
<li><a href="#module-structure-of-the-new-code-generator">Module structure of the new code generator</a><ul>
<li><a href="#basic-datatypes-and-infrastructure">Basic datatypes and infrastructure</a></li>
<li><a href="#analyses-and-transformations">Analyses and transformations</a></li>
<li><a href="#linking-the-pipeline">Linking the pipeline</a></li>
<li><a href="#dead-code">Dead code</a></li>
</ul></li>
<li><a href="#historical-page-2">Historical page</a></li>
</ul></li>
<li><a href="#design-of-the-new-code-generator">Design of the new code generator</a><ul>
<li><a href="#overview-4">Overview</a></li>
<li><a href="#the-cmm-pipeline">The Cmm pipeline</a><ul>
<li><a href="#branches-to-continuations-and-the-adams-optimisation">Branches to continuations and the &quot;Adams optimisation&quot;</a></li>
</ul></li>
<li><a href="#runtime-system">Runtime system</a></li>
</ul></li>
<li><a href="#note-historical-page">NOTE: Historical page</a></li>
<li><a href="#stupidity-in-the-new-code-generator">Stupidity in the New Code Generator</a><ul>
<li><a href="#cantankerous-comparisons">Cantankerous Comparisons</a></li>
<li><a href="#dead-stackheap-checks">Dead stack/heap checks</a></li>
<li><a href="#instruction-reordering">Instruction reordering</a></li>
<li><a href="#stack-space-overuse">Stack space overuse</a></li>
<li><a href="#double-temp-use-means-no-inlinining">Double temp-use means no inlinining?</a></li>
<li><a href="#stupid-spills">Stupid spills</a></li>
<li><a href="#noppy-proc-points">Noppy proc-points</a></li>
<li><a href="#lots-of-temporary-variables">Lots of temporary variables</a></li>
<li><a href="#double-proc-points">Double proc points</a></li>
<li><a href="#rewriting-stacks">Rewriting stacks</a></li>
<li><a href="#spilling-hpsp">Spilling Hp/Sp</a></li>
<li><a href="#up-and-down">Up and Down</a></li>
<li><a href="#sp-is-generally-stupid">Sp is generally stupid</a></li>
<li><a href="#heap-and-r1-aliasing">Heap and R1 aliasing</a></li>
<li><a href="#historical-page-3">Historical page</a></li>
</ul></li>
<li><a href="#ghcs-glorious-new-code-generator">GHC's glorious new code generator</a><ul>
<li><a href="#workflow-for-the-new-code-generator-and-hoopl">Workflow for the new code generator and Hoopl</a></li>
<li><a href="#status-report-april-2011">Status report April 2011</a></li>
</ul></li>
<li><a href="#old-code-generator-prior-to-ghc-7.8">Old Code Generator (prior to GHC 7.8)</a><ul>
<li><a href="#storage-manager-representations">Storage manager representations</a></li>
<li><a href="#generated-cmm-naming-convention">Generated Cmm Naming Convention</a></li>
<li><a href="#modules">Modules</a><ul>
<li><a href="#section-6"></a></li>
<li><a href="#section-7"></a></li>
<li><a href="#section-8"></a></li>
<li><a href="#section-9"></a></li>
<li><a href="#section-10"></a></li>
<li><a href="#memory-and-register-management">Memory and Register Management</a></li>
<li><a href="#function-calls-and-parameter-passing">Function Calls and Parameter Passing</a></li>
<li><a href="#misc-utilities">Misc utilities</a></li>
<li><a href="#special-runtime-support">Special runtime support</a></li>
</ul></li>
</ul></li>
<li><a href="#ordering-the-core-to-core-optimisation-passes">Ordering the Core-to-Core optimisation passes</a><ul>
<li><a href="#this-ordering-obeys-all-the-constraints-except-5">This ordering obeys all the constraints except (5)</a></li>
<li><a href="#constraints-1">Constraints</a><ul>
<li><a href="#float-in-before-strictness">1. float-in before strictness</a></li>
<li><a href="#dont-simplify-between-float-in-and-strictness">2. Don't simplify between float-in and strictness</a></li>
<li><a href="#want-full-laziness-before-foldrbuild">3. Want full-laziness before foldr/build</a></li>
<li><a href="#want-strictness-after-foldrbuild">4. Want strictness after foldr/build</a></li>
<li><a href="#want-full-laziness-after-strictness">5. Want full laziness after strictness</a></li>
<li><a href="#want-float-in-after-foldrbuild">6. Want float-in after foldr/build</a></li>
<li><a href="#want-simplify-after-float-inwards">7. Want simplify after float-inwards</a></li>
<li><a href="#if-full-laziness-is-ever-done-after-strictness">8. If full laziness is ever done after strictness</a></li>
<li><a href="#ignore-inline-pragmas-flag-for-final-simplification">9. Ignore-inline-pragmas flag for final simplification</a></li>
<li><a href="#run-float-inwards-once-more-after-strictness-simplify">10. Run Float Inwards once more after strictness-simplify</a></li>
</ul></li>
</ul></li>
<li><a href="#overall-organisation-of-ghc">Overall organisation of GHC</a></li>
<li><a href="#ghc-source-code">GHC source code</a></li>
<li><a href="#updates">Updates</a><ul>
<li><a href="#unique">Unique</a></li>
<li><a href="#redesign-2014">Redesign (2014)</a></li>
</ul></li>
<li><a href="#the-data-type-and-its-friends">The data type  and its friends</a><ul>
<li><a href="#views-of-types">Views of types</a></li>
<li><a href="#the-representation-of">The representation of </a></li>
<li><a href="#overloaded-types">Overloaded types</a></li>
<li><a href="#classifying-types">Classifying types</a></li>
</ul></li>
<li><a href="#package-compatibility">Package Compatibility</a><ul>
<li><a href="#dont-reorganise-packages">1. Don't reorganise packages</a></li>
<li><a href="#provide-older-versions-of-base-with-a-new-ghc-release">2. Provide older version(s) of base with a new GHC release</a></li>
<li><a href="#allow-packages-to-re-export-modules">4. Allow packages to re-export modules</a></li>
<li><a href="#provide-backwards-compatible-versions-of-base">4.1 Provide backwards-compatible versions of base</a></li>
<li><a href="#rename-base-and-provide-a-compatibility-wrapper">4.2 Rename base, and provide a compatibility wrapper</a></li>
<li><a href="#dont-rename-base">4.3 Don't rename base</a></li>
<li><a href="#do-some-kind-of-providesrequires-interface-in-cabal">5. Do some kind of provides/requires interface in Cabal</a><ul>
<li><a href="#make-api-specifications-more-symmetric">5.1 Make API specifications more symmetric</a></li>
<li><a href="#make-api-specifications-explicit">5.2 Make API specifications explicit</a></li>
<li><a href="#make-api-specifications-more-specific">5.3 Make API specifications more specific</a></li>
</ul></li>
<li><a href="#distributions-at-the-hackage-level">6. Distributions at the Hackage level</a></li>
<li><a href="#allow-package-overlaps">7. Allow package overlaps</a></li>
<li><a href="#the-problem-of-lax-version-dependencies">The problem of lax version dependencies</a></li>
</ul></li>
<li><a href="#note-about-this-page">Note about this page</a></li>
<li><a href="#explicit-package-imports">Explicit package imports</a><ul>
<li><a href="#is-the-from-compulsory">Is the 'from <package>' compulsory?</a></li>
<li><a href="#package-versions">Package versions</a></li>
<li><a href="#importing-from-the-home-package">Importing from the home package</a></li>
<li><a href="#the-as-p-alias">The 'as P' alias</a></li>
<li><a href="#qualified-names">Qualified names</a></li>
<li><a href="#exporting-modules-from-other-packages">Exporting modules from other packages</a></li>
<li><a href="#syntax">Syntax</a><ul>
<li><a href="#syntax-formalised-and-summarised">Syntax formalised and summarised</a></li>
<li><a href="#proposal-for-package-mounting">Proposal for Package Mounting</a></li>
<li><a href="#evaluation">Evaluation</a></li>
<li><a href="#note-on-package-grafting">Note on Package Grafting</a></li>
</ul></li>
<li><a href="#alternative-proposal-for-packages-with-explicit-namespaces">Alternative Proposal for Packages (with explicit namespaces)</a></li>
<li><a href="#a-different-but-related-problem">A different, but related, problem</a></li>
<li><a href="#proposal">Proposal</a><ul>
<li><a href="#naming-a-namespace">Naming a namespace</a></li>
<li><a href="#what-namespaces-are-available-by-default">What namespaces are available by default?</a></li>
<li><a href="#namespace-resolution">Namespace resolution</a></li>
<li><a href="#syntax-1">Syntax</a></li>
<li><a href="#exports-1">Exports</a></li>
<li><a href="#implicit-imports">Implicit imports</a></li>
<li><a href="#exposed-vs-hidden-packages">Exposed vs Hidden packages</a></li>
<li><a href="#what-if-you-wanted-to-import-a.b.c-from-p1-and-a.b.c-from-p2-into-the-same-module">What if you wanted to import A.B.C from P1 and A.B.C from P2 into the <em>same</em> module?</a></li>
</ul></li>
</ul></li>
<li><a href="#package-reorg">Package Reorg</a><ul>
<li><a href="#goals-1">Goals</a></li>
<li><a href="#proposal-1">Proposal</a><ul>
<li><a href="#what-is-in-the-core-packages">What is in the Core Packages?</a></li>
<li><a href="#requirements-to-libraries-to-be-included-in-core-set">Requirements to libraries to be included in core set</a></li>
<li><a href="#the-base-package">The base package</a></li>
<li><a href="#other-packages">Other packages</a></li>
</ul></li>
<li><a href="#testing-1">Testing</a></li>
<li><a href="#implementation-specific-notes">Implementation-specific notes</a><ul>
<li><a href="#notes-about-ghc">Notes about GHC</a></li>
<li><a href="#notes-about-hugs">Notes about Hugs</a></li>
</ul></li>
</ul></li>
<li><a href="#commentary-the-package-system">Commentary: The Package System</a><ul>
<li><a href="#architecture">Architecture</a></li>
<li><a href="#identifying-packages">Identifying Packages</a></li>
<li><a href="#design-constraints">Design constraints</a></li>
<li><a href="#the-plan-1">The Plan</a><ul>
<li><a href="#detecting-abi-incompatibility">Detecting ABI incompatibility</a></li>
<li><a href="#allowing-abi-compatibilty">Allowing ABI compatibilty</a></li>
</ul></li>
</ul></li>
<li><a href="#the-parser">The Parser</a><ul>
<li><a href="#principles">Principles</a></li>
<li><a href="#avoiding-right-recursion">Avoiding right-recursion</a></li>
<li><a href="#indentation">Indentation</a></li>
<li><a href="#syntax-extensions">Syntax extensions</a></li>
</ul></li>
<li><a href="#pinned-objects">Pinned Objects</a></li>
<li><a href="#overview-5">Overview</a></li>
<li><a href="#the-driver-pipeline">The driver pipeline</a></li>
<li><a href="#the-compiler-pipeline">The compiler pipeline</a></li>
<li><a href="#video">Video</a></li>
<li><a href="#platforms">Platforms</a><ul>
<li><a href="#limitations">Limitations</a></li>
<li><a href="#macros">Macros</a></li>
</ul></li>
<li><a href="#pointer-tagging-1">Pointer Tagging</a><ul>
<li><a href="#meaning-of-the-tag-bits">Meaning of the tag bits</a></li>
<li><a href="#optimisations-enabled-by-tag-bits">Optimisations enabled by tag bits</a></li>
<li><a href="#garbage-collection-with-tagged-pointers">Garbage collection with tagged pointers</a></li>
<li><a href="#invariants">Invariants</a></li>
<li><a href="#compacting-gc">Compacting GC</a></li>
<li><a href="#dealing-with-tags-in-the-code">Dealing with tags in the code</a></li>
</ul></li>
<li><a href="#position-independent-code-and-dynamic-linking">Position-Independent Code and Dynamic Linking</a><ul>
<li><a href="#how-to-access-symbols">How to access symbols</a></li>
<li><a href="#clabel.labeldynamic">CLabel.labelDynamic</a></li>
<li><a href="#info-tables-1">Info Tables</a></li>
<li><a href="#imported-labels-in-srts-windows">Imported labels in SRTs (Windows)</a></li>
<li><a href="#pic-and-dynamic-linking-support-in-the-ncg">PIC and dynamic linking support in the NCG</a></li>
<li><a href="#how-things-are-done-on-different-platforms">How things are done on different platforms</a><ul>
<li><a href="#position-dependent-code">Position dependent code</a></li>
<li><a href="#position-independent-code">Position independent code</a></li>
</ul></li>
<li><a href="#linking-on-elf">Linking on ELF</a></li>
<li><a href="#mangling-dynamic-library-names">Mangling dynamic library names</a></li>
</ul></li>
<li><a href="#ghc-commentary-the-c-code-generator">GHC Commentary: The C code generator</a><ul>
<li><a href="#header-files">Header files</a></li>
<li><a href="#prototypes">Prototypes</a></li>
</ul></li>
<li><a href="#primitive-operations-primops">Primitive Operations (!PrimOps)</a><ul>
<li><a href="#the-primops.txt.pp-file">The primops.txt.pp file</a></li>
<li><a href="#implementation-of-primops">Implementation of !PrimOps</a><ul>
<li><a href="#inline-primops">Inline !PrimOps</a></li>
<li><a href="#out-of-line-primops">Out-of-line !PrimOps</a></li>
<li><a href="#foreign-out-of-line-primops-and-foreign-import-prim">Foreign out-of-line !PrimOps and `foreign import prim`</a></li>
</ul></li>
<li><a href="#adding-a-new-primop">Adding a new !PrimOp</a></li>
</ul></li>
<li><a href="#profiling">Profiling</a><ul>
<li><a href="#cost-centre-profiling">Cost-centre profiling</a></li>
<li><a href="#ticky-ticky-profiling">Ticky-ticky profiling</a></li>
</ul></li>
<li><a href="#and">, , and </a><ul>
<li><a href="#the-module-and-modulename-types">The `Module` and `ModuleName` types</a></li>
<li><a href="#the-type-2">The  type</a></li>
</ul></li>
<li><a href="#recompilation-avoidance">Recompilation Avoidance</a><ul>
<li><a href="#what-is-recompilation-avoidance">What is recompilation avoidance?</a></li>
<li><a href="#example-2">Example</a></li>
<li><a href="#why-do-we-need-recompilation-avoidance">Why do we need recompilation avoidance?</a><ul>
<li><a href="#ghci-and---make">GHCi and `--make`</a></li>
<li><a href="#make">`make`</a></li>
</ul></li>
<li><a href="#how-does-it-work">How does it work?</a><ul>
<li><a href="#deciding-whether-to-recompile">Deciding whether to recompile</a></li>
<li><a href="#example-3">Example</a></li>
<li><a href="#how-does-fingerprinting-work">How does fingerprinting work?</a></li>
<li><a href="#mutually-recursive-groups-of-entities">Mutually recursive groups of entities</a></li>
<li><a href="#fixities">Fixities</a></li>
<li><a href="#instances">Instances</a></li>
<li><a href="#orphans">Orphans</a></li>
<li><a href="#rules">Rules</a></li>
<li><a href="#on-ordering">On ordering</a></li>
<li><a href="#packages">Packages</a></li>
<li><a href="#package-version-changes">Package version changes</a></li>
</ul></li>
<li><a href="#interface-stability">Interface stability</a></li>
</ul></li>
<li><a href="#the-register-allocator-1">The Register Allocator</a><ul>
<li><a href="#overview-6">Overview</a></li>
<li><a href="#code-map">Code map</a></li>
<li><a href="#references-2">References</a></li>
<li><a href="#register-pressure-in-haskell-code">Register pressure in Haskell code</a></li>
<li><a href="#hackingdebugging">Hacking/Debugging</a></li>
<li><a href="#runtime-performance">Runtime performance</a></li>
<li><a href="#possible-improvements">Possible Improvements</a></li>
</ul></li>
<li><a href="#haskell-excecution-registers">Haskell Excecution: Registers</a></li>
<li><a href="#relevant-ghc-parts-for-demand-analysis-results">Relevant GHC parts for Demand Analysis results</a></li>
<li><a href="#remembered-sets">Remembered Sets</a><ul>
<li><a href="#remembered-set-maintenance-during-mutation">Remembered set maintenance during mutation</a><ul>
<li><a href="#thunk-updates">Thunk Updates</a></li>
<li><a href="#mutable-objects-mut_var-mvar">Mutable objects: MUT_VAR, MVAR</a></li>
<li><a href="#arrays-mut_arr_ptrs">Arrays: MUT_ARR_PTRS</a></li>
<li><a href="#threads-tso">Threads: TSO</a></li>
</ul></li>
<li><a href="#remembered-set-maintenance-during-gc">Remembered set maintenance during GC</a></li>
</ul></li>
<li><a href="#the-renamer">The renamer</a><ul>
<li><a href="#the-global-renamer-environment">The global renamer environment, </a></li>
<li><a href="#unused-imports">Unused imports</a></li>
<li><a href="#name-space-management">Name Space Management</a></li>
<li><a href="#rebindable-syntax">Rebindable syntax</a></li>
</ul></li>
<li><a href="#replacing-the-native-code-generator">Replacing the Native Code Generator</a></li>
<li><a href="#resource-limits">Resource Limits</a><ul>
<li><a href="#code-generation-changes">Code generation changes</a><ul>
<li><a href="#dynamic-closure-allocation">Dynamic closure allocation</a></li>
<li><a href="#caf-allocation">CAF Allocation</a></li>
<li><a href="#thunk-code">Thunk code</a></li>
<li><a href="#foreign-calls">Foreign calls</a></li>
</ul></li>
<li><a href="#case-split">Case split</a></li>
<li><a href="#front-end-changes">Front-end changes</a></li>
</ul></li>
<li><a href="#garbage-collection-roots">Garbage Collection Roots</a></li>
<li><a href="#ghc-source-tree-roadmap-rts">GHC Source Tree Roadmap: rts/</a><ul>
<li><a href="#subdirectories-of-rts">Subdirectories of rts/</a></li>
<li><a href="#haskell-execution">Haskell Execution</a></li>
<li><a href="#the-wikicommentaryrtsstorage-storage-manager">The [wiki:Commentary/Rts/Storage Storage Manager]</a></li>
<li><a href="#data-structures">Data Structures</a></li>
<li><a href="#the-wikicommentaryrtsscheduler-scheduler">The [wiki:Commentary/Rts/Scheduler Scheduler]</a></li>
<li><a href="#c-files-the-wikicommentaryrtsffi-ffi">C files: the [wiki:Commentary/Rts/FFI FFI]</a></li>
<li><a href="#the-wikicommentaryrtsinterpreter-byte-code-interpreter">The [wiki:Commentary/Rts/Interpreter Byte-code Interpreter]</a></li>
<li><a href="#wikicommentaryprofiling-profiling">[wiki:Commentary/Profiling Profiling]</a></li>
<li><a href="#rts-debugging">RTS Debugging</a></li>
<li><a href="#the-front-panel">The Front Panel</a></li>
<li><a href="#other">Other</a></li>
<li><a href="#old-stuff">OLD stuff</a></li>
</ul></li>
<li><a href="#sanity-checking">Sanity Checking</a></li>
<li><a href="#the-scheduler">The Scheduler</a><ul>
<li><a href="#os-threads">OS Threads</a></li>
<li><a href="#haskell-threads">Haskell threads</a></li>
</ul></li>
<li><a href="#seq-magic">Seq magic</a><ul>
<li><a href="#the-baseline-position">The baseline position</a><ul>
<li><a href="#problem-1-trac-1031">Problem 1 (Trac #1031)</a></li>
<li><a href="#problem-2-trac-2273">Problem 2 (Trac #2273)</a></li>
<li><a href="#problem-3-trac-5262">Problem 3 (Trac #5262)</a></li>
<li><a href="#problem-4-seq-in-the-io-monad">Problem 4: seq in the IO monad</a></li>
<li><a href="#problem-5-the-need-for-special-rules">Problem 5: the need for special rules</a></li>
</ul></li>
</ul></li>
<li><a href="#a-better-way">A better way</a></li>
<li><a href="#the-ghc-commentary-signals">The GHC Commentary: Signals</a><ul>
<li><a href="#signal-handling-in-the-rts">Signal handling in the RTS</a><ul>
<li><a href="#the-timer-signal">The timer signal</a></li>
</ul></li>
<li><a href="#the-interrupt-signal">The interrupt signal</a></li>
<li><a href="#signal-handling-in-haskell-code">Signal handling in Haskell code</a></li>
<li><a href="#rts-alarm-signals-and-foreign-libraries">RTS Alarm Signals and Foreign Libraries</a></li>
</ul></li>
<li><a href="#slop">Slop</a><ul>
<li><a href="#why-do-we-want-to-avoid-slop">Why do we want to avoid slop?</a></li>
<li><a href="#how-does-slop-arise">How does slop arise?</a></li>
<li><a href="#what-do-we-do-about-it">What do we do about it?</a></li>
</ul></li>
<li><a href="#layout-of-important-files-and-directories">Layout of important files and directories</a><ul>
<li><a href="#files-in-top">Files in `$(TOP)`</a></li>
<li><a href="#libraries">`libraries/`</a></li>
<li><a href="#compiler-docs-ghc">`compiler/`, `docs/`, `ghc/`</a></li>
<li><a href="#rts">`rts/`</a></li>
<li><a href="#includes">`includes/`</a></li>
<li><a href="#utils-libffi">`utils/`, `libffi/`</a></li>
<li><a href="#driver">`driver/`</a></li>
<li><a href="#ghc-tarballs-windows-only">`ghc-tarballs/` (Windows only)</a></li>
<li><a href="#testsuite-nofib">`testsuite/`, `nofib/`</a></li>
<li><a href="#mk-rules">`mk/`, `rules/`</a></li>
<li><a href="#distrib">`distrib/`</a></li>
<li><a href="#stuff-that-appears-only-in-a-build-tree">Stuff that appears only in a build tree</a><ul>
<li><a href="#inplace">`inplace/`</a></li>
<li><a href="#dist">`.../dist*/`</a></li>
</ul></li>
<li><a href="#stack-layout-1">Stack Layout</a><ul>
<li><a href="#representing-stack-slots">Representing Stack Slots</a></li>
<li><a href="#laying-out-the-stack">Laying out the stack</a></li>
<li><a href="#a-greedy-algorithm">A greedy algorithm</a></li>
</ul></li>
</ul></li>
<li><a href="#layout-of-the-stack">Layout of the stack</a><ul>
<li><a href="#info-tables-for-stack-frames">Info tables for stack frames</a></li>
<li><a href="#layout-of-the-payload">Layout of the payload</a></li>
<li><a href="#kinds-of-stack-frame">Kinds of Stack Frame</a></li>
</ul></li>
<li><a href="#the-stg-syntax-data-types">The STG syntax data types</a></li>
<li><a href="#ghc-commentary-software-transactional-memory-stm">GHC Commentary: Software Transactional Memory (STM)</a></li>
<li><a href="#background">Background</a><ul>
<li><a href="#definitions">Definitions</a><ul>
<li><a href="#useful-rts-terms">Useful RTS terms</a></li>
<li><a href="#transactional-memory-terms">Transactional Memory terms</a></li>
</ul></li>
</ul></li>
<li><a href="#overview-of-features">Overview of Features</a><ul>
<li><a href="#reading-and-writing">Reading and Writing</a></li>
<li><a href="#blocking">Blocking</a></li>
<li><a href="#choice">Choice</a></li>
<li><a href="#data-invariants">Data Invariants</a></li>
<li><a href="#exceptions">Exceptions</a></li>
</ul></li>
<li><a href="#overview-of-the-implementation">Overview of the Implementation</a><ul>
<li><a href="#transactions-that-read-and-write.">Transactions that Read and Write.</a><ul>
<li><a href="#transactional-record">Transactional Record</a></li>
<li><a href="#starting">Starting</a></li>
<li><a href="#reading">Reading</a></li>
<li><a href="#writing">Writing</a></li>
<li><a href="#validation">Validation</a></li>
<li><a href="#committing">Committing</a></li>
<li><a href="#aborting">Aborting</a></li>
<li><a href="#exceptions-1">Exceptions</a></li>
</ul></li>
<li><a href="#blocking-with">Blocking with </a></li>
<li><a href="#choice-with">Choice with </a></li>
<li><a href="#invariants-1">Invariants</a><ul>
<li><a href="#details">Details</a></li>
<li><a href="#changes-from-choice">Changes from Choice</a></li>
</ul></li>
<li><a href="#other-details">Other Details</a><ul>
<li><a href="#detecting-long-running-transactions">Detecting Long Running Transactions</a></li>
<li><a href="#transaction-state">Transaction State</a></li>
<li><a href="#gc-and-aba">GC and ABA</a></li>
<li><a href="#management-of-s">Management of s</a></li>
<li><a href="#tokens-and-version-numbers.">Tokens and Version Numbers.</a></li>
<li><a href="#implementation-invariants">Implementation Invariants</a></li>
<li><a href="#fine-grain-locking">Fine Grain Locking</a></li>
</ul></li>
<li><a href="#bibliography">Bibliography</a></li>
</ul></li>
<li><a href="#ghc-commentary-storage">GHC Commentary: Storage</a></li>
<li><a href="#general-overview">General overview</a></li>
<li><a href="#important-note">IMPORTANT NOTE</a></li>
<li><a href="#the-demand-analyzer">The demand analyzer</a><ul>
<li><a href="#important-datatypes">Important datatypes</a></li>
</ul></li>
<li><a href="#symbol-names">Symbol Names</a><ul>
<li><a href="#tuples">Tuples</a></li>
<li><a href="#unboxed-tuples">Unboxed Tuples</a></li>
<li><a href="#alphanumeric-characters">Alphanumeric Characters</a></li>
<li><a href="#constructor-characters">Constructor Characters</a></li>
<li><a href="#variable-characters">Variable Characters</a></li>
<li><a href="#other-1">Other</a></li>
<li><a href="#examples">Examples</a></li>
</ul></li>
<li><a href="#the-monad-for-renaming-typechecking-desugaring">The monad for renaming, typechecking, desugaring</a></li>
<li><a href="#kirstens-sketchy-notes-on-getting-ticky-to-work">Kirsten's sketchy notes on getting ticky to work</a></li>
<li><a href="#the-ghc-commentary-checking-types">The GHC Commentary: Checking Types</a><ul>
<li><a href="#the-overall-flow-of-things">The Overall Flow of Things</a><ul>
<li><a href="#entry-points-into-the-type-checker">Entry Points Into the Type Checker</a></li>
<li><a href="#renaming-and-type-checking-a-module">Renaming and Type Checking a Module</a></li>
</ul></li>
<li><a href="#type-checking-a-declaration-group">Type Checking a Declaration Group</a></li>
<li><a href="#type-checking-type-and-class-declarations">Type checking Type and Class Declarations</a></li>
<li><a href="#more-details">More Details</a><ul>
<li><a href="#types-variables-and-zonking">Types Variables and Zonking</a></li>
<li><a href="#type-representation">Type Representation</a></li>
<li><a href="#type-checking-environment">Type Checking Environment</a></li>
<li><a href="#expressions-1">Expressions</a></li>
<li><a href="#handling-of-dictionaries-and-method-instances">Handling of Dictionaries and Method Instances</a></li>
</ul></li>
<li><a href="#connection-with-ghcs-constraint-solver">Connection with GHC's Constraint Solver</a></li>
<li><a href="#generating-evidence">Generating Evidence</a></li>
<li><a href="#the-solver">The Solver</a><ul>
<li><a href="#given-constraints">Given Constraints</a></li>
<li><a href="#derived-constraints">Derived Constraints</a></li>
<li><a href="#wanted-constraints">Wanted Constraints</a></li>
</ul></li>
</ul></li>
<li><a href="#the-data-type-and-its-friends-1">The data type  and its friends</a><ul>
<li><a href="#views-of-types-1">Views of types</a></li>
<li><a href="#the-representation-of-1">The representation of </a></li>
<li><a href="#overloaded-types-1">Overloaded types</a></li>
<li><a href="#classifying-types-1">Classifying types</a></li>
<li><a href="#unique-1">Unique</a></li>
<li><a href="#current-design">Current design</a><ul>
<li><a href="#known-key-things">Known-key things</a></li>
<li><a href="#interface-files-1">Interface files</a></li>
</ul></li>
<li><a href="#redesign-2014-1">Redesign (2014)</a></li>
</ul></li>
<li><a href="#unpacking-primitive-fields">Unpacking primitive fields</a><ul>
<li><a href="#goals-and-non-goals">Goals and non-goals</a></li>
<li><a href="#detailed-design">Detailed design</a></li>
<li><a href="#benchmarks">Benchmarks</a></li>
</ul></li>
<li><a href="#unused-imports-1">Unused imports</a><ul>
<li><a href="#the-current-story">The current story</a></li>
<li><a href="#examples-1">Examples</a></li>
<li><a href="#specfication">Specfication</a></li>
<li><a href="#implementation-1">Implementation</a></li>
<li><a href="#algorithm">Algorithm</a></li>
</ul></li>
<li><a href="#updates-1">Updates</a></li>
<li><a href="#the-user-manual">The user manual</a></li>
<li><a href="#ghc-boot-library-version-history">GHC Boot Library Version History</a></li>
<li><a href="#ghc-commentary-weak-pointers-and-finalizers">GHC Commentary: Weak Pointers and Finalizers</a></li>
<li><a href="#work-in-progress-on-the-llvm-backend">Work in Progress on the LLVM Backend</a><ul>
<li><a href="#llvm-ir-representation">LLVM IR Representation</a></li>
<li><a href="#tables_next_to_code-1">TABLES_NEXT_TO_CODE</a></li>
<li><a href="#llvm-alias-analysis-pass">LLVM Alias Analysis Pass</a></li>
<li><a href="#optimise-llvm-for-the-type-of-code-ghc-produces">Optimise LLVM for the type of Code GHC produces</a></li>
<li><a href="#update-the-back-end-to-use-the-new-cmm-data-types-new-code-generator">Update the Back-end to use the new Cmm data types / New Code Generator</a></li>
<li><a href="#llvms-link-time-optimisations">LLVM's Link Time Optimisations</a></li>
<li><a href="#llvm-cross-compiler-port">LLVM Cross Compiler / Port</a></li>
<li><a href="#get-rid-of-proc-point-splitting">Get rid of Proc Point Splitting</a></li>
<li><a href="#dont-pass-around-dead-stg-registers">Don't Pass Around Dead STG Registers</a></li>
</ul></li>
<li><a href="#wired-in-and-known-key-things">Wired-in and known-key things</a><ul>
<li><a href="#wired-in-things">Wired-in things</a></li>
<li><a href="#known-key-things-1">Known-key things</a></li>
<li><a href="#initialisation">Initialisation</a></li>
<li><a href="#orig-rdrname-things">`Orig` `RdrName` things</a></li>
</ul></li>
<li><a href="#ghc-commentary-the-word">GHC Commentary: The Word</a></li>
</ul>
</div>
<h1 id="ghc-source-code-abbreviations">GHC Source Code Abbreviations</h1>
<p>Certain abbreviations are used pervasively throughout the GHC source code. This page gives a partial list of them and their expansion:</p>
<ul>
<li><strong>ANF</strong>: A-normal form</li>
</ul>
<ul>
<li><strong>CAF</strong>: Constant Applicative Form</li>
</ul>
<ul>
<li><strong>Class</strong>: Type Class</li>
</ul>
<ul>
<li><strong>Cmm</strong>: The final IR used in GHC, based on the C-- language</li>
</ul>
<ul>
<li><strong>Core</strong>: GHC core language. Based on System FC (variant of System F). Represents a type-checked and desugared program in some (out of several) intermediate compilation step</li>
</ul>
<ul>
<li><strong>CoreFV</strong>: Free variables in core</li>
</ul>
<ul>
<li><strong>!CoreLint</strong>: Type and sanity-checking of core. (Lint: Jargon for a program analysis that looks for bug-suspicious code.)</li>
</ul>
<ul>
<li><strong>!CoreSubst</strong>: Substitution in core</li>
</ul>
<ul>
<li><strong>!CoreSyn</strong>: Core abstract syntax</li>
</ul>
<ul>
<li><strong>!DataCon</strong>: Data constructor</li>
</ul>
<ul>
<li><strong>Ds</strong>: Desugarer</li>
</ul>
<ul>
<li><strong>Gbl</strong>: Global</li>
</ul>
<ul>
<li><strong>Hs</strong>: Haskell Syntax (generally as opposed to Core, for example, Expr vs !HsExpr)</li>
</ul>
<ul>
<li><strong>Hsc</strong>: Haskell compiler. Means it Deals with compiling a single module and no more.</li>
</ul>
<ul>
<li><strong>!HsSyn</strong>: Haskell abstract syntax</li>
</ul>
<ul>
<li><strong>Id</strong>: Synonym for Var, but indicating a term variable</li>
</ul>
<ul>
<li><strong>Iface</strong>: Interface, as in Haskell interface (.hi) files</li>
</ul>
<ul>
<li><strong>!IfaceSyn</strong>: Interface abstract syntax</li>
</ul>
<ul>
<li><strong>LHs</strong>: Located Haskell something</li>
</ul>
<ul>
<li><strong>Loc</strong>: Location, as in !SrcLoc</li>
</ul>
<ul>
<li><strong>Located</strong>: Something annotated with a !SrcSpan</li>
</ul>
<ul>
<li><strong>Lcl</strong>: Local</li>
</ul>
<ul>
<li><strong>nativeGen</strong>: Native code generator (generates assembly from Cmm)</li>
</ul>
<ul>
<li><strong>Occ</strong>: Occurrence</li>
</ul>
<p><code> * However, in the context of </code><a href="http://hackage.haskell.org/trac/ghc/wiki/Commentary/Compiler/RdrNameType#TheOccNametype"><code>OccName</code></a><code>, &quot;occurrence&quot; actually means &quot;classified (i.e. as a type name, value name, etc) but not qualified and not yet resolved&quot;</code></p>
<ul>
<li><strong>PId</strong>: Package ID</li>
</ul>
<ul>
<li><strong>!PprCore</strong>: Pretty-printing core</li>
</ul>
<ul>
<li><strong>Rdr</strong>: Parser (or reader)</li>
</ul>
<ul>
<li><strong>Rn</strong>: Rename or Renamer</li>
</ul>
<ul>
<li><strong>Rts</strong>: Run Time System</li>
</ul>
<ul>
<li><strong>!SimplCore</strong>: Simplify core (the so-called simplifier belongs to this, as does the strictness analyser)</li>
</ul>
<ul>
<li><strong>!SrcLoc</strong>: Source location (filename, line number, character position)</li>
</ul>
<ul>
<li><strong>!SrcSpan</strong>: Source location span (filename, start line number and character position, end line number and character position)</li>
</ul>
<ul>
<li><strong>STG</strong>: [Commentary/Compiler/StgSynType Spineless Tagless G-machine]</li>
</ul>
<ul>
<li><strong>Tc</strong>: !TypeCheck{ing,er}</li>
</ul>
<ul>
<li><strong>TSO</strong>: <a href="https://ghc.haskell.org/trac/ghc/wiki/Commentary/Rts/Storage/HeapObjects#ThreadStateObjects">Thread State Object</a></li>
</ul>
<ul>
<li><strong>!TyCon</strong>: Type constructor</li>
</ul>
<ul>
<li><strong>!TyThing</strong>: Something that is type-checkable</li>
</ul>
<ul>
<li><strong>Ty</strong>: Type</li>
</ul>
<ul>
<li><strong>!TyVar</strong>: Synonym for Var, but indicating a type variable</li>
</ul>
<ul>
<li><strong>Var</strong>: A variable with some information about its type (or kind)</li>
</ul>
<h1 id="aging-in-the-generational-gc">Aging in the generational GC</h1>
<p>Aging is an important technique in generational GC: the idea is that objects that have only recently been allocated have not had sufficient chance to die, and so promoting them immediately to the next generation may lead to retention of unnecessary data. The problem is amplified if the prematurely promoted objects are thunks that are subsequently updated, leading to retention of an arbitrary amount of live data until the next collection of the old generation, which may be a long time coming.</p>
<p>The idea is that instead of promoting live objects directly from generation 0 into generation 1, they stay in generation 0 for a &quot;while&quot;, and if they live long enough, they get promoted. The simplest way is to segment the objects in generation 0 by the number of collections they have survived, up to a maximum. GHC 6.12 used to do this: each generation had a tunable number of <em>steps</em>. Objects were initially promoted to step 0, copied through each subsequent step on following GC cycles, and then eventually promoted to the next generation.</p>
<p>Measurement we made showed that the optimal number of steps was somewhere between 1 and 3 (2 was almost always better than either 1 or 3). In priniciple it is possible to have a fractional number of steps, although GHC 6.12 only supported integral numbers.</p>
<p>In GHC 6.13 and later, we made the following change: each block now points to the generation to which objects in that block will be copied in the next GC (the `dest` field of `bdescr`). This lets us decide on a block-by-block basis which objects to promote and which to retain in a generation, and lets us implement fractional numbers of steps. At the same time, we dropped the notion of explicit steps, so each generation just has a single list of blocks. This means that we can no longer do aging of more than 2 GC cycles, but since the measurements showed that this was unlikely to be beneficial, and the new structure is much simpler, we felt it was worthwhile.</p>
<p>Blocks in the nursery have a `dest` field pointing to generation 0, and blocks of live objects in generation 0 have a `dest` field pointing to generation 1. This gives us the same effect as 2 steps did in the GHC 6.12, except that intermediate generations (e.g. gen 1 in a 3-gen setup) now only have one step rather than 2. We could implement aging in the intermediate generations too if that turns out to be beneficial (more than 2 generations is rarely better than 2, according to our measurements).</p>
<h1 id="improving-llvm-alias-analysis">Improving LLVM Alias Analysis</h1>
<p>This page tracks the information and progress relevant to improving the alias analysis pass for the LLVM backend of GHC.</p>
<p>This correspond to bug #5567.</p>
<h2 id="llvm-alias-analysis-infrastructure">LLVM Alias Analysis Infrastructure</h2>
<p>Some links to the various documentation on LLVM's AA support:</p>
<p><code>* </code><a href="http://llvm.org/docs/AliasAnalysis.html"><code>LLVM</code> <code>Alias</code> <code>Analysis</code> <code>Infrastructure</code></a><br />
<code>* </code><a href="http://llvm.org/docs/Passes.html"><code>LLVM's</code> <code>Analysis</code> <code>and</code> <code>Transform</code> <code>Passes</code></a><br />
<code>* </code><a href="http://llvm.org/docs/GetElementPtr.html"><code>The</code> <code>Often</code> <code>Misunderstood</code> <code>GEP</code> <code>Instruction</code></a><br />
<code>* </code><a href="http://llvm.org/docs/LangRef.html"><code>LLVM</code> <code>Language</code> <code>Reference</code></a><br />
<code>* </code><a href="http://groups.google.com/group/llvm-dev/browse_thread/thread/2a5944692508bcc2/363c96bb1c6a506d?show_docid=363c96bb1c6a506d&amp;pli=1"><code>LLVM</code> <code>Dev</code> <code>List:</code> <code>Comparison</code> <code>of</code> <code>Alias</code> <code>Analysis</code> <code>in</code> <code>LLVM</code></a></p>
<h2 id="maxs-work">Max's Work</h2>
<p>Max had a crack at writing a custom alias analysis pass for LLVM, relevant links are:</p>
<p><code>* </code><a href="http://lists.cs.uiuc.edu/pipermail/llvmdev/2011-September/043603.html"><code>Email</code> <code>to</code> <code>LLVM</code> <code>dev</code></a><br />
<code>* </code><a href="http://blog.omega-prime.co.uk/?p=135"><code>Blog</code> <code>post</code> <code>about</code> <code>results</code></a><br />
<code>* </code><a href="https://github.com/bgamari/ghc-llvm-analyses"><code>A</code> <code>port</code> <code>to</code> <code>LLVM</code> <code>3.6</code></a></p>
<h2 id="tbaa">TBAA</h2>
<p>LLVM as of version 2.9 includes Type Based Alias Analysis. This mean using metadata you can specify a type hierarchy (with alias properties between types) and annotate your code with these types to improve the alias information. This should allow us to improve the alias analysis without any changes to LLVM itself like Max made.</p>
<p><code>* </code><a href="http://llvm.org/docs/LangRef.html#tbaa"><code>LLVM</code> <code>TBBA</code> <code>Doc</code></a></p>
<h2 id="stg-cmm-alias-properties">STG / Cmm Alias Properties</h2>
<p><strong>Question</strong> (David Terei): What alias properties does the codegen obey? Sp and Hp never alias? R<n> registers never alias? ....</p>
<p><strong>Answer</strong> (Simon Marlow): Sp[] and Hp[] never alias, R[] never aliases with Sp[], and that's about it.</p>
<p></p>
<p><em>' Simon</em>': As long as it propagates properly, such that every F(Sp) is a stack pointer, where F() is any expression context except a dereference. That is, we better be sure that</p>
<p></p>
<p>is &quot;stack&quot;, not &quot;heap&quot;.</p>
<h2 id="how-to-track-tbaa-information">How to Track TBAA information</h2>
<p>Really to be sound and support Cmm in full we would need to track and propagate TBAA information. It's Types after all! At the moment we don't. We simply rely on the fact that the Cmm code generated for loads and stores is nearly always in the form of:</p>
<p></p>
<p>That is to say, it has the values it depends on for the pointer derivation in-lined in the load or store expression. It is very rarely of the form:</p>
<p></p>
<p>And when it is, 'it is' (unconfirmed) always deriving a &quot;heap&quot; pointer, &quot;stack&quot; pointers are always of the in-line variety. This assumption if true allows us to look at just a store or load in isolation to properly Type it.</p>
<p>There are two ways to type this 'properly'.</p>
<p>1. Do data flow analysis. This is the only proper way to do it but also annoying. 2. Do block local analysis. Instead of doing full blow data flow analysis, just track the type of pointers stored to CmmLocal regs at the block level. This is safe but just may miss some opportunities when a CmmLocal's value is assigned in another block... My hunch is this is quite rare so this method should be fairly effective (and easier to implement and quicker to run that 1.)</p>
<h2 id="llvm-type-system">LLVM type system</h2>
<p>The above aliasing information can be encoded as follows:</p>
<p></p>
<p>The fact that `R[]` never aliases with `Sp[]` is never used as the one way relation isn't expressible in LLVM.</p>
<p>Stores/loads needs to be annotated with `!tbaa` and one of the above four types e.g.</p>
<p></p>
<h2 id="problems-optmisations-to-solve">Problems / Optmisations to Solve</h2>
<h3 id="llvm-optimisations">LLVM Optimisations</h3>
<p>Roman reported that running 'opt -std-compile-opts' gives much better code than running 'opt -O3'.</p>
<p><strong>Following is from Roman Leschinskiy</strong></p>
<p>'-O2 -std-compile-opts' does the trick but it's obviously overkill because it essentially executes the whole optimisation pipeline twice. The crucial passes seem to be loop rotation and loop invariant code motion. These are already executed twice by -O2 but it seems that they don't have enough information then and that something interesting happens in later passes which allows them to work much better the third time.</p>
<h3 id="safe-loads-speculative-load">Safe Loads (speculative load)</h3>
<p>We want to allow LLVM to speculatively hoist loads out of conditional blocks. Relevant LLVM source code is here:</p>
<p><code>* </code><a href="http://llvm.org/docs/doxygen/html/SimplifyCFG_8cpp_source.html"><code>SimplifyCFG</code> <code>Source</code> <code>Code</code></a><br />
<code>* </code><a href="http://llvm.org/docs/doxygen/html/namespacellvm.html#a4899ff634bf732c16dd22ecfdafdea7d"><code>llvm::isSafeToSpeculativelyExecute</code></a><br />
<code>* </code><a href="http://lists.cs.uiuc.edu/pipermail/llvmdev/2012-January/046958.html"><code>LLVM</code> <code>Mailing</code> <code>List</code> <code>Discussion</code> <code>about</code> <code>'Safe</code> <code>loads'</code></a></p>
<p><strong>Following is from Roman Leshchinskiy</strong></p>
<p>I've poked around a bit and things are rather complicated. So far I've identified two problems. Here is a small example function:</p>
<p></p>
<p>This is the interesting C-- bit:</p>
<p></p>
<p>Look at what indexDoubleArray# compiles to: F64[I32[Sp + 12] + ((R1 &lt;&lt; 3) + 8)]. We would very much like LLVM to hoist the I32[Sp+12] bit (i.e., loading the pointer to the ByteArray data) out of the loop because that might allow all sorts of wonderful optimisation such as promoting it to a register. But alas, this doesn't happen, LLVM leaves the load in the loop. Why? Because it assumes that the load might fail (for instance, if Sp is NULL) and so can't move it past conditionals. We know, of course, that this particular load can't fail and so can be executed speculatively but there doesn't seem to be a way of communicating this to LLVM.</p>
<p>As a quick experiment, I hacked LLVM to accept &quot;safe&quot; annotations on loads and then manually annotated the LLVM assembly generated by GHC and that helped quite a bit. I suppose that's the way to go - we'll have to get this into LLVM in some form and then the backend will have to generate those annotations for loads which can't fail. I assume they are loads through the stack pointer and perhaps the heap pointer unless we're loading newly allocated memory (those loads can't be moved past heap checks). In any case, the stack pointer is the most important thing. I can also imagine annotating pointers (such as Sp) rather than instructions but that doesn't seem to be the LLVM way and it's also less flexible.</p>
<h3 id="ghc-heap-check-case-merging">GHC Heap Check (case merging)</h3>
<p>See bug #1498</p>
<p><strong>Following is from Roman Leshchinskiy</strong></p>
<p>I investigated heap check a bit more and it seems to me that it's largely GHC's fault. LLVM does do loop unswitching which correctly pulls out loop-invariant heap checks but that happens fairly late in its pipeline and heap checks interfere with optimisations before that.</p>
<p>However, we really shouldn't be generating those heap checks in the first place. Here is a small example loop:</p>
<p></p>
<p>This is the C-- that GHC generates:</p>
<p></p>
<p>Note how in each loop iteration, we add 12 to Hp, then do the heap check and then subtract 12 from Hp again. I really don't think we should be generating that and then relying on LLVM to optimise it away.</p>
<p>This happens because GHC commons up heap checks for case alternatives and does just one check before evaluating the case. The relevant comment from CgCase.lhs is this:</p>
<p>A more interesting situation is this:</p>
<p></p>
<p>where !x! indicates a possible heap-check point. The heap checks in the alternatives <strong>can</strong> be omitted, in which case the topmost heapcheck will take their worst case into account.</p>
<p>This certainly makes sense if A allocates. But with vector-based code at least, a lot of the time neither A nor C will allocate <strong>and</strong> C will tail-call A again so by pushing the heap check into !A!, we are now doing it <strong>in</strong> the loop rather than at the end.</p>
<p>It seems to me that we should only do this if A actually allocates and leave the heap checks in the alternatives if it doesn't (perhaps we could also use a common heap check if <strong>all</strong> alternatives allocate). I tried to hack this and see what happens but found the code in CgCase and friends largely incomprehensible. What would I have to change to implement this (perhaps controlled by a command line flag) and is it a good idea at all?</p>
<h1 id="ghc-commentary-the-ghc-api">GHC Commentary: The GHC API</h1>
<p>This section of the commentary describes everything between [wiki:Commentary/Compiler/HscMain HscMain] and the front-end; that is, the parts of GHC that coordinate the compilation of multiple modules.</p>
<p>The GHC API is rather stateful; the state of an interaction with GHC is stored in an abstract value of type . The only fundamental reason for this choice is that the  models the state of the RTS's linker, which must be single-threaded.</p>
<p>Although the GHC API apparently supports multiple clients, because each can be interacting with a different , in fact it only supports one client that is actually executing code, because the [wiki:Commentary/Rts/Interpreter#Linker RTS linker] has a single global symbol table.</p>
<p>This part of the commentary is not a tutorial on <em>using</em> the GHC API: for that, see <a href="http://haskell.org/haskellwiki/GHC/As_a_library">Using GHC as a Library</a>. Here we are going to talk about the implementation.</p>
<p>A typical interaction with the GHC API goes something like the following:</p>
<p><code>* You probably want to wrap the whole program in </code><code> to get error messages</code><br />
<code>* Create a new session: </code><br />
<code>* Set the flags: </code><code>, </code><code>.</code><br />
<code>* Add some </code><em><code>targets</code></em><code>: </code><code>, </code><code>, </code><br />
<code>* Perform </code><a href="ref(Dependency_Analysis)" title="wikilink"><code>ref(Dependency</code> <code>Analysis)</code></a><code>: </code><br />
<code>* Load (compile) the source files: </code></p>
<p>Warning: Initializing GHC is tricky! Here is a template that seems to initialize GHC and a session. Derived from ghc's Main.main function.</p>
<p></p>
<p>You must pass the path to  as an argument to .</p>
<p>The  field of  tells the compiler what kind of output to generate from compilation. There is unfortunately some overlap between this and the  passed to ; we hope to clean this up in the future, but for now it's probably a good idea to make sure that these two settings are consisent. That is, if , then , if  then .</p>
<h2 id="targets">Targets</h2>
<p>The targets specify the source files or modules at the top of the dependency tree. For a Haskell program there is often just a single target , but for a library the targets would consist of every visible module in the library.</p>
<p>The  type is defined in <a href="GhcFile(compiler/main/HscTypes.lhs)" class="uri" title="wikilink">GhcFile(compiler/main/HscTypes.lhs)</a>. Note that a  includes not just the file or module name, but also optionally the complete source text of the module as a : this is to support an interactive development environment where the source file is being edited, and the in-memory copy of the source file is to be used in preference to the version on disk.</p>
<h2 id="dependency-analysis">Dependency Analysis</h2>
<p>The dependency analysis phase determines all the Haskell source files that are to be compiled or loaded in the current session, by traversing the transitive dependencies of the targets. This process is called the <em>downsweep</em> because we are traversing the dependency tree downwards from the targets. (The <em>upsweep</em>, where we compile all these files happens in the opposite direction of course).</p>
<p>The  function takes the targets and returns a list of  consisting of all the modules to be compiled/loaded.</p>
<h2 id="the-modsummary-type">The !ModSummary type</h2>
<p>A  (defined in <a href="GhcFile(compiler/main/HscTypes.lhs)" class="uri" title="wikilink">GhcFile(compiler/main/HscTypes.lhs)</a>) contains various information about a module:</p>
<p><code>* Its </code><code>, which includes the package that it belongs to</code><br />
<code>* Its </code><code>, which lists the pathnames of all the files associated with the module</code><br />
<code>* The modules that it imports</code><br />
<code>* The time it was last modified</code><br />
<code>* ... some other things</code></p>
<p>We collect  information for all the modules we are interested in during the <em>downsweep</em>, below. Extracting the information about the module name and the imports from a source file is the job of <a href="GhcFile(compiler/main/HeaderInfo.hs)" class="uri" title="wikilink">GhcFile(compiler/main/HeaderInfo.hs)</a> which partially parses the source file.</p>
<p>Converting a given module name into a  is done by  in <a href="GhcFile(compiler/main/GHC.hs)" class="uri" title="wikilink">GhcFile(compiler/main/GHC.hs)</a>. Similarly, if we have a filename rather than a module name, we generate a  using .</p>
<h2 id="loading-compiling-the-modules">Loading (compiling) the Modules</h2>
<p>When the dependency analysis is complete, we can load these modules by calling . The same interface is used regardless of whether we are loading modules into GHCi with the  command, or compiling a program with : we always end up calling .</p>
<p>The process in principle is fairly simple:</p>
<p><code>* Visit each module in the dependency tree from the bottom up, invoking [wiki:Commentary/Compiler/HscMain HscMain]</code><br />
<code>  to compile it (the </code><em><code>upsweep</code></em><code>).</code><br />
<code>* Finally, link all the code together.  In GHCi this involves loading all the object code into memory and linking it</code><br />
<code>  with the [wiki:Commentary/Rts/Interpreter#Linker RTS linker], and then linking all the byte-code together.  In</code><br />
<code>  </code><code> mode this involves invoking the external linker to link the object code into a binary.</code></p>
<p>The process is made more tricky in practice for two reasons:</p>
<p><code>* We might not need to compile certain modules, if none of their dependencies have changed.  GHC's </code><br />
<code>  [wiki:Commentary/Compiler/RecompilationAvoidance recompilation checker] determines whether a module really needs</code><br />
<code>  to be compiled or not.</code><br />
<code>* In GHCi, we might just be reloading the program after making some changes, so we don't even want to re-link</code><br />
<code>  modules for which no dependencies have changed.</code></p>
<h1 id="ghc-commentary-asynchronous-exceptions">GHC Commentary: Asynchronous Exceptions</h1>
<h1 id="ghc-commentary-backends">GHC Commentary: Backends</h1>
<p>After [wiki:Commentary/Compiler/CmmType Cmm] has been generated, we have a choice of targets to compile to:</p>
<p><code>* [wiki:Commentary/Compiler/Backends/PprC The C code generator]</code><br />
<code>* [wiki:Commentary/Compiler/Backends/NCG  The native code generator]</code><br />
<code>* [wiki:Commentary/Compiler/Backends/LLVM The LLVM code generator]</code><br />
<code>* [wiki:Commentary/Compiler/Backends/GHCi The GHCi code generator]</code></p>
<p>These backends are completely interchangeable. Our preferred route is the native code generator. The C code generator is used for portable, non-optimised, or unregisterised compilation (Note that the LLVM backend also supports building GHC in unregisterised mode as well as registerised mode so it is usually the preferred route for porting GHC).</p>
<h1 id="types-in-the-back-end-aka-the-rep-swamp">Types in the back end (aka &quot;The `Rep` swamp&quot;)</h1>
<p>I have completed a major representation change, affecting both old and new code generators, of the various `Rep` types. It's pervasive in that it touches a lot of files; and in the native code-gen very many lines are changed. The new situation is much cleaner.</p>
<p>Here are the highlights of the new design.</p>
<h2 id="cmmtype">`CmmType`</h2>
<p>There is a new type `CmmType`, defined in module `CmmExpr`, which is just what it sounds like: it's the type of a `CmmExpr` or a `CmmReg`.</p>
<p><code>  * A `CmmType` is </code><em><code>abstract</code></em><code>: its representation is private to `CmmExpr`.  That makes it easy to change representation.</code><br />
<code>  * A `CmmType` is actually just a pair of a `Width` and a category (`CmmCat`).</code><br />
<code>  * The `Width` type is exported and widely used in pattern-matching, but it does what it says on the tin: width only.  </code><br />
<code>  * In contrast, the `CmmCat` type is entirely private to `CmmExpr`.  It is just an enumeration that allows us to distinguish: floats, gc pointers, and other. </code></p>
<p>Other important points are these:</p>
<p><code>* Each `LocalReg` has a `CmmType` attached; this replaces the previous unsavoury combination of `MachRep` and `CmmKind`.  Indeed, both of the latter are gone entirely.</code></p>
<p><code>* Notice that a `CmmType` accurately knows about gc-pointer-hood. Ultimately we will abandon static-reference-table generation in STG syntax, and instead generate SRTs from the Cmm code.  We'll need to update the RTS `.cmm` files to declare pointer-hood.</code></p>
<p><code>* The type `TyCon.PrimRep` remains; it enumerates the representations that a Haskell value can take.  Differences from `CmmType`:</code><br />
<code>  * `PrimRep` contains `VoidRep`, but `CmmType` has no zero-width form.</code><br />
<code>  * `CmmType` includes sub-word width values (e.g. 8-bit) which `PrimRep` does not.</code><br />
<code>  The function `primRepCmmType` converts a non-void `PrimRep` to a `CmmType`.</code></p>
<p><code>* `CmmLint` is complains if you assign a gc-ptr to a non-gc-ptr and vice versa.  It treats &quot;gc-ptr + constant&quot; as a gc-ptr.  </code></p>
<p><code>   </code><em><code>NB:</code> <code>you'd</code> <code>better</code> <code>not</code> <code>make</code> <code>an</code> <code>interior</code> <code>pointer</code> <code>live</code> <code>across</code> <code>a</code> <code>call</code></em><code>, else we'll save it on the stack and treat it as a GC root.  It's not clear how to guarantee this doesn't happen as the result of some optimisation.</code></p>
<p><strong>Parsing `.cmm` RTS files.</strong> The global register `P0` is a gc-pointer version of `R0`. They both map to the same physical register, though!</p>
<h2 id="the-machop-type">The `MachOp` type</h2>
<p>The `MachOp` type enumerates (in machine-independent form) the available machine instructions. The principle they embody is that <em>everything except the width is embodied in the opcode</em>. In particular, we have</p>
<p><code>* `MO_S_Lt`, `MO_U_Lt`, and `MO_F_Lt` for comparison (signed, unsigned, and float).</code><br />
<code>* `MO_SS_Conv`, `MO_SF_Conv` etc, for conversion (`SS` is signed-to-signed, `SF` is signed-to-float, etc).</code></p>
<p>These constructor all take `Width` arguments.</p>
<p>The `MachOp` data type is defined in `CmmExpr`, not in a separate `MachOp` module.</p>
<h2 id="foreign-calls-and-hints">Foreign calls and hints</h2>
<p>In the new Cmm representation (`ZipCfgCmmRep`), but not the old one, arguments and results to all calls, including foreign ones, are ordinary `CmmExpr` or `CmmReg` respectively. The extra information we need for foreign calls (is this signed? is this an address?) are kept in the calling convention. Specifically:</p>
<p><code>* `MidUnsafeCall` calls a `MidCallTarget`</code><br />
<code>* `MidCallTarget` is either a `CallishMachOp` or a `ForeignTarget`</code><br />
<code>* In the latter case we supply a `CmmExpr` (the function to call) and a `ForeignConvention`</code><br />
<code>* A `ForeignConvention` contains the C calling convention (stdcall, ccall etc), and a list of `ForiegnHints` for arguments and for results. (We might want to rename this type.)</code></p>
<p>This simple change was horribly pervasive. The old Cmm rep (and Michael Adams's stuff) still has arguments and results being (argument,hint) pairs, as before.</p>
<h2 id="native-code-generation-and-the-size-type">Native code generation and the `Size` type</h2>
<p>The native code generator has an instruction data type for each architecture. Many of the instructions in these data types used to have a `MachRep` argument, but now have a `Size` argument instead. In fact, so far as the native code generators are concerned, these `Size` types (which can be machine-specific) are simply a plug-in replacement for `MachRep`, with one big difference: <strong>`Size` is completely local to the native code generator</strong> and hence can be changed at will without affecting the rest of the compiler.</p>
<p>`Size` is badly named, but I inherited the name from the previous code.</p>
<p>I rather think that many instructions should have a `Width` parameter, not a `Size` parameter. But I didn't feel confident to change this. Generally speaking the NCG is a huge swamp and needs re-factoring. I'm working on getting Backtraces in GHC. Progress can be seen here: <a href="https://github.com/abacathoo/ghc" class="uri">https://github.com/abacathoo/ghc</a></p>
<h1 id="the-block-allocator">The Block Allocator</h1>
<p>Source: <a href="GhcFile(includes/rts/storage/Block.h)" class="uri" title="wikilink">GhcFile(includes/rts/storage/Block.h)</a>, <a href="GhcFile(rts/sm/BlockAlloc.h)" class="uri" title="wikilink">GhcFile(rts/sm/BlockAlloc.h)</a>, <a href="GhcFile(rts/sm/BlockAlloc.c)" class="uri" title="wikilink">GhcFile(rts/sm/BlockAlloc.c)</a>, <a href="GhcFile(includes/rts/storage/MBlock.h)" class="uri" title="wikilink">GhcFile(includes/rts/storage/MBlock.h)</a>, <a href="GhcFile(rts/sm/MBlock.c)" class="uri" title="wikilink">GhcFile(rts/sm/MBlock.c)</a>.</p>
<p>The block allocator is where the storage manager derives much of its flexibilty. Rather than keep our heap in a single contiguous region of memory, or one contiguous region per generation, we manage linked lists of memory blocks. Managing contiguous regions is difficult, especially when you want to change the size of some of the areas. A block-structured storage arrangement has several advantages:</p>
<p><code>* resizing areas of memory is easy: just chain more blocks onto the list.</code></p>
<p><code>* managing large objects without copying is easy: allocate each one a complete block, and use the block linkage to</code><br />
<code>  chain them together.</code></p>
<p><code>* free memory can be recycled faster, because a block is a block.</code></p>
<p>The concept relies on the property that most data objects are significantly smaller than a block, and only rarely do we need to allocate objects that approach or exceed the size of a block.</p>
<h2 id="structure-of-blocks">Structure of blocks</h2>
<p>We want to allocate memory in units of a small block (around 4k, say). Furthermore, we want each block to have an associated small structure called a <em>block descriptor</em>, which contains information about the block: its link field, which generation it belongs to, and so on. This is similar to the well-known &quot;BiBOP&quot; (Big Bag of Pages) technique, where objects with similar tags are collected together on a page so as to avoid needing to store an individual tag with each object.</p>
<p>We want a function `Bdescr(p)`, that, given an arbitrary pointer into a block, returns the address of the block descriptor that corresponds to the block containing that pointer.</p>
<p>There are two options:</p>
<p><code>* Put the block descriptor at the start of the block.  `Bdescr(p) = p &amp; ~BLOCK_SIZE`.  This option has problems if</code><br />
<code>  we need to allocate a contiguous region larger than a single block (GHC does this occasionally when allocating</code><br />
<code>  a large number of objects in one go).</code></p>
<p><code>* Allocate memory in larger units (a </code><em><code>megablock</code></em><code>), divide the megablock into blocks, and put all the block</code><br />
<code>  descriptors at the beginning.  The megablock is aligned, so that the address of the block descriptor for</code><br />
<code>  a block is a simple function of its address.  The 'Bdescr' function is more complicated than the first</code><br />
<code>  method, but it is easier to allocate contiguous regions (unless the contiguous region is larger than</code><br />
<code>  a megablock...).</code></p>
<p>We adopt the second approach. The following diagram shows a megablock:</p>
<p><a href="Image(sm-block.png)" class="uri" title="wikilink">Image(sm-block.png)</a></p>
<p>We currently have megablocks of 1Mb in size (m = 20) with blocks of 4k in size (k = 12), and these sizes are easy to change (<a href="GhcFile(includes/rts/Constants.h)" class="uri" title="wikilink">GhcFile(includes/rts/Constants.h)</a>).</p>
<p>Block descriptors are currently 32 or 64 bytes depending on the word size (d = 5 or 6). The block descriptor itself is the structure `bdescr` defined in <a href="GhcFile(includes/rts/storage/Block.h)" class="uri" title="wikilink">GhcFile(includes/rts/storage/Block.h)</a>, and that file also defines the `Bdescr()` macro.</p>
<p>The block allocator has a the following structure:</p>
<p><code>* At the bottom, talking to the OS, is the megablock allocator (</code><a href="GhcFile(rts/sm/MBlock.c)" title="wikilink"><code>GhcFile(rts/sm/MBlock.c)</code></a><code>, </code><a href="GhcFile(includes/rts/storage/MBlock.h)" title="wikilink"><code>GhcFile(includes/rts/storage/MBlock.h)</code></a><code>).</code><br />
<code>  It is responsible for delivering megablocks, correctly aligned, to the upper layers.  It is also responsible for</code><br />
<code>  implementing [wiki:Commentary/HeapAlloced HEAP_ALLOCED()]: the predicate that tests whether a pointer points to dynamically allocated memory</code><br />
<code>  or not.  This is implemented as a simple bitmap lookup on a 32-bit machine, and something more complex on</code><br />
<code>  64-bit addressed machines.  See </code><a href="GhcFile(includes/rts/storage/MBlock.h)" title="wikilink"><code>GhcFile(includes/rts/storage/MBlock.h)</code></a><code> for details.</code><br />
<code>  </code><a href="br" title="wikilink"><code>br</code></a><a href="br" title="wikilink"><code>br</code></a><br />
<code>  Currently, megablocks are never freed back to the OS, except at the end of the program.  This is a potential</code><br />
<code>  improvement that could be made.</code></p>
<p><code>* Sitting on top of the megablock allocator is the block layer (</code><a href="GhcFile(includes/rts/storage/Block.h)" title="wikilink"><code>GhcFile(includes/rts/storage/Block.h)</code></a><code>, </code><a href="GhcFile(rts/sm/BlockAlloc.c)" title="wikilink"><code>GhcFile(rts/sm/BlockAlloc.c)</code></a><code>).</code><br />
<code>  This layer is responsible for providing:</code></p>
<p></p>
<p><code> These functions allocate and deallocate a block </code><em><code>group</code></em><code>: a contiguous sequence of blocks (the degenerate, and common, case</code><br />
<code> is a single block).  The block allocator is responsible for keeping track of free blocks.  Currently it does this by</code><br />
<code> maintaining an ordered (by address) list of free blocks, with contiguous blocks coallesced.  However this is certanly</code><br />
<code> not optimal, and has been shown to be a bottleneck in certain cases - improving this allocation scheme would be good.</code></p>
<h1 id="ghc-commentary-garbage-collecting-cafs">GHC Commentary: Garbage Collecting CAFs</h1>
<p>Files: <a href="GhcFile(rts/sm/GC.c)" class="uri" title="wikilink">GhcFile(rts/sm/GC.c)</a>, function scavange_srt in <a href="GhcFile(rts/sm/Scav.h)" class="uri" title="wikilink">GhcFile(rts/sm/Scav.h)</a></p>
<p>Constant Applicative Forms, or CAFs for short, are top-level values defined in a program. Essentially, they are objects that are not allocated dynamically at run-time but, instead, are part of the static data of the program. Sometimes, a CAF may refer to many values in the heap. To avoid memory leaks in such situations, we need to know when a CAF is never going to be used again, and so we can deallocate the values that it refers to.</p>
<p>See Note [CAF management] in <a href="GhcFile(rts/sm/Storage.c)" class="uri" title="wikilink">GhcFile(rts/sm/Storage.c)</a> for more information.</p>
<h2 id="static-reference-tables">Static Reference Tables</h2>
<p>File: <a href="GhcFile(includes/rts/storage/InfoTables.h)" class="uri" title="wikilink">GhcFile(includes/rts/storage/InfoTables.h)</a></p>
<p>The info table of various closures may contain information about what static objects are referenced by the closure. This information is stored in two parts:</p>
<p><code> 1. a static reference table (SRT), which is an array of references to static objects</code><br />
<code> 2. a bitmask which specifies which of the objects are actually used by the closure.</code></p>
<p>There are two different ways to access this information depending on the size of the SRT:</p>
<p><code> * &quot;small&quot;: if </code><code> is a small bitmap, not all 1s, then GET_FUN?_SRT contains the SRT.</code><br />
<code> * &quot;large&quot;: if </code><code> is all 1s, then GET_FUN?_SRT contains a large bitmap, and the actual SRT.</code></p>
<h2 id="evacuating-static-objects">Evacuating Static Objects</h2>
<p>Files: <a href="GhcFile(rts/sm/GCThread.h)" class="uri" title="wikilink">GhcFile(rts/sm/GCThread.h)</a>, <a href="GhcFile(rts/sm/Evac.c)" class="uri" title="wikilink">GhcFile(rts/sm/Evac.c)</a>, <a href="GhcFile(rts/sm/GC.c)" class="uri" title="wikilink">GhcFile(rts/sm/GC.c)</a></p>
<p>While scavenging objects, we also process (aka &quot;evacuate&quot;) any static objects that need to be kept alive. When a GC thread discovers a live static object, it places it on its  list. Later, this list is used to scavange the static objects, potentially finding more live objects. Note that this process might find more static objects, and thus further extend the  list.</p>
<p>When a static object is scavenged, it is removed from  and placed on another list, called . Later, we use this list to &quot;clean up&quot; the liveness markers from these static objects, so that we can repeat the process on the next garbage collection. Note that we can't &quot;clean up&quot; the liveness markers as we go along because we use them to notice cycles among the static objects.</p>
<h1 id="calling-convention">Calling Convention</h1>
<p>Entry conventions are very conventional: the first N argumements in registers and the rest on the stack.</p>
<h1 id="return-convention">Return Convention</h1>
<p>All returns are now <em>direct</em>; that is, a return is made by jumping to the code associated with the [wiki:Commentary/Rts/Storage/HeapObjects#InfoTables info table] of the topmost [wiki:Commentary/Rts/Storage/Stack stack frame].</p>
<p>GHC used to have a more complex return convention called vectored returns in which some stack frames pointed to vectors of return addresses; this was dropped in GHC 6.8 after measurements that showed it was not (any longer) worthwhile.</p>
<h2 id="historical-page">Historical page</h2>
<p>This page is a bunch of notes on the new code generator. It is outdated and is here only for historical reasons.It should probably be removed. See [wiki:Commentary/Compiler/CodeGen Code Generator] page for a description of current code generator.</p>
<h1 id="cleanup-after-the-new-codegen-is-enabled">Cleanup after the new codegen is enabled</h1>
<p>The new codegen was enabled by default in 832077ca5393d298324cb6b0a2cb501e27209768. Now that the switch has been made, we can remove all the cruft associated with the old code generator. There are dependencies between some of the components, so we have to do things in the right order. Here is a list of the cleanup tasks, and notes about dependencies:</p>
<h2 id="independent-tasks">Independent tasks</h2>
<p><code>* Use `BlockId` or `Label` consistently, currently we use a mixture of the two.  Maybe get rid of the `BlockId` module.</code></p>
<p><code>* Remove live-var and CAF lists from `StgSyn`, and then clean up `CoreToStg`</code></p>
<p><code>* DONE: Remove the SRT pass in `simplStg/SRT.lhs`</code></p>
<p><code>* DONE: remove RET_DYN from the RTS</code></p>
<p><code>* DONE: remove `-fnew-codegen`, related `HscMain` bits and the `CodeGen` module.</code></p>
<p><code>* DONE: remove `CmmOpt.cmmMiniInline`, it is not used any more</code></p>
<p><code>* Fix the layering: `cmm` modules should not depend on `codeGen/StgCmm*`</code></p>
<h2 id="towards-removing-codegencg">Towards removing codeGen/Cg*</h2>
<p><code>* DONE: `CmmParse` should produce new `Cmm`. </code><br />
<code>  * We will probably want two kinds of `.cmm` file, one that is to be fed through `CmmLayoutStack` and one that isn't.</code><br />
<code>  * primops will be fed through `CmmLayoutStack`, and will use the native calling convention, with the code generator inserting the copyin/copyout for us.</code></p>
<p><code>* DONE: Remove all the `Cg*` modules</code></p>
<h2 id="towards-removing-oldcmm">Towards removing `OldCmm`</h2>
<p><code>* IN PROGRESS (Simon M): Change the NCG over to consume new `Cmm`.  We possibly also want the generated native code to use the Hoopl Block representation, although that will mean changing branch instructions to have both true and false targets, rather than true and fallthrough as we have now.</code></p>
<p><code>* Remove `cmm/CmmCvt` (this will save some compile-time too)</code></p>
<p><code>* Remove `cmm/OldCmm*`, `cmm/PprOldCmm` etc.</code></p>
<h2 id="later">Later</h2>
<p><code>* Do the new SRT story (!ToDo: write a wiki page about this)</code></p>
<h1 id="cmm-implementing-exception-handling">Cmm: Implementing Exception Handling</h1>
<p>The IEEE 754 specification for floating point numbers defines exceptions for certain floating point operations, including:</p>
<p><code>* range violation (overflow, underflow); </code><br />
<code>* rounding errors (inexact); </code><br />
<code>* invalid operation (invalid operand, such as comparison with a `NaN` value, the square root of a negative number or division of zero by zero); and,</code><br />
<code>* zero divide (a special case of an invalid operation).  </code></p>
<p>Many architectures support floating point exceptions by including a special register as an addition to other exception handling registers. The IBM PPC includes the `FPSCR` (&quot;Floating Point Status Control Register&quot;); the Intel x86 processors use the `MXCSR` register. When the PPC performs a floating point operation it checks for possible errors and sets the `FPSCR`. Some processors allow a flag in the Foating-Point Unit (FPU) status and control register to be set that will disable some exceptions or the entire FPU exception handling facility. Some processors disable the FPU after an exception has occurred while others, notably Intel's x86 and x87 processors, continue to perform FPU operations. Depending on whether quiet !NaNs (QNaNs) or signaling !NaNs (SNaNs) are used by the software, an FPU exception may signal an interrupt for the software to pass to its own exception handler.</p>
<p>Some higher level languages provide facilities to handle these exceptions, including Ada, Fortran (F90 and later), C++ and C (C99, fenv.h, float.h on certain compilers); others may handle such exceptions without exposing a low-level interface. There are three reasons to handle FPU exceptions, and these reasons apply similarly to other exceptions:</p>
<p><code>* the facilities provide greater control; </code><br />
<code>* the facilities are efficient--more efficient than a higher-level software solution; and, </code><br />
<code>* FPU exceptions may be unavoidable, especially if several FPU operations are serially performed at the machine level so the higher level software has no opportunity to check the results in between operations. </code></p>
<h4 id="an-integral-exception-example">An Integral Exception Example</h4>
<p>There has been at least one problem in GHC that would benefit from exception handling--in some cases, for `Integral`s. See bug ticket #1042. The bug occurs in `show`ing the number, in [GhcFile(libraries/base/GHC/Show.lhs) GHC.Show], `showSignedInt`, before conversion from base_2 to base_10, where a negative `Int` (always `Int32`) is negated in order to process it as a positive value when converting it to a string, base_10, causing an overflow error on some architectures. (Bear in mind that it would show up here in the example for #1042 because the function would be evaluated in GHCi here; the negation is the problem and the exception shows up in the <em>next</em> instruction on that operand, here `DIV`.)</p>
<p>The exception example in #1042 does not occur on PowerPC machines, which dutifully print the two's complement of : `0`. (`-2147483648` is the minimum bound for signed Ints, so negating it should properly become, bitwise, a positive `2147483647` (all but bit 31 set); once negated again when divided by `-1` this would be `0`; `-0` is converted to `0`.) On some architectures such as Intel 64 and IA-32, negating the minimum bound does not wrap around to `0` but overflows, which is reported as a floating point &quot;overflow&quot; (`#O`) exception: the `NEG` instruction modifies the `OF` flag (bit 11) in the `EFLAGS` register--curiously enough, the `DIV` and `IDIV` instructions have <em>undefined</em> effects on the `OF` flag.</p>
<p>The workaround was to avoid negating `minBound` `Int`s; note that no Intel instructions allow one to modify the `OF` flag directly. Alternative solutions might be to</p>
<p><code>1. mask the &quot;exception&quot; by clearing the interrupt flag, `IF`, using the `CLI` instruction; or, </code><br />
<code>1. conditionally unset the flag by using the `PUSHF` instruction on the `EFLAGS` register to push its lower word (bits 15-0, including the offending bit 11 (`OF`)) onto the stack, reset the `OF` bit, then push that back onto the stack and pop it into EFLAGS with `POPF`.  Depending on variable register used, the assembler output would look similar to:</code></p>
<p></p>
<h4 id="a-floating-point-exception-example">A Floating Point Exception Example</h4>
<p>There was a long message thread on the Haskell-prime mailing list, &quot;realToFrac Issues,&quot; beginning with <a href="http://www.haskell.org/pipermail/haskell-prime/2006-February/000791.html">John Meacham's message</a> and ending with <a href="http://www.haskell.org/pipermail/haskell-prime/2006-March/000840.html">Simon Marlow's message</a>. The following code for converting a Float to a Double will <em>fail</em> to produce a floating point exception or NaN on x86 machines (recall that 0.0/0.0 is NaN <em>and</em> a definite FPU exception):</p>
<p>[in GHCi-6.6 on PowerPC, OS X]: </p>
<p>This bug is not due to the lack of FPU exceptions in Cmm but bears mention as the internal conversion performed in 'realToFrac' on 'Float's would benefit from FPU exceptions: with Haskell-support for FPU exceptions this realToFrac would be able to issue an exception for NaN, Infinity or rounding errors when converting a Float to a Double and vice versa. There is a related problem with rounding errors in the functions 'encodeFloat', 'decodeFloat', 'encodeDouble' and 'decodeDouble', see [wiki:ReplacingGMPNotes/TheCurrentGMPImplementation].</p>
<p>On 5 May 2008, Isaac Dupree asked</p>
<p><code> Is there documentation (e.g. on the GHC Commentary somewhere I can't</code><br />
<code> find) an explanation of what C-- &quot;kinds&quot; are or how they're useful/used? </code></p>
<p>Probably not. GHC Cmm is a sort of pidgin version of C-- 2.0, and true C-- kinds are explained in the <a href="http://www.cminusminus.org/code.html">C-- specification, section 5.1</a>.</p>
<p><code> When I was portabilizing that code area a while ago I had ignorantly </code><br />
<code> changed some of the uses of &quot;kind&quot; to &quot;hint&quot; for consistency (both names </code><br />
<code> had been being used for the same thing via type-synonym.) and because I </code><br />
<code> could guess how the code make sense if it was, informally, a hint about </code><br />
<code> what to do.</code></p>
<p>Hint was the word used originally, and several people (including reviewers) objected to it on the grounds that the 'hints' are actually mandatory to get the compiler to do what you want (e.g., pass arguments in floating-point registers). So we changed the name to 'kind'.</p>
<p>If you like dense, indigestible academic papers full of formalism, there's <a href="http://www.cs.tufts.edu/~nr/pubs/staged-abstract.html">one I'm quite proud of</a>. It explains in detail how kinds are useful for specifying and implementing procedure calling conventions, which is the use to which they are put within GHC.</p>
<p>Norman Ramsey</p>
<h3 id="note-to-reader">Note To Reader</h3>
<p>This page was written with more detail than usual since you may need to know how to work with Cmm as a programming language. Cmm is the basis for the future of GHC, Native Code Generation, and if you are interested in hacking Cmm at least this page might help reduce your learning curve. As a finer detail, if you read the [wiki:Commentary/Compiler/HscMain Compiler pipeline] wiki page or glanced at the diagram there you may have noticed that whether you are working backward from an `intermediate C` (Haskell-C &quot;HC&quot;, `.hc`) file or an Assembler file you get to Cmm before you get to the STG language, the Simplifier or anything else. In other words, for really low-level debugging you may have an easier time if you know what Cmm is about. Cmm also has opportunities for implementing small and easy hacks, such as little optimisations and implementing new Cmm Primitive Operations.</p>
<p>A portion of the [wiki:Commentary/Rts RTS] is written in Cmm: <a href="GhcFile(rts/Apply.cmm)" class="uri" title="wikilink">GhcFile(rts/Apply.cmm)</a>, <a href="GhcFile(rts/Exception.cmm)" class="uri" title="wikilink">GhcFile(rts/Exception.cmm)</a>, <a href="GhcFile(rts/HeapStackCheck.cmm)" class="uri" title="wikilink">GhcFile(rts/HeapStackCheck.cmm)</a>, <a href="GhcFile(rts/PrimOps.cmm)" class="uri" title="wikilink">GhcFile(rts/PrimOps.cmm)</a>, <a href="GhcFile(rts/StgMiscClosures.cmm)" class="uri" title="wikilink">GhcFile(rts/StgMiscClosures.cmm)</a>, <a href="GhcFile(rts/StgStartup.cmm)" class="uri" title="wikilink">GhcFile(rts/StgStartup.cmm)</a> and <a href="GhcFile(StgStdThunks.cmm)" class="uri" title="wikilink">GhcFile(StgStdThunks.cmm)</a>. (For notes related to `PrimOps.cmm` see the [wiki:Commentary/PrimOps PrimOps] page; for much of the rest, see the [wiki:Commentary/Rts/HaskellExecution HaskellExecution] page.) Cmm is optimised before GHC outputs either HC or Assembler. The C compiler (from HC, pretty printed by <a href="GhcFile(compiler/cmm/PprC.hs)" class="uri" title="wikilink">GhcFile(compiler/cmm/PprC.hs)</a>) and the [wiki:Commentary/Compiler/Backends/NCG Native Code Generator] (NCG) [wiki:Commentary/Compiler/Backends Backends] are closely tied to data representations and transformations performed in Cmm. In GHC, Cmm roughly performs a function similar to the intermediate <a href="http://gcc.gnu.org/onlinedocs/gccint/RTL.html">Register Transfer Language (RTL)</a> in GCC.</p>
<h1 id="table-of-contents">Table of Contents</h1>
<p><code> 1. [wiki:Commentary/Compiler/CmmType#AdditionsinCmm Additions in Cmm]</code><br />
<code> 1. [wiki:Commentary/Compiler/CmmType#CompilingCmmwithGHC Compiling Cmm with GHC]</code><br />
<code> 1. [wiki:Commentary/Compiler/CmmType#BasicCmm Basic Cmm]</code><br />
<code>  1. [wiki:Commentary/Compiler/CmmType#CodeBlocksinCmm Code Blocks in Cmm]</code><br />
<code>    * [wiki:Commentary/Compiler/CmmType#BasicBlocksandProcedures Basic Blocks and Procedures]</code><br />
<code>  1. [wiki:Commentary/Compiler/CmmType#VariablesRegistersandTypes Variables, Registers and Types]</code><br />
<code>    1. [wiki:Commentary/Compiler/CmmType#LocalRegisters Local Registers]</code><br />
<code>    1. [wiki:Commentary/Compiler/CmmType#GlobalRegistersandHints Global Registers and Hints]</code><br />
<code>    1. [wiki:Commentary/Compiler/CmmType#DeclarationandInitialisation Declaration and Initialisation]</code><br />
<code>    1. [wiki:Commentary/Compiler/CmmType#MemoryAccess Memory Access]</code><br />
<code>  1. [wiki:Commentary/Compiler/CmmType#LiteralsandLabels Literals and Labels]</code><br />
<code>    * [wiki:Commentary/Compiler/CmmType#Labels Labels]</code><br />
<code>  1. [wiki:Commentary/Compiler/CmmType#SectionsandDirectives Sections and Directives]</code><br />
<code>    * [wiki:Commentary/Compiler/CmmType#TargetDirective Target Directive]</code><br />
<code>  1. [wiki:Commentary/Compiler/CmmType#Expressions Expressions]</code><br />
<code>    * [wiki:Commentary/Compiler/CmmType#QuasioperatorSyntax Quasi-operator Syntax]</code><br />
<code>  1. [wiki:Commentary/Compiler/CmmType#StatementsandCalls Statements and Calls]</code><br />
<code>    * [wiki:Commentary/Compiler/CmmType#CmmCalls Cmm Calls]</code><br />
<code>  1. [wiki:Commentary/Compiler/CmmType#OperatorsandPrimitiveOperations Operators and Primitive Operations]</code><br />
<code>    1. [wiki:Commentary/Compiler/CmmType#Operators Operators]</code><br />
<code>    1. [wiki:Commentary/Compiler/CmmType#PrimitiveOperations Primitive Operations]</code><br />
<code> 1. [wiki:Commentary/Compiler/CmmType#CmmDesign:ObservationsandAreasforPotentialImprovement Cmm Design: Observations and Areas for Potential Improvement]</code></p>
<h1 id="the-cmm-language">The Cmm language</h1>
<p>`Cmm` is the GHC implementation of the `C--` language; it is also the extension of Cmm source code files: `.cmm` (see [wiki:Commentary/Rts/Cmm What the hell is a .cmm file?]). The GHC [wiki:Commentary/Compiler/CodeGen Code Generator] (`CodeGen`) compiles the STG program into `C--` code, represented by the `Cmm` data type. This data type follows the <a href="http://www.cminusminus.org/">definition of `C--`</a> pretty closely but there are some remarkable differences. For a discussion of the Cmm implementation noting most of those differences, see the [wiki:Commentary/Compiler/CmmType#BasicCmm Basic Cmm] section, below.</p>
<p><code>* </code><a href="GhcFile(compiler/cmm/Cmm.hs)" title="wikilink"><code>GhcFile(compiler/cmm/Cmm.hs)</code></a><code>: the main data type definition.</code><br />
<code>* </code><a href="GhcFile(compiler/cmm/MachOp.hs)" title="wikilink"><code>GhcFile(compiler/cmm/MachOp.hs)</code></a><code>: data types defining the machine operations (e.g. floating point divide) provided by `Cmm`.</code><br />
<code>* </code><a href="GhcFile(compiler/cmm/CLabel.hs)" title="wikilink"><code>GhcFile(compiler/cmm/CLabel.hs)</code></a><code>: data type for top-level `Cmm` labels.</code></p>
<p><code>* </code><a href="GhcFile(compiler/cmm/PprCmm.hs)" title="wikilink"><code>GhcFile(compiler/cmm/PprCmm.hs)</code></a><code>: pretty-printer for `Cmm`.</code><br />
<code>* </code><a href="GhcFile(compiler/cmm/CmmUtils.hs)" title="wikilink"><code>GhcFile(compiler/cmm/CmmUtils.hs)</code></a><code>: operations over `Cmm`</code></p>
<p><code>* </code><a href="GhcFile(compiler/cmm/CmmLint.hs)" title="wikilink"><code>GhcFile(compiler/cmm/CmmLint.hs)</code></a><code>: a consistency checker.</code><br />
<code>* </code><a href="GhcFile(compiler/cmm/CmmOpt.hs)" title="wikilink"><code>GhcFile(compiler/cmm/CmmOpt.hs)</code></a><code>: an optimiser for `Cmm`.</code></p>
<p><code>* </code><a href="GhcFile(compiler/cmm/CmmParse.y)" title="wikilink"><code>GhcFile(compiler/cmm/CmmParse.y)</code></a><code>, </code><a href="GhcFile(compiler/cmm/CmmLex.x)" title="wikilink"><code>GhcFile(compiler/cmm/CmmLex.x)</code></a><code>: parser and lexer for [wiki:Commentary/Rts/Cmm .cmm files].</code></p>
<p><code>* </code><a href="GhcFile(compiler/cmm/PprC.hs)" title="wikilink"><code>GhcFile(compiler/cmm/PprC.hs)</code></a><code>: pretty-print `Cmm` in C syntax, when compiling via C.</code></p>
<h2 id="additions-in-cmm">Additions in Cmm</h2>
<p>Although both Cmm and C-- allow foreign calls, the `.cmm` syntax includes the </p>
<p>The [R2] part is the (set of) register(s) that you need to save over the call.</p>
<p>Other additions to C-- are noted throughout the [wiki:Commentary/Compiler/CmmType#BasicCmm Basic Cmm] section, below.</p>
<h2 id="compiling-cmm-with-ghc">Compiling Cmm with GHC</h2>
<p>GHC is able to compile `.cmm` files with a minimum of user-effort. To compile `.cmm` files, simply invoke the main GHC driver but remember to:</p>
<p><code>* add the option `-dcmm-lint` if you have handwritten Cmm code;</code><br />
<code>* add appropriate includes, especially </code><a href="GhcFile(includes/Cmm.h)" title="wikilink"><code>GhcFile(includes/Cmm.h)</code></a><code> if you are using Cmm macros or GHC defines for certain types, such as `W_` for `bits32` or `bits64` (depending on the machine word size)--`Cmm.h` is in the `/includes` directory of every GHC distribution, i.e., `usr/local/lib/ghc-6.6/includes`; and,</code><br />
<code>* if you do include GHC header files, remember to pass the code through the C preprocessor by adding the `-cpp` option.</code></p>
<p>For additional fun, you may pass GHC the `-keep-s-file` option to keep the temporary assembler file in your compile directory. For example:  This will only work with very basic Cmm files. If you noticed that GHC currently provides no `-keep-cmm-file` option and `-keep-tmp-files` does not save a `.cmm` file and you are thinking about redirecting output from `-ddump-cmm`, beware. The output from `-ddump-cmm` contains equal-lines and dash-lines separating Cmm Blocks and Basic Blocks; these are unparseable. The parser also cannot handle `const` sections. For example, the parser will fail on the first `0` or alphabetic token after `const`:  Although GHC's Cmm pretty printer outputs C-- standard parenthetical list of arguments after procedure names, i.e., `()`, the Cmm parser will fail at the `(` token. For example:  The Cmm procedure names in <a href="GhcFile(rts/PrimOps.cmm)" class="uri" title="wikilink">GhcFile(rts/PrimOps.cmm)</a> are not followed by a (possibly empty) parenthetical list of arguments; all their arguments are Global (STG) Registers, anyway, see [wiki:Commentary/Compiler/CmmType#VariablesRegistersandTypes Variables, Registers and Types], below. Don't be confused by the procedure definitions in other handwritten `.cmm` files in the RTS, such as <a href="GhcFile(rts/Apply.cmm)" class="uri" title="wikilink">GhcFile(rts/Apply.cmm)</a>: all-uppercase procedure invocations are special reserved tokens in <a href="GhcFile(compiler/cmm/CmmLex.x)" class="uri" title="wikilink">GhcFile(compiler/cmm/CmmLex.x)</a> and <a href="GhcFile(compiler/cmm/CmmParse.y)" class="uri" title="wikilink">GhcFile(compiler/cmm/CmmParse.y)</a>. For example, `INFO_TABLE` is parsed as one of the tokens in the Alex `info` predicate: </p>
<p>GHC's Cmm parser also cannot parse nested code blocks. For example:  The C-- specification example in section 4.6.2, &quot;Procedures as section contents&quot; also will not parse in Cmm:  Note that if `p (bits32 i) { ... }` were written as a Cmm-parseable procedure, as `p { ... }`, the parse error would occur at the closing curly bracket for the `section &quot;data&quot; { ... p { ... } }`&lt;- here.</p>
<h2 id="basic-cmm">Basic Cmm</h2>
<p>FIXME: The links in this section are dead. But the files can be found here: <a href="http://www.cs.tufts.edu/~nr/c--/index.html">1</a>. Relevant discussion about the documentations of C--: <a href="https://mail.haskell.org/pipermail/ghc-devs/2014-September/006301.html">2</a></p>
<p>Cmm is a high level assembler with a syntax style similar to C. This section describes Cmm by working up from assembler--the C-- papers and specification work down from C. At the least, you should know what a &quot;high level&quot; assembler is, see <a href="http://webster.cs.ucr.edu/AsmTools/HLA/HLADoc/HLARef/HLARef3.html#1035157">&quot;What is a High Level Assembler?&quot;</a>. Cmm is different than other high level assembler languages in that it was designed to be a semi-portable intermediate language for compilers; most other high level assemblers are designed to make the tedium of assembly language more convenient and intelligible to humans. If you are completely new to C--, I highly recommend these papers listed on the <a href="http://cminusminus.org/papers.html">C-- Papers</a> page:</p>
<p><code>* </code><a href="http://cminusminus.org/abstracts/ppdp.html"><code>C--:</code> <code>A</code> <code>Portable</code> <code>Assembly</code> <code>Language</code> <code>that</code> <code>Supports</code> <code>Garbage</code> <code>Collection</code> <code>(1999)</code></a><code> (Paper page with Abstract)</code><br />
<code>* </code><a href="http://cminusminus.org/abstracts/pal-ifl.html"><code>C--:</code> <code>A</code> <code>Portable</code> <code>Assembly</code> <code>Language</code> <code>(1997)</code></a><code> (Paper page with Abstract)</code><br />
<code>* </code><a href="http://cminusminus.org/abstracts/c--pldi-00.html"><code>A</code> <code>Single</code> <code>Intermediate</code> <code>Language</code> <code>That</code> <code>Supports</code> <code>Multiple</code> <code>Implementations</code> <code>of</code> <code>Exceptions</code> <code>(2000)</code></a><code> (Paper page with Abstract)</code><br />
<code>* </code><a href="http://cminusminus.org/extern/man2.pdf"><code>The</code> <code>C--</code> <code>Language</code> <code>Specification</code> <code>Version</code> <code>2.0</code> <code>(CVS</code> <code>Revision</code> <code>1.128,</code> <code>23</code> <code>February</code> <code>2005)</code></a><code> (PDF)</code></p>
<p>Cmm is not a stand alone C-- compiler; it is an implementation of C-- embedded in the GHC compiler. One difference between Cmm and a C-- compiler like <a href="http://cminusminus.org/code.html">Quick C--</a> is this: Cmm uses the C preprocessor (cpp). Cpp lets Cmm <em>integrate</em> with C code, especially the C header defines in <a href="GhcFile(includes)" class="uri" title="wikilink">GhcFile(includes)</a>, and among many other consequences it makes the C-- `import` and `export` statements irrelevant; in fact, according to <a href="GhcFile(compiler/cmm/CmmParse.y)" class="uri" title="wikilink">GhcFile(compiler/cmm/CmmParse.y)</a> they are ignored. The most significant action taken by the Cmm modules in the Compiler is to optimise Cmm, through <a href="GhcFile(compiler/cmm/CmmOpt.hs)" class="uri" title="wikilink">GhcFile(compiler/cmm/CmmOpt.hs)</a>. The Cmm Optimiser generally runs a few simplification passes over primitive Cmm operations, inlines simple Cmm expressions that do not contain global registers (these would be left to one of the [wiki:Commentary/Compiler/Backends Backends], which currently cannot handle inlines with global registers) and performs a simple loop optimisation.</p>
<h3 id="code-blocks-in-cmm">Code Blocks in Cmm</h3>
<p>The Haskell representation of Cmm separates contiguous code into:</p>
<p><code>* </code><em><code>modules</code></em><code> (compilation units; a `.cmm` file); and</code><br />
<code>* </code><em><code>basic</code> <code>blocks</code></em></p>
<p>Cmm modules contain static data elements (see [wiki:Commentary/Compiler/CmmType#LiteralsandLabels Literals and Labels]) and [wiki:Commentary/Compiler/CmmType#BasicBlocks:Procedures Basic Blocks], collected together in `Cmm`, a type synonym for `GenCmm`, defined in <a href="GhcFile(compiler/cmm/Cmm.hs)" class="uri" title="wikilink">GhcFile(compiler/cmm/Cmm.hs)</a>:  `CmmStmt` is described in [wiki:Commentary/Compiler/CmmType#StatementsandCalls Statements and Calls];<a href="BR" class="uri" title="wikilink">BR</a> `Section` is described in [wiki:Commentary/Compiler/CmmType#SectionsandDirectives Sections and Directives];<a href="BR" class="uri" title="wikilink">BR</a> the static data in `[d]` is [`CmmStatic`] from the type synonym `Cmm`;<a href="BR" class="uri" title="wikilink">BR</a> `CmmStatic` is described in [wiki:Commentary/Compiler/CmmType#LiteralsandLabels Literals and Labels].</p>
<h4 id="basic-blocks-and-procedures">Basic Blocks and Procedures</h4>
<p>Cmm procedures are represented by the first constructor in `GenCmmTop d i`:  For a description of Cmm labels and the `CLabel` data type, see the subsection [wiki:Commentary/Compiler/CmmType#LiteralsandLabels Literals and Labels], below.</p>
<p>Cmm Basic Blocks are labeled blocks of Cmm code ending in an explicit jump. Sections (see [wiki:Commentary/Compiler/CmmType#SectionsandDirectives Sections and Directives]) have no jumps--in Cmm, Sections cannot contain nested Procedures (see, e.g., [wiki:Commentary/Compiler/CmmType#CompilingCmmwithGHC Compiling Cmm with GHC]). Basic Blocks encapsulate parts of Procedures. The data type `GenBasicBlock` and the type synonym `CmmBasicBlock` encapsulate Basic Blocks; they are defined in <a href="GhcFile(compiler/cmm/Cmm.hs)" class="uri" title="wikilink">GhcFile(compiler/cmm/Cmm.hs)</a>:  The `BlockId` data type simply carries a `Unique` with each Basic Block. For descriptions of `Unique`, see</p>
<p><code>* the [wiki:Commentary/Compiler/Renamer Renamer] page;</code><br />
<code>* the [wiki:Commentary/Compiler/WiredIn#Knownkeythings Known Key Things] section of the [wiki:Commentary/Compiler/WiredIn Wired-in and Known Key Things] page; and, </code><br />
<code>* the [wiki:Commentary/Compiler/EntityTypes#Typevariablesandtermvariables Type variables and term variables] section of the [wiki:Commentary/Compiler/EntityTypes Entity Types] page.</code></p>
<h3 id="variables-registers-and-types">Variables, Registers and Types</h3>
<p>Like other high level assembly languages, all variables in C-- are machine registers, separated into different types according to bit length (8, 16, 32, 64, 80, 128) and register type (integral or floating point). The C-- standard specifies little more type information about a register than its bit length: there are no distinguishing types for signed or unsigned integrals, or for &quot;pointers&quot; (registers holding a memory address). A C-- standard compiler supports additional information on the type of a register value through compiler <em>hints</em>. In a foreign call, a `&quot;signed&quot; bits8` would be sign-extended and may be passed as a 32-bit value. Cmm diverges from the C-- specification on this point somewhat (see below). C-- and Cmm do not represent special registers, such as a Condition Register (`CR`) or floating point unit (FPU) status and control register (`FPSCR` on the PowerPC, `MXCSR` on Intel x86 processors), as these are a matter for the [wiki:Commentary/Compiler/Backends Backends].</p>
<p>C-- and Cmm hide the actual number of registers available on a particular machine by assuming an &quot;infinite&quot; supply of registers. A backend, such as the NCG or C compiler on GHC, will later optimise the number of registers used and assign the Cmm variables to actual machine registers; the NCG temporarily stores any overflow in a small memory stack called the <em>spill stack</em>, while the C compiler relies on C's own runtime system. Haskell handles Cmm registers with three data types: `LocalReg`, `GlobalReg` and `CmmReg`. `LocalReg`s and `GlobalRegs` are collected together in a single `Cmm` data type: </p>
<h4 id="local-registers">Local Registers</h4>
<p>Local Registers exist within the scope of a Procedure:  For a list of references with information on `Unique`, see the [wiki:Commentary/Compiler/CmmType#BasicBlocksandProcedures Basic Blocks and Procedures] section, above.</p>
<p>A `MachRep`, the type of a machine register, is defined in <a href="GhcFile(compiler/cmm/MachOp.hs)" class="uri" title="wikilink">GhcFile(compiler/cmm/MachOp.hs)</a>:  There is currently no register for floating point vectors, such as `F128`. The types of Cmm variables are defined in the Happy parser file <a href="GhcFile(compiler/cmm/CmmParse.y)" class="uri" title="wikilink">GhcFile(compiler/cmm/CmmParse.y)</a> and the Alex lexer file <a href="GhcFile(compiler/cmm/CmmLex.x)" class="uri" title="wikilink">GhcFile(compiler/cmm/CmmLex.x)</a>. (Happy and Alex will compile these into `CmmParse.hs` and `CmmLex.hs`, respectively.) Cmm recognises the following `C--` types as parseable tokens, listed next to their corresponding s in <a href="GhcFile(includes/Cmm.h)" class="uri" title="wikilink">GhcFile(includes/Cmm.h)</a> and their STG types: || <strong>Cmm Token</strong> || <strong>Cmm.h #define</strong> || <strong>STG type</strong> || || `bits8` || `I8` || `StgChar` or `StgWord8` || || `bits16` || `I16` || `StgWord16` || || `bits32` || `I32`, `CInt`, `CLong` || `StgWord32`; `StgWord` (depending on architecture) || || `bits64` || `I64`, `CInt`, `CLong`, `L_` || `StgWord64`; `StgWord` (depending on architecture) || || `float32` || `F_` || `StgFloat` || || `float64` || `D_` || `StgDouble` ||</p>
<p><a href="GhcFile(includes/Cmm.h)" class="uri" title="wikilink">GhcFile(includes/Cmm.h)</a> also defines `L_` for `bits64`, so `F_`, `D_` and `L_` correspond to the `GlobalReg` data type constructors `FloatReg`, `DoubleReg` and `LongReg`. Note that although GHC may generate other register types supported by the `MachRep` data type, such as `I128`, they are not parseable tokens. That is, they are internal to GHC. The special defines `CInt` and `CLong` are used for compatibility with C on the target architecture, typically for making `foreign &quot;C&quot;` calls.</p>
<p><strong>Note</strong>: Even Cmm types that are not explicit variables (Cmm literals and results of Cmm expressions) have implicit `MachRep`s, in the same way as you would use temporary registers to hold labelled constants or intermediate values in assembler functions. See:</p>
<p><code>* [wiki:Commentary/Compiler/CmmType#LiteralsandLabels Literals and Labels] for information related to the Cmm literals `CmmInt` and `CmmFloat`; and,</code><br />
<code>* [wiki:Commentary/Compiler/CmmType#Expressions Expressions], regarding the `cmmExprRep` function defined in </code><a href="GhcFile(compiler/cmm/Cmm.hs)" title="wikilink"><code>GhcFile(compiler/cmm/Cmm.hs)</code></a><code>.</code></p>
<h4 id="global-registers-and-hints">Global Registers and Hints</h4>
<p>These are universal both to a Cmm module and to the whole compiled program. Variables are global if they are declared at the top-level of a compilation unit (outside any procedure). Global Variables are marked as external symbols with the `.globl` assembler directive. In Cmm, global registers are used for special STG registers and specific registers for passing arguments and returning values. The Haskell representation of Global Variables (Registers) is the `GlobalReg` data type, defined in <a href="GhcFile(compiler/cmm/Cmm.hs)" class="uri" title="wikilink">GhcFile(compiler/cmm/Cmm.hs)</a>:  For a description of the `Hp` and `Sp` <em>virtual registers</em>, see [wiki:Commentary/Rts/HaskellExecution The Haskell Execution Model] page. General `GlobalReg`s are clearly visible in Cmm code according to the following syntax defined in <a href="GhcFile(compiler/cmm/CmmLex.x)" class="uri" title="wikilink">GhcFile(compiler/cmm/CmmLex.x)</a>: || <strong>`GlobalReg` Constructor</strong> || <strong>Syntax</strong> || <strong>Examples</strong> || || `VanillaReg Int` || `R ++ Int` || `R1`, `R10` || || `FloatReg Int` || `F ++ Int` || `F1`, `F10` || || `DoubleReg Int` || `D ++ Int` || `D1`, `D10` || || `LongReg Int` || `L ++ Int` || `L1`, `L10` || General `GlobalRegs` numbers are decimal integers, see the `parseInteger` function in <a href="GhcFile(compiler/utils/StringBuffer.lhs)" class="uri" title="wikilink">GhcFile(compiler/utils/StringBuffer.lhs)</a>. The remainder of the `GlobalReg` constructors, from `Sp` to `BaseReg` are lexical tokens exactly like their name in the data type; `PicBaseReg` does not have a lexical token since it is used only inside the NCG. See [wiki:Commentary/PositionIndependentCode Position Independent Code and Dynamic Linking] for an in-depth description of PIC implementations in the NCG.</p>
<p>`GlobalRegs` are a very special case in Cmm, partly because they must conform to the STG register convention and the target C calling convention. That the Cmm parser recognises `R1` and `F3` as `GlobalRegs` is only the first step. The main files to look at for more information on this delicate topic are:</p>
<p><code>* </code><a href="GhcFile(compiler/codeGen/CgCallConv.hs)" title="wikilink"><code>GhcFile(compiler/codeGen/CgCallConv.hs)</code></a><code> (the section on &quot;Register assignment&quot;)</code><br />
<code>* </code><a href="GhcFile(includes/stg/Regs.h)" title="wikilink"><code>GhcFile(includes/stg/Regs.h)</code></a><code> (defining STG registers)</code><br />
<code>* </code><a href="GhcFile(includes/stg/MachRegs.h)" title="wikilink"><code>GhcFile(includes/stg/MachRegs.h)</code></a><code> (target-specific mapping of machine registers for </code><em><code>registerised</code></em><code> builds of GHC)</code><br />
<code>* </code><a href="GhcFile(rts/PrimOps.cmm)" title="wikilink"><code>GhcFile(rts/PrimOps.cmm)</code></a><code> (examples of `GlobalReg` register usage for out-of-line primops)</code></p>
<p>All arguments to out-of-line !PrimOps in <a href="GhcFile(rts/PrimOps.cmm)" class="uri" title="wikilink">GhcFile(rts/PrimOps.cmm)</a> are STG registers.</p>
<p>Cmm recognises all C-- syntax with regard to <em>hints</em>. For example:  Hints are represented in Haskell as `MachHint`s, defined near `MachRep` in <a href="GhcFile(compiler/cmm/MachOp.hs)" class="uri" title="wikilink">GhcFile(compiler/cmm/MachOp.hs)</a>: </p>
<p>Although the C-- specification does not allow the C-- type system to statically distinguish between floats, signed ints, unsigned ints or pointers, Cmm does. Cmm `MachRep`s carry the float or int kind of a variable, either within a local block or in a global register. `GlobalReg` includes separate constructors for `Vanilla`, `Float`, `Double` and `Long`. Cmm still does not distinguish between signed ints, unsigned ints and pointers (addresses) at the register level, as these are given <em>hint</em> pseudo-types or their real type is determined as they run through primitive operations. `MachHint`s still follow the C-- specification and carry kind information as an aide to the backend optimisers.</p>
<p>Global Registers in Cmm currently have a problem with inlining: because neither <a href="GhcFile(compiler/cmm/PprC.hs)" class="uri" title="wikilink">GhcFile(compiler/cmm/PprC.hs)</a> nor the NCG are able to keep Global Registers from clashing with C argument passing registers, Cmm expressions that contain Global Registers cannot be inlined into an argument position of a foreign call. For more thorough notes on inlining, see the comments in <a href="GhcFile(compiler/cmm/CmmOpt.hs)" class="uri" title="wikilink">GhcFile(compiler/cmm/CmmOpt.hs)</a>.</p>
<h4 id="declaration-and-initialisation">Declaration and Initialisation</h4>
<p>Cmm variables hold the same values registers do in assembly languages but may be declared in a similar way to variables in C. As in C--, they may actually be declared anywhere in the scope for which they are visible (a block or file)--for Cmm, this is done by the `loopDecls` function in <a href="GhcFile(compiler/cmm/CmmParse.y)" class="uri" title="wikilink">GhcFile(compiler/cmm/CmmParse.y)</a>. In <a href="GhcFile(compiler/rts/PrimOps.cmm)" class="uri" title="wikilink">GhcFile(compiler/rts/PrimOps.cmm)</a>, you will see Cmm variable declarations like this one:  Remember that Cmm code is run through the C preprocessor. `W_` will be transformed into `bits32`, `bits64` or whatever is the `bits`<em>size</em> of the machine word, as defined in <a href="GhcFile(includes/Cmm.h)" class="uri" title="wikilink">GhcFile(includes/Cmm.h)</a>. In Haskell code, you may use the <a href="GhcFile(compiler/cmm/MachOp.hs)" class="uri" title="wikilink">GhcFile(compiler/cmm/MachOp.hs)</a> functions `wordRep` and `halfWordRep` to dynamically determine the machine word size. For a description of word sizes in GHC, see the [wiki:Commentary/Rts/Word Word] page.</p>
<p>The variables `w`, `code` and `val` should be real registers. With the above declaration the variables are uninitialised. Initialisation requires an assignment <em>statement</em>. Cmm does not recognise C-- &quot;`{` <em>literal</em>, ... `}`&quot; initialisation syntax, such as `bits32{10}` or `bits32[3] {1, 2, 3}`. Cmm does recognise initialisation with a literal:  The typical method seems to be to declare variables and then initialise them just before their first use. (Remember that you may declare a variable anywhere in a procedure and use it in an expression before it is initialised but you must initialise it before using it anywhere else--statements, for example.)</p>
<h4 id="memory-access">Memory Access</h4>
<p>If the value in `w` were the address of a memory location, you would obtain the value at that location similar to Intel assembler syntax. In Cmm, you would write:  compare the above statement to indirect addressing in Intel assembler: </p>
<p>The code between the brackets (`w` in `[w]`, above) is an <em>expression</em>. See the [wiki:Commentary/Compiler/CmmType#Expressions Expressions] section. For now, consider the similarity between the Cmm-version of indexed memory addressing syntax, here:  and the corresponding Intel assembler indexed memory addressing syntax, here:  You will generally not see this type of syntax in either handwritten or GHC-produced Cmm code, although it is allowed; it simply shows up in macros. C-- also allows the `*` (multiplication) operator in addressing expressions, for an approximation of <em>scaled</em> addressing (`[base * (2^n)]`); for example, `n` (the &quot;scale&quot;) must be `0`, `1`, `2` or `4`. C-- itself would not enforce alignment or limits on the scale. Cmm, however, could not process it: since the NCG currently outputs GNU Assembler syntax, the Cmm or NCG optimisers would have to reduce `n` in (`* n`) to an absolute address or relative offset, or to an expression using only `+` or `-`. This is not currently the case and would be difficult to implement where one of the operands to the `*` is a relative address not visible in the code block. <a href="GhcFile(includes/Cmm.h)" class="uri" title="wikilink">GhcFile(includes/Cmm.h)</a> defines macros to perform the calculation with a constant. For example:  is used in:  The function `cmmMachOpFold` in <a href="GhcFile(compiler/cmm/CmmOpt.hs)" class="uri" title="wikilink">GhcFile(compiler/cmm/CmmOpt.hs)</a> will reduce the resulting expression `Sp + (n * SIZEOF_W)` to `Sp + N`, where `N` is a constant. A very large number of macros for accessing STG struct fields and the like are produced by <a href="GhcFile(includes/mkDerivedConstants.c)" class="uri" title="wikilink">GhcFile(includes/mkDerivedConstants.c)</a> and output into the file `includes/DerivedConstants.h` when GHC is compiled.</p>
<p>Of course, all this also holds true for the reverse (when an assignment is made to a memory address):  or, for an example of a macro from `DerivedConstants.h`:  this will be transformed to: </p>
<h3 id="literals-and-labels">Literals and Labels</h3>
<p>Cmm literals are exactly like C-- literals, including the Haskell-style type syntax, for example: `0x00000001::bits32`. Cmm literals may be used for initialisation by assignment or in expressions. The `CmmLit` and `CmmStatic` data types, defined in <a href="GhcFile(compiler/cmm/Cmm.hs)" class="uri" title="wikilink">GhcFile(compiler/cmm/Cmm.hs)</a> together represent Cmm literals, static information and Cmm labels:  Note how the `CmmLit` constructor `CmmInt Integer MachRep` contains sign information in the `Integer`, the representation of the literal itself: this conforms to the C-- specification, where integral literals contain sign information. For an example of a function using `CmmInt` sign information, see `cmmMachOpFold` in <a href="GhcFile(compiler/cmm/CmmOpt.hs)" class="uri" title="wikilink">GhcFile(compiler/cmm/CmmOpt.hs)</a>, where sign-operations are performed on the `Integer`.</p>
<p>The `MachRep` of a literal, such as `CmmInt Integer MachRep` or `CmmFloat Rational MachRep` may not always require the size defined by `MachRep`. The NCG optimiser, <a href="GhcFile(compiler/nativeGen/MachCodeGen.hs)" class="uri" title="wikilink">GhcFile(compiler/nativeGen/MachCodeGen.hs)</a>, will test a literal such as `1::bits32` (in Haskell, `CmmInt (1::Integer) I32`) for whether it would fit into the bit-size of Assembler instruction literals on that particular architecture with a function defined in <a href="GhcFile(compiler/nativeGen/MachRegs.lhs)" class="uri" title="wikilink">GhcFile(compiler/nativeGen/MachRegs.lhs)</a>, such as `fits16Bits` on the PPC. If the Integer literal fits, the function `makeImmediate` will truncate it to the specified size if possible and store it in a NCG data type, `Imm`, specifically `Maybe Imm`. (These are also defined in <a href="GhcFile(compiler/nativeGen/MachRegs.lhs)" class="uri" title="wikilink">GhcFile(compiler/nativeGen/MachRegs.lhs)</a>.)</p>
<p>The Haskell representation of Cmm separates unchangeable Cmm values into a separate data type, `CmmStatic`, defined in <a href="GhcFile(compiler/cmm/Cmm.hs)" class="uri" title="wikilink">GhcFile(compiler/cmm/Cmm.hs)</a>:  Note the `CmmAlign` constructor: this maps to the assembler directive `.align N` to set alignment for a data item (hopefully one you remembered to label). This is the same as the `align` directive noted in Section 4.5 of the <a href="http://cminusminus.org/extern/man2.pdf">C-- specification (PDF)</a>. In the current implementation of Cmm the `align` directive seems superfluous because <a href="GhcFile(compiler/nativeGen/PprMach.hs)" class="uri" title="wikilink">GhcFile(compiler/nativeGen/PprMach.hs)</a> translates `Section`s to assembler with alignment directives corresponding to the target architecture (see [wiki:Commentary/Compiler/CmmType#SectionsandDirectives Sections and Directives], below).</p>
<h4 id="labels">Labels</h4>
<p>Remember that C--/Cmm names consist of a string where the first character is:</p>
<p><code>* ASCII alphabetic (uppercase or lowercase);</code><br />
<code>* an underscore:    `_` ;</code><br />
<code>* a period:         `.` ;</code><br />
<code>* a dollar sign:    `$` ; or,</code><br />
<code>* a commercial at:  `@` .</code></p>
<p>Cmm labels conform to the C-- specification. C--/Cmm uses labels to refer to memory locations in code--if you use a data directive but do not give it a label, you will have no means of referring to the memory! For `GlobalReg`s (transformed to assembler `.globl`), labels serve as both symbols and labels (in the assembler meaning of the terms). The Haskell representation of Cmm Labels is contained in the `CmmLit` data type, see [wiki:Commentary/Compiler/CmmType#Literals Literals] section, above. Note how Cmm Labels are `CLabel`s with address information. The `Clabel` data type, defined in <a href="GhcFile(compiler/cmm/CLabel.hs)" class="uri" title="wikilink">GhcFile(compiler/cmm/CLabel.hs)</a>, is used throughout the Compiler for symbol information in binary files. Here it is: </p>
<h3 id="sections-and-directives">Sections and Directives</h3>
<p>The Haskell representation of Cmm Section directives, in <a href="GhcFile(compiler/cmm/Cmm.hs)" class="uri" title="wikilink">GhcFile(compiler/cmm/Cmm.hs)</a> as the first part of the &quot;Static Data&quot; section, is:  Cmm supports the following directives, corresponding to the assembler directives pretty-printed by the `pprSectionHeader` function in <a href="GhcFile(compiler/nativeGen/PprMach.hs)" class="uri" title="wikilink">GhcFile(compiler/nativeGen/PprMach.hs)</a>: || <strong>`Section` Constructor</strong> || <strong>Cmm section directive</strong> || <strong>Assembler Directive</strong> || || `Text` || `&quot;text&quot;` || `.text` || || `Data` || `&quot;data&quot;` || `.data` || || `ReadOnlyData` || `&quot;rodata&quot;` || `.rodata`<a href="BR" class="uri" title="wikilink">BR</a>(generally; varies by arch,OS) || || `RelocatableReadOnlyData` || no parse (GHC internal), output: `&quot;relreadonly&quot;` || `.const_data`<a href="BR" class="uri" title="wikilink">BR</a>`.section .rodata`<a href="BR" class="uri" title="wikilink">BR</a>(generally; varies by arch,OS) || || `UninitialisedData` || `&quot;bss&quot;`, output: `&quot;uninitialised&quot;` || `.bss` || || `ReadOnlyData16` || no parse (GHC internal), output: none || `.const`<a href="BR" class="uri" title="wikilink">BR</a>`.section .rodata`<a href="BR" class="uri" title="wikilink">BR</a>(generally; on x86_64:<a href="BR" class="uri" title="wikilink">BR</a>`.section .rodata.cst16`) || You probably already noticed I omitted the alignment directives (for clarity). For example, `pprSectionHeader` would pretty-print `ReadOnlyData` as  on an i386 with the Darwin OS. If you are really on the ball you might have noticed that the `PprMach.hs` output of &quot;`.section .data`&quot; and the like is really playing it safe since on most OS's, using GNU Assembler, the `.data` directive is equivalent to `.section __DATA .data`, or simply `.section .data`. Note that `OtherSection String` is not a catch-all for the Cmm parser. If you wrote:  The Cmm parser (through GHC) would panic, complaining, &quot;`PprMach.pprSectionHeader: unknown section`.&quot;</p>
<p>While the C-- specification allows a bare `data` keyword directive, Cmm does not: </p>
<p>Cmm does not recognise the C-- &quot;`stack`&quot; declaration for allocating memory on the system stack.</p>
<p>GHC-produced Cmm code is replete with `data` sections, each of which is stored in `.data` section of the binary code. This contributes significantly to the large binary size for GHC-compiled code.</p>
<p><code> ==== Target Directive ====</code></p>
<p>The C-- specification defines a special `target` directive, in section 4.7. The `target` directive is essentially a code block defining the properties of the target architecture:  This is essentially a custom-coded version of the GNU Assembler (`as`) `.machine` directive, which is essentially the same as passing the `-arch [cpu_type]` option to `as`.</p>
<p>Cmm does not support the `target` directive. This is partly due GHC generally lacking cross-compiler capabilities. Should GHC move toward adding cross-compilation capabilities, the `target` might not be a bad thing to add. Target architecture parameters are currently handled through the [wiki:Attic/Building/BuildSystem Build System], which partly sets such architectural parameters through <a href="GhcFile(includes/mkDerivedConstants.c)" class="uri" title="wikilink">GhcFile(includes/mkDerivedConstants.c)</a> and <a href="GhcFile(includes/ghcconfig.h)" class="uri" title="wikilink">GhcFile(includes/ghcconfig.h)</a>.</p>
<h3 id="expressions">Expressions</h3>
<p>Expressions in Cmm follow the C-- specification. They have:</p>
<p><code>* no side-effects; and,</code><br />
<code>* one result: </code><br />
<code>  * a </code><em><code>k</code></em><code>-bit value</code><a href="BR" title="wikilink"><code>BR</code></a><code>--these expressions map to the `MachOp` data type, defined in </code><a href="GhcFile(compiler/cmm/MachOp.hs)" title="wikilink"><code>GhcFile(compiler/cmm/MachOp.hs)</code></a><code>, see [wiki:Commentary/Compiler/CmmType#OperatorsandPrimitiveOperations Operators and Primitive Operations], the </code><em><code>k</code></em><code>-bit value may be:</code><br />
<code>    * a Cmm literal (`CmmLit`); or,</code><br />
<code>    * a Cmm variable (`CmmReg`, see [wiki:Commentary/Compiler/CmmType#VariablesRegistersandTypes Variables, Registers and Types]);</code><a href="BR" title="wikilink"><code>BRor</code></a><code>, </code><br />
<code>  * a boolean condition.</code></p>
<p>Cmm expressions may include</p>
<p><code>* a literal or a name (`CmmLit` contains both, see [wiki:Commentary/Compiler/CmmType#LiteralsandLabels Literals and Labels], above);</code><br />
<code>* a memory reference (`CmmLoad` and `CmmReg`, see [wiki:Commentary/Compiler/CmmType#MemoryAccess Memory Access], above);</code><br />
<code>* an operator (a `MachOp`, in `CmmMachOp`, below); or,</code><br />
<code>* another expression (a `[CmmExpr]`, in `CmmMachOp`, below).</code></p>
<p>These are all included as constructors in the `CmmExpr` data type, defined in <a href="GhcFile(compiler/cmm/Cmm.hs)" class="uri" title="wikilink">GhcFile(compiler/cmm/Cmm.hs)</a>:  Note that `CmmRegOff reg i` is only shorthand for a specific `CmmMachOp` application:  The function `cmmRegRep` is described below. Note: the original comment following `CmmExpr` in <a href="GhcFile(compiler/cmm/Cmm.hs)" class="uri" title="wikilink">GhcFile(compiler/cmm/Cmm.hs)</a> is erroneous (cf., `mangleIndexTree` in <a href="GhcFile(compiler/nativeGen/MachCodeGen.hs)" class="uri" title="wikilink">GhcFile(compiler/nativeGen/MachCodeGen.hs)</a>) but makes the same point described here. The offset, `(CmmLit (CmmInt i rep))`, is a literal (`CmmLit`), not a name (`CLabel`). A `CmmExpr` for an offset must be reducible to a `CmmInt` <em>in Haskell</em>; in other words, offsets in Cmm expressions may not be external symbols whose addresses are not resolvable in the current context.</p>
<p>Boolean comparisons are not boolean conditions. Boolean comparisons involve relational operators, such as `&gt;`, `&lt;` and `==`, and map to `MachOp`s that are converted to comparison followed by branch instructions. For example, `&lt;` would map to `MO_S_Lt` for signed operands, <a href="GhcFile(compiler/nativeGen/MachCodeGen.hs)" class="uri" title="wikilink">GhcFile(compiler/nativeGen/MachCodeGen.hs)</a> would transform `MO_S_Lt` into the `LTT` constructor of the `Cond` union data type defined in <a href="GhcFile(compiler/nativeGen/MachInstrs.hs)" class="uri" title="wikilink">GhcFile(compiler/nativeGen/MachInstrs.hs)</a> and <a href="GhcFile(compiler/nativeGen/PprMach.hs)" class="uri" title="wikilink">GhcFile(compiler/nativeGen/PprMach.hs)</a> would transform `LTT` to the distinguishing comparison type for an assembler comparison instruction. You already know that the result of a comparison instruction is actually a change in the state of the Condition Register (CR), so Cmm boolean expressions do have a kind of side-effect but that is to be expected. In fact, it is necessary since at the least a conditional expression becomes two assembler instructions, in PPC Assembler:  This condition mapping does have an unfortunate consequence: conditional expressions do not fold into single instructions. In Cmm, as in C--, expressions with relational operators may evaluate to an integral (`0`, nonzero) instead of evaluating to a boolean type. For certain cases, such as an arithmetic operation immediately followed by a comparison, extended mnemonics such as `addi.` might eliminate the comparison instruction. See [wiki:Commentary/Compiler/CmmType#CmmDesignObservationsandAreasforPotentialImprovement Cmm Design: Observations and Areas for Potential Improvement] for more discussion and potential solutions to this situation.</p>
<p>Boolean conditions include: `&amp;&amp;`, `||`, `!` and parenthetical combinations of boolean conditions. The `if expr { }` and `if expr { } else { }` statements contain boolean conditions. The C-- type produced by conditional expressions is `bool`, in Cmm, type `BoolExpr` in <a href="GhcFile(compiler/cmm/CmmParse.y)" class="uri" title="wikilink">GhcFile(compiler/cmm/CmmParse.y)</a>:  The type `BoolExpr` maps to the `CmmCondBranch` or `CmmBranch` constructors of type `CmmStmt`, defined in <a href="GhcFile(compiler/cmm/Cmm.hs)" class="uri" title="wikilink">GhcFile(compiler/cmm/Cmm.hs)</a>, see [wiki:Commentary/Compiler/CmmType#StatementsandCalls Statements and Calls].</p>
<p>The `CmmExpr` constructor `CmmMachOp MachOp [CmmExpr]` is the core of every operator-based expression; the key here is `MachOp`, which in turn depends on the type of `MachRep` for each operand. See [wiki:Commentary/Compiler/CmmType#FundamentalandPrimitiveOperators Fundamental and PrimitiveOperators]. In order to process `CmmExpr`s, the data type comes with a deconstructor function to obtain the relevant `MachRep`s, defined in <a href="GhcFile(compiler/cmm/Cmm.hs)" class="uri" title="wikilink">GhcFile(compiler/cmm/Cmm.hs)</a>:  The deconstructors `cmmLitRep` and `cmmRegRep` (with its supporting deconstructor `localRegRep`) are also defined in <a href="GhcFile(compiler/cmm/Cmm.hs)" class="uri" title="wikilink">GhcFile(compiler/cmm/Cmm.hs)</a>.</p>
<p>In PPC Assembler you might add two 32-bit integrals by:  while in Cmm you might write:  Remember that the assignment operator, `=`, is a statement since it has the &quot;side effect&quot; of modifying the value in `res`. The `+` expression in the above statement, for a 32-bit architecture, would be represented in Haskell as:  The `expr` production rule in the Cmm Parser <a href="GhcFile(compiler/cmm/CmmParse.y)" class="uri" title="wikilink">GhcFile(compiler/cmm/CmmParse.y)</a> maps tokens to &quot;values&quot;, such as `+` to an addition operation, `MO_Add`. The `mkMachOp` function in the Parser determines the `MachOp` type in `CmmMachOp MachOp [CmmExpr]` from the token value and the `MachRep` type of the `head` variable. Notice that the simple `+` operator did not contain sign information, only the `MachRep`. For `expr`, signed and other `MachOps`, see the `machOps` function in <a href="GhcFile(compiler/cmm/CmmParse.y)" class="uri" title="wikilink">GhcFile(compiler/cmm/CmmParse.y)</a>. Here is a table of operators and the corresponding `MachOp`s recognised by Cmm (listed in order of precedence): || <strong>Operator</strong> || <strong>`MachOp`</strong> || || `/` || `MO_U_Quot` || || `*` || `MO_Mul` || || `%` || `MO_U_Rem` || || `-` || `MO_Sub` || || `+` || `MO_Add` || || `&gt;&gt;` || `MO_U_Shr` || || `&lt;&lt;` || `MO_Shl` || || `&amp;` || `MO_And` || || `^` || `MO_Xor` || || `|` || `MO_Or` || || `&gt;=` || `MO_U_Ge` || || `&gt;` || `MO_U_Gt` || || `&lt;=` || `MO_U_Le` || || `&lt;` || `MO_U_Lt` || || `!=` || `MO_Ne` || || `==` || `MO_Eq` || || `~` || `MO_Not` || || `-` || `MO_S_Neg` ||</p>
<h4 id="quasi-operator-syntax">Quasi-operator Syntax</h4>
<p>If you read to the end of `expr` in <a href="GhcFile(compiler/cmm/CmmParse.y)" class="uri" title="wikilink">GhcFile(compiler/cmm/CmmParse.y)</a>, in the next production rule, `expr0`, you will notice that Cmm expressions also recognise a set of name (not symbol) based operators that would probably be better understood as <em>quasi-operators</em>. The syntax for these quasi-operators is in some cases similar to syntax for Cmm statements and generally conform to the C-- specification, sections 3.3.2 (`expr`) and 7.4.1 (syntax of primitive operators), <em>except that</em> 3. <em>and, by the equivalence of the two,</em> 1. <em>may return</em> <strong>multiple</strong> '' arguments''. In Cmm, quasi-operators may have side effects. The syntax for quasi-operators may be:</p>
<p><code>1. `expr0` </code><code> `expr0`</code><a href="BR" title="wikilink"><code>BR</code></a><code>(just like infix-functions in Haskell);</code><br />
<code>1. `type[ expression ]`</code><a href="BR" title="wikilink"><code>BR</code></a><code>(the memory access quasi-expression described in [wiki:Commentary/Compiler/CmmType#MemoryAccess Memory Access]; the Haskell representation of this syntax is `CmmLoad CmmExpr MachRep`); </code><br />
<code>1. `%name( exprs0 )`</code><a href="BR" title="wikilink"><code>BR</code></a><code>(standard prefix form, similar to C-- </code><em><code>statement</code></em><code> syntax for procedures but with the distinguishing prefix `%`; in Cmm this is </code><em><code>also</code> <code>used</code> <code>as</code> <code>statement</code> <code>syntax</code> <code>for</code> <code>calls,</code> <code>which</code> <code>are</code> <code>really</code> <code>built-in</code> <code>procedures</code></em><code>, see [wiki:Commentary/Compiler/CmmType#CmmCalls Cmm Calls]) </code></p>
<p>A `expr0` may be a literal (`CmmLit`) integral, floating point, string or a `CmmReg` (the production rule `reg`: a `name` for a local register (`LocalReg`) or a `GlobalReg`).</p>
<p>Note that the `name` in `expr0` syntax types 1. and 3. must be a known <em>primitive</em> (primitive operation), see [wiki:Commentary/Compiler/CmmType#OperatorsandPrimitiveOperations Operators and Primitive Operations]. The first and third syntax types are interchangeable:  The primitive operations allowed by Cmm are listed in the `machOps` production rule, in <a href="GhcFile(compiler/cmm/CmmParse.y)" class="uri" title="wikilink">GhcFile(compiler/cmm/CmmParse.y)</a>, and largely correspond to `MachOp` data type constructors, in <a href="GhcFile(compiler/cmm/MachOp.hs)" class="uri" title="wikilink">GhcFile(compiler/cmm/MachOp.hs)</a>, with a few additions. The primitive operations distinguish between signed, unsigned and floating point types.</p>
<p>Cmm adds some expression macros that map to Haskell Cmm functions. They are listed under `exprMacros` in <a href="GhcFile(compiler/cmm/CmmParse.y)" class="uri" title="wikilink">GhcFile(compiler/cmm/CmmParse.y)</a> and include:</p>
<p><code>* `ENTRY_CODE`</code><br />
<code>* `INFO_PTR`</code><br />
<code>* `STD_INFO`</code><br />
<code>* `FUN_INFO`</code><br />
<code>* `GET_ENTRY`</code><br />
<code>* `GET_STD_INFO`</code><br />
<code>* `GET_FUN_INFO`</code><br />
<code>* `INFO_TYPE`</code><br />
<code>* `INFO_PTRS`</code><br />
<code>* `INFO_NPTRS`</code><br />
<code>* `RET_VEC`</code></p>
<h3 id="statements-and-calls">Statements and Calls</h3>
<p>Cmm Statements generally conform to the C-- specification, with a few exceptions noted below. Cmm Statements implement:</p>
<p><code>* no-op; the empty statement: `;`</code><br />
<code>* C-- (C99/C++ style) comments: `// ... \n` and `/* ... */`</code><br />
<code>* the assignment operator: `=`</code><br />
<code>* store operation (assignment to a memory location): `type[expr] =`</code><br />
<code>* control flow within procedures (`goto`) and between procedures (`jump`, returns) (note: returns are </code><em><code>only</code></em><code> Cmm macros)</code><br />
<code>* foreign calls (`foreign &quot;C&quot; ...`) and calls to Cmm Primitive Operations (`%`)</code><br />
<code>* procedure calls and tail calls</code><br />
<code>* conditional statement (`if ... { ... } else { ... }`)</code><br />
<code>* tabled conditional (`switch`)</code></p>
<p>Cmm does not implement the C-- specification for Spans (sec. 6.1) or Continuations (sec. 6.7).<a href="BR" class="uri" title="wikilink">BR</a> Although Cmm supports primitive operations that may have side effects (see [wiki:Commentary/Compiler/CmmType#PrimitiveOperations Primitive Operations], below), it does not parse the syntax `%%` form mentioned in section 6.3 of the C-- specification. Use the `%name(arg1,arg2)` expression-syntax instead. <a href="BR" class="uri" title="wikilink">BR</a> Cmm does not implement the `return` statement (C-- spec, sec. 6.8.2) but provides a set of macros that return a list of tuples of a `CgRep` and a `CmmExpr`: `[(CgRep,CmmExpr)]`. For a description of `CgRep`, see comments in <a href="GhcFile(compiler/codeGen/SMRep.lhs)" class="uri" title="wikilink">GhcFile(compiler/codeGen/SMRep.lhs)</a>. The return macros are defined at the end of the production rule `stmtMacros` in <a href="GhcFile(compiler/cmm/CmmParse.y)" class="uri" title="wikilink">GhcFile(compiler/cmm/CmmParse.y)</a>:</p>
<p><code>* `RET_P`</code><br />
<code>* `RET_N`</code><br />
<code>* `RET_PP`</code><br />
<code>* `RET_NN`</code><br />
<code>* `RET_NP`</code><br />
<code>* `RET_PPP`</code><br />
<code>* `RET_NNP`</code><br />
<code>* `RET_NNNP`</code><br />
<code>* `RET_NPNP`</code></p>
<p>In the above macros, `P` stands for `PtrArg` and `N` stands for `NonPtrArg`; both are `CgRep` constructors. These return macros provide greater control for the [wiki:Commentary/Compiler/CodeGen CodeGen] and integrate with the RTS but limit the number and type of return arguments in Cmm: you may only return according to these macros! The returns are processed by the `emitRetUT` function in <a href="GhcFile(compiler/cmm/CmmParse.y)" class="uri" title="wikilink">GhcFile(compiler/cmm/CmmParse.y)</a>, which in turn calls several functions from <a href="GhcFile(compiler/codeGen/CgMonad.lhs)" class="uri" title="wikilink">GhcFile(compiler/codeGen/CgMonad.lhs)</a>, notably `emitStmts`, which is the core Code Generator function for emitting `CmmStmt` data.</p>
<p>The Haskell representation of Cmm Statements is the data type `CmmStmt`, defined in <a href="GhcFile(compiler/cmm/Cmm.hs)" class="uri" title="wikilink">GhcFile(compiler/cmm/Cmm.hs)</a>:  Note how the constructor `CmmJump` contains `[LocalReg]`: this is the Cmm implementation of the C-- `jump` statement for calling another procedure where the parameters are the arguments passed to the other procedure. None of the parameters contain the address--in assembler, a label--of the caller, to return control to the caller. The `CmmCall` constructor also lacks a parameter to store the caller's address. Cmm implements C-- jump nesting and matching returns by <em>tail calls</em>, as described in section 6.8 of the C-- specification. Tail calls are managed through the [wiki:Commentary/Compiler/CodeGen CodeGen], see <a href="GhcFile(compiler/codeGen/CgTailCall.lhs)" class="uri" title="wikilink">GhcFile(compiler/codeGen/CgTailCall.lhs)</a>. You may have already noticed that the call target of the `CmmJump` is a `CmmExpr`: this is the Cmm implementation of computed procedure addresses, for example:  The computed procedure address, in this case `(bits32[x+4])`, should always be the first instruction of a `Cmm` procedure. You cannot obtain the address of a code block <em>within</em> a procedure and `jump` to it, as an alternative way of computing a <em>continuation</em>.</p>
<p>`CmmBranch BlockId` represents an unconditional branch to another [wiki:Commentary/Compiler/CmmType#BasicBlocksandProcedures Basic Block] in the same procedure. There are two unconditional branches in Cmm/C--:</p>
<p><code>1. `goto` statement; and</code><br />
<code>1. a branch from the `else` portion of an `if-then-else` statement.</code></p>
<p>`CmmCondBranch CmmExpr BlockId` represents a conditional branch to another [wiki:Commentary/Compiler/CmmType#BasicBlocksandProcedures Basic Block] in the same procedure. This is the `if expr` statement where `expr` is a `CmmExpr`, used in both the unary `if` and `if-then-else` statements. `CmmCondBranch` maps to more complex Assembler instruction sets or HC code (<a href="GhcFile(compiler/cmm/PprC.hs)" class="uri" title="wikilink">GhcFile(compiler/cmm/PprC.hs)</a>). For assembler, labels are created for each new Basic Block. During parsing, conditional statements map to the `BoolExpr` data type which guides the encoding of assembler instruction sets.</p>
<p>`CmmSwitch` represents the `switch` statement. It is parsed and created as with the `doSwitch` function in <a href="GhcFile(compiler/cmm/CmmParse.y)" class="uri" title="wikilink">GhcFile(compiler/cmm/CmmParse.y)</a> or created from `case` expressions with the `emitSwitch` and `mk_switch` functions in <a href="GhcFile(compiler/codeGen/CgUtils.hs)" class="uri" title="wikilink">GhcFile(compiler/codeGen/CgUtils.hs)</a>. In the NCG, a `CmmSwitch` is generated as a jump table using the `genSwitch` function in <a href="GhcFile(compiler/nativeGen/MachCodeGen.hs)" class="uri" title="wikilink">GhcFile(compiler/nativeGen/MachCodeGen.hs)</a>. There is currently no implementation of any optimisations, such as a cascade of comparisons for switches with a wide deviation in values or binary search for very wide value ranges--for output to HC, earlier versions of GCC could not handle large if-trees, anyway.</p>
<h4 id="cmm-calls">Cmm Calls</h4>
<p>Cmm calls include both calls to foreign functions and calls to Cmm quasi-operators using expression syntax (see [wiki:Commentary/Compiler/CmmType#QuasioperatorSyntax Quasi-operator Syntax]). Although Cmm does not implement any of the control flow statements of C-- specification (section 6.8.1), foreign calls from Cmm are one of the most complex components of the system due to various differences between the Cmm and C calling conventions.</p>
<p>The data type, `CmmCallTarget` is defined in <a href="GhcFile(compiler/cmm/Cmm.hs)" class="uri" title="wikilink">GhcFile(compiler/cmm/Cmm.hs)</a> as:  `CCallConv` is defined in <a href="GhcFile(compiler/prelude/ForeignCall.lhs)" class="uri" title="wikilink">GhcFile(compiler/prelude/ForeignCall.lhs)</a>; for information on register assignments, see comments in <a href="GhcFile(compiler/codeGen/CgCallConv.hs)" class="uri" title="wikilink">GhcFile(compiler/codeGen/CgCallConv.hs)</a>.</p>
<p>`CallishMachOp` is defined in <a href="GhcFile(compiler/cmm/MachOp.hs)" class="uri" title="wikilink">GhcFile(compiler/cmm/MachOp.hs)</a>; see, also, below [wiki:Commentary/Compiler/CmmType#PrimitiveOperations Primitive Operations]. `CallishMachOp`s are generally used for floating point computations (without implementing any floating point exceptions). Here is an example of using a `CallishMachOp` (not yet implemented): </p>
<h3 id="operators-and-primitive-operations">Operators and Primitive Operations</h3>
<p>Cmm generally conforms to the C-- specification for operators and &quot;primitive operations&quot;. The C-- specification, in section 7.4, refers to both of these as &quot;primitive operations&quot; but there are really two different types:</p>
<p><code>* </code><em><code>operators</code></em><code>, as I refer to them, are: </code><br />
<code>  * parseable tokens, such as `+`,`-`,`*` or `/`; </code><br />
<code>  * generally map to a single machine instruction or part of a machine instruction;</code><br />
<code>  * have no side effects; and, </code><br />
<code>  * are represented in Haskell using the `MachOp` data type; </code><br />
<code>* </code><em><code>primitive</code> <code>operations</code></em><code> (Cmm </code><em><code>quasi-operators</code></em><code>) are special, usually inlined, procedures, represented in Haskell using the `CallishMachOp` data type; primitive operations may have side effects.</code></p>
<p>The `MachOp` and `CallishMachOp` data types are defined in <a href="GhcFile(compiler/cmm/MachOp.hs)" class="uri" title="wikilink">GhcFile(compiler/cmm/MachOp.hs)</a>.</p>
<p>Both Cmm Operators and Primitive Operations are handled in Haskell as [wiki:Commentary/PrimOps#InlinePrimOps Inline PrimOps], though what I am calling Cmm <em>primitive operations</em> may be implemented as out-of-line foreign calls.</p>
<h4 id="operators">Operators</h4>
<p> Each `MachOp` generally corresponds to a machine instruction but may have its value precomputed in the Cmm, NCG or HC optimisers.</p>
<h4 id="primitive-operations">Primitive Operations</h4>
<p>Primitive Operations generally involve more than one machine instruction and may not always be inlined.</p>
<p> For an example, the floating point sine function, `sinFloat#` in <a href="GhcFile(compiler/prelude/primops.txt.pp)" class="uri" title="wikilink">GhcFile(compiler/prelude/primops.txt.pp)</a> is piped through the `callishOp` function in <a href="GhcFile(compiler/codeGen/CgPrimOp.hs)" class="uri" title="wikilink">GhcFile(compiler/codeGen/CgPrimOp.hs)</a> to become `Just MO_F32_Sin`. The `CallishMachOp` constructor `MO_F32_Sin` is piped through a platform specific function such as <a href="GhcFile(compiler/nativeGen/X86/CodeGen.hs)" class="uri" title="wikilink">GhcFile(compiler/nativeGen/X86/CodeGen.hs)</a> on X86, where the function `genCCall` will call `outOfLineFloatOp` to issue a call to a C function such as `sin`.</p>
<h2 id="cmm-design-observations-and-areas-for-potential-improvement">Cmm Design: Observations and Areas for Potential Improvement</h2>
<p>&quot;If the application of a primitive operator causes a system exception, such as division by zero, this is an unchecked run-time error. (A future version of this specification may provide a way for a program to recover from such an exception.)&quot; C-- spec, Section 7.4. Cmm may be able to implement a partial solution to this problem, following the paper: <a href="http://cminusminus.org/abstracts/c--pldi-00.html">A Single Intermediate Language That Supports Multiple Implementations of Exceptions (2000)</a>. (TODO: write notes to wiki and test fix.)</p>
<p>The IEEE 754 specification for floating point numbers defines exceptions for certain floating point operations, including:</p>
<p><code>* range violation (overflow, underflow); </code><br />
<code>* rounding errors (inexact); </code><br />
<code>* invalid operation (invalid operand, such as comparison with a `NaN` value, the square root of a negative number or division of zero by zero); and,</code><br />
<code>* zero divide (a special case of an invalid operation).  </code></p>
<p>Many architectures support floating point exceptions by including a special register as an addition to other exception handling registers. The IBM PPC includes the `FPSCR` (&quot;Floating Point Status Control Register&quot;); the Intel x86 processors use the `MXCSR` register. When the PPC performs a floating point operation it checks for possible errors and sets the `FPSCR`. Some processors allow a flag in the Foating-Point Unit (FPU) status and control register to be set that will disable some exceptions or the entire FPU exception handling facility. Some processors disable the FPU after an exception has occurred while others, notably Intel's x86 and x87 processors, continue to perform FPU operations. Depending on whether quiet !NaNs (QNaNs) or signaling !NaNs (SNaNs) are used by the software, an FPU exception may signal an interrupt for the software to pass to its own exception handler.</p>
<p>Some higher level languages provide facilities to handle these exceptions, including Ada, Fortran (F90 and later), C++ and C (C99, fenv.h, float.h on certain compilers); others may handle such exceptions without exposing a low-level interface. There are three reasons to handle FPU exceptions, and these reasons apply similarly to other exceptions:</p>
<p><code>* the facilities provide greater control; </code><br />
<code>* the facilities are efficient--more efficient than a higher-level software solution; and, </code><br />
<code>* FPU exceptions may be unavoidable, especially if several FPU operations are serially performed at the machine level so the higher level software has no opportunity to check the results in between operations. </code></p>
<p>A potential solution to the problem of implementing Cmm exceptions, especially for floating point operations, is at [wiki:Commentary/CmmExceptions Cmm: Implementing Exception Handling].</p>
<p>The C-- Language Specification mentions over 75 primitive operators. The Specification lists separate operators for integral and floating point (signed) arithmetic (including carry, borrow and overflow checking), logical comparisons and conversions (from one size float to another, from float to integral and vice versa, etc.). C-- also includes special operators for floating point number values, such as `NaN`, `mzero`<em>k</em> and `pzero`<em>k</em>, and rounding modes; integral kinds also include bitwise operators, unsigned variants, and bit extraction for width changing and sign or zero-extension. A C-- implementation may conveniently map each of these operators to a machine instruction, or to a simulated operation on architectures that do not support a single instruction. There seem to be two main problems with the current GHC-implementation of Cmm:</p>
<p><code>1. not enough operators</code><br />
<code>1. no implementation of vector (SIMD) registers (though there is a `I128` `MachRep`)</code></p>
<p>If a particular architecture supports it, assembler includes instructions such as mnemonics with the `.` (&quot;dot&quot;) suffix (`add., fsub.`), which set the Condition Register (CR) thereby saving you at least one instruction. (Extended mnemonics can save you even more.) Extended mnemonics with side effects may be implemented as new `CallishMachOps`, see [wiki:Commentary/Compiler/CmmType#PrimitiveOperations Primitive Operations] and [wiki:Commentary/Compiler/CmmType#CmmCalls Cmm Calls]. Assembler also supports machine exceptions, especially exceptions for floating-point operations, invalid storage access or misalignment (effective address alignment). The current implementation of Cmm cannot model such exceptions through flow control because no flow control is implemented, see [wiki:Commentary/Compiler/CmmType#CmmCalls Cmm Calls].</p>
<p>Hiding the kinds of registers on a machine eliminates the ability to handle floating point exceptions at the Cmm level and to explicitly vectorize (use SIMD extensions). The argument for exposing vector types may be a special case since such low-level operations are exposed at the C-level, as new types of variables or &quot;intrinsics,&quot; that are C-language extensions provided by special header files and compiler support (`vector unsigned int` or `__m128i`, `vector float` or `__m128`) and operations (`vec_add()`, `+` (with at least one vector operand), `_mm_add_epi32()`).</p>
<h1 id="ghc-commentary-what-the-hell-is-a-.cmm-file">GHC Commentary: What the hell is a `.cmm` file?</h1>
<p>A `.cmm` file is rather like C--. The syntax is almost C-- (a few constructs are missing), and it is augmented with some macros that are expanded by GHC's code generator (eg. `INFO_TABLE()`). A `.cmm` file is compiled by GHC itself: the syntax is parsed by <a href="GhcFile(compiler/cmm/CmmParse.y)" class="uri" title="wikilink">GhcFile(compiler/cmm/CmmParse.y)</a> and <a href="GhcFile(compiler/cmm/CmmLex.x)" class="uri" title="wikilink">GhcFile(compiler/cmm/CmmLex.x)</a> into the [wiki:Commentary/Compiler/CmmType Cmm] data type, where it is then passed through one of the [wiki:Commentary/Compiler/Backends back-ends].</p>
<p>We use the C preprocessor on `.cmm` files, making extensive use of macros to make writing this low-level code a bit less tedious and error-prone. Most of our C-- macros are in <a href="GhcFile(includes/Cmm.h)" class="uri" title="wikilink">GhcFile(includes/Cmm.h)</a>. One useful fact about the macros is `P_` is an alias for `gcptr`, and you should not use it for non-garbage-collected pointers.</p>
<h2 id="reading-references">Reading references</h2>
<p>Reading material for learning Cmm is somewhat scattered, so I (Arash) have created a list of useful links. Since the Cmm language is changing as GHC changes, I have prioritized resources that are not too old. (<em>Feel free to add/remove/modify this list! :)</em>)</p>
<p><code>* An overview of Cmm is given in </code><a href="https://davidterei.com/downloads/papers/terei:2009:honours_thesis.pdf"><code>David</code> <code>Terei's</code> <code>bachelor</code> <code>thesis</code></a><code> (chapter 2.4.3).</code><br />
<code>* The comments in the beginning of </code><a href="GhcFile(compiler/cmm/CmmParse.y)" title="wikilink"><code>GhcFile(compiler/cmm/CmmParse.y)</code></a><code> is super-useful and kept up to date. The rest of the file contains the </code><em><code>grammar</code></em><code> of the language. Afraid of grammars? Edward Yang wrote this fantastic </code><a href="http://blog.ezyang.com/2013/07/no-grammar-no-problem/"><code>blog</code> <code>post</code></a><code> on how to understand the constructs of Cmm by using the grammar.  </code><br />
<code>* Cmm has a preprocessor like the one in C and many of the macros are defined in </code><a href="GhcFile(includes/Cmm.h)" title="wikilink"><code>GhcFile(includes/Cmm.h)</code></a><code>. </code><br />
<code>* In 2012, Simon Marlow extended the Cmm language by adding a new high-level syntax which can be used when you don't need low-level access (like registers). The </code><a href="https://github.com/ghc/ghc/commit/a7c0387d20c1c9994d1100b14fbb8fb4e28a259e"><code>commit</code></a><code> explains the details.</code><br />
<code>* Cmm is also described [wiki:Commentary/Compiler/CmmType on this wiki], but it is written before the new syntax was introduced.</code><br />
<code>* Stack frame types are created using `INFO_TABLE_RET`, the syntax can be confusing since there are both </code><em><code>arguments</code></em><code> and </code><em><code>fields</code></em><code>, I (Arash) have not seen anything like it in other programming languages. I tried to explain it in my </code><a href="http://arashrouhani.com/papers/master-thesis.pdf"><code>master</code> <code>thesis</code></a><code> (sections 4.2 and 4.2.1).</code></p>
<h2 id="other-information">Other information</h2>
<p>It can take time to learn Cmm. One unintuitive thing to watch out for is that there are no function calls in low-level cmm code. The new syntax from 2012 allows function calls but you should know that they are kind of magical.</p>
<p>We say that <strong>Cmm</strong> is GHC's implementation of <strong>C--</strong>. This naming scheme is not done consistently everywhere, unfortunately. If you are interested in C-- (which have diverged from Cmm), you can check out the <a href="http://www.cminusminus.org/">website</a> and the <a href="http://www.cs.tufts.edu/~nr/c--/extern/man2.pdf">specification</a>.</p>
<h1 id="code-generator">Code Generator</h1>
<p>This page describes code generator (&quot;codegen&quot;) in GHC. It is meant to reflect current state of the implementation. If you notice any inaccuracies please update the page (if you know how) or complain on ghc-devs.</p>
<h2 id="a-brief-history-of-code-generator">A brief history of code generator</h2>
<p>You might occasionally hear about &quot;old&quot; and &quot;new&quot; code generator. GHC 7.6 and earlier used the old code generator. New code generator was being developed since 2007 and it was [changeset:832077ca5393d298324cb6b0a2cb501e27209768/ghc enabled by default on 31 August 2012] after the release of GHC 7.6.1. The first stable GHC to use the new code generator is 7.8.1 released in early 2014. The commentary on the old code generator can be found [wiki:Commentary/Compiler/OldCodeGen here]. Notes from the development process of the new code generator are located in a couple of pages on the wiki - to find them go to [wiki:TitleIndex Index] and look for pages starting with &quot;!NewCodeGen&quot;.</p>
<p>There are some plans for the future development of code generator. One plan is to expand the capability of the pipeline so that it does native code generation too so that existing backends can be discarded - see [wiki:Commentary/Compiler/IntegratedCodeGen IntegratedCodeGen] for discussion of the design. It is hard to say if this will ever happen as currently there is no work being done on that subject and in the meanwhile there was an alternative proposal to [wiki:Commentary/Compiler/Backends/LLVM/ReplacingNCG replace native code generator with LLVM].</p>
<h2 id="overview">Overview</h2>
<p>The goal of the code generator is to convert program from [wiki:Commentary/Compiler/GeneratedCode STG] representation to [wiki:Commentary/Compiler/CmmType Cmm] representation. STG is a functional language with explicit stack. Cmm is a low-level imperative language - something between C and assembly - that is suitable for machine code generation. Note that terminology might be a bit confusing here: the term &quot;code generator&quot; can refer both to STG-&gt;Cmm pass and the whole STG-&gt;Cmm-&gt;assembly pass. The Cmm-&gt;assembly conversion is performed by one the backends, eg. NCG (Native Code Generator or LLVM.</p>
<p>The top-most entry point to the codegen is located in <a href="GhcFile(compiler/main/HscMain.hs)" class="uri" title="wikilink">GhcFile(compiler/main/HscMain.hs)</a> in the `tryNewCodegen` function. Code generation is done in two stages:</p>
<p><code> 1. Convert STG to Cmm with implicit stack, and native Cmm calls. This whole stage lives in </code><a href="GhcFile(compiler/codeGen)" title="wikilink"><code>GhcFile(compiler/codeGen)</code></a><code> directory with the entry point being `codeGen` function in </code><a href="GhcFile(compiler/codeGen/StgCmm.hs)" title="wikilink"><code>GhcFile(compiler/codeGen/StgCmm.hs)</code></a><code> module.</code><br />
<code> 2. Optimise the Cmm, and CPS-convert it to have an explicit stack, and no native calls. This lives in </code><a href="GhcFile(compiler/cmm)" title="wikilink"><code>GhcFile(compiler/cmm)</code></a><code> directory with the `cmmPipeline` function from </code><a href="GhcFile(compiler/cmm/CmmPipeline.hs)" title="wikilink"><code>GhcFile(compiler/cmm/CmmPipeline.hs)</code></a><code> module being the entry point.</code></p>
<p>The CPS-converted Cmm is fed to one of the backends. This is done by `codeOutput` function (<a href="GhcFile(compiler/main/CodeOutput.lhs)" class="uri" title="wikilink">GhcFile(compiler/main/CodeOutput.lhs)</a> called from `hscGenHardCode` after returning from `tryNewCodegen`.</p>
<h2 id="first-stage-stg-to-cmm-conversion">First stage: STG to Cmm conversion</h2>
<p><code>* </code><strong><code>Code</code> <code>generator</code></strong><code> converts STG to `CmmGraph`.  Implemented in `StgCmm*` modules (in directory `codeGen`). </code><br />
<code>  * `Cmm.CmmGraph` is pretty much a Hoopl graph of `CmmNode.CmmNode` nodes. Control transfer instructions are always the last node of a basic block.</code><br />
<code>  * Parameter passing is made explicit; the calling convention depends on the target architecture.  The key function is `CmmCallConv.assignArgumentsPos`. </code><br />
<code>    * Parameters are passed in virtual registers R1, R2 etc. [These map 1-1 to real registers.] </code><br />
<code>    * Overflow parameters are passed on the stack using explicit memory stores, to locations described abstractly using the [wiki:Commentary/Compiler/StackAreas </code><em><code>Stack</code> <code>Area</code></em><code> abstraction].   </code><br />
<code>    * Making the calling convention explicit includes an explicit store instruction of the return address, which is stored explicitly on the stack in the same way as overflow parameters. This is done (obscurely) in `StgCmmMonad.mkCall`.</code></p>
<h2 id="second-stage-the-cmm-pipeline">Second stage: the Cmm pipeline</h2>
<p>The core of the Cmm pipeline is implemented by the `cpsTop` function in <a href="GhcFile(compiler/cmm/CmmPipeline.hs)" class="uri" title="wikilink">GhcFile(compiler/cmm/CmmPipeline.hs)</a> module. Below is a high-level overview of the pipeline. See source code comments in respective modules for a more in-depth explanation of each pass.</p>
<p><code>* </code><strong><code>Control</code> <code>Flow</code> <code>Optimisations</code></strong><code>, implemented in `CmmContFlowOpt`, simplifies the control flow graph by:</code><br />
<code>  * Eliminating blocks that have only one predecessor by concatenating them with that predecessor</code><br />
<code>  * Shortcuting targets of branches and calls (see Note [What is shortcutting])</code><br />
<code> </code><br />
<code>If a block becomes unreachable because of shortcutting it is eliminated from the graph. However, </code><strong><code>it</code> <code>is</code> <code>theoretically</code> <code>possible</code> <code>that</code> <code>this</code> <code>pass</code> <code>will</code> <code>produce</code> <code>unreachable</code> <code>blocks</code></strong><code>. The reason is the label renaming pass performed after block concatenation has been completed.</code></p>
<p><code>This pass might be optionally called for the second time at the end of the pipeline.</code></p>
<p><code>* </code><strong><code>Common</code> <code>Block</code> <code>Elimination</code></strong><code>, implemented in `CmmCommonBlockElim`, eliminates blocks that are identical (except for the label on their first node). Since this pass traverses blocks in depth-first order any unreachable blocks introduced by Control Flow Optimisations are eliminated. </code><strong><code>This</code> <code>pass</code> <code>is</code> <code>optional.</code></strong></p>
<p><code>* </code><strong><code>Determine</code> <code>proc-points</code></strong><code>, implemented in `CmmProcPoint`. The idea behind the &quot;proc-point splitting&quot; is that we first determine proc-points, ie. blocks in the graph that can be turned into entry points of procedures, and then split a larger function into many smaller ones, each having a proc-point as its entry point. This is required for the LLVM backend. The proc-point splitting itself is done later in the pipeline, but here we only determine the set of proc-points. We first call `callProcPoints`, which assumes that entry point to a Cmm graph and every continuation of a call is a procpoint. If we are splitting proc-points we update the list of proc-points by calling `minimalProcPointSet`, which adds all blocks reachable from more than one block in the graph. The set of proc-points is required by the stack layout pass.</code></p>
<p><code>* </code><strong><code>Figure</code> <code>out</code> <code>the</code> <code>stack</code> <code>layout</code></strong><code>, implemented in `CmmStackLayout`. The job of this pass is to:</code><br />
<code>  * replace references to abstract stack Areas with fixed offsets from Sp.</code><br />
<code>  * replace the !CmmHighStackMark constant used in the stack check with</code><br />
<code>    the maximum stack usage of the proc.</code><br />
<code>  * save any variables that are live across a call, and reload them as</code><br />
<code>  necessary.</code><br />
<strong><code>Important</code></strong><code>: It may happen that stack layout will invalidate the computed set of proc-points by making a proc-point unreachable. This unreachable block is eliminated by one of subsequent passes that performs depth-first traversal of a graph: sinking pass (if optimisations are enabled), proc-point analysis (if optimisations are disabled and we're doing proc-point splitting) or at the very end of the pipeline (if optimisations are disabled and we're not doing proc-point splitting). This means that starting from this point in the pipeline we have inconsistent data and subsequent steps must be prepared for it.</code><br />
<br />
<code>* </code><strong><code>Sinking</code> <code>assignments</code></strong><code>, implemented in `CmmSink`, performs these optimizations:</code><br />
<code>  * moves assignments closer to their uses, to reduce register pressure</code><br />
<code>  * pushes assignments into a single branch of a conditional if possible</code><br />
<code>  * inlines assignments to registers that are mentioned only once</code><br />
<code>  * discards dead assignments</code><br />
<strong><code>This</code> <code>pass</code> <code>is</code> <code>optional.</code></strong><code> It currently does not eliminate dead code in loops (#8327) and has some other minor deficiencies (eg. #8336).</code></p>
<p><code> * </code><strong><code>CAF</code> <code>analysis</code></strong><code>, implemented in `CmmBuildInfoTables`. Computed CAF information is returned from `cmmPipeline` and used to create Static Reference Tables (SRT). See [wiki:Commentary/Rts/Storage/GC/CAFs here] for some more detail on CAFs and SRTs. This pass is implemented using Hoopl (see below).</code></p>
<p><code> * </code><strong><code>Proc-point</code> <code>analysis</code> <code>and</code> <code>splitting</code></strong><code> (only when splitting proc-points), implemented by `procPointAnalysis` in `CmmProcPoint`, takes a list of proc-points and for each block and determines from which proc-point the block is reachable. This is implemented using Hoopl.</code><br />
<code> Then the call to `splitAtProcPoints` splits the Cmm graph into multiple Cmm graphs (each represents a single function) and build info tables to each of them.</code><br />
<code> When doing this we must be prepared for the fact that a proc-point does not actually exist in the graph since it was removed by stack layout pass (see #8205).</code></p>
<p><code> * </code><strong><code>Attach</code> <code>continuations'</code> <code>info</code> <code>tables</code></strong><code> (only when NOT splitting proc-points), implemented by `attachContInfoTables` in `CmmProcPoint` attaches info tables for the continuations of calls in the graph. </code><em><code>[PLEASE</code> <code>WRITE</code> <code>MORE</code> <code>IF</code> <code>YOU</code> <code>KNOW</code> <code>WHY</code> <code>THIS</code> <code>IS</code> <code>!DONE]</code></em></p>
<p><code> * </code><strong><code>Update</code> <code>info</code> <code>tables</code> <code>to</code> <code>include</code> <code>stack</code> <code>liveness</code></strong><code>, implemented by `setInfoTableStackMap` in `CmmLayoutStack`. Populates info tables of each Cmm function with stack usage information. Uses stack maps created by the stack layout pass.</code></p>
<p><code> * </code><strong><code>Control</code> <code>Flow</code> <code>Optimisations</code></strong><code>, same as the beginning of the pipeline, but this pass runs only with `-O1` and `-O2`. Since this pass might produce unreachable blocks it is followed by a call to `removeUnreachableBlocksProc` (also in `CmmContFlowOpt.hs`)</code></p>
<h2 id="dumping-and-debugging-cmm">Dumping and debugging Cmm</h2>
<p>You can dump the generated Cmm code using `-ddump-cmm` flag. This is helpful for debugging Cmm problems. Cmm dump is divided into several sections:</p>
<p></p>
<p>&quot;Cmm produced by new codegen&quot; is emited in `HscMain` module after converting STG to Cmm. This Cmm has not been processed in any way by the Cmm pipeline. If you see that something is incorrect in that dump it means that the problem is located in the STG-&gt;Cmm pass. The last section, &quot;Output Cmm&quot;, is also dumped in `HscMain` but this is done after the Cmm has been processed by the whole Cmm pipeline. All other sections are dumped by the Cmm pipeline. You can dump only selected passes with more specific flags. For example, if you know (or suspect) that the sinking pass is performing some incorrect transformations you can make the dump shorter by adding `-ddump-cmm-sp -ddump-cmm-sink` flags. This will produce only the &quot;Layout Stack&quot; dump (just before sinking pass) and &quot;Sink assignments&quot; dump (just after the sinking pass) allowing you to focus on the changes introduced by the sinking pass.</p>
<h2 id="register-allocator-code">Register Allocator Code</h2>
<p>The register allocator code is split into two main sections, the register allocator proper and a generic graph coloring library. The graph coloring library is also used by the Stg-&gt;Cmm converter.</p>
<h3 id="the-register-allocator">The register allocator</h3>
<p><code>* </code><a href="GhcFile(compiler/nativeGen/RegLiveness.hs)" title="wikilink"><code>GhcFile(compiler/nativeGen/RegLiveness.hs)</code></a><code> </code><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  Defines </code><code> and </code><code> which carry native machine instructions annotated with register liveness information. It also provides functions to annotate native code (</code><code>) with this liveness information, and to slurp out sets of register conflicts for feeding into the coloring allocator.</code></p>
<p><code>* </code><a href="GhcFile(compiler/nativeGen/RegAllocColor.hs)" title="wikilink"><code>GhcFile(compiler/nativeGen/RegAllocColor.hs)</code></a><code> </code><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  Defines </code><code>, the main driver function for the graph coloring allocator. The driver accepts </code><code>s which use virtual regs, and produces</code><code> which use real machine regs. This module also provides functions to help build and deep seq the register conflict graph.</code></p>
<p><code>* </code><a href="GhcFile(compiler/nativeGen/RegAllocLinear.hs)" title="wikilink"><code>GhcFile(compiler/nativeGen/RegAllocLinear.hs)</code></a><code> </code><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  Defines the linear scan allocator. Its interface is identical to the coloring allocator.</code></p>
<p><code>* </code><a href="GhcFile(compiler/nativeGen/RegAllocInfo.hs)" title="wikilink"><code>GhcFile(compiler/nativeGen/RegAllocInfo.hs)</code></a><code> </code><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  Defines the register information function, </code><code>, which takes a set of real and virtual registers and returns the actual registers used by a particular </code><code>; register allocation is in AT&amp;T syntax order (source, destination), in an internal function, </code><code>; defines the </code><code> data type</code><a href="BR" title="wikilink"><code>BR</code></a><a href="BR" title="wikilink"><code>BR</code></a></p>
<p><code>* </code><a href="GhcFile(compiler/nativeGen/RegSpillCost.hs)" title="wikilink"><code>GhcFile(compiler/nativeGen/RegSpillCost.hs)</code></a><code> </code><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  Defines </code><code> which is responsible for selecting a virtual reg to spill to the stack when not enough real regs are available.</code></p>
<p><code>* </code><a href="GhcFile(compiler/nativeGen/RegSpill.hs)" title="wikilink"><code>GhcFile(compiler/nativeGen/RegSpill.hs)</code></a><code> </code><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  Defines </code><code> which takes </code><code>s and inserts spill/reload instructions virtual regs that wouldn't fit in real regs. </code><code>'s strategy is to simply inserts spill/reloads for every use/def of a particular virtual reg. This inefficient code is cleaned up by the spill cleaner after allocation.</code><br />
<br />
<code>* </code><a href="GhcFile(compiler/nativeGen/RegSpillClean.hs)" title="wikilink"><code>GhcFile(compiler/nativeGen/RegSpillClean.hs)</code></a><code> </code><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  The spill cleaner is run after real regs have been allocated. It erases spill/reload instructions inserted by </code><code> that weren't strictly nessesary.</code></p>
<p><code>* </code><a href="GhcFile(compiler/nativeGen/RegAllocStats.hs)" title="wikilink"><code>GhcFile(compiler/nativeGen/RegAllocStats.hs)</code></a><code> </code><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  Defines data types and pretty printers used for collecting statistics and debugging info from the coloring allocator.</code></p>
<h3 id="graph-coloring">Graph coloring</h3>
<p><code>* </code><a href="GhcFile(compiler/utils/GraphBase.hs)" title="wikilink"><code>GhcFile(compiler/utils/GraphBase.hs)</code></a><code> </code><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  Defines the basic </code><code>, </code><code> and </code><code> types used by the coloring algorithm.</code></p>
<p><code>* </code><a href="GhcFile(compiler/utils/GraphColor.hs)" title="wikilink"><code>GhcFile(compiler/utils/GraphColor.hs)</code></a><code> </code><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  Defines the function </code><code> which is responsible for assigning colors (real regs) to nodes (virtual regs) in the register conflict graph.</code></p>
<p><code>* </code><a href="GhcFile(compiler/utils/GraphOps.hs)" title="wikilink"><code>GhcFile(compiler/utils/GraphOps.hs)</code></a><code> </code><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  Defines functions to perform basic operations on the graphs such as adding, deleting, and coalescing nodes.</code></p>
<p><code>* </code><a href="GhcFile(compiler/utils/GraphPps.hs)" title="wikilink"><code>GhcFile(compiler/utils/GraphPps.hs)</code></a><code> </code><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  Defines functions for pretty print graphs in human readable-ish and graphviz format.</code></p>
<h3 id="miscellanea">Miscellanea</h3>
<p><code>* </code><a href="GhcFile(compiler/nativeGen/RegCoalesce.hs)" title="wikilink"><code>GhcFile(compiler/nativeGen/RegCoalesce.hs)</code></a><code> </code><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  Defines a function </code><code> that does aggressive coalescing directly on </code><code>, without using the graph. This isn't used at the moment but has been left in incase we want to rejig the allocator when the new CPS converter comes online.</code></p>
<p><code>* </code><a href="GhcFile(compiler/nativeGen/RegArchBase.hs)" title="wikilink"><code>GhcFile(compiler/nativeGen/RegArchBase.hs)</code></a><code> </code><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  Defines utils for calculating whether a register in the conflict graph is trivially colorable, in a generic way which handles aliasing between register classes. This module is not used directly by GHC.</code></p>
<p><code>* </code><a href="GhcFile(compiler/nativeGen/RegArchX86.hs)" title="wikilink"><code>GhcFile(compiler/nativeGen/RegArchX86.hs)</code></a><code> </code><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  Contains a description of the aliasing constraints between the register sets on x86. This module is not used directly by GHC.</code></p>
<p><a href="PageOutline" class="uri" title="wikilink">PageOutline</a></p>
<h1 id="the-ghc-commentary---coding-style-guidelines-for-the-compiler">The GHC Commentary - Coding Style Guidelines for the compiler</h1>
<p>This is a rough description of some of the coding practices and style that we use for Haskell code inside . For run-time system code see the [wiki:Commentary/Rts/Conventions Coding Style Guidelines for RTS C code]. Also see the wiki page on [wiki:WorkingConventions Working Conventions] for issues related to version control, workflow, testing, bug tracking and other miscellany.</p>
<h2 id="general-style">General Style</h2>
<p>The general rule is to stick to the same coding style as is already used in the file you're editing. If you must make stylistic changes, commit them separately from functional changes, so that someone looking back through the change logs can easily distinguish them.</p>
<p>It's much better to write code that is transparent than to write code that is short.</p>
<p>Often it's better to write out the code longhand than to reuse a generic abstraction (not always, of course). Sometimes it's better to duplicate some similar code than to try to construct an elaborate generalisation with only two instances. Remember: other people have to be able to quickly understand what you've done, and overuse of abstractions just serves to obscure the <em>really</em> tricky stuff, and there's no shortage of that in GHC.</p>
<h2 id="comments">Comments</h2>
<p>There are two kinds of comments in source code, comments that describe the interface (i.e. how is this supposed to be used) and comments that describe the implementation (e.g. subtle gotchas).</p>
<h3 id="comments-on-top-level-entities">Comments on top-level entities</h3>
<p>Every top-level entity should have a Haddock comment that describes what it does and, if needed, why it's there. Example:</p>
<p></p>
<p>We use Haddock so that the comment is included in the generated HTML documentation.</p>
<p>There's a bit of a broken window effect going on, but please try to follow this rule for new functions you add.</p>
<h3 id="comments-in-the-source-code">Comments in the source code</h3>
<p>Commenting is good but</p>
<p><code> * long comments </code><em><code>interleaved</code> <code>with</code> <code>the</code> <code>code</code></em><code> can make the code itself incredibly hard to read, and</code><br />
<code> * long comments </code><em><code>detached</code> <code>from</code> <code>the</code> <code>code</code></em><code> are easy to miss when you are editing the code itself, and soon become out of date or even misleading.</code></p>
<p>We have adopted a style that seems to help. Here's an example:  Notice that</p>
<p><code>* </code><strong><code>Interleaved</code> <code>with</code> <code>the</code> <code>code</code></strong><code> is a short link `Note [Float coercions]`. You can't miss it when you are editing the code, but you can still see the code itself.</code><br />
<code>* </code><strong><code>Detached</code> <code>from</code> <code>the</code> <code>code</code></strong><code> is the linked comment, starting with the same string `Note [Float coercions]`.  It can be long, and often includes examples.</code></p>
<p>The standard format &quot;`Note [Float coercions]`&quot; serves like an URL, to point to an out-of-line comment. Usually the target is in the same module, but not always. Sometimes we say </p>
<p>Please use this technique. It's robust, and survives successive changes to the same lines of code. When you are changing code, it draws attention to non-obvious things you might want to bear in mind. When you encounter the note itself you can search for the string to find the code that implements the thoughts contained in the comment.</p>
<h3 id="comments-and-examples">Comments and examples</h3>
<p>When writing a comment to explain a subtle point, consider including an example code snippet that illustrates the point. For example, the above `Note [Float coercions]` continues thus:  These kind of code snippets are extremely helpful to illustrate the point in a concrete way. Other ways of making the comment concrete are:</p>
<p><code>* Cite a particular Trac ticket that this bit of code deals with</code><br />
<code>* Cite a test case in the test suite that illustrates it</code></p>
<h3 id="longer-comments-or-architectural-commentary">Longer comments or architectural commentary</h3>
<p>Comments with a broad scope, describing the architecture or workings of more than one module, belong here in the commentary rather than in the code. Put the URL for the relevant commentary page in a comment in the code itself, and also put URLs for all relevant commentary pages in a comment at the top of each module.</p>
<h3 id="commit-messages">Commit messages</h3>
<p>Please do not use commit messages to describe how something works, or give examples, <em>even if the patch is devoted to a single change</em>. The information is harder to find in a commit message, and (much worse) there is no explicit indication in the code that there is carefully-written information available about that particular line of code. Instead, you can refer to the Note from the commit message.</p>
<p>Commit messages can nevertheless contain substantial information, but it is usually of a global nature. E.g. &quot;This patch modifies 20 files to implement a new form of inlining pragma&quot;. They are also a useful place to say which ticket is fixed by the commit, summarise the changes embodied in the commit etc.</p>
<p>In short, commit messages describe <em>changes</em>, whereas comment explain the code <em>as it now is</em>.</p>
<h2 id="warnings">Warnings</h2>
<p>We are aiming to make the GHC code warning-free, for all warnings turned on by  The build automatically sets these flags for all source files (see `mk/warnings.mk`).</p>
<p>The [wiki:TestingPatches validate script], which is used to test the build before commiting, additionally sets the `-Werror` flag, so that the code <strong>must</strong> be warning-free to pass validation. The `-Werror` flag is not set during normal builds, so warnings will be printed but won't halt the build.</p>
<p>Currently we are some way from our goal, so some modules have a  pragma; you are encouraged to remove this pragma and fix any warnings when working on a module.</p>
<h2 id="exports-and-imports">Exports and Imports</h2>
<h3 id="exports">Exports</h3>
<p> We usually (99% of the time) include an export list. The only exceptions are perhaps where the export list would list absolutely everything in the module, and even then sometimes we do it anyway.</p>
<p>It's helpful to give type signatures inside comments in the export list, but hard to keep them consistent, so we don't always do that.</p>
<h3 id="imports">Imports</h3>
<p>List imports in the following order:</p>
<p><code>* Local to this subsystem (or directory) first </code><br />
<code>* Compiler imports, generally ordered from specific to generic (ie. modules from utils/ and basicTypes/ usually come last) </code><br />
<code>* Library imports </code><br />
<code>* Standard Haskell 98 imports last </code></p>
<p></p>
<p>Import library modules from the [wiki:Commentary/Libraries boot packages] only (boot packages are those packages in the file [source:packages] that have a '-' in the &quot;tag&quot; column). Use `#defines `in `HsVersions.h` when the modules names differ between versions of GHC. For code inside `#ifdef GHCI`, don't worry about GHC versioning issues, because this code is only ever compiled by the this very version of GHC.</p>
<p><strong>Do not use explicit import lists</strong>, except to resolve name clashes. There are several reasons for this:</p>
<p><code>* They slow down development: almost every change is accompanied by an import list change.</code></p>
<p><code>* They cause spurious conflicts between developers.</code></p>
<p><code>* They lead to useless warnings about unused imports, and time wasted trying to</code><br />
<code>  keep the import declarations &quot;minimal&quot;.</code></p>
<p><code>* GHC's warnings are useful for detecting unnecessary imports: see `-fwarn-unused-imports`.</code></p>
<p><code>* TAGS is a good way to find out where an identifier is defined (use `make tags` in `ghc/compiler`,</code><br />
<code>  and hit `M-.` in emacs).</code></p>
<p>If the module can be compiled multiple ways (eg. GHCI vs. non-GHCI), make sure the imports are properly `#ifdefed` too, so as to avoid spurious unused import warnings.</p>
<h2 id="compiler-versions-and-language-extensions">Compiler versions and language extensions</h2>
<p>GHC must be compilable and validate by the previous two major GHC releases, and itself. It isn't necessary for it to be compilable by every intermediate development version.</p>
<p>To maintain compatibility, use [wiki:Commentary/CodingStyle#HsVersions.h HsVersions.h] (see below) where possible, and try to avoid using #ifdef in the source itself.</p>
<h3 id="section"></h3>
<p> is a CPP header file containing a number of macros that help smooth out the differences between compiler versions. It defines, for example, macros for library module names which have moved between versions. Take a look <a href="GhcFile(compiler/HsVersions.h)" class="uri" title="wikilink">GhcFile(compiler/HsVersions.h)</a>. </p>
<h3 id="literate-haskell">Literate Haskell</h3>
<p>In GHC we use a mixture of literate () and non-literate () source. I (Simon M.) prefer to use non-literate style, because I think the } clutter up the source too much, and I like to use Haddock-style comments (we haven't tried processing the whole of GHC with Haddock yet, though).</p>
<h3 id="the-c-preprocessor-cpp">The C Preprocessor (CPP)</h3>
<p>Whenever possible we try to avoid using CPP, as it can hide code from the compiler (which means changes that work on one platform can break the build on another) and code using CPP can be harder to understand.</p>
<p>The following CPP symbols are used throughout the compiler:</p>
<p><strong><code>DEBUG</code></strong><code>:: </code><br />
<code> Used to enables extra checks and debugging output in the compiler. The ASSERT macro (see </code><code>) provides assertions which disappear when DEBUG is not defined. </code></p>
<p><code>However, whenever possible, it is better to use `debugIsOn` from the `Util` module, which is defined to be `True` when `DEBUG` is defined and `False` otherwise.  The ideal way to provide debugging output is to use a Haskell expression &quot;`when debugIsOn $ ...`&quot; to arrange that the compiler will be silent when `DEBUG` is off (unless of course something goes wrong or the verbosity level is nonzero). When option `-O` is used, GHC will easily sweep away the unreachable code.</code></p>
<p><code>As a last resort, debugging code can be placed inside `#ifdef DEBUG`, but since this strategy guarantees that only a fraction of the code is seen be the compiler on any one compilation, it is to be avoided when possible.</code></p>
<p><code>Regarding performance, a good rule of thumb is that `DEBUG` shouldn't add more than about 10-20% to the compilation time. This is the case at the moment. If it gets too expensive, we won't use it. For more expensive runtime checks, consider adding a flag - see for example `-dcore-lint`.</code></p>
<p><strong>Trap, pitfall for using the ASSERT macro</strong>:</p>
<p>The ASSERT macro uses CPP, and if you are unwise enough to try to write assertions using primed variables (), one possible outcome is that CPP silently fails to expand the ASSERT, and you get this very baffling error message:  Now you can Google for this error message :-)</p>
<p><strong><code>GHCI</code></strong><code>:: </code><br />
<code> Enables GHCi support, including the byte code generator and interactive user interface. This isn't the default, because the compiler needs to be bootstrapped with itself in order for GHCi to work properly. The reason is that the byte-code compiler and linker are quite closely tied to the runtime system, so it is essential that GHCi is linked with the most up-to-date RTS. Another reason is that the representation of certain datatypes must be consistent between GHCi and its libraries, and if these were inconsistent then disaster could follow. </code></p>
<h3 id="platform-tests">Platform tests</h3>
<p>Please refer to [wiki:Commentary/PlatformNaming Platforms and Conventions] wiki page for an overview of how to handle target specific code in GHC.</p>
<h2 id="tabs-vs-spaces">Tabs vs Spaces</h2>
<p>GHCs source code is indented with a mixture of tabs and spaces, and is standardised on a tabstop of 8.</p>
<p>Most of the Haskell source code in GHC is free of tabs. We'd like to move away from tabs in the long term, and so a git hook on darcs.haskell.org will reject series of commits that add tabs to a file that is currently tab-free. (Note that there are no restrictions on adding tabs to a file already containing them.)</p>
<p>In order to avoid angering this git hook, you should set your editor to indent using spaces rather than tabs:</p>
<p><code> * In Emacs, add `(setq-default indent-tabs-mode nil)` to your `.emacs` file (</code><a href="http://cscs.umich.edu/~rlr/Misc/emacs_tabs.htm"><code>more</code> <code>discussion</code></a><code>)</code><br />
<code> * In Sublime Text, save the following to files at `Packages/User/Haskell.sublime-settings` and `Packages/User/Literate Haskell.sublime-settings`:</code></p>
<p></p>
<p><code> * In !TextMate, in the tabs pop-up menu at the bottom of the window, select &quot;Soft Tabs&quot;, as show in the following screenshot where the blue rectangle is:</code></p>
<p><code> </code><a href="Image(TextMate-tabs-menu.png)" title="wikilink"><code>Image(TextMate-tabs-menu.png)</code></a><code>  </code></p>
<p><code>Alternatively, open the Bundle Editor and add a new Preference called Indentation to the bundle editor. Give it the following contents:</code></p>
<p></p>
<h1 id="coercions-in-ghcs-core-language">Coercions in GHC's core language</h1>
<p>Ever since coercions were introduced into GHC's Core language I have treated</p>
<p><code>* Coercions like types</code><br />
<code>* Coercion variables like type variables</code></p>
<p>In particular, casts, coercion applications, and coercion abstractoins are all erased before we generate code.</p>
<p>I now think that this is the wrong approach. This note describes why.</p>
<h2 id="difficulties-with-the-current-approach">Difficulties with the current approach</h2>
<p>Ther are two problems with the current approach</p>
<p><code>* Equality evidence variables (&quot;type variables&quot;) are treated differently to dictionary evidence variables (&quot;term varaibles&quot;). This leads to lots of tiresome non-uniformities.</code><br />
<code>* In an abstraction `/\a\x:a.e` the type variable `a` can appear in the type of a term-variable binder `x`.  In contrast `x` can't appear in the type of another binder.  Coercion binders behave exactly like term binders in this way, and quite unlike type binders.</code><br />
<code>* More seriously, we don't have a decent way to handle superclass equalities.</code></p>
<p>The last problem is the one that triggered this note, and needs a bit more explanation. Consider  The dictionary for C looks like this:  Now imagine typechecking a function like this  The Core program we generate looks something like this:  The `nd` binding extracts the `Num` superclass dictionary from the `C` dictionary; the case expression is called a <em>superclass selector</em>.</p>
<p>Now suppose that we needed to use the equality superclass rather than the `Num` superclass:  The obvious translation would look like this:  But Core doesn't (currently) have a let-binding form that binds a coercion variable, and whose right-hand side is a term (in this example, a case expression) rather than a literal coercion! So the current plan is to generate this instead:  This non-uniformity of equality and dictionary evidence is extremely awkward in the desugarer. Moreover, it means that we can't abstract the superclass selector; we'd really like to have:  And it interacts poorly with the class-op rules that GHC uses to simplify dictinary selectors. Imagine the call  ...unfinished...</p>
<h2 id="main-proposal">Main proposal</h2>
<p>Recall our basic types  Note that</p>
<p><code>* `Var` can be a type variable, coercion variable, or term variable.  You can tell which with a dynamic test (e.g. `isId :: Var -&gt; Bool`).</code></p>
<p><code>* `Lam` is used for type abstractions, coercion abstractions, and value abstractions.  The `Var` can tell you which.</code></p>
<p><code>* Type applications (in a term) look like `(App f (Type t))`.  The `(Type t)` part must literally appear there,  with no intervening junk.  This is not statically enforced, but it turns out to be much more convenient than having a constructor `TyApp CoreExpr Type`.</code></p>
<p>OK now the new proposal is to <em>treat equality evidence just like any other sort of evidence</em>.</p>
<p><code>  * A coercion variable is treated like term-level identifier, not a type-level identifier. (More on what that means below.)</code></p>
<p><code>  * A coercion is an `CoreExpr`, of form `Coercion g`, whose type is `(s ~ t)`, of form `PredTy (EqPred s t)`.</code></p>
<p><code>  * Unlike type applications, coercion applications are not required to have a `(Coercion g)` as the argument.  For example, suppose we have</code></p>
<p></p>
<p><code>  Then the term `(f x (id (x~Int) c))` would be fine. Notice that the coercion argument is an appplication of the identity function.  (Yes it's a bit contrived.)  In `CoreExpr` form it would look like:</code></p>
<p></p>
<p><code>  * Similarly a let-binding can bind a coercion</code></p>
<p></p>
<p><code>  * Coercion application is call-by value.  Ditto let-bindings.  You must have the evidence before calling the function.</code><br />
<br />
<code>  * So it doesn't make sense to have recursive coercion bindings.</code></p>
<p><code>  * If we see `Let (NonRec c (Coercion g)) e` we can substitute `(Coercion g)` for any term-level occurrences of `c` in the term `e`, and `g` for `c` in any occurrences of `c` in coercions inside `e`.  (This seems a bit messy.)</code></p>
<h1 id="parsing-of-command-line-arguments">Parsing of command line arguments</h1>
<p>GHC's many flavours of command line flags make the code interpreting them rather involved. The following provides a brief overview of the processing of these options. Since the addition of the interactive front-end to GHC, there are two kinds of flags: static and dynamic. Static flags can only be set once on the command line. They remain the same throughout the whole GHC session (so for example you cannot change them within GHCi using `:set` or with `OPTIONS_GHC` pragma in the source code). Dynamic flags are the opposite: they can be changed in GHCi sessions using `:set` command or `OPTIONS_GHC` pragma in the source code. There are few static flags and it is likely that in the future there will be even less. Thus, you won't see many static flag references in the source code, but you will see a lot of functions that use dynamic flags.</p>
<p>Command line flags are described by Flag data type defined in <a href="GhcFile(compiler/main/CmdLineParser.hs)" class="uri" title="wikilink">GhcFile(compiler/main/CmdLineParser.hs)</a>:</p>
<p></p>
<p>This file contains functions that actually parse the command line parameters.</p>
<h2 id="static-flags">Static flags</h2>
<p>Static flags are managed by functions in <a href="GhcFile(compiler/main/StaticFlags.hs)" class="uri" title="wikilink">GhcFile(compiler/main/StaticFlags.hs)</a>.</p>
<p>Function `parseStaticFlags ::` is an entry point for parsing static flags. It is called by the `main :: IO ()` function of GHC in <a href="GhcFile(ghc/Main.hs)" class="uri" title="wikilink">GhcFile(ghc/Main.hs)</a>. Two global IORefs are used to parse static flags: `v_opt_C_ready` and `v_opt_C`. These are defined using `GLOBAL_VAR` macro from <a href="GhcFile(compiler/HsVersions.h)" class="uri" title="wikilink">GhcFile(compiler/HsVersions.h)</a>. First IORef is a flag that checks whether the static flags are parsed at the right time. Initialized to `False`, it is set to `True` after the parsing is done. `v_opt_C` is a `[String]` used to store parsed flags (see `addOpt` and `removeOpt` functions).</p>
<p>In <a href="GhcFile(compiler/main/StaticFlags.hs)" class="uri" title="wikilink">GhcFile(compiler/main/StaticFlags.hs)</a>, `flagsStatic :: [Flag IO]` defines a list of static flags and what actions should be taken when these flags are encountered (see `Flag` data type above). It also contains some helper functions to check whether particular flags have been set. Functions `staticFlags :: [String]` and `packed_staticFlags :: [FastString]` return a list of parsed command line static flags, provided that parsing has been done (checking the value of `v_opt_C_ready`).</p>
<h2 id="dynamic-flags">Dynamic flags</h2>
<p>They are managed by functions in <a href="GhcFile(compiler/main/DynFlags.hs)" class="uri" title="wikilink">GhcFile(compiler/main/DynFlags.hs)</a> file. Looking from the top you will find data types used to described enabled dynamic flags: `DumpFlag`, `GeneralFlag`, `WarningFlag`, `Language`, `SafeHaskellMode`, `ExtensionFlag` and finally `DynFlags`. Function `defaultDynFlags :: Settings -&gt; DynFlags` initializes some of the flags to default values. Available dynamic flags and their respective actions are defined by `dynamic_flags :: [Flag (CmdLineP DynFlags)]`. Also, `fWarningFlags :: [FlagSpec WarningFlag]`, `fFlags :: [FlagSpec GeneralFlag]`, `xFlags :: [FlagSpec ExtensionFlag]` and a few more smaller functions define even more flags needed for example for language extensions, warnings and other things. These flags are descibred by the data type `FlagSpec f`:</p>
<p> Flags described by `FlagSpec` can be reversed, e.g. flags that start with `-f` prefix are reversed by using `-fno-` prefix instead.</p>
<h1 id="the-ghc-commentary">The GHC Commentary</h1>
<p>This tree of wiki pages is a &quot;commentary&quot; on the GHC source code. It contains all the explanatory material that doesn't belong in comments in the source code itself, because the material is wide-ranging, usually covers multiple source files, and is more architectural in nature. The commentary can also be considered a design document for GHC.</p>
<p>For the beginners there is [wiki:Newcomers a short getting started guide].</p>
<p>For the dedicated, there are [wiki:AboutVideos videos of Simon and Simon giving an overview of GHC], at the 2006 [wiki:Hackathon GHC Hackathon].</p>
<p>Also check out the [wiki:ReadingList GHC Reading List], which gives lots of background reading that will help you understand the actual implementation. Here's <a href="http://www.stephendiehl.com/posts/essential_compilers.html">another reading list</a> from Stephen Diehl.</p>
<h2 id="editing-the-commentary">Editing the Commentary</h2>
<p>Please feel free to add material to the rest of the wiki: don't worry too much about accuracy (in due course someone will edit your contribution). When unsure though please indicate this and its best to ask on the GHC mailing list so you can correct the commentary. Please give some thought to where in the commentary your contribution belongs. GHC has an older commentary (non wiki based) that read like a single coherent narrative, made sure to define terms before using them, and introduced concepts in the order which made them easiest to understand. Please do try to preserve those properties in this wiki commentary. If you're unsure or in a hurry, consider creating a wiki page outside the commentary and linking to it from the commentary (or the &quot;contributed documentation&quot; section below).</p>
<p>Try to link to source files as much as possible by using this macro: . Also try to add appropriate links to other parts of the commentary.</p>
<h2 id="contents">Contents</h2>
<p><code>* [wiki:Commentary/GettingStarted Getting Started]</code><br />
<code>  * [wiki:Commentary/SourceTree Source Tree Roadmap]</code><br />
<code>  * [wiki:Commentary/ModuleStructure Module Structure]</code><br />
<code>  * [wiki:Commentary/CodingStyle Coding Style]</code><br />
<code>  * [wiki:Commentary/Abbreviations Abbreviations in GHC]</code><br />
<code>  * [wiki:Commentary/PlatformNaming Platforms and their Naming Convention]</code></p>
<p><code>* [wiki:Commentary/Compiler The Compiler]</code></p>
<p><code>* [wiki:Commentary/Libraries The Libraries on which GHC depends]</code><br />
<code>  * [wiki:Commentary/Libraries/Integer The Integer libraries (`integer-gmp` and `integer-simple`)]</code></p>
<p><code>* [wiki:Commentary/Rts The Runtime System (RTS)]</code><br />
<code>   * [wiki:Commentary/Rts/Conventions RTS Coding Conventions]</code><br />
<code>   * [wiki:Commentary/Rts/HaskellExecution The Haskell Execution Model]</code><br />
<code>   * [wiki:Commentary/Rts/Storage The memory layout of heap and stack objects]</code><br />
<br />
<code>* Cross-cutting concerns: topics which span both the compiler and the runtime system</code><br />
<code>   * [wiki:Commentary/Profiling Profiling]</code><br />
<code>   * [wiki:Commentary/Compiler/WiredIn Wired-in and known-key things]</code><br />
<code>   * [wiki:Commentary/PrimOps Primitive Operations (PrimOps)]</code><br />
<code>   * [wiki:Commentary/Packages The Package System]</code><br />
<br />
<code>* [wiki:Commentary/UserManual The User Manual] (formatting guidelines etc)</code></p>
<h2 id="contributed-documentation">Contributed Documentation</h2>
<p>The above commentary covers the source code of GHC. For material that doesn't concern this topic (such as proposals, work-in-progress and status reports) or that don't fit into the existing structure, you will find them below. Feel free to add new material here but please categorise it correctly.</p>
<p><code>* General Notes on the GHC compiler</code><br />
<code>  * Edward Yang's blog post about </code><a href="http://blog.ezyang.com/2011/04/tracing-the-compilation-of-hello-factorial/"><code>the</code> <code>entire</code> <code>complilation</code> <code>pipeline</code> <code>for</code> <code>`factorial`</code></a><br />
<code>  * [wiki:AddingNewPrimitiveOperations New Prim Ops]: How to add new primitive operations to GHC Haskell.</code><br />
<code>  * [wiki:ReplacingGMPNotes Replacing GMP]: Notes from an effort to replace GMP with another Bignum library.</code><br />
<code>  * [wiki:ExternalCore External Core]: Describes the process of bringing External Core up to speed. Once finished, this will simply describe what External Core is, and how it works. </code><br />
<code>  * </code><a href="http://sourceforge.net/apps/mediawiki/developers/index.php?title=ScrapYourBoilerplate"><code>The</code> <code>Scrap</code> <code>your</code> <code>boilerplate</code> <code>homepage</code></a><code>.</code><br />
<code>  * [wiki:Commentary/Compiler/OptOrdering Optimisation Ordering] Describe the ordering and interaction of optimisation passes (Old).</code><br />
<code>  * </code><a href="https://github.com/takenobu-hs/haskell-ghc-illustrated"><code>GHC</code> <code>Illustrated</code></a><code> (follow the PDF link), a very insightful tutorial on GHC's internals.</code><br />
<code>  * </code><a href="https://ocharles.org.uk/blog/pages/2014-12-01-24-days-of-ghc-extensions.html"><code>Ollie</code> <code>Charles's</code> <code>24</code> <code>days</code> <code>of</code> <code>GHC</code> <code>Extensions</code></a><code>, and </code><a href="http://augustss.blogspot.com/2014/12/a-commentary-on-24-days-of-ghc.html"><code>Lennart</code> <code>Augstsson's</code> <code>commentary</code></a><br />
<code>    * </code><a href="https://ocharles.org.uk/blog/posts/2014-12-01-24-days-of-ghc-extensions.html"><code>Welcome</code></a><br />
<code>    * </code><a href="https://ocharles.org.uk/blog/guest-posts/2014-12-23-static-pointers.html"><code>Static</code> <code>Pointers</code></a><br />
<code>    * </code><a href="https://ocharles.org.uk/blog/guest-posts/2014-12-22-template-haskell.html"><code>Template</code> <code>Haskell</code></a><br />
<code>    * </code><a href="https://ocharles.org.uk/blog/guest-posts/2014-12-21-arrows.html"><code>Arrows</code></a><br />
<code>    * </code><a href="https://ocharles.org.uk/blog/guest-posts/2014-12-20-scoped-type-variables.html"><code>Scoped</code> <code>Type</code> <code>Variables</code></a><br />
<code>    * </code><a href="https://ocharles.org.uk/blog/guest-posts/2014-12-19-existential-quantification.html"><code>Existential</code> <code>Quantification</code></a><br />
<code>    * </code><a href="https://ocharles.org.uk/blog/guest-posts/2014-12-18-rank-n-types.html"><code>Rank</code> <code>N</code> <code>Types</code></a><br />
<code>    * </code><a href="https://ocharles.org.uk/blog/posts/2014-12-17-overloaded-strings.html"><code>Overloaded</code> <code>Strings</code></a><br />
<code>    * </code><a href="https://ocharles.org.uk/blog/posts/2014-12-16-derive-generic.html"><code>DeriveGeneric</code></a><br />
<code>    * </code><a href="https://ocharles.org.uk/blog/guest-posts/2014-12-15-deriving.html"><code>Deriving</code></a><br />
<code>    * </code><a href="https://ocharles.org.uk/blog/posts/2014-12-14-functional-dependencies.html"><code>Functional</code> <code>Dependencies</code></a><br />
<code>    * </code><a href="https://ocharles.org.uk/blog/posts/2014-12-13-multi-param-type-classes.html"><code>Multi-parameter</code> <code>Type</code> <code>Classes</code></a><br />
<code>    * </code><a href="https://ocharles.org.uk/blog/posts/2014-12-12-type-families.html"><code>Type</code> <code>Families</code></a><br />
<code>    * </code><a href="https://ocharles.org.uk/blog/posts/2014-12-11-implicit-params.html"><code>Implicit</code> <code>Parameters</code></a><br />
<code>    * </code><a href="https://ocharles.org.uk/blog/posts/2014-12-10-nullary-type-classes.html"><code>Nullary</code> <code>Type</code> <code>Classes</code></a><br />
<code>    * </code><a href="https://ocharles.org.uk/blog/posts/2014-12-09-recursive-do.html"><code>Recursive</code> <code>Do</code></a><br />
<code>    * </code><a href="https://ocharles.org.uk/blog/posts/2014-12-08-type-operators.html"><code>Type</code> <code>Operators</code></a><br />
<code>    * </code><a href="https://ocharles.org.uk/blog/guest-posts/2014-12-07-list-comprehensions.html"><code>List</code> <code>Comprehensions</code></a><br />
<code>    * </code><a href="https://ocharles.org.uk/blog/guest-posts/2014-12-06-rebindable-syntax.html"><code>Rebindable</code> <code>Syntax</code></a><br />
<code>    * </code><a href="https://ocharles.org.uk/blog/posts/2014-12-05-bang-patterns.html"><code>Bang</code> <code>Patterns</code></a><br />
<code>    * </code><a href="https://ocharles.org.uk/blog/posts/2014-12-04-record-wildcards.html"><code>Record</code> <code>Wildcards</code></a><br />
<code>    * </code><a href="https://ocharles.org.uk/blog/posts/2014-12-03-pattern-synonyms.html"><code>Pattern</code> <code>Synonyms</code></a><br />
<code>    * </code><a href="https://ocharles.org.uk/blog/posts/2014-12-02-view-patterns.html"><code>View</code> <code>Patterns</code></a><br />
<code>    * </code><a href="https://ocharles.org.uk/blog/posts/2014-12-24-conclusion.html"><code>Thanks</code></a><br />
<code>  * [wiki:Commentary/Rts/CompilerWays]: Compiler </code><em><code>ways</code></em><code> in GHC, what, how, and where</code></p>
<p><code>* Notes on implemented GHC features:</code><br />
<code>  * </code><a href="https://www.fpcomplete.com/tutorial-preview/4431/z0KpB0ai2R"><code>Evaluation</code> <code>order</code> <code>and</code> <code>state</code> <code>tokens</code></a><code>: notes written by Michael Snoyberg in response to #9390.</code><br />
<code>  * [wiki:FoldrBuildNotes Notes on fusion] (eg foldr/build)</code><br />
<code>  * [wiki:OverloadedLists Overloaded list syntax] allows you to use list notation for things other than lists.</code><br />
<code>  * [wiki:GhcKinds Kind polymorphism and data type promotion]</code><br />
<code>  * [wiki:KindFact A kind for class constraints. Implemented as ConstraintKinds]</code><br />
<code>  * [wiki:Commentary/Compiler/Backends/LLVM LLVM back end]</code><br />
<code>  * [wiki:Commentary/Compiler/GenericDeriving Support for generic programming]</code><br />
<code>  * [wiki:TemplateHaskell Notes about Template Haskell]</code><br />
<code>  * [wiki:RewriteRules Rewrite Rules]: Notes about the implementation of RULEs in GHC</code><br />
<code>  * [wiki:MonadComprehensions Monad Comprehensions]: Translation rules and some implementation details </code><br />
<code>  * [wiki:HaddockComments Haddock]: Some notes about how the Haddock comment support is implemented.  </code><br />
<code>  * [wiki:IntermediateTypes Intermediate Types]: Notes about the type system of GHC's new intermediate language (in the HEAD since ICFP'06)  </code><br />
<code>  * [wiki:TypeFunctions Type families/type functions]: Notes concerning the implementation of type families, associated types, and equality constraints as well as the extension of the type checker with a contraint solver for equality constraints.</code><br />
<code>  * [wiki:Commentary/Compiler/SeqMagic Magic to do with `seq` and friends]</code><br />
<code>  * [wiki:NewPlugins Compiler plug-ins]</code><br />
<code>  * [wiki:MemcpyOptimizations memcpy/memmove/memset optimizations]  </code><br />
<code>  * [wiki:BackEndNotes Backend Ideas]: Some ideas and notes about the back end.</code><br />
<code>  * [wiki:Commentary/Compiler/NewCodeGen Notes about the new code generator]</code><br />
<code>  * [wiki:Commentary/Compiler/HooplPerformance A record of improvements made to the performance of the Hoopl library for dataflow optimisation]</code><br />
<code>  * [wiki:DataParallel DPH]: Notes about the implementation of Data Parallel Haskell</code><br />
<code>  * [wiki:SafeHaskell Safe Haskell]: The design of the GHC Safe Haskell extension</code><br />
<code>  * [wiki:SQLLikeComprehensions SQL-Like Comprehensions]: Notes on SPJs &quot;Comprehensive Comprehensions&quot; (!TransformComprehensions)</code><br />
<code>  * [wiki:DeferErrorsToRuntime Deferring compilation type errors to runtime (`-fdefer-type-errors`)]</code><br />
<code>  * [wiki:Commentary/Compiler/Demand Demand analyser] Notes on the meanings, worker-wrapper splitting of demand signatures and relevant components of the compiler</code><br />
<code>  * [wiki:NewAxioms Closed type families]</code><br />
<code>  * [wiki:OneShot] The magic `oneShot` function.</code><br />
<code>  * [wiki:Commentary/Compiler/DeriveFunctor Deriving Functor, Foldable, and Traversable]</code></p>
<p><code>* Notes on proposed or in progress (but out of tree) GHC compiler features:</code><br />
<code>  * [wiki:LanguageStrict Making Haskell strict]</code><br />
<code>  * [wiki:PatternMatchCheck Improving pattern-match overlap and exhaustiveness checks]</code><br />
<code>  * [wiki:GhcAstAnnotations Source-locations on HsSyn]</code><br />
<code>  * [wiki:CabalDependency How GHC inter-operates with Cabal] and [wiki:Backpack]</code><br />
<code>  * [wiki:StaticValues] and ticket #7015</code><br />
<code>  * [wiki:PartialTypeSignatures Partial type signatures] and its ticket #9478</code><br />
<code>  * [wiki:LateLamLift Late lambda-lifting], and its ticket #9476</code><br />
<code>  * [wiki:Roles Roles in Haskell]</code><br />
<code>  * [wiki:DependentHaskell Dependent types in Haskell]</code><br />
<code>  * [wiki:NestedCPR Nested CPR analysis]</code><br />
<code>  * [wiki:TemplateHaskell/Annotations Giving Template Haskell full access to annotations]</code><br />
<code>  * [wiki:FunDeps Checking consistency of functional dependencies]</code><br />
<code>  * [wiki:Commentary/GSoCMultipleInstances Allowing multiple instances of the same package to be installed], each instance having different dependencies</code><br />
<code>  * [wiki:Commentary/Contracts Contracts in Haskell]</code><br />
<code>  * [wiki:Holes Agda-style holes in terms] which supports writing partial programs.</code><br />
<code>  * [wiki:Records Records]</code><br />
<code>  * </code><a href="http://haskell.org/haskellwiki/GHC/CouldAndHPCHaskell"><code>Cloud</code> <code>Haskell</code></a><br />
<code>  * [wiki:PackageLanguage A modular package language for Haskell] Scott Kilpatrick and Derek Dreyer are designing a new </code></p>
<h1 id="compiler-and-runtime-system-ways-in-ghc">Compiler and runtime system ways in GHC</h1>
<p>GHC can compile programs in different <em>ways</em>. For instance, a program might be compiled with profiling enabled (`-prof`), or for multithreaded execution (`-threaded`), or maybe making some debugging tools available (`-debug`, see Debugging/RuntimeSystem for a description).</p>
<p>There are two types of GHC ways, RTS-only ways and full ways.</p>
<ul>
<li><strong>Runtime system (RTS) ways</strong> affect the way that the runtime system is built. As an example, `-threaded` is a runtime system way. When you compile a program with `-threaded`, it will be linked to a (precompiled) version of the RTS with multithreading enabled.</li>
</ul>
<p>Obviously, the compiler's RTS must have been built for this way (the threaded RTS is activated by default BTW). In customised builds, an RTS way can be added in the build configuration `mk/build.mk` (see <a href="GhcFile(mk/build.mk.sample)" class="uri" title="wikilink">GhcFile(mk/build.mk.sample)</a>), by adding its <em>short name</em> to the variable `GhcRTSWays`.</p>
<ul>
<li><strong>Full ways</strong></li>
</ul>
<p>Full compiler ways are ways which affect both the generated code and the runtime system that runs it.</p>
<p>The profiling way `-prof` is such a way. The machine code of a program compiled for profiling differs from a normal version's code by all code that gathers the profiling information, and the runtime system has additional functionality to access and report this information. Therefore, all libraries used in a profiling-enabled program need to also have profiling enabled, i.e. a separate library version for profiling needs to be installed to compile the program with `prof`. (If the library was installed without this profiling version, the program cannot be linked).</p>
<p>In customised builds, a full way is added in the build configuration `mk/build.mk` by adding its tag to the variable `GhcLibWays`.</p>
<h2 id="available-ways-in-a-standard-ghc">Available ways in a standard GHC</h2>
<p>Ways are identified internally by a way name, and enabled by specific compilation flags. In addition, there are short names (tags) for the available ways, mainly used by the build system.</p>
<p>Here is a table of available ways in a standard GHC, as of May 2015.</p>
<p>||=Way flag =||= Way name =||= Tag =||= Type =||= Description =|| ||= - =|| - || `v` || Full || (vanilla way) default || ||=`-threaded` =|| WayThreaded || `thr` || RTS || multithreaded runtime system || ||=`-debug` =|| WayDebug || `debug` || RTS || debugging, enables trace messages and extra checks || ||=`-prof` =|| WayProf || `p` || Full || profiling, enables cost centre stacks and profiling reports || ||=`-eventlog` =|| WayEventLog || `l` || RTS || Event logging (for ghc-events, threadscope, and EdenTV) || ||=`-dyn` =|| WayDyn || `dyn` || Full || Dynamic linking ||</p>
<p>The standard (<em>vanilla</em>) way of GHC has a name (<em>vanilla</em>), but it could (probably?) even be switched off in a custom build if desired. Obviously, the libraries would still need to be built in the vanilla way for all RTS-only ways, so one would need `GhcLibWays=v` when building any other RTS-only way.</p>
<p>The code (see below) contains another way, for Glasgow parallel Haskell, which is currently unmaintained (`WayPar`).</p>
<h3 id="ways-for-parallel-execution-on-clusters-and-multicores">Ways for parallel execution on clusters and multicores</h3>
<p>The parallel Haskell runtime system for Eden (available from <a href="http://github.com/jberthold/ghc" class="uri">http://github.com/jberthold/ghc</a>) defines several RTS-only ways for Eden. All these ways execute the RTS in multiple instances with distributed heaps, they differ in the communication substrate (and consequently in the platform).</p>
<p>||=Way flag =||= Way name =||= Tag =||= Type =||= communication (OS) =|| ||=`-parpvm` =|| WayParPvm ||`pp`|| RTS || PVM (Linux) || ||=`-parmpi` =|| WayParMPI ||`pm`|| RTS || MPI (Linux) || ||=`-parcp` =|| WayParCp ||`pc`|| RTS || OS-native shared memory (Windows/Linux) || ||=`-parms` =|| WayParMSlot ||`ms`|| RTS || Windows mail slots (Windows) ||</p>
<h2 id="combining-ways">Combining ways</h2>
<p>The alert reader might have noticed that combinations like &quot;threaded with dynamic linking&quot; or &quot;profiled with eventlog&quot; are not covered in the table. Some ways can be used together (most prominently, debugging can be used together with any other way), others are mutually excluding each other (like profiling with eventlog).</p>
<p>The allowed combinations are defined inside the compiler, in <a href="GhcFile(compiler/main/DynFlags.hs)" class="uri" title="wikilink">GhcFile(compiler/main/DynFlags.hs)</a>. Which brings us to discussing some of the internals.</p>
<h1 id="internals">Internals</h1>
<p>Ways are defined in <a href="GhcFile(compiler/main/DynFlags.hs)" class="uri" title="wikilink">GhcFile(compiler/main/DynFlags.hs)</a> as a Haskell data structure `Way`.</p>
<p>Function `dynamic_flags` defines the actual flag strings for the ghc invocation (like `-prof`, `-threaded`), which activate the respective `Way`.</p>
<p>The short name tags for ways are defined in `wayTag`. The tags are used in the suffixes of *.o and *.a files for RTS and libraries, for instance `*.p_o` for profiling, `*.l_o` for eventlog.</p>
<p>A number of other functions in there customise behaviour depending on the ways. Note `wayOptc` which sets some options for the C compiler, like `-DTRACING` for the `-eventlog` way.</p>
<p>However, this is not the full truth. For instance, there is no `-DDEBUG` for the debug way here, but the RTS is full of `#ifdef DEBUG`.</p>
<p>In <a href="GhcFile(mk/ways.mk)" class="uri" title="wikilink">GhcFile(mk/ways.mk)</a>, we find all the short names and all combinations enumerated, and some more options are defined here (`WAY_*_HC_OPTS`). These definitions are for the driver script, and pass on the right (long-name) options to the Haskell compiler to activate what is inside DynFlags (like -prof for WAY_p_HC_OPTS). Here we find ```WAY_debug_HC_OPTS= -static -optc-DDEBUG -ticky -DTICKY_TICKY``` so we can learn that ticky profiling is activated by compiling with `debug`.</p>
<p>(TODO be more precise on where the options from ways.mk are used.)</p>
<h1 id="ghc-commentary-the-compiler">GHC Commentary: The Compiler</h1>
<p>The compiler itself is written entirely in Haskell, and lives in the many sub-directories of the <a href="GhcFile(compiler)" class="uri" title="wikilink">GhcFile(compiler)</a> directory.</p>
<p><code>* [wiki:ModuleDependencies Compiler Module Dependencies] (deals with the arcane mutual recursions among GHC's many data types)</code><br />
<code>* [wiki:Commentary/CodingStyle Coding guidelines]</code></p>
<p><code>* [wiki:Commentary/Compiler/CommandLineArgs Command line arguments] </code><br />
<code>* [wiki:Commentary/Pipeline The compilation pipeline]</code></p>
<p><code>* </code><strong><code>Compiling</code> <code>one</code> <code>module:</code> <code>!HscMain</code></strong><br />
<code>  * [wiki:Commentary/Compiler/HscMain Overview] gives the big picture. </code><br />
<code>  * Some details of the [wiki:Commentary/Compiler/Parser parser]</code><br />
<code>  * Some details of the [wiki:Commentary/Compiler/Renamer renamer]</code><br />
<code>  * Some details of the [wiki:Commentary/Compiler/TypeChecker typechecker]</code><br />
<code>  * Some details of the [wiki:Commentary/Compiler/Core2CorePipeline simplifier]</code><br />
<code>  * Some details of the [wiki:Commentary/Compiler/CodeGen code generator] converts STG to Cmm</code><br />
<code>  * [wiki:Commentary/Compiler/Backends Backends] convert Cmm to native code:</code><br />
<code>    * [wiki:Commentary/Compiler/Backends/PprC C code generator]</code><br />
<code>    * [wiki:Commentary/Compiler/Backends/NCG Native code generator]</code><br />
<code>    * [wiki:Commentary/Compiler/Backends/LLVM LLVM backend]</code><br />
<code>    * [wiki:Commentary/Compiler/Backends/GHCi GHCi backend]</code><br />
<code>  * A guide to the [wiki:Commentary/Compiler/GeneratedCode generated assembly code]</code></p>
<p><code>* [wiki:Commentary/Compiler/KeyDataTypes Key data types]</code><br />
<code>  * [wiki:Commentary/Compiler/HsSynType The source language: HsSyn] </code><br />
<code>  * [wiki:Commentary/Compiler/RdrNameType RdrNames, Modules, and OccNames]</code><br />
<code>  * [wiki:Commentary/Compiler/ModuleTypes ModIface, ModDetails, ModGuts]</code><br />
<code>  * [wiki:Commentary/Compiler/NameType Names]</code><br />
<code>  * [wiki:Commentary/Compiler/EntityTypes Entities]: variables, type constructors, data constructors, and classes.</code><br />
<code>  * Types: </code><br />
<code>    * [wiki:Commentary/Compiler/TypeType Types]</code><br />
<code>    * [wiki:Commentary/Compiler/Kinds Kinds]</code><br />
<code>    * [wiki:Commentary/Compiler/FC Equality types and coercions]</code><br />
<code>  * [wiki:Commentary/Compiler/CoreSynType The core language]</code><br />
<code>  * [wiki:Commentary/Compiler/StgSynType The STG language]</code><br />
<code>  * [wiki:Commentary/Compiler/CmmType The Cmm language]</code><br />
<code>  * [wiki:Commentary/Compiler/BackEndTypes Back end types]</code><br />
<br />
<code>* [wiki:Commentary/Compiler/Driver Compiling more than one module at once]</code><br />
<code>* [wiki:Commentary/Compiler/DataTypes How data type declarations are compiled]</code><br />
<code>* [wiki:Commentary/Compiler/API The GHC API]</code><br />
<code>* [wiki:Commentary/Compiler/SymbolNames Symbol names and the Z-encoding]</code><br />
<code>* [wiki:TemplateHaskell/Conversions Template Haskell]</code><br />
<code>* [wiki:Commentary/Compiler/WiredIn Wired-in and known-key things]</code><br />
<code>* [wiki:Commentary/Compiler/Packages Packages]</code><br />
<code>* [wiki:Commentary/Compiler/RecompilationAvoidance Recompilation Avoidance]</code></p>
<p>Case studies:</p>
<p><code>* [wiki:Commentary/Compiler/CaseStudies/Bool Implementation of wired-in Bool data type]</code></p>
<h2 id="overall-structure">Overall Structure</h2>
<p>Here is a block diagram of its top-level structure:</p>
<p><a href="Image(ghc-top.png)" class="uri" title="wikilink">Image(ghc-top.png)</a></p>
<p>The part called [wiki:Commentary/Compiler/HscMain HscMain] deals with compiling a single module. On top of this is built the <strong>compilation manager</strong> (in blue) that manages the compilation of multiple modules. It exports an interface called the <strong>GHC API</strong>. On top of this API are four small front ends:</p>
<p><code>* GHCi, the interactive environment, is implemented in </code><a href="GhcFile(ghc/InteractiveUI.hs)" title="wikilink"><code>GhcFile(ghc/InteractiveUI.hs)</code></a><code> and </code><a href="GhcFile(compiler/main/InteractiveEval.hs)" title="wikilink"><code>GhcFile(compiler/main/InteractiveEval.hs)</code></a><code>. It sits squarely on top of the GHC API.</code><br />
<br />
<code>* </code><code> is almost a trivial client of the GHC API, and is implemented in </code><a href="GhcFile(compiler/main/GhcMake.hs)" title="wikilink"><code>GhcFile(compiler/main/GhcMake.hs)</code></a><code>. </code></p>
<p><code>* </code><code>, the Makefile dependency generator, is also a client of the GHC API and is implemented in </code><a href="GhcFile(compiler/main/DriverMkDepend.hs)" title="wikilink"><code>GhcFile(compiler/main/DriverMkDepend.hs)</code></a><code>. </code></p>
<p><code>* The &quot;one-shot&quot; mode, where GHC compiles each file on the command line separately (eg. </code><code>). This mode bypasses the GHC API, and is implemented</code><br />
<code>  directly on top of [wiki:Commentary/Compiler/HscMain HscMain], since it compiles only one file at a time. In fact, this is all that   </code><br />
<code>  GHC consisted of prior to version 5.00 when GHCi and `--make` were introduced.</code></p>
<p>GHC is packaged as a single binary in which all of these front-ends are present, selected by the command-line flags indicated above. There is a single command-line interface implemented in <a href="GhcFile(ghc/Main.hs)" class="uri" title="wikilink">GhcFile(ghc/Main.hs)</a>.</p>
<p>In addition, GHC is compiled, without its front ends, as a <em>library</em> which can be imported by any Haskell program; see [wiki:Commentary/Compiler/API the GHC API]. Package keys, installed package IDs, ABI hashes, package names and versions, Nix-style hashes, ... there's so many different identifiers, what do they all mean? I think the biggest source of confusion (for myself included) is keeping straight not only what these terms mean, but also what people want them to mean in the future, and what we //actually// care about. So I want to help clarify this a bit, by clearly separating the //problem you are trying to solve// from //how you are solving the problem//.</p>
<p>The content here overlaps with wiki:Commentary/Packages but is looking at the latest iteration of the multi-instances and Backpack work.</p>
<p>See also `Note [The identifier lexicon]` in `compiler/basicTypes/Module.hs`.</p>
<p>Some relevant tickets: #10622</p>
<h2 id="what-problems-do-we-need-to-solve">What problems do we need to solve?</h2>
<p>When we come up with identification schemes for packages, we are trying to solve a few problems:</p>
<p><code>[SYMBOL]::</code><br />
<code>  What symbol names should we put in the binary? (e.g., the &quot;foozm0zi1&quot; in   &quot;foozm0zi1_A_DZCF_closure&quot;)</code><br />
<code>       - It must be unique enough that for all libraries we would</code><br />
<code>         like to be able to link together, there should not be</code><br />
<code>         conflicts.</code><br />
<code>       - HOWEVER, it must be stable enough that if we make a minor</code><br />
<code>         source code change, we don't have to gratuitously recompile</code><br />
<code>         every dependency.</code></p>
<p><code>[ABI]::</code><br />
<code> When can I swap out one compiled package with another WITHOUT recompiling, i.e. what is the ABI of the package? Equal ABIs implies equal symbols, though not vice versa. ABI is usually computed after compilation is complete.</code><br />
<code>       - ABI can serve as correctness condition: if we link against a specific ABI, we can be sure that anything with an equivalent ABI won't cause our package to segfault.</code><br />
<code>       - ABI can also serve as an indirection: we linked against an ABI, anything that is compatible can be hotswapped in without compilation. In practice, this capability is rarely used by users because it's quite hard to compile a package multiple times with the same ABI, because (1) compilation is nondeterministic, and (2) even if no types change, a change in implementation can cause a different exported unfolding, which is ABI relevant.</code></p>
<p><code>[SOURCE]::</code><br />
<code> What is the unit of distribution? In other words, when a maintainer uploads an sdist to Hackage, how do you identify that source tarball?</code><br />
<code>       - On Hackage, a package name plus version uniquely identifies an</code><br />
<code>         sdist.  This is enforced by community standards; in a local</code><br />
<code>         development environment, this may not hold since devs will edit</code><br />
<code>         code without updating the version number. Call this [WEAK SOURCE].</code><br />
<code>       - Alternately, a cryptographic hash of the source code uniquely</code><br />
<code>         identifies the stream of bytes.  This is enforced by math. Call this [STRONG SOURCE].</code></p>
<p><code>[LIBRARY]::</code><br />
<code>  When you build a library, you get an `libfoo.so` file. What identifies an OS level library?</code></p>
<p><code>[NIX]::</code><br />
<code>  What is the full set of source which I can use to reproduceably build a build product?</code><br />
<code>       - In today's Cabal, you could approximate this by taking [WEAK SOURCE] of a package, as well as all of its transitive dependencies. Call this [WEAK NIX].</code><br />
<code>       - The Nix approach is to ensure deterministic builds by taking the hash of the source [STRONG SOURCE] and also recursively including the [NIX] of each direct dependency. Call this [STRONG NIX].</code><br />
<code>       - Note that [ABI] does NOT imply [NIX]; a package might be binary compatible but do something different, and in a Nix model they should be recorded differently.</code></p>
<p><code>[TYPES]::</code><br />
<code>  When are two types the same?  If there are from differing packages, they are obviously different; if they are from the same package, they might still be different if the dependencies were different in each case.</code><br />
<code>       - Types show up in error message, so this is a USER VISIBLE</code><br />
<code>         notion.  Many people have (cogently) argued that this should</code><br />
<code>         be AS SIMPLE as possible, because there's nothing worse</code><br />
<code>         than being told that Data.ByteString.ByteString is not</code><br />
<code>         equal to Data.ByteString.ByteString (because they were from</code><br />
<code>         different packages.)</code></p>
<h2 id="current-mechanisms">Current mechanisms</h2>
<p>Today, we have a lot of different MECHANISMS for identifying these:</p>
<p><code>   Package Name::</code><br />
<code>       Something like &quot;lens&quot;</code></p>
<p><code>   Package Version::</code><br />
<code>       Something like &quot;0.1.2&quot;</code></p>
<p><code>   (Source) Package ID::</code><br />
<code>       Package name plus version.  With Hackage today, this identifies a unit of distribution: given a package ID you can download a source tarball [SOURCE] of a package (but not build it). Pre-GHC 7.10, the package ID was used for library identification, symbols and type-checking ([LIBRARY], [SYMBOL] and [TYPES]), but this is no longer the case.</code></p>
<p><code>   Installed Package ID::</code><br />
<code>       Package name, package version, and the output of ghc --abi-hash.  This is currently used to uniquely identify a built package, although technically it only identifies [ABI].</code></p>
<p><code>   Package Key (new in 7.10)::</code><br />
<code>       Hash of package name, package version, the package keys of all</code><br />
<code>       textual dependencies the package included, and in Backpack</code><br />
<code>       a mapping from hole name to module by package key.</code><br />
<code>       In GHC 7.10 this is used for library identification, symbols and type-checking ([LIBRARY], [SYMBOL] and [TYPES]).  Because it includes package keys of textual dependencies, it also distinguishes between different dependency resolutions, ala [WEAK NIX].</code></p>
<h2 id="new-concepts-for-backpack">New concepts for Backpack</h2>
<p>First, we have to take the concept of an InstalledPackageId and make it more precise, having it identity components rather than packages.</p>
<p><code>   Component ID::</code><br />
<code>       The package name, the package version, the name of the component (blank in the case of the default library component), and the hash of source code sdist tarball, selected Cabal flags (not the command line flags), GHC flags, hashes of direct dependencies of the component (the `build-depends` of the library in the Cabal file).</code></p>
<p>Then in Backpack we have these concepts:</p>
<p><code>   Indefinite/definite unit::</code><br />
<code>       An indefinite unit is a single unit which hasn't been instantiated; a definite unit is one that has an instantiation of its holes.  Units without holes are both definite and indefinite (they can be used for both contexts).</code></p>
<p><code>   Indefinite unit record (in &quot;logical&quot; indefinite unit database)::</code><br />
<code>       An indefinite unit record is the most general result of type-checking a unit without any of its holes instantiated.  It consists of the types of the modules in the unit (ModIfaces) as well as the source code of the unit (so that it can be recompiled into a definite unit). Indefinite unit records can be installed in the &quot;indefinite unit database.&quot;</code></p>
<p><code>   Definite unit record (previously installed package record, in the definite unit database, previously the installed package database)::</code><br />
<code>       A definite unit record is a fully-instantiated unit with its associated library. It consists of the types and objects of the compiled unit; they also contain metadata for their associated package.  Definite unit records can be installed in the &quot;definite unit database&quot; (previously known as the &quot;installed package database.&quot;)</code></p>
<p>To handle these, we need some new identifiers:</p>
<p><code>   Unit Id (previously named Package Key)::</code><br />
<code>       For Backpack units, the unit ID is the component ID plus a mapping from holes to modules (unit key plus module name). For non-Backpack units, the unit ID is equivalent to the component source hash (the hole mapping is empty). These serve the role of [SYMBOL, LIBRARY, TYPES]. (Partially definite unit keys can occur on-the-fly during type checking.) When all of the requirements are filled (so there is no occurrence of HOLE), the unit key serves as the primary key for the installed unit database. (We might call this an &quot;installed unit ID&quot; in this context) The unit ID &quot;HOLE&quot; is a distinguished unit ID, which is for the &quot;hole package&quot;, representing modules which are not yet implemented (there is not actually a unit named hole, it's just a notational convention).</code></p>
<p><code>   Module::</code><br />
<code>       A unit ID plus a module name.</code></p>
<h2 id="features">Features</h2>
<p>There are a number of enhancements proposed for how Cabal handles packages, which have often been conflated together. I want to clearly separate them out here:</p>
<p><code> Non-destructive installs::</code><br />
<code>   If I have package foo-0.2 compiled against bar-0.1, and a different build compiled against bar-0.2, I should be able to put them in the same installed package database.  THIS IS HIGH PRIORITY.</code></p>
<p><code> Views::</code><br />
<code>   If I have package foo compiled against bar-0.1, and baz compiled against bar-0.2, these two packages aren't usable together (modulo private dependencies, see below).  Views are a UI paradigm making it easier for users to work in a universe where foo is available, or a universe where baz is available, but not both simultaneously. Cabal sandboxes are views but without a shared installed package database.  This is lower priority, because if you use cabal-install to get a coherent dependency set, you'll never see both foo and baz at the same time; the primary benefit of this is to assist with direct use of GHC/GHCi, however, it is generally believed that non-destructive installs will make it difficult to use GHC/GHCi by itself.</code></p>
<p><code> Private dependencies::</code><br />
<code>   If I have a package foo-0.2 which depends on a library bar-0.1, but not in any externally visible way, it should be allowed for a client to separately use bar-0.2. This is LOW priority; amusingly, in 7.10, this is already supported by GHC, but not by Cabal.</code></p>
<p><code> Hot swappable libraries::</code><br />
<code>   If I install a library and it's assigned ABI hash 123abc, and then I install a number of libraries that depend on it, hot swappable library means that I can replace that installed library with another version with the same ABI hash, and everything will keep working. This feature is accidentally supported by GHC today, but no one uses it (because ABIs are not stable enough); we are willing to break this mode of use to support other features.</code></p>
<h2 id="constraints">Constraints</h2>
<p>For an implementer, it is best if each problem is solved separately. However, Simon has argued strongly it is best if we REDUCE the amount of package naming concepts. You can see this in pre-7.10 GHC, where the package ID (package name + version) was used fulfill many functions: linker symbols, type identity as well as being a unit of distribution.</p>
<p>So the way I want to go about arguing for the necessity of a given identifier is by showing that it is IMPOSSIBLE (by the intended functions) for a single identifier to serve both roles. Here are the main constraints:</p>
<p><code>- [SYMBOL] and [STRONG NIX]/[STRONG SOURCE] don't play nicely together.  If you modify your source code, a [STRONG NIX/SOURCE] identifier must change; if this means [SYMBOL] changes too, you will have to recompile everything. However, you can work around this problem by using fake identifiers during development to avoid recompilation, recompiling with the correct NIX identifier when it's finally time to install.</code></p>
<p><code>- [SOURCE] and [TYPES] are incompatible under non-destructive installs and private dependencies. With private dependencies (which GHC supports!), I may link against the multiple instances of the same source but compiled against different dependencies; we MUST NOT consider these types to be the same. Note: GHC used to use package ID for both of these; so coherence was guaranteed by requiring destructive installs.</code></p>
<p><code>- [NIX] and [TYPES] are incompatible under Backpack.  In Backpack, a library author may distribute a package with the explicit intent that it may be used in the same client multiple times with different instantiations of its holes; these types must be kept distinct.</code></p>
<h1 id="rts-configurations">RTS Configurations</h1>
<p>The RTS can be built in several different ways, corresponding to global CPP defines. The flavour of the RTS is chosen by GHC when compiling a Haskell program, in response to certain command-line options: , , etc.</p>
<p>The CPP symbols and their corresponding command-line flags are:</p>
<p><code>::</code><br />
<code> Enables profiling.</code><a href="br" title="wikilink"><code>br</code></a><br />
<code> GHC option: </code><a href="br" title="wikilink"><code>br</code></a><br />
<code> RTS suffix: </code></p>
<p><code>::</code><br />
<code> Enables multithreading in the RTS, bound threads, and SMP execution.</code><a href="br" title="wikilink"><code>br</code></a><br />
<code> GHC option: </code><a href="br" title="wikilink"><code>br</code></a><br />
<code> RTS suffix: </code></p>
<p><code>::</code><br />
<code> Enables extra debugging code, assertions, traces, and the </code><code> options.</code><a href="br" title="wikilink"><code>br</code></a><br />
<code> GHC option: </code><a href="br" title="wikilink"><code>br</code></a><br />
<code> RTS suffix: </code></p>
<p><code>::</code><br />
<code> Enables RTS tracing and event logging, see </code><a href="GhcFile(rts/Trace.c)" title="wikilink"><code>GhcFile(rts/Trace.c)</code></a><code>.  Implied by `DEBUG`.</code><a href="br" title="wikilink"><code>br</code></a><br />
<code> GHC option: </code><a href="br" title="wikilink"><code>br</code></a><br />
<code> RTS suffix: </code></p>
<p>So for example,  is the version of the runtime compiled with  and , and will be linked in if you use the  and  options to GHC.</p>
<p>The ways that the RTS is built in are controlled by the  Makefile variable.</p>
<h2 id="combinations">Combinations</h2>
<p>All combinations are allowed. Only some are built by default though; see [source:mk/config.mk.in] to see how the `GhcRTSWays` variable is set.</p>
<h2 id="other-configuration-options">Other configuration options</h2>
<p><code>::</code><br />
<code> Disabled the use of hardware registers for the stack pointer (`Sp`), heap pointer (`Hp`), etc.  This is</code><br />
<code> enabled when building &quot;unregisterised&quot; code, which is controlled by the `GhcUnregisterised` build option.</code><br />
<code> Typically this is necessary when building GHC on a platform for which there is no native code generator</code><br />
<code> and LLVM does not have a GHC calling convention.</code></p>
<p><code>::</code><br />
<code> Enables the use of the RTS &quot;mini-interpreter&quot;, which simulates tail-calls.  Again, this is enabled by</code><br />
<code> `GhcUnregisterised` in the build system.</code></p>
<p><code>::</code><br />
<code> Controls whether the info table is placed directly before the entry code for a closure or return continuation.</code><br />
<code> This is normally turned on if the platform supports it, but is turned off by `GhcUnregisterised`.</code></p>
<h1 id="contracts-for-haskell">Contracts for Haskell</h1>
<h2 id="involved">Involved</h2>
<p><code>* Simon Peyton-Jones</code><br />
<code>* Dimitrios Vytiniotis</code><br />
<code>* Koen Claessen</code><br />
<code>* Charles-Pierre Astolfi</code></p>
<h2 id="overview-1">Overview</h2>
<p>Contracts, just as types, give a specification of the arguments and return values of a function. For example we can give to head the following contract:</p>
<p></p>
<p>Where Ok means that the result of head is not an error/exception as long as the argument isn't.</p>
<p>Any Haskell boolean expression can be used in a contract, for example  is a contract that means that for every a which is an actual integer (not an error), then fac a &gt;= a</p>
<p>We can also use a higher-order contracts:  This contract means that if we apply map to a non-empty list with a function that takes a non-negative integer and returns an positive integer then map returns a list of values without errors.</p>
<p>For a formal introduction, one can read [1].</p>
<h2 id="the-plan">The plan</h2>
<p>Verifying that a function satisfies a given contract is obviously undecidable, but that does not mean that we can't prove anything interesting. Our plan is to translate Haskell programs to first-order logic (with equality) and then use Koen's automated theorem prover to check contract satisfaction. Given that first-order logic is only semi-decidable, the theorem prover can (and in fact does) hang when fed with contracts that are in contradiction with the function definition.</p>
<h2 id="current-status">Current status</h2>
<p>The current status is described in [3] and some code and examples can be found in [2]. Note that given it's just a prototype the input syntax is slightly different from Haskell. In the end, we should get a ghc extension for contracts.</p>
<h2 id="questions">Questions</h2>
<p><code>* Do we need cfness predicate anymore? It was important in the POPL paper but is still relevant?</code><br />
<code>* UNR should be renamed to a less confusing name.</code><br />
<code>* Hoare logic vs liquid types</code><br />
<code>* Semantics &amp; domain theory to prove the correctness of the translation</code><br />
<code>* Unfolding for proving contracts on recursive functions</code></p>
<h2 id="references">References</h2>
<p>[1] : <a href="http://research.microsoft.com/en-us/um/people/simonpj/papers/verify/index.htm" class="uri">http://research.microsoft.com/en-us/um/people/simonpj/papers/verify/index.htm</a> <a href="BR" class="uri" title="wikilink">BR</a> [2] : <a href="https://github.com/cpa/haskellcontracts" class="uri">https://github.com/cpa/haskellcontracts</a> and <a href="https://github.com/cpa/haskellcontracts-examples" class="uri">https://github.com/cpa/haskellcontracts-examples</a> <a href="BR" class="uri" title="wikilink">BR</a> [3] : <a href="https://github.com/cpa/haskellcontracts/blob/master/draft2.pdf" class="uri">https://github.com/cpa/haskellcontracts/blob/master/draft2.pdf</a></p>
<h1 id="the-ghc-commentary-coding-style-guidelines-for-rts-c-code">The GHC Commentary: Coding Style Guidelines for RTS C code</h1>
<h2 id="comments-1">Comments</h2>
<p>These coding style guidelines are mainly intended for use in  and . See [wiki:Commentary/CodingStyle Coding Style Guidelines] for code in .</p>
<p>These are just suggestions. They're not set in stone. Some of them are probably misguided. If you disagree with them, feel free to modify this document (and make your commit message reasonably informative) or mail someone (eg. <script type="text/javascript">
<!--
h='&#104;&#x61;&#x73;&#x6b;&#x65;&#108;&#108;&#46;&#x6f;&#114;&#x67;';a='&#64;';n='&#x67;&#108;&#x61;&#x73;&#x67;&#x6f;&#x77;&#x2d;&#104;&#x61;&#x73;&#x6b;&#x65;&#108;&#108;&#x2d;&#x75;&#x73;&#x65;&#114;&#x73;';e=n+a+h;
document.write('<a h'+'ref'+'="ma'+'ilto'+':'+e+'" clas'+'s="em' + 'ail">'+'&#84;&#104;&#x65;&#32;&#x47;&#72;&#x43;&#32;&#x6d;&#x61;&#x69;&#108;&#x69;&#110;&#x67;&#32;&#108;&#x69;&#x73;&#116;'+'<\/'+'a'+'>');
// -->
</script><noscript>&#84;&#104;&#x65;&#32;&#x47;&#72;&#x43;&#32;&#x6d;&#x61;&#x69;&#108;&#x69;&#110;&#x67;&#32;&#108;&#x69;&#x73;&#116;&#32;&#40;&#x67;&#108;&#x61;&#x73;&#x67;&#x6f;&#x77;&#x2d;&#104;&#x61;&#x73;&#x6b;&#x65;&#108;&#108;&#x2d;&#x75;&#x73;&#x65;&#114;&#x73;&#32;&#x61;&#116;&#32;&#104;&#x61;&#x73;&#x6b;&#x65;&#108;&#108;&#32;&#100;&#x6f;&#116;&#32;&#x6f;&#114;&#x67;&#x29;</noscript>)</p>
<h2 id="references-1">References</h2>
<p>If you haven't read them already, you might like to check the following. Where they conflict with our suggestions, they're probably right.</p>
<p><code>* The C99 standard.  One reasonable reference is </code><a href="http://home.tiscalinet.ch/t_wolf/tw/c/c9x_changes.html"><code>here</code></a><code>.</code></p>
<p><code>* Writing Solid Code, Microsoft Press. (Highly recommended.)</code></p>
<p><code>* Autoconf documentation. See also</code><br />
<code>  </code><a href="http://peti.gmd.de/autoconf-archive/"><code>The</code> <code>autoconf</code> <code>macro</code> <code>archive</code></a><br />
<code>  and </code><a href="http://www.cyclic.com/cyclic-pages/autoconf.html"><code>Cyclic</code> <code>Software's</code> <code>description</code></a><code>.</code></p>
<p><code>* </code><a href="http://www.cs.arizona.edu/~mccann/cstyle.html"><code>Indian</code> <code>Hill</code> <code>C</code> <code>Style</code> <code>and</code> <code>Coding</code> <code>Standards</code></a></p>
<p><code>* </code><a href="http://www.cs.umd.edu/users/cml/cstyle/"><code>A</code> <code>list</code> <code>of</code> <code>C</code> <code>programming</code> <code>style</code> <code>links</code></a></p>
<p><code>* </code><a href="http://www.lysator.liu.se/c/c-www.html"><code>A</code> <code>very</code> <code>large</code> <code>list</code> <code>of</code> <code>C</code> <code>programming</code> <code>links</code></a></p>
<h2 id="portability-issues">Portability issues</h2>
<h3 id="which-c-standard">Which C Standard?</h3>
<p>We try to stick to C99 where possible. We use the following C99 features relative to C89, some of which were previously GCC extensions (possibly with different syntax):</p>
<p><code>* Variable length arrays as the last field of a struct.  GCC has</code><br />
<code>  a similar extension, but the syntax is slightly different: in GCC you</code><br />
<code>  would declare the array as </code><code>, whereas in C99 it is</code><br />
<code>  declared as </code><code>.</code></p>
<p><code>* Inline annotations on functions (see later)</code></p>
<p><code>* Labeled elements in initialisers.  Again, GCC has a slightly</code><br />
<code>  different syntax from C99 here, and we stick with the GCC syntax</code><br />
<code>  until GCC implements the C99 proposal.</code></p>
<p><code>* C++-style comments.  These are part of the C99 standard, and we</code><br />
<code>  prefer to use them whenever possible.</code></p>
<p>In addition we use ANSI-C-style function declarations and prototypes exclusively. Every function should have a prototype; static function prototypes may be placed near the top of the file in which they are declared, and external prototypes are usually placed in a header file with the same basename as the source file (although there are exceptions to this rule, particularly when several source files together implement a subsystem which is described by a single external header file).</p>
<p><code>* We use the following GCC extensions, but surround them with </code><code>:</code><br />
<code>  * Function attributes (mostly just </code><code> and </code><code>)</code><br />
<code>  * Inline assembly.</code></p>
<h3 id="other-portability-conventions">Other portability conventions</h3>
<p><code>* char can be signed or unsigned - always say which you mean</code></p>
<p><code>* Our POSIX policy: try to write code that only uses POSIX</code><br />
<code>  (</code><a href="http://www.opengroup.org/onlinepubs/009695399/toc.htm"><code>IEEE</code> <code>Std</code> <code>1003.1</code></a><code>)</code><br />
<code>  interfaces and APIs.  We used to define </code><code> by</code><br />
<code>  default, but found that this caused more problems than it solved, so</code><br />
<code>  now we require any code that is POSIX-compliant to explicitly say so</code><br />
<code>  by having </code><code> at the top.  Try to do this</code><br />
<code>  whenever possible.</code></p>
<p><code>* Some architectures have memory alignment constraints.  Others don't</code><br />
<code>  have any constraints but go faster if you align things.  These</code><br />
<code>  macros (from </code><code>) tell you which alignment to use</code></p>
<p></p>
<p><code>* Use </code><code>, </code><code> and </code><code> when</code><br />
<code>  reading/writing ints and ptrs to the stack or heap.  Note that, by</code><br />
<code>  definition, </code><code>, </code><code> and </code><code> are the</code><br />
<code>  same size and have the same alignment constraints even if</code><br />
<code>  </code><code> on that platform.</code></p>
<p><code>* Use </code><code>, </code><code>, etc when you need a certain</code><br />
<code>  minimum number of bits in a type.  Use </code><code> and </code><code> when</code><br />
<code>  there's no particular constraint.  ANSI C only guarantees that ints</code><br />
<code>  are at least 16 bits but within GHC we assume they are 32 bits.</code></p>
<p><code>* Use </code><code> and </code><code> for floating point values</code><br />
<code>  which will go on/have come from the stack or heap.  Note that</code><br />
<code>  </code><code> may occupy more than one </code><code>, but it will</code><br />
<code>  always be a whole number multiple.</code></p>
<p><code>* Use </code><code>, </code><code> to read </code><br />
<code>  and </code><code> values from the stack/heap, and</code><br />
<code>  </code><code> / </code><code> to assign</code><br />
<code>  StgFloat/StgDouble values to heap/stack locations.  These macros</code><br />
<code>  take care of alignment restrictions.</code></p>
<p><code>* Heap/Stack locations are always </code><code> aligned; the</code><br />
<code>  alignment requirements of an </code><code> may be more than that</code><br />
<code>  of </code><code>, but we don't pad misaligned </code><br />
<code>  because doing so would be too much hassle (see </code><code> &amp; co</code><br />
<code>  above).</code></p>
<p><code>*  Avoid conditional code like this:</code></p>
<p></p>
<p><code> Instead, add an appropriate test to the configure.ac script and use</code><br />
<code> the result of that test instead.</code></p>
<p></p>
<p><code> The problem is that things change from one version of an OS to</code><br />
<code> another - things get added, things get deleted, things get broken,</code><br />
<code> some things are optional extras.  Using &quot;feature tests&quot; instead of</code><br />
<code> &quot;system tests&quot; makes things a lot less brittle.  Things also tend to</code><br />
<code> get documented better.</code></p>
<h2 id="debuggingrobustness-tricks">Debugging/robustness tricks</h2>
<p>Anyone who has tried to debug a garbage collector or code generator will tell you: &quot;If a program is going to crash, it should crash as soon, as noisily and as often as possible.&quot; There's nothing worse than trying to find a bug which only shows up when running GHC on itself and doesn't manifest itself until 10 seconds after the actual cause of the problem.</p>
<p>We put all our debugging code inside . The general policy is we don't ship code with debugging checks and assertions in it, but we do run with those checks in place when developing and testing. Anything inside  should not slow down the code by more than a factor of 2.</p>
<p>We also have more expensive &quot;sanity checking&quot; code for hardcore debugging - this can slow down the code by a large factor, but is only enabled on demand by a command-line flag. General sanity checking in the RTS is currently enabled with the  RTS flag.</p>
<p>There are a number of RTS flags which control debugging output and sanity checking in various parts of the system when  is defined. For example, to get the scheduler to be verbose about what it is doing, you would say . See  and  for the full set of debugging flags. To check one of these flags in the code, write:  would check the  flag before generating the output (and the code is removed altogether if  is not defined).</p>
<p>All debugging output should go to .</p>
<p>Particular guidelines for writing robust code:</p>
<p><code>* Use assertions.  Use lots of assertions.  If you write a comment</code><br />
<code>  that says &quot;takes a +ve number&quot; add an assertion.  If you're casting</code><br />
<code>  an int to a nat, add an assertion.  If you're casting an int to a</code><br />
<code>  char, add an assertion.  We use the </code><code> macro for writing</code><br />
<code>  assertions; it goes away when </code><code> is not defined.</code></p>
<p><code>* Write special debugging code to check the integrity of your data</code><br />
<code>  structures.  (Most of the runtime checking code is in</code><br />
<code>  </code><code>) Add extra assertions which call this code at</code><br />
<code>  the start and end of any code that operates on your data</code><br />
<code>  structures.</code></p>
<p><code>* When you find a hard-to-spot bug, try to think of some assertions,</code><br />
<code>  sanity checks or whatever that would have made the bug easier to</code><br />
<code>  find.</code></p>
<p><code>* When defining an enumeration, it's a good idea not to use 0 for</code><br />
<code>  normal values.  Instead, make 0 raise an internal error.  The idea</code><br />
<code>  here is to make it easier to detect pointer-related errors on the</code><br />
<code>  assumption that random pointers are more likely to point to a 0</code><br />
<code>  than to anything else.</code></p>
<p></p>
<p><code>* Use </code><code> or </code><code> whenever you write a piece of</code><br />
<code>  incomplete/broken code.</code></p>
<p><code>* When testing, try to make infrequent things happen often.  For</code><br />
<code>  example, make a context switch/gc/etc happen every time a context</code><br />
<code>  switch/gc/etc can happen.  The system will run like a pig but it'll</code><br />
<code>  catch a lot of bugs.</code></p>
<h2 id="syntactic-details">Syntactic details</h2>
<p><code>* Please keep to 80 columns: the line has to be drawn somewhere, and</code><br />
<code>  by keeping it to 80 columns we can ensure that code looks OK on</code><br />
<code>  everyone's screen.  Long lines are hard to read, and a sign that</code><br />
<code>  the code needs to be restructured anyway.</code></p>
<p><code>* An indentation width of 4 is preferred (don't use actual tab characters, use spaces).</code></p>
<p><code>* </code><strong><code>Important:</code></strong><code> Put &quot;redundant&quot; braces or parens in your code.</code><br />
<code>   Omitting braces and parens leads to very hard to spot bugs -</code><br />
<code>   especially if you use macros (and you might have noticed that GHC</code><br />
<code>   does this a lot!)</code></p>
<p><code> In particular, put braces round the body of for loops, while loops,</code><br />
<code> if statements, etc.  even if they &quot;aren't needed&quot; because it's</code><br />
<code> really hard to find the resulting bug if you mess up.  Indent them</code><br />
<code> any way you like but put them in there!</code></p>
<p><code> * When defining a macro, always put parens round args - just in case.</code><br />
<code>   For example, write:</code></p>
<p></p>
<p><code> instead of</code></p>
<p></p>
<p><code>* Don't declare and initialize variables at the same time.</code><br />
<code>  Separating the declaration and initialization takes more lines, but</code><br />
<code>  make the code clearer.</code></p>
<p><code>* Don't define macros that expand to a list of statements.  You could</code><br />
<code>  just use braces as in:</code></p>
<p></p>
<p><code> (but it's usually better to use an inline function instead - see above).</code></p>
<p><code>* Don't even write macros that expand to 0 statements - they can mess</code><br />
<code>  you up as well.  Use the </code><code> macro instead.</code></p>
<p></p>
<p><code>* This code</code></p>
<p></p>
<p><code> looks like it declares two pointers but, in fact, only p is a pointer.</code><br />
<code> It's safer to write this:</code></p>
<p></p>
<p><code> You could also write this:</code></p>
<p></p>
<p><code> but it is preferrable to split the declarations.</code></p>
<p><code>* Try to use ANSI C's enum feature when defining lists of constants</code><br />
<code>  of the same type.  Among other benefits, you'll notice that gdb</code><br />
<code>  uses the name instead of its (usually inscrutable) number when</code><br />
<code>  printing values with enum types and gdb will let you use the name</code><br />
<code>  in expressions you type.</code></p>
<p><code> Examples:</code></p>
<p></p>
<p><code> instead of</code></p>
<p></p>
<p><code> and </code></p>
<p></p>
<p><code> instead of</code></p>
<p></p>
<p><code>* When commenting out large chunks of code, use </code><code> </code><br />
<code>  rather than </code><code> because C doesn't have</code><br />
<code>  nested comments.</code></p>
<p><code>* When declaring a typedef for a struct, give the struct a name as</code><br />
<code>  well, so that other headers can forward-reference the struct name</code><br />
<code>  and it becomes possible to have opaque pointers to the struct.  Our</code><br />
<code>  convention is to name the struct the same as the typedef, but add a</code><br />
<code>  leading underscore.  For example:</code></p>
<p></p>
<p><code>* Do not use </code><code> instead of explicit comparison against </code><br />
<code>  or </code><code>; the latter is much clearer.</code></p>
<p><code>* Please write comments in English.  Especially avoid Klingon.</code></p>
<h2 id="inline-functions">Inline functions</h2>
<p>Use inline functions instead of macros if possible - they're a lot less tricky to get right and don't suffer from the usual problems of side effects, evaluation order, multiple evaluation, etc.</p>
<p><code>* Inline functions get the naming issue right.  E.g. they</code><br />
<code>  can have local variables which (in an expression context)</code><br />
<code>  macros can't.</code></p>
<p><code>* Inline functions have call-by-value semantics whereas macros are</code><br />
<code>  call-by-name.  You can be bitten by duplicated computation if you</code><br />
<code>  aren't careful.</code></p>
<p><code>* You can use inline functions from inside gdb if you compile with</code><br />
<code>  -O0 or -fkeep-inline-functions.  If you use macros, you'd better know</code><br />
<code>  what they expand to.</code></p>
<p><code> However, note that macros can serve as both l-values and r-values and</code><br />
<code> can be &quot;polymorphic&quot; as these examples show:</code></p>
<p></p>
<p>There are three macros to do inline portably. Don't use `inline` directly, use these instead:</p>
<p>`INLINE_HEADER`</p>
<p><code> An inline function in a header file.  This is just like a macro.  We never emit</code><br />
<code> a standalone copy of the function, so it </code><em><code>must</code></em><code> be inlined everywhere.</code></p>
<p>`STATIC_INLINE`</p>
<p><code> An inline function in a C source file.  Again, it is always inlined, and we never</code><br />
<code> emit a standalone copy. </code></p>
<p>`EXTERN_INLINE`</p>
<p><code> A function which is optionally inlined.  The C compiler is told to inline if possible,</code><br />
<code> but we also generated a standalone copy of the function just in case (see source:rts/Inlines.c).</code></p>
<h2 id="source-control-issues">Source-control issues</h2>
<p><code>* Don't be tempted to re-indent or re-organise large chunks of code -</code><br />
<code>  it generates large diffs in which it's hard to see whether anything</code><br />
<code>  else was changed, and causes extra conflicts when moving patches to</code><br />
<code>  another branch.</code><br />
<code>  </code><a href="BR" title="wikilink"><code>BR</code></a><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  If you must re-indent or re-organise, don't include any functional</code><br />
<code>  changes that commit and give advance warning that you're about to do</code><br />
<code>  it in case anyone else is changing that file.  For more details on</code><br />
<code>  source control conventions, see [wiki:WorkingConventions/Git].</code></p>
<p>for file in *; do</p>
<p><code>   iconv -f ascii -t utf-8 &quot;$file&quot; -o &quot;${file%.txt}.wiki&quot;</code></p>
<p>done</p>
<h1 id="copying-gc">Copying GC</h1>
<p>GHC uses copying GC by default, while it requires more memory than [wiki:Commentary/Rts/Storage/GC/Compaction mark/compact], it is faster.</p>
<p>The basic copying scheme is <a href="http://en.wikipedia.org/wiki/Cheney%27s_algorithm">Cheney's Algorithm</a>. Starting from the [wiki:Commentary/Rts/Storage/GC/Roots roots], we visit each live object:</p>
<p><code>* The object is </code><em><code>evacuated</code></em><code> (copied) to its destination generation.   The destination is given by `bd-&gt;dest` pointer in the `bdescr` of the</code><br />
<code>  block in which it lives; typically an object is promoted to the next highest generation, but the basic policy is affected by  [wiki:Commentary/Rts/Storage/GC/Aging aging] and [wiki:Commentary/Rts/Storage/GC/EagerPromotion eager promotion].</code></p>
<p><code>* The header word of the original object is replaced by a </code><em><code>forwarding</code> <code>pointer</code></em><code>.  The forwarding pointer is just the pointer to the new copy, with the least significant bit set to 1 so that forwarding pointers can be distinguished from info table pointers.</code></p>
<p><code>* We scan objects that have been evacuated, and </code><em><code>scavenge</code></em><code> each one.  Scavenging involves evacuating each of the pointers</code><br />
<code>  in the object, replacing each pointer with a pointer to the evacuated copy.</code></p>
<p><code>* When there are no more objects to be scavenged, the algorithm is complete.  The memory containing the evacuated objects is retained, all the memory containing the old objects and forwarding pointers is discarded.</code></p>
<p>Evacuation is implemented in the file <a href="GhcFile(rts/sm/Evac.c)" class="uri" title="wikilink">GhcFile(rts/sm/Evac.c)</a>.<a href="br" class="uri" title="wikilink">br</a> Scavenging is implemented in the file <a href="GhcFile(rts/sm/Scav.c)" class="uri" title="wikilink">GhcFile(rts/sm/Scav.c)</a>.<a href="br" class="uri" title="wikilink">br</a></p>
<p>The principle APIs are</p>
<p><code>`void evacuate (StgClosure **p)`::</code><br />
<code>  which evacuates the object pointed to by the pointer at `p`, and updates `p` to point to the new location.</code></p>
<p><code>`void scavenge_block (bdescr *bd)`::</code><br />
<code>  which scavenges all the objects in the block `bd` (objects between `bd-&gt;u.scan` and `bd-&gt;free` are assumed to</code><br />
<code>  be unscavenged so far).</code></p>
<p>= Core-to-Core optimization pipeline</p>
<p>After the source program has been [wiki:Commentary/Compiler/TypeChecker typechecked] it is desugared into GHC's intermediate language [wiki:Commentary/Compiler/CoreSynType Core]. The Core representation of a program is then optimized by a series of correctness preserving Core-to-Core passes. This page describes the overall structure of the Core-to-Core optimization pipeline. Detailed descriptions of optimizations are available [wiki:Commentary/Compiler/Core2CorePipeline#Furtherreading in the published papers]. An overview of the whole compiler pipeline is available [wiki:Commentary/Compiler/HscMain here].</p>
<p>== Optimizations during desugaring</p>
<p>At the end of desugaring we run the `simpleOptPgm` function that performs some simple optimizations: eliminating dead bindings, and inlining non-recursive bindings that are used only once or where the RHS is trivial. The rest of Core optimisations is performed by the Core-to-Core pipeline.</p>
<p>== The pipeline</p>
<p>The structure of the Core-to-Core pipeline is determined in the `getCoreToDo` function in the <a href="GhcFile(compiler/simplCore/SimplCore.lhs)" class="uri" title="wikilink">GhcFile(compiler/simplCore/SimplCore.lhs)</a> module. Below is an ordered list of performed optimisations. These are enabled by default with `-O1` and `-O2` unless the description says a specific flag is required. The simplifier, which the pipeline description below often refers to, is described in detail in [wiki:Commentary/Compiler/Core2CorePipeline#Simplifier the next section].</p>
<p><code> * </code><strong><code>Static</code> <code>Argument</code> <code>Transformation</code></strong><code>: tries to remove redundant arguments to recursive calls, turning them into free variables in those calls.  Only enabled with `-fstatic-argument-transformation`.  If run this pass is preceded with a &quot;gentle&quot; run of the simplifier.</code></p>
<p><code> * </code><strong><code>Vectorisation</code></strong><code>: run the [wiki:DataParallel Data Parallel Haskell] [wiki:DataParallel/Vectorisation vectoriser]. Only enabled with `-fvectorise`. TODO: does `-Odph` imply `fvectorise`?</code></p>
<p><code> * </code><strong><code>Simplifier,</code> <code>gentle</code> <code>run</code></strong></p>
<p><code> * </code><strong><code>Specialisation</code></strong><code>: specialisation attempts to eliminate overloading. More details can be found in the comments in </code><a href="GhcFile(compiler/specialise/Specialise.lhs)" title="wikilink"><code>GhcFile(compiler/specialise/Specialise.lhs)</code></a><code>.</code></p>
<p><code> * </code><strong><code>Full</code> <code>laziness,</code> <code>1st</code> <code>pass</code></strong><code>: floats let-bindings outside of lambdas. This pass includes annotating bindings with level information and then running the float-out pass. In this first pass of the full laziness we don't float partial applications and bindings that contain free variables - this will be done by the second pass later in the pipeline. See &quot;Further Reading&quot; section below for pointers where to find the description of the full laziness algorithm.</code></p>
<p><code> * </code><strong><code>Float</code> <code>in,</code> <code>1st</code> <code>pass</code></strong><code>: the opposite of full laziness, this pass floats let-bindings as close to their use sites as possible. It will not undo the full laziness by sinking bindings inside a lambda, unless the lambda is one-shot. At this stage we have not yet run the demand analysis, so we only have demand information for things that we imported.</code></p>
<p><code> * </code><strong><code>Simplifier,</code> <code>main</code> <code>run</code></strong><code>: run the main passes of the simplifier (phases 2, 1 and 0). Phase 0 is run with at least 3 iterations.</code></p>
<p><code> * </code><strong><code>Call</code> <code>arity</code></strong><code>: attempts to eta-expand local functions based on how they are used. If run, this pass is followed by a 0 phase of the simplifier. See Notes in </code><a href="GhcFile(compiler/simplCore/CallArity.hs)" title="wikilink"><code>GhcFile(compiler/simplCore/CallArity.hs)</code></a><code> and the relevant paper.</code></p>
<p><code> * </code><strong><code>Demand</code> <code>analysis,</code> <code>1st</code> <code>pass</code></strong><code> (a.k.a. strictness analysis): runs the demand analyser followed by worker-wrapper transformation and 0 phase of the simplifier. This pass tries to determine if some expressions are certain to be used and whether they will be used once or many times (cardinality analysis). We currently don't have means of saying that a binding is certain to be used many times. We can only determine that it is certain to be one-shot (ie. used only once) or probable to be one shot. Demand analysis pass only annotates Core with strictness information. This information is later used by worker/wrapper pass to perform transformations. CPR analysis is also done during demand analysis.</code></p>
<p><code> * </code><strong><code>Full</code> <code>laziness,</code> <code>2nd</code> <code>pass</code></strong><code>: another full-laziness pass. This time partial applications and functions with free variables are floated out.</code></p>
<p><code> * </code><strong><code>Common</code> <code>Sub-expression-elimination</code></strong><code>: eliminates expressions that are identical.</code></p>
<p><code> * </code><strong><code>Float</code> <code>in,</code> <code>2nd</code> <code>pass</code></strong></p>
<p><code> * </code><strong><code>Check</code> <code>rules,</code> <code>1st</code> <code>pass</code></strong><code>: this pass is not for optimisation but for troubleshooting the rules. It is only enabled with `-frule-check` flag that accepts a string pattern. This pass looks for rules beginning with that string pattern that could have fired but didn't and prints them to stdout.</code></p>
<p><code> * </code><strong><code>Liberate</code> <code>case</code></strong><code>: unrolls recursive functions once in their own RHS, to avoid repeated case analysis of free variables. It's a bit like the call-pattern specialisation but for free variables rather than arguments. Followed by a phase 0 simplifier run. Only enabled with `-fliberate-case` flag.</code></p>
<p><code> * </code><strong><code>Call-pattern</code> <code>specialisation</code></strong><code>: Only enabled with `-fspec-constr` flag. TODO: explain what it does.</code></p>
<p><code> * </code><strong><code>Check</code> <code>rules,</code> <code>2nd</code> <code>pass</code></strong></p>
<p><code> * </code><strong><code>Simplifier,</code> <code>final</code></strong><code>: final 0 phase of the simplifier.</code></p>
<p><code> * </code><strong><code>Damand</code> <code>analysis,</code> <code>2nd</code> <code>pass</code></strong><code> (a.k.a. late demand analysis): this pass consists of demand analysis followed by worker-wrapper transformation and phase 0 of the simplifier. The reason for this pass is that some opportunities for discovering strictness were not visible earlier; and optimisations like call-pattern specialisation can create functions with unused arguments which are eliminated by late demand analysis. Only run with `-flate-dmd-anal`. FIXME: but the cardinality paper says something else, namely that the late pass is meant to detect single entry thunks. Is it still the case in the current implementation?</code></p>
<p><code> * </code><strong><code>Check</code> <code>rules,</code> <code>3rd</code> <code>pass</code></strong></p>
<p>The plugin mechanism allows to modify the above pipeline dynamically.</p>
<p>== Simplifier</p>
<p>Simplifier is the workhorse of the Core-to-Core optimisation pipeline. It performs all the local transformations: (TODO: this list is most likely not comprehensive)</p>
<p><code>- constant folding</code><br />
<code>- applying the rewrite rules</code><br />
<code>- inlining</code><br />
<code>- case of case</code><br />
<code>- case of known constructor</code><br />
<code>- eta expansion and eta reduction</code><br />
<code>- combining adjacent casts</code><br />
<code>- pushing a cast out of the way of an application e.g. </code></p>
<p>Video: <a href="http://www.youtube.com/watch?v=EQA69dvkQIk&amp;list=PLBkRCigjPwyeCSD_DFxpd246YIF7_RDDI">GHC Core language</a> (14'04&quot;)</p>
<h1 id="the-type">The  type</h1>
<p>The Core language is GHC's central data types. Core is a very small, explicitly-typed, variant of System F. The exact variant is called [wiki:Commentary/Compiler/FC System FC], which embodies equality constraints and coercions.</p>
<p>The  type, and the functions that operate over it, gets an entire directory <a href="GhcFile(compiler/coreSyn)" class="uri" title="wikilink">GhcFile(compiler/coreSyn)</a>:</p>
<p><code> * </code><a href="GhcFile(compiler/coreSyn/CoreSyn.hs)" title="wikilink"><code>GhcFile(compiler/coreSyn/CoreSyn.hs)</code></a><code>: the data type itself.</code></p>
<p><code> * </code><a href="GhcFile(compiler/coreSyn/PprCore.hs)" title="wikilink"><code>GhcFile(compiler/coreSyn/PprCore.hs)</code></a><code>: pretty-printing.</code><br />
<code> * </code><a href="GhcFile(compiler/coreSyn/CoreFVs.hs)" title="wikilink"><code>GhcFile(compiler/coreSyn/CoreFVs.hs)</code></a><code>: finding free variables.</code><br />
<code> * </code><a href="GhcFile(compiler/coreSyn/CoreSubst.hs)" title="wikilink"><code>GhcFile(compiler/coreSyn/CoreSubst.hs)</code></a><code>: substitution.</code><br />
<code> * </code><a href="GhcFile(compiler/coreSyn/CoreUtils.hs)" title="wikilink"><code>GhcFile(compiler/coreSyn/CoreUtils.hs)</code></a><code>: a variety of other useful functions over Core.</code></p>
<p><code> * </code><a href="GhcFile(compiler/coreSyn/CoreUnfold.hs)" title="wikilink"><code>GhcFile(compiler/coreSyn/CoreUnfold.hs)</code></a><code>: dealing with &quot;unfoldings&quot;.</code></p>
<p><code> * </code><a href="GhcFile(compiler/coreSyn/CoreLint.hs)" title="wikilink"><code>GhcFile(compiler/coreSyn/CoreLint.hs)</code></a><code>: type-check the Core program. This is an incredibly-valuable consistency check, enabled by the flag </code><code>.</code></p>
<p><code> * </code><a href="GhcFile(compiler/coreSyn/CoreTidy.hs)" title="wikilink"><code>GhcFile(compiler/coreSyn/CoreTidy.hs)</code></a><code>: part of the [wiki:Commentary/Compiler/HscMain the CoreTidy pass] (the rest is in </code><a href="GhcFile(compiler/main/TidyPgm.hs)" title="wikilink"><code>GhcFile(compiler/main/TidyPgm.hs)</code></a><code>).</code><br />
<code> * </code><a href="GhcFile(compiler/coreSyn/CorePrep.hs)" title="wikilink"><code>GhcFile(compiler/coreSyn/CorePrep.hs)</code></a><code>: [wiki:Commentary/Compiler/HscMain the CorePrep pass]</code></p>
<p>Here is the entire Core type <a href="GhcFile(compiler/coreSyn/CoreSyn.hs)" class="uri" title="wikilink">GhcFile(compiler/coreSyn/CoreSyn.hs)</a>:  That's it. All of Haskell gets compiled through this tiny core.</p>
<p> is parameterised over the type of its <em>binders</em>, . This facility is used only rarely, and always temporarily; for example, the let-floater  pass attaches a binding level to every binder. By far the most important type is , which is  with  binders. If you want to learn more about such AST-parametrization, I encourage you to read a blog post about it: <a href="http://blog.ezyang.com/2013/05/the-ast-typing-problem" class="uri">http://blog.ezyang.com/2013/05/the-ast-typing-problem</a> .</p>
<p>Binder is used (as the name suggest) to bind a variable to an expression. The  data type is parametrized by the binder type. The most common one is the  where  comes from <a href="GhcFile(compiler/basicTypes/Var.hs)" class="uri" title="wikilink">GhcFile(compiler/basicTypes/Var.hs)</a>, which in fact is a  with some extra informations attached (like types).</p>
<p>Here are some notes about the individual constructors of .</p>
<p><code>* </code><code> represents variables.  The </code><code> it contains is essentially an [wiki:Commentary/Compiler/RdrNameType#TheOccNametype OccName] plus a </code><code>; however, equality </code><code> on </code><code>s is based only on their </code><code>'s, so </code><em><code>two</code> <code>s</code> <code>with</code> <code>different</code> <code>types</code> <code>may</code> <code>be</code> <code>-equal</code></em><code>.</code></p>
<p><code>* </code><code> is used for both term and type abstraction (small and big lambdas).</code></p>
<p><code>* </code><code> appears only in type-argument positions (e.g. </code><code>).  To emphasise this, the type synonym </code><code> is used as documentation when we expect that a </code><code> constructor may show up.  Anything not called </code><code> should not use a </code><code> constructor. Additional GHC Core uses so called type-lambdas, they are like lambdas, but instead of taking a real argument, they take a type instead. You should not confuse them with TypeFamilies, because type-lambdas are working on a value level, while type families are functions on the type level. The simplies example for a type-lambda usage is a polymorphic one: </code><code>. It will be represented in Core as </code><code>, where </code><code> is a *type argument*, so when specyfying the argument of </code><code> we can refer to </code><code>. This is how polymorphism is represented in Core.</code></p>
<p><code>* </code><code> handles both recursive and non-recursive let-bindings; see the the two constructors for </code><code>. The Let constructor contains both binders as well as the resulting expression. The resulting expression is the </code><code> in expression </code><code>.</code></p>
<p><code>* </code><code> expressions need [wiki:Commentary/Compiler/CoreSynType#Caseexpressions more explanation].</code></p>
<p><code>* </code><code> is used for an [wiki:Commentary/Compiler/FC FC cast expression].  </code><code> is a synonym for </code><code>.</code></p>
<p><code>* </code><code> is used to represent all the kinds of source annotation we support: profiling SCCs, HPC ticks, and GHCi breakpoints. Was named </code><code> some time ago.</code></p>
<h2 id="case-expressions">Case expressions</h2>
<p>Case expressions are the most complicated bit of . In the term :</p>
<p><code>* </code><code> is the scrutinee</code><br />
<code>* </code><code> is the </code><strong><code>case</code> <code>binder</code></strong><code> (see notes below)</code><br />
<code>* </code><code> is the type of the entire case expression (redundant once [wiki:Commentary/Compiler/FC FC] is in HEAD -- was for GADTs)</code><br />
<code>* </code><code> is a list of the case alternatives</code></p>
<p>A case expression can scrutinise</p>
<p><code>* </code><strong><code>a</code> <code>data</code> <code>type</code></strong><code> (the alternatives are </code><code>s), or </code><br />
<code>* </code><strong><code>a</code> <code>primitive</code> <code>literal</code> <code>type</code></strong><code> (the alternatives are </code><code>s), or </code><br />
<code>* </code><strong><code>a</code> <code>value</code> <code>of</code> <code>any</code> <code>type</code> <code>at</code> <code>all</code></strong><code> (if there is one </code><code> alternative).</code></p>
<p>A case expression is <strong>always strict</strong>, even if there is only one alternative, and it is . (This differs from Haskell!) So  will call , rather then returning .</p>
<p>The  field, called the <strong>case binder</strong>, is an unusual feature of GHC's case expressions. The idea is that <em>in any right-hand side, the case binder is bound to the value of the scrutinee</em>. If the scrutinee was always atomic nothing would be gained, but real expressiveness is added when the scrutinee is not atomic. Here is a slightly contrived example:  (Here, &quot;&quot; is the case binder; at least that is the syntax used by the Core pretty printer.) This expression evaluates ; if the result is , it returns , otherwise it returns the reversed list appended to itself. Since the returned value of  is present in the implementation, it makes sense to have a name for it!</p>
<p>The most common application is to model call-by-value, by using  instead of . For example, here is how we might compile the call  if we knew that  was strict: </p>
<p>Case expressions have several invariants</p>
<p><code>* The </code><code> type is the same as the type of any of the right-hand sides (up to refining unification -- coreRefineTys in </code><a href="GhcFile(compiler/types/Unify.hs)" title="wikilink"><code>GhcFile(compiler/types/Unify.hs)</code></a><code> -- in pre-[wiki:Commentary/Compiler/FC FC]).</code><br />
<br />
<code>* If there is a </code><code> alternative, it must appear first.  This makes finding a </code><code> alternative easy, when it exists.</code></p>
<p><code>* The remaining non-DEFAULT alternatives must appear in order of</code><br />
<code>   * tag, for </code><code>s</code><br />
<code>   * lit, for </code><code>s</code><br />
<code>This makes finding the relevant constructor easy, and makes comparison easier too.</code></p>
<p><code>* The list of alternatives is </code><strong><code>always</code> <code>exhaustive</code></strong><code>, meaning that it covers </code><strong><code>all</code> <code>reachable</code> <code>cases</code></strong><code>.  Note, however, that an &quot;exhausive&quot; case does not necessarily mention all constructors:</code></p>
<p></p>
<p><code>The inner case does not need a </code><code> alternative, because x can't be </code><code> at that program point. Furthermore, GADT type-refinement might mean that some alternatives are not reachable, and hence can be discarded.  </code></p>
<h2 id="shadowing">Shadowing</h2>
<p>One of the important things when working with Core is that variable shadowing is allowed. In other words, it is possible to come across a definition of a variable that has the same name (`realUnique`) as some other one that is already in scope. One of the possible ways to deal with that is to use `Subst` (substitution environment from <a href="GhcFile(compiler/coreSyn/CoreSubst.hs)" class="uri" title="wikilink">GhcFile(compiler/coreSyn/CoreSubst.hs)</a>), which maintains the list of variables in scope and makes it possible to clone (i.e. rename) only the variables that actually capture names of some earlier ones. For some more explanations about this approach see <a href="http://research.microsoft.com/%7Esimonpj/Papers/inlining/index.htm">Secrets of the Glasgow Haskell Compiler inliner (JFP'02)</a> (section 4 on name capture).</p>
<h2 id="human-readable-core-generation">Human readable Core generation</h2>
<p>If you are interested in the way Core is translated into human readable form, you should check the sources for <a href="GhcFile(compiler/coreSyn/PprCore.hs)" class="uri" title="wikilink">GhcFile(compiler/coreSyn/PprCore.hs)</a>. It is especially usefull if you want to see how the Core data types are being build, especially when there is no Show instance defined for them.</p>
<h1 id="cps-conversion">CPS Conversion</h1>
<p>This part of the compiler is now merged in ghc-HEAD.</p>
<h2 id="overview-2">Overview</h2>
<p>This pass takes Cmm with native proceedure calls and an implicit stack and produces Cmm with only tail calls implemented as jumps and an explicit stack. In a word, it does CPS conversion. (All right, so that's two words.)</p>
<h2 id="design-aspects">Design Aspects</h2>
<p><code>* Proc-Point Analysis</code><br />
<code>* Calling Conventions</code><br />
<code>* Live Value Analysis</code><br />
<code>* Stack Layout</code></p>
<h2 id="simple-design">Simple Design</h2>
<p><code>* Split blocks into multiple blocks at function calls</code><br />
<code>  * TODO: eliminate extra jump at block ends when there is already a jump at the end of the call</code><br />
<code>* Do liveness analysis</code><br />
<code>* Split every block into a separate function</code><br />
<code>* Pass all live values as parameters (probably slow)</code><br />
<code>  * Must arrange for both the caller and callee to know argument order</code><br />
<code>    * Simple design: callee just chooses some order and all callers must comply</code><br />
<code>  * Eventually could be passed implicitly but keeping things explicit makes things easier</code><br />
<code>  * Evantually could use a custom calling convention</code><br />
<code>  * Actual syntax is probably virtual.  (I.e. in an external table, not in actual syntax because that would require changes to the type for Cmm code)</code><br />
<code>    * Input code:</code><br />
<code>      </code><br />
<code>    * Output code:</code><br />
<code>      </code><br />
<code>* Save live values before a call in the continuation</code><br />
<code>  * Must arrange for both the caller and callee to know field order</code><br />
<code>    * Simple design: callee just chooses some order and all callers must comply</code><br />
<code>  * Eventually needs to be optimized to reduce continuation shuffling</code><br />
<code>    * Can register allocation algorithms be unified with this into one framework?</code></p>
<h2 id="to-be-worked-out">To be worked out</h2>
<p><code>* The continuations for </code><code> and </code><code> are different.</code><br />
<code>  </code><br />
<code>  * Could make a for each that shuffles the arguments into a common format.</code><br />
<code>  * Could make one branch primary and shuffle the other to match it, but that might entail unnecessary memory writes.</code></p>
<h2 id="pipeline">Pipeline</h2>
<p><code>* CPS</code><br />
<code>  * Make closures and stacks manifest</code><br />
<code>  * Makes all calls are tail calls</code><br />
<code>* Parameter Elimination</code><br />
<code>  * Makes calling convention explicit</code><br />
<code>  * For externally visible functions calling conventions is machine specific, but not backend specific because functions compiled from different backends must be be able to call eachother</code><br />
<code>  * For local functions calling convention can be left up to the backend because it can take advantage of register allocation.</code><br />
<code>    * However, the first first draft will specify the standard calling convention for all functions even local ones because:</code><br />
<code>      * It's simpler</code><br />
<code>      * The C code generator can't handle function parameters because of the Evil Mangler</code><br />
<code>      * The NCG doesn't yet understand parameters</code></p>
<h2 id="todo">TODO</h2>
<p><code>* Downstream</code><br />
<code>  * Argument passing convention</code><br />
<code>  * Stack check</code><br />
<code>    * Needs some way to synchronize the branch label with the heap check</code><br />
<code>* Midstream</code><br />
<code>  * Support </code><code> (needed by rts/Apply.cmm)</code><br />
<code>  * More factoring and cleanup/documentation</code><br />
<code>  * Wiki document the designed choosen</code><br />
<code>  * Better stack slot selection</code><br />
<code>  * Foreign function calls</code><br />
<code>  * Garbage collector</code><br />
<code>  * Proc points</code><br />
<code>    * May cause new blocks</code><br />
<code>    * May cause new functions</code><br />
<code>    * Lives could be passes either on stack or in arguments</code><br />
<code>  * Proc grouping of blocks</code><br />
<code>* Upstream</code><br />
<code>  * Have </code><code> emit C-- with functions.</code></p>
<h2 id="current-pipeline">Current Pipeline</h2>
<h3 id="section-1"></h3>
<p>The / pipeline and the / pipeline can each independantly use the CPS pass. However, they currently bypass it untill the CPS code becomes stablized, but they must both use the  pass. This pass converts the header on each function from a  to a .</p>
<h2 id="non-cps-changes">Non-CPS Changes</h2>
<p><code>* Cmm Syntax Changes</code><br />
<code>  * The returns parameters of a function call must be surrounded by parenthesis.</code><br />
<code>    For example</code></p>
<p></p>
<p><code>    This is simply to avoid shift-reduce conflicts with assignment.</code><br />
<code>    Future revisions to the parser may eliminate the need for this.</code></p>
<p><code>  * Variable declarations may are annotated to indicate</code><br />
<code>    whether they are GC followable pointers.</code></p>
<p></p>
<p><code>  * The bitmap of a </code><code> is now specified using</code><br />
<code>    a parameter like syntax.</code></p>
<p></p>
<p><code>    Note that these are not real parameters, they are the stack layout</code><br />
<code>    of the continuation.  Also, until the CPS algorithm</code><br />
<code>    gets properly hooked into the </code><code> path the parameter names are not used.</code><br />
<code>  * The return values of a function call may only be </code><code>.</code><br />
<code>    This is due to changes in the </code><code> data type.</code></p>
<p><code>* Cmm Data Type Changes</code><br />
<code>  * The return parameters of a </code><code> are </code><code> instead of </code><code>.</code><br />
<code>    This is because a </code><code> doesn't have a well defined pointerhood,</code><br />
<code>    and the return values will become parameters to continuations where</code><br />
<code>    their pointerhood will be needed.</code><br />
<code>  * The type of info tables is now a separate parameter to </code><br />
<code>    * Before</code></p>
<p></p>
<p><code>    * After</code></p>
<p></p>
<p><code>    This is to support using either </code><code> or </code><br />
<code>    as the header of a </code><code>.</code><br />
<code>    * Before info table conversion use </code></p>
<p></p>
<p><code>    * After info table conversion use </code></p>
<p></p>
<p><code>    Same for </code><code> and </code><code>.</code><br />
<code>  * New type aliases </code><code>, </code><code> and </code><code>.</code><br />
<code>    Respectively these are the actual parameters of a function call,</code><br />
<code>    the formal parameters of a function, and the</code><br />
<code>    return results of a function call with pointerhood annotation</code><br />
<code>    (CPS may convert these to formal parameter of the call's continuation).</code></p>
<h2 id="notes">Notes</h2>
<p><code>* Changed the parameter to a </code><code> to be </code><code> instead of </code><br />
<code>  * </code><code> are </code><br />
<code>  * This field seems to not have been being used; it only require a type change</code><br />
<code>* GC can be cleaned up b/c of the CPS</code><br />
<code>  * Before</code></p>
<p></p>
<p><code>  * After</code></p>
<p></p>
<p><code>* We need the NCG to do aliasing analysis.  At present the CPS pass will generate the following, and will assume that the NCG can figure out when the loads and stores can be eliminated.  (The global saves part of a </code><code> is dead b/c of this.)</code></p>
<p></p>
<p><code>* Simple calls</code><br />
<code>  * Before</code></p>
<p></p>
<p><code>  * Output of CPS</code></p>
<p></p>
<p><code>  * Optimization by the NCG</code></p>
<p></p>
<h2 id="loopholes">Loopholes</h2>
<p>There are a number of deviations from what one might expect from a CPS algorithm due to the need to encode existing optimizations and idioms.</p>
<h3 id="gc-blocks">GC Blocks</h3>
<p>For obvious reasons, the stack used by GC blocks does not count tward the maximum amount of stack used by the function.</p>
<p>This loophole is overloaded by the GC <strong>functions</strong> so they don't create their own infinite loop. The main block is marked as being the GC block so its stack usage doesn't get checked.</p>
<h3 id="update-frames">Update Frames</h3>
<p>Update frame have to be pushed onto the stack at the begining of an update function. We could do this by wrapping the update function inside another function that just does the work of calling that other function, but since updates are so common we don't want to pay the cost of that extra jump. Thus a function can be annotated with a frame that should be pushed on entry.</p>
<p>Note that while the frame is equivalent to a tail call at the end of the function, the frame must be pushed at the beginning of the function because parts of the blackhole code look for these update frames to determine what thunks are under evaluation.</p>
<h3 id="user-defined-continuations">User defined continuations</h3>
<p>Pushing an update frame on the stack requires the ability to define a function that will pull that frame from the stack and have access to any values within the frame. This is done with user-defined continuations.</p>
<h3 id="branches-to-continuations">Branches to continuations</h3>
<p>A GC block for a heap check after a call should only take one or two instructions. However the natural code:  would generate a trivial continuation for the  call as well as a trivial continuation for the  call that just calls the proc point .</p>
<p>We solve this by changing the syntax to </p>
<p>Now the  call has the same return signature as  and can use the same continuation. (A call followed by a  thus gets optimized down to just the call.)</p>
<h2 id="not-in-scope-of-current-work">Not in Scope of Current Work</h2>
<p>Improvements that could be made but that will not be implemented durring the curent effort.</p>
<h3 id="static-reference-table-handling-srt">Static Reference Table Handling (SRT)</h3>
<p>As it stands, each function and thus each call site must be annotated with a bitmap and a pointer or offset to the SRT shared by the function. This does not interact with the stack in any way so it ought to be outside the scope of the CPS algorithm. However there is some level of interaction because</p>
<p><code>1. the SRT information on each call site needs to be attached to the resulting continuation and</code><br />
<code>2. functions read from a Cmm file might need to be annotated with that SRT info.</code></p>
<p>The first is a concern for correctness but may be handled by treating the SRT info as opaque data. The second is a concern for ease of use and thus the likelyhood of mistakes in hand written C-- code. At the moment it appears that all of the C-- functions in the runtime system (RTS) use a null SRT so for now we'll just have the CPS algorithm treat the SRT info as opaque.</p>
<p>In the future it would be nice to have a more satisfactory way to handle both these issues.</p>
<h3 id="cmm-optimization-assumed-by-cps">Cmm Optimization assumed by CPS</h3>
<p>In order to simplify the CPS pass, it makes some assumptions about the optimizer.</p>
<p><code>* The CPS pass may generate more blocks than strictly necessary.  In particular,</code><br />
<code>  it might be possible to join together two blocks when the second block is only</code><br />
<code>  entered by the first block.  This is a simple optimization that needs to be implemented.</code><br />
<code>* The CPS pass may generate more loads and stores than strictly necessary.  In particular,</code><br />
<code>  it may load a local register only to store it back to the same stack location a few</code><br />
<code>  statements later.  There may be intervening branches.  The optimizer</code><br />
<code>  needs to be extended to eliminate these load store pairs.</code></p>
<h2 id="notes-on-future-development">Notes on future development</h2>
<h3 id="handling-gc">Handling GC</h3>
<p></p>
<p></p>
<h1 id="the-ghc-commentary-data-types-and-data-constructors">The GHC Commentary: Data types and data constructors</h1>
<p>This chapter was thoroughly changed Feb 2003. If you are interested in how a particular data type is implemented take a look at [wiki:Commentary/Compiler/CaseStudies/Bool this case study].</p>
<h2 id="data-types">Data types</h2>
<p>Consider the following data type declaration:  The user's source program mentions only the constructors `MkT` and `Nil`. However, these constructors actually <em>do</em> something in addition to building a data value. For a start, `MkT` evaluates its arguments. Secondly, with the flag `-funbox-strict-fields` GHC will flatten (or unbox) the strict fields. So we may imagine that there's the <em>source</em> constructor `MkT` and the <em>representation</em> constructor `MkT`, and things start to get pretty confusing.</p>
<p>GHC now generates three unique `Name`s for each data constructor:  Recall that each occurrence name (OccName) is a pair of a string and a name space (see [wiki:Commentary/Compiler/RdrNameType#TheOccNametype RdrNames, Modules, and OccNames]), and two OccNames are considered the same only if both components match. That is what distinguishes the name of the name of the DataCon from the name of its worker Id. To keep things unambiguous, in what follows we'll write &quot;MkT{d}&quot; for the source data con, and &quot;MkT{v}&quot; for the worker Id. (Indeed, when you dump stuff with &quot;-ddumpXXX&quot;, if you also add &quot;-dppr-debug&quot; you'll get stuff like &quot;Foo {- d rMv -}&quot;. The &quot;d&quot; part is the name space; the &quot;rMv&quot; is the unique key.)</p>
<p>Each of these three names gets a distinct unique key in GHC's name cache.</p>
<h1 id="the-life-cycle-of-a-data-type">The life cycle of a data type</h1>
<p>Suppose the Haskell source looks like this:  When the parser reads it in, it decides which name space each lexeme comes from, thus:  Notice that in the Haskell source <em>all data contructors are named via the &quot;source data con&quot; MkT{d}</em>, whether in pattern matching or in expressions.</p>
<p>In the translated source produced by the type checker (-ddump-tc), the program looks like this: </p>
<p>Notice that the type checker replaces the occurrence of MkT by the <em>wrapper</em>, but the occurrence of Nil by the <em>worker</em>. Reason: Nil doesn't have a wrapper because there is nothing to do in the wrapper (this is the vastly common case).</p>
<p>Though they are not printed out by &quot;-ddump-tc&quot;, behind the scenes, there are also the following: the data type declaration and the wrapper function for MkT.  Here, the <em>wrapper</em> $WMkT evaluates and takes apart the argument p, evaluates the argument t, and builds a three-field data value with the <em>worker</em> constructor MkT{v}. (There are more notes below about the unboxing of strict fields.) The worker $WMkT is called an <em>implicit binding</em>, because it's introduced implicitly by the data type declaration (record selectors are also implicit bindings, for example). Implicit bindings are injected into the code just before emitting code or External Core.</p>
<p>After desugaring into Core (-ddump-ds), the definition of f looks like this:  Notice the way that pattern matching has been desugared to take account of the fact that the &quot;real&quot; data constructor MkT has three fields.</p>
<p>By the time the simplifier has had a go at it, f will be transformed to:  Which is highly cool.</p>
<h2 id="the-constructor-wrapper-functions">The constructor wrapper functions</h2>
<p>The wrapper functions are automatically generated by GHC, and are really emitted into the result code (albeit only after CorePre; see `CorePrep.mkImplicitBinds`). The wrapper functions are inlined very vigorously, so you will not see many occurrences of the wrapper functions in an optimised program, but you may see some. For example, if your Haskell source has  then `$WMkT` will not be inlined (because it is not applied to anything). That is why we generate real top-level bindings for the wrapper functions, and generate code for them.</p>
<h2 id="the-constructor-worker-functions">The constructor worker functions</h2>
<p>Saturated applications of the constructor worker function MkT{v} are treated specially by the code generator; they really do allocation. However, we do want a single, shared, top-level definition for top-level nullary constructors (like True and False). Furthermore, what if the code generator encounters a non-saturated application of a worker? E.g. (`map Just xs`). We could declare that to be an error (CorePrep should saturate them). But instead we currently generate a top-level defintion for each constructor worker, whether nullary or not. It takes the form:  This is a real hack. The occurrence on the RHS is saturated, so the code generator (both the one that generates abstract C and the byte-code generator) treats it as a special case and allocates a MkT; it does not make a recursive call! So now there's a top-level curried version of the worker which is available to anyone who wants it.</p>
<p>This strange definition is not emitted into External Core. Indeed, you might argue that we should instead pass the list of `TyCon`s to the code generator and have it generate magic bindings directly. As it stands, it's a real hack: see the code in CorePrep.mkImplicitBinds.</p>
<h2 id="external-core">External Core</h2>
<p>When emitting External Core, we should see this for our running example:  Notice that it makes perfect sense as a program all by itself. Constructors look like constructors (albeit not identical to the original Haskell ones).</p>
<p>When reading in External Core, the parser is careful to read it back in just as it was before it was spat out, namely: </p>
<h2 id="unboxing-strict-fields">Unboxing strict fields</h2>
<p>If GHC unboxes strict fields (as in the first argument of MkT above), it also transforms source-language case expressions. Suppose you write this in your Haskell source:  GHC will desugar this to the following Core code:  The local let-binding reboxes the pair because it may be mentioned in the case alternative. This may well be a bad idea, which is why `-funbox-strict-fields` is an experimental feature.</p>
<p>It's essential that when importing a type `T` defined in some external module `M`, GHC knows what representation was used for that type, and that in turn depends on whether module M was compiled with `-funbox-strict-fields`. So when writing an interface file, GHC therefore records with each data type whether its strict fields (if any) should be unboxed.</p>
<h2 id="labels-and-info-tables">Labels and info tables</h2>
<p><em>Quick rough notes: SLPJ March 2003.</em></p>
<p>Every data constructor `C` has two info tables:</p>
<p><code>   * The static info table (label `C_static_info`), used for statically-allocated constructors.</code><br />
<code>   * The dynamic info table (label `C_con_info`), used for dynamically-allocated constructors. </code></p>
<p>Statically-allocated constructors are not moved by the garbage collector, and therefore have a different closure type from dynamically-allocated constructors; hence they need a distinct info table. Both info tables share the same entry code, but since the entry code is physically juxtaposed with the info table, it must be duplicated (`C_static_entry` and `C_con_entry` respectively).</p>
<p><a href="PageOutline" class="uri" title="wikilink">PageOutline</a></p>
<h1 id="demand-analyser-in-ghc">Demand analyser in GHC</h1>
<p>This page explains basics of the so-called demand analysis in GHC, comprising strictness and absence analyses. Meanings of demand signatures are explained and examples are provided. Also, components of the compiler possibly affected by the results of the demand analysis are listed with explanations provided.</p>
<p><code>* The </code><a href="http://research.microsoft.com/en-us/um/people/simonpj/papers/demand-anal/demand.ps"><code>demand-analyser</code> <code>draft</code> <code>paper</code></a><code> is as yet unpublished, but gives the most accurate overview of the way GHC's demand analyser works.</code></p>
<hr />
<h2 id="demand-signatures">Demand signatures</h2>
<p>Let us compile the following program with `-O2 -ddump-stranal` flags:</p>
<p></p>
<p>The resulting demand signature for function `f` will be the following one:</p>
<p></p>
<p>This should be read as &quot;`f` puts stricts demands on both its arguments (hence, `S`); `f` might use its first and second arguments. but in the second argument (which is a product), the second component is ignored&quot;. The suffix `m` in the demand signature indicates that the function returns <strong>CPR</strong>, a constructed product result (for more information on CPR see the JFP paper <a href="http://research.microsoft.com/en-us/um/people/simonpj/Papers/cpr/index.htm">Constructed Product Result Analysis for Haskell</a>).</p>
<p>Current implementation of demand analysis in Haskell performs annotation of all binders with demands, put on them in the context of their use. For functions, it is assumed, that the result of the function is used strictly. The analysis infers strictness and usage information separately, as two components of a cartesian product domain. The same analysis also performs inference CPR and bottoming properties for functions, which can be read from the suffix of the signature. Demand signatures of inner definitions may also include <em>demand environments</em> that indicate demands, which a closure puts to its free variables, once strictly used, e.g. the signature</p>
<p></p>
<p>indicates that the function has one parameter, which is used lazily (hence `<L,U>`), however, when its result is used strictly, the free variable `skY` in its body is also used strictly.</p>
<h3 id="demand-descriptions">Demand descriptions</h3>
<p>Strictness demands</p>
<p><code> * `B` -- a </code><em><code>hyperstrict</code></em><code> demand. The expression `e` puts this demand on its argument `x` if every evaluation of `e` is guaranteed to diverge, regardless of the value of the argument. We call this demand </code><em><code>hyperstrict</code></em><code> because it is safe to evaluate `x` to arbitrary depth before evaluating `e`. This demand is polymorphic with respect to function calls and can be seen as `B = C(B) = C(C(B)) = ...` for an arbitrary depth.</code><br />
<code> </code><br />
<code> * `L`  -- a </code><em><code>lazy</code></em><code> demand. If an expression `e` places demand `L` on a variable  `x`, we can deduce nothing about how `e` uses `x`. `L` is the completely uninformative demand, the top element of the lattice.</code></p>
<p><code> * `S` -- a </code><em><code>head-strict</code></em><code> demand.  If `e` places demand `S` on `x` then `e` evaluates `x` to at least head-normal form; that is, to the outermost constructor of `x`.  This demand is typically placed by the `seq` function on its first argument. The demand `S(L ... L)` places a lazy demand on all the components, and so is equivalent to `S`; hence the identity `S = S(L ... L)`. Another identity is for functions, which states that `S = C(L)`. Indeed, if a function is certainly called, it is evaluated at lest up to the head normal form, i.e., </code><em><code>strictly</code></em><code>. However, its result may be used lazily.</code></p>
<p><code> * `S(s1 ... sn)` -- a structured strictness demand on a product.  It is at least head-strict, and perhaps more.</code></p>
<p><code> * `C(s)`  -- a </code><em><code>call-demand</code></em><code>, when placed on a binder `x`, indicates that the value is a function, which is always called and its result is used according to the demand `s`. </code></p>
<p>Absence/usage demands</p>
<p><code> * `A` -- when placed on a binder `x` it means that `x` is definitely unused.</code></p>
<p><code> * `U` -- the value is used on some execution path.  This demand is a top of usage domain.</code></p>
<p><code> * `H` -- a </code><em><code>head-used</code></em><code> demand. Indicates that a product value is used itself, however its components are certainly ignored. This demand is typically placed by the `seq` function on its first argument. This demand is polymorphic with respect to products and functions. For a product, the head-used demand is expanded as `U(A, ..., A)` and for functions it can be read as `C(A)`, as the function is called (i.e., evaluated to at least a head-normal form), but its result is ignored.</code></p>
<p><code> * `U(u1 ... un)` -- a structured usage demand on a product. It is at least head-used, and perhaps more.</code></p>
<p><code> * `C(u)` -- a </code><em><code>call-demand</code></em><code> for usage information. When put on a binder `x`, indicates that `x` in all executions paths where `x` is used, it is </code><em><code>applied</code></em><code> to some argument, and the result of the application is used with a demand `u`.</code></p>
<p>Additional information (demand signature suffix)</p>
<p><code> * `m`  -- a function returns a </code><a href="http://research.microsoft.com/en-us/um/people/simonpj/Papers/cpr/index.htm"><code>constructed</code> <code>product</code> <code>result</code></a></p>
<p><code> * `b` -- the function is a </code><em><code>bottoming</code></em><code> one, i.e., some decoration of `error` and friends.</code></p>
<h2 id="worker-wrapper-split">Worker-Wrapper split</h2>
<p>Demand analysis in GHC drives the <em>worker-wrapper transformation</em>, which exposes specialised calling conventions to the rest of the compiler. In particular, the worker-wrapper transformation implements the unboxing optimisation.</p>
<p>The worker-wrapper transformation splits each function `f` into a <em>wrapper</em>, with the ordinary calling convention, and a <em>worker</em>, with a specialised calling convention. The wrapper serves as an impedance-matcher to the worker; it simply calls the worker using the specialised calling convention. The transformation can be expressed directly in GHC's intermediate language. Suppose that `f` is defined thus:  and that we know that `f` is strict in its argument (the pair, that is), and uses its components. What worker-wrapper split shall we make? Here is one possibility:  Now the wrapper, `f`, can be inlined at every call site, so that the caller evaluates `p`, passing only the components to the worker `$wf`, thereby implementing the unboxing transformation.</p>
<p>But what if `f` did not use `a`, or `b`? Then it would be silly to pass them to the worker `$wf`. Hence the need for absence analysis. Suppose, then, that we know that `b` is not needed. Then we can transform to:  Since `b` is not needed, we can avoid passing it from the wrapper to the worker; while in the worker, we can use `error &quot;abs&quot;` instead of `b`.</p>
<p>In short, the worker-wrapper transformation allows the knowledge gained from strictness and absence analysis to be exposed to the rest of the compiler simply by performing a local transformation on the function definition. Then ordinary inlining and case elimination will do the rest, transformations the compiler does anyway.</p>
<h2 id="relevant-compiler-parts">Relevant compiler parts</h2>
<p>Multiple parts of GHC are sensitive to changes in the nature of demand signatures and results of the demand analysis, which might cause unexpected errors when hacking into demands. [wiki:Commentary/Compiler/Demand/RelevantParts This list] enumerates the parts of the compiler that are sensitive to demand, with brief summaries of how so.</p>
<h1 id="support-for-deriving-and-instances">Support for deriving , , and  instances</h1>
<p><a href="PageOutline" class="uri" title="wikilink">PageOutline</a></p>
<p>GHC 6.12.1 introduces an extension to the  mechanism allowing for automatic derivation of , , and  instances using the , , and  extensions, respectively. Twan van Laarhoven <a href="https://mail.haskell.org/pipermail/haskell-prime/2007-March/002137.html">first proposed this feature</a> in 2007, and <a href="https://ghc.haskell.org/trac/ghc/ticket/2953">opened a related GHC Trac ticket</a> in 2009.</p>
<h2 id="example">Example</h2>
<p></p>
<p>The derived code would look something like this:</p>
<p></p>
<h2 id="algorithm-description">Algorithm description</h2>
<p>, , and  all operate using the same underlying mechanism. GHC inspects the arguments of each constructor and derives some operation to perform on each argument, which depends of the type of the argument itself. In a  instance, for example  would be applied to occurrences of the last type parameter, but  would be applied to other type parameters. Typically, there are five cases to consider. (Suppose we have a data type .)</p>
<p>1. Terms whose type does not mention  2. Terms whose type mentions  3. Occurrences of  4. Tuple values 5. Function values</p>
<p>After this is done, the new terms are combined in some way. For instance,  instances combine terms in a derived  definition by applying the appropriate constructor to all terms, whereas in  instances, a derived  definition would  the terms together.</p>
<h3 id="section-2"></h3>
<p>A comment in <a href="http://git.haskell.org/ghc.git/blob/9f968e97a0de9c2509da00f6337b612dd72a0389:/compiler/typecheck/TcGenDeriv.hs#l1476">TcGenDeriv.hs</a> lays out the basic structure of , which derives an implementation for .</p>
<p></p>
<p> is special in that it can recurse into function types, whereas  and  cannot (see the section on covariant and contravariant positions).</p>
<h3 id="section-3"></h3>
<p>Another comment in <a href="http://git.haskell.org/ghc.git/blob/9f968e97a0de9c2509da00f6337b612dd72a0389:/compiler/typecheck/TcGenDeriv.hs#l1725">TcGenDeriv.hs</a> reveals the underlying mechanism behind :</p>
<p></p>
<p>In addition to ,  also generates a definition for  as of GHC 7.8.1 (addressing <a href="https://ghc.haskell.org/trac/ghc/ticket/7436">#7436</a>). The pseudo-definition for  would look something like this:</p>
<p></p>
<h3 id="section-4"></h3>
<p>From <a href="http://git.haskell.org/ghc.git/blob/9f968e97a0de9c2509da00f6337b612dd72a0389:/compiler/typecheck/TcGenDeriv.hs#l1800">TcGenDeriv.hs</a>:</p>
<p></p>
<h3 id="covariant-and-contravariant-positions">Covariant and contravariant positions</h3>
<p>One challenge of deriving  instances for arbitrary data types is handling function types. To illustrate this, note that these all can have derived  instances:</p>
<p></p>
<p>but none of these can:</p>
<p></p>
<p>In , , and , all occurrences of the type variable  are in <em>covariant</em> positions (i.e., the  values are produced), whereas in , , and , all occurrences of  are in <em>contravariant</em> positions (i.e., the  values are consumed). If we have a function , we can't apply  to an  value in a contravariant position, which precludes a  instance.</p>
<p>Most type variables appear in covariant positions. Functions are special in that the lefthand side of a function arrow reverses variance. If a function type  appears in a covariant position (e.g.,  above), then  is in a contravariant position and  is in a covariant position. Similarly, if  appears in a contravariant position (e.g.,  above), then  is in a covariant position and  is in a contravariant position.</p>
<p>If we annotate covariant positions with  (for positive) and contravariant positions with  (for negative), then we can examine the above examples with the following pseudo-type signatures:</p>
<p></p>
<p>Since , , and  all use the last type parameter in at least one  position, GHC would reject a derived  instance for each of them.</p>
<h2 id="requirements-for-legal-instances">Requirements for legal instances</h2>
<p>This mechanism cannot derive , , or  instances for all data types. Currently, GHC checks if a data type meets the following criteria:</p>
<p>1. The data type has at least one type parameter. (For example,  cannot have a  instance.) 2. The data type's last type parameter cannot be used contravariantly. (see the section on covariant and contravariant positions.) 3. The data type's last type parameter cannot be used in the &quot;wrong place&quot; in any constructor's data arguments. For example, in , the type parameter  is only ever used as the last type argument in  and , so both  and  values can be ped. However, in , the type variable  appears in a position other than the last, so trying to  an  value would not typecheck.</p>
<p><code>  Note that there are two exceptions to this rule: tuple and function types.</code></p>
<p>4. The data type's last type variable cannot used in a  constraint. For example,  would be rejected.</p>
<p>In addition, GHC performs checks for certain classes only:</p>
<p>1. For derived  and  instances, a data type cannot use function types. This restriction does not apply to derived  instances, however. 2. For derived  and  instances, the data type's last type variable must be truly universally quantified, i.e., it must not have any class or equality constraints. This means that the following is legal:</p>
<p></p>
<p><code>  but the following is not legal:</code></p>
<p></p>
<p><code>  This restriction does not apply to derived </code><code> instances. See the following section for more details.</code></p>
<h3 id="relaxed-universality-check-for">Relaxed universality check for </h3>
<p> and  cannot be used with data types that use existential constraints, since the type signatures of  and  make this impossible. However,  instances are unique in that they do not produce constraints, but only consume them. Therefore, it is permissible to derive  instances for constrained data types (e.g., GADTs).</p>
<p>For example, consider the following GADT:</p>
<p></p>
<p>In the type signatures for  and , the  parameter appears both in an argument and the result type, so pattern-matching on a value of  must not impose any constraints, as neither  nor  would typecheck.</p>
<p>, however, only mentions  in argument types:</p>
<p></p>
<p>Therefore, a derived  instance for  typechecks:</p>
<p></p>
<p>Deriving  instances for GADTs with equality constraints could become murky, however. Consider this GADT:</p>
<p></p>
<p>All four  constructors have the same &quot;shape&quot; in that they all take an argument of type  (or , to which  is constrained to be equal). Does that mean all four constructors would have their arguments folded over? While it is possible to derive perfectly valid code which would do so:</p>
<p></p>
<p>it is much harder to determine which arguments are equivalent to . Also consider this case:</p>
<p></p>
<p>For all we know, it may be that . Does this mean that the  argument in  should be folded over?</p>
<p>To avoid these thorny edge cases, we only consider constructor arguments (1) whose types are <em>syntactically</em> equivalent to the last type parameter and (2) in cases when the last type parameter is a truly universally polymorphic. In the above  example, only  fits the bill, so the derived  instance is actually:</p>
<p></p>
<p>To expound more on the meaning of criterion (2), we want not only to avoid cases like , but also something like this:</p>
<p></p>
<p>In this example, the last type variable is instantiated with , which contains one type variable  applied to another type variable . We would <em>not</em> fold over the argument of type  in this case, because the last type variable should be <em>simple</em>, i.e., contain only a single variable without any application.</p>
<p>For the original discussion on this proposal, see <a href="https://ghc.haskell.org/trac/ghc/ticket/10447">#10447</a>.</p>
<h2 id="alternative-strategy-for-deriving-foldable-and-traversable">Alternative strategy for deriving `Foldable` and `Traversable`</h2>
<p>We adapt the algorithms for `-XDeriveFoldable` and `-XDeriveTraversable` based on that of `-XDeriveFunctor`. However, there is an important difference between deriving the former two typeclasses and the latter one (as of GHC 8.2, addressing <a href="https://ghc.haskell.org/trac/ghc/ticket/11174">Trac #11174</a>), which is best illustrated by the following scenario:</p>
<p></p>
<p>The generated code for the `Functor` instance is straightforward:</p>
<p></p>
<p>But if we use too similar of a strategy for deriving the `Foldable` and `Traversable` instances, we end up with this code:</p>
<p></p>
<p>This is unsatisfying for two reasons:</p>
<p>1. The `Traversable` instance doesn't typecheck! `Int#` is of kind `#`, but `pure` expects an argument whose type is of kind `*`. This effectively prevents `Traversable` from being derived for any datatype with an unlifted argument type (see <a href="https://ghc.haskell.org/trac/ghc/ticket/11174">Trac #11174</a>).</p>
<p>2. The generated code contains superfluous expressions. By the `Monoid` laws, we can reduce `f a &lt;&gt; mempty` to `f a`, and by the `Applicative` laws, we can reduce `fmap WithInt (f a) &lt;*&gt; pure i` to `fmap (\b -&gt; WithInt b i) (f a)`.</p>
<p>We can fix both of these issues by incorporating a slight twist to the usual algorithm that we use for `-XDeriveFunctor`. The differences can be summarized as follows:</p>
<p>1. In the generated expression, we only fold over arguments whose types mention the last type parameter. Any other argument types will simply produce useless `mempty`s or `pure`s, so they can be safely ignored.</p>
<p>2. In the case of `-XDeriveTraversable`, instead of applying `ConName`, we apply `\b_i ... b_k -&gt; ConName a_1 ... a_n`, where</p>
<ul>
<li>`ConName` has `n` arguments</li>
<li>`{b_i, ..., b_k}` is a subset of `{a_1, ..., a_n}` whose indices correspond to the arguments whose types mention the last type parameter. As a consequence, taking the difference of `{a_1, ..., a_n}` and `{b_i, ..., b_k}` yields the all the argument values of `ConName` whose types do not mention the last type parameter. Note that `[i, ..., k]` is a strictly increasing</li>
</ul>
<p><a href="PageOutline" class="uri" title="wikilink">PageOutline</a></p>
<h1 id="llvm-back-end-design">LLVM Back-end Design</h1>
<p>The current design tries to fit into GHC's pipeline stages as an alternative to the C and NCG back-ends as seamlessly as possible. This allows for quicker development and focus on the core task of LLVM code generation.</p>
<p>The LLVM pipeline works as follows:</p>
<p><code> * New path for LLVM generation, separate from C and NCG. (path forks at compiler/main/CodeOutput.lhs, same place where C and NCG fork).</code><br />
<code> * LLVM code generation will output LLVM assembly code.</code><br />
<code> * The LLVM assembly code is translated to an object file as follows</code><br />
<code>    * The LLVM optimizer is run which is a series of bitcode to bitcode optimization passes (using the </code><code> tool).</code><br />
<code>    * Finally an object file is created from the LLVM bitcode (using the </code><code> tool)</code><br />
<code> * This brings the LLVM path back to the other back-ends.</code><br />
<code> * The final state is the Link stage, which uses the system linker as with the other back-ends.</code></p>
<p>Here is a diagram of the pipeline:</p>
<p></p>
<p>This approach was the easiest and thus quickest way to initially implement the LLVM back-end. Now that it is working, there is some room for additional optimisations. A potential optimisation would be to add a new linker phase for LLVM. Instead of each module just being compiled to native object code ASAP, it would be better to keep them in the LLVM bitcode format and link all the modules together using the LLVM linker. This enable all of LLVM's link time optimisations. All the user program LLVM bitcode will then be compiled to a native object file and linked with the runtime using the native system linker.</p>
<h1 id="implementation">Implementation</h1>
<h2 id="framework">Framework</h2>
<p><code> * New </code><strong><code>-fllvm</code></strong><code> code generation pipeline, involved modifying:</code><br />
<code>   * </code><code> - Selects appropriate back-end for code generation (C, NCG, LLVM).</code><br />
<code>   * </code><code> - Stores GHC configuration (command line options, compile time options... ect). Added `HscLlvm` target type.</code><br />
<code>   * </code><code> - Stores modules/files to compile for ghc. Added new LLVM files and directory stored under `llvmGen`, and new CPP flag to enable the LLVM code generator (`-DLLVM`).</code><br />
<code>   * </code><code> - Added new `GhcWithLlvmCodeGen` option which can be set in `build.mk` to `YES` to enable the LLVM code generator.</code><br />
<code>   * </code><code> - Added `LlvmAs` phase to invoke the compilation of LLVM bitcode/IR to an object file. After this phase linking can occur.</code><br />
<code>   * </code><code> - Added code for new `LlvmAs`, `LlvmOpt` and `LlvmLlc` phases.</code><br />
<code>     * </code><code> - Invokes `llvm-as` tool to compile a llvm assembly file ('.ll') to a bitcode file (`.bc`).</code><br />
<code>     * </code><code> - Invokes the llvm `opt` tool to optimise the module. Just use the llvm standard optimisation groups of `O1`, `O2`, `O3`, depending on the optimisation level passed to 'ghc' by the user.</code><br />
<code>     * </code><code> - Invokes the llvm `llc` tool to generate the machine code ('.s' file) from the optimised bitcode. 'As' stage runs next, part of existing 'ghc' pipeline.</code><br />
<code>   * </code><code> - Stores the path and default settings of the system tools needed, so for LLVM back-end this is `llvm-as`, `opt` and `llc`.</code></p>
<p>The LLVM pipeline works as specified above. Code generation phase occurs, using the  option data the appropriate generator is selected (which is the Llvm back-end is `-fllvm` has been specified on the command line). After code generation, the next phase is determined, this is done from the `HscLlvm` target data constructor which is selected at ghc startup by . The next phase is `LlvmAs` which will compile the text IR to an LLVM bitcode file (equivalent to `llvm-as` tool). After this the `LlvmLlc` phase is run, which produces a native object file from the llvm bitcode file (equivalanet to the `llc` tool). At this stage, the output from all three back-ends should be 'equivalent'. After this phase, the `StopLn`, or linking phase occurs which should result in the end result. Compiling some Haskell code with the c-backend and some with the llvm-backend and linking them together is supported.</p>
<h2 id="llvm-code-generation">LLVM Code Generation</h2>
<p>For LLVM code generation we need a method for representing and generating LLVM code. The <a href="http://llvm.org/docs/FAQ.html#langirgen">LLVM FAQ</a> suggest the following possible approaches:</p>
<p><code> * Call into LLVM Libraries using FFI (can probably use </code><a href="http://hackage.haskell.org/package/llvm"><code>Haskell</code> <code>LLVM</code> <code>Bindings</code></a><code>) </code><br />
<code> * Emit LLVM Assembly (approach taken by </code><a href="http://www.cs.uu.nl/wiki/Ehc/WebHome"><code>EHC's</code></a><code> LLVM Back-end, can use the </code><a href="https://subversion.cs.uu.nl/repos/project.UHC.pub/trunk/EHC/src/ehc/LLVM.cag"><code>module</code></a><code> developed by them for this) </code><br />
<code> * Emit LLVM Bitcode (can't see any reason to do this)</code></p>
<p>The approach taken was to use the LLVM module from <a href="http://www.cs.uu.nl/wiki/Ehc/WebHome">EHC</a>. This module contains an abstract syntax representation of LLVM Assembly and the ability to pretty print it. It has been heavily modified to increase its language coverage as it was missing several LLVM constructs which were needed. Ideally we would like to add a second pretty printer which calls into the LLVM C++ API to generate LLVM Bitcode. This should hopefully decrease the compile times and make the back-end more resilient to future changes to LLVM Assembly. The LLVM Haskell binding (first option) wasn't used as it represents LLVM at a very high level, which isn't appropriate for the back-end.</p>
<h2 id="register-pinning">Register Pinning</h2>
<p>The new back-end supports a custom calling convention to place the STG virtual registers into specific hardware registers. The current approach taken by the C back-end and NCG of having a fixed assignment of STG virtual registers to hardware registers for performance gains is not implemented in the LLVM back-end. Instead, it uses a custom calling convention to support something semantically equivalent to register pinning. The custom calling convention passes the first N variables in specific hardware registers, thus guaranteeing on all function entries that the STG virtual registers can be found in the expected hardware registers. This approach is believed to provide better performance than the register pinning used by NCG/C back-ends as it keeps the STG virtual registers mostly in hardware registers but allows the register allocator more flexibility and access to all machine registers.</p>
<p>For some more information about the use of a custom calling convention see <a href="http://www.nondot.org/sabre/LLVMNotes/GlobalRegisterVariables.txt">here (Discussion between Chris Lattner and David Terei)</a></p>
<h2 id="code-generation">Code Generation</h2>
<p>Code generation consists of translating a list of `GenCmmTop` data types to LLVM code. `GenCmmTop` has the following form:</p>
<p></p>
<p>That is, it consists of two types, static data and functions. Each can largely be handled separately. Just enough information is needed such that pointers can be constructed to them and in many cases this information can be gathered from assumptions and constraints on Cmm.</p>
<p>After all the polymorphic types are bound we get this: </p>
<p>The code generator lives in `llvmGen` with the driver being `llvmGen/LlvmCodeGen.lhs`.</p>
<p>A large part of the code generation is keeping track of defined variables/functions and their type. An `LlvmEnv` construct is used for this. It is simply a dictionary storing function/variable names with their corresponding type information. This is used to create correct references/pointers between variables and functions.</p>
<h3 id="unregisterised-vs.-registerised">Unregisterised Vs. Registerised</h3>
<p>Code generation can take place in two general modes, `unregisterised` and `registerised`. There are two major differences from a back-end code generation point of view. Firstly, in unregisterised mode a optimisation feature called  is disabled. This means that the `h` field of `CmmProc` is empty. In registerised mode it instead contains the `CmmStatic` data for the procedures info table which must be placed just before the procedure in the generated code so that both the info table and procedure can be accessed through one pointer. This optimisation can be disabled separately though in `registerised` mode.</p>
<p>The other major change is the use of pinned global registers. The `Cmm` language includes a concept called registers. These are used like machine registers or variables in C to store the result of expressions. Unlike `LLVM` they are mutable. `Cmm` includes two types of registers as you can see below:</p>
<p></p>
<p>A `LocalReg` is a temporary general purpose register used in a procedure with scope of a single procedure. A `GlobalReg` on the other hand has global scope and a specific use. They are used just like machine registers, with a Stack Pointer and Heap Pointer registers creating a virtual machine (`STG`). `GlobalReg` is of the form:</p>
<p></p>
<p>In unregisterised mode these global registers are all just stored in memory in the heap. A specific pass operating on Cmm that takes place just before code generation thus transforms code such as:</p>
<p></p>
<p>into the following unregisterised form for code generation:</p>
<p></p>
<p>Where `MainCapability` is a label to the start of a RTS defined structure storing all the global registers.</p>
<p>In registerised mode as many of these global registers are assigned permanently to fixed hardware registers. This is done as it greatly improves performance. As these registers are accessed very frequently needing to load and store to memory for accessing adds a great cost. So for example on `x86` the following map between `Cmm` global registers and `x86` hardware registers exists:</p>
<p></p>
<p>These are all the available `callee save` registers on x86. `callee save` are used as in ghc generated code now saving and restoring of these registers are needed due to there new special use and because GHC uses continuation passing style, so a `'ret'` statement is never actually generated. And since they are `callee save`, foreign code can also be called without any need to handle the `Cmm` registers.</p>
<h2 id="cmmdata">!CmmData</h2>
<p>`CmmData` takes the following form:</p>
<p></p>
<p>Code generation takes place mainly in , driven by the main Llvm compiler driver, }.</p>
<p>The code generation for data occurs in two phases, firstly the types and all data is generated except for address values. Then the address values are resolved. This two step method is used as in the first pass, we don't know if a address refers to an external address or a procedure/data structure in the current LLVM module. We also need the type information in LLVM to create a pointer.</p>
<h3 id="st-pass-generation">1st Pass : Generation</h3>
<p>All `CmmStatic` is translated to LLVM structures.</p>
<h2 id="cmmstaticlit">!CmmStaticLit</h2>
<p>These are translated when possible as follows:</p>
<p><code> * `CmmInt` -&gt; Reduced to Int and then an appropriate `LMInt` of correct size is created. As LLVM supports any bit size, this is very straight forward.</code><br />
<code> * `CmmFloat` -&gt; Translated to a double, detecting NAN and INFINITY correctly. Then correct LLVM type (`float`, `double`, `float80`, `float128`) is selected.</code><br />
<code> * `CmmLabel` -&gt; Left untranslated at first, later resolved once we have determined types. As pointers are cast to word size ints, we can still determine types.</code><br />
<code> * `CmmLabelOff` -&gt; As above.</code><br />
<code> * `CmmLabelDiffOff` -&gt; As above.</code><br />
<code> * `CmmBlock` -&gt; `BlockId` is changed to a `CLabel` and then treated as a `CmmLabel` static type.</code><br />
<code> * `CmmHighStackMark` -&gt; Panic occurs if this type is encountered.</code></p>
<h4 id="cmmuninitialised">!CmmUninitialised</h4>
<p>For this, a zeroed array of `8bit` values is created of correct size.</p>
<h4 id="cmmalign-cmmdatalabel">!CmmAlign &amp; !CmmDataLabel</h4>
<p>The LLVM back-end can't handle `CmmAlign` or `CmmDataLabel`. A panic occurs if either is encountered. A `CmmDataLabel` is expected at the very start of each list of `CmmStatic`. It is removed and used as the name for the structure and constant instance.</p>
<h4 id="cmmstring">!CmmString</h4>
<p>This is translated into a LLVM string. Ascii characters are used when they are printable, escaped hex values otherwise. A null termination is added.</p>
<h3 id="nd-pass-resolution">2nd Pass : Resolution</h3>
<p>After the first pass, all types have been determined and all data translated except for address values (CLabel's). All generated llvm data is added to a Map of string to `LlvmType`, string being the data structure name. All `CmmProc's` are added to the map as well, they don't need to be properly passed though, just their names retrieved as they have a constant type of void return and no parameters.</p>
<p>Now appropriate pointers can be generated using the type information from the map and LLVM's `getelementptr` instruction. These are then all passed to int's to allow the types of structures to be determined in advance. If a pointer doesn't have a match in the Map, it is assumed to refer to an external (outside of this module) address. An external reference is declared for this address as:</p>
<p></p>
<p>Where i32 is the pointer size. (i64 if on 64 bit).</p>
<h2 id="cmmproc">!CmmProc</h2>
<p>A Cmm procedure is made up of a list of basic blocks, with each basic block being comprised of a list of CmmStmt</p>
<h1 id="desugaring-instance-declarations">Desugaring instance declarations</h1>
<p>These notes compare various ways of desugaring Haskell instance declarations. The tradeoffs are more complicated than I thought!</p>
<h2 id="basic-stuff">Basic stuff</h2>
<p> These desugar to the following Core:  (Notation: I am omitting foralls, big lambdas, and type arguments. I'm also using `f x = e` rather than `f = \x.e`.)</p>
<p>Points worth noting:</p>
<p><code>* The class gives rise to an eponymous data type (in GHC it is actually</code><br />
<code>  called `:TC`), the dictionary. </code></p>
<p><code>* There is an eponymous top-level selector function for each class method, </code><br />
<code>  `opF` and `opG` in this case.</code></p>
<p><code>* The default method for `opG` becomes a top-level function `$dmopG`.</code><br />
<code>  It takes the `(C a)` dictionary a argument because the RHS is allowed to call</code><br />
<code>  other methods of C.</code></p>
<p><code>* The instance declaration defines a dictionary `dCInt`.  Notice  </code><br />
<code>  that it's recursive, because we must pass `dCInt` to `opGI`.</code></p>
<p><code>* Crucially, the simplifier is careful not to choose `dCInt` as</code><br />
<code>  a loop breaker, and hence if it sees `case dCInt of ...` it</code><br />
<code>  can simplify the `case`. </code></p>
<p><code>* If `$dmopG` is inlined, the recursion is broken anyway.</code></p>
<h2 id="dictionary-functions">Dictionary functions</h2>
<p>Now consider an instance declaration that has a context:  Here is one way to desugar it.  Notice that</p>
<p><code>* If we inline the selector `opF` in `opF d_as`, then</code><br />
<code>  we can simplify `opfl` to give a directly-recursive function:</code></p>
<p></p>
<p><code>  This is important.</code></p>
<p><code>* The BAD THING is that `dCList` is big, and hence won't be inlined.</code><br />
<code>  That's bad because it means that if we see</code></p>
<p></p>
<p><code>  we don't get to call `opfl` directly. Instead we'll call `dCList`, build </code><br />
<code>  the dictionary, do the selection, etc.  So specialiation won't happen,</code><br />
<code>  even when all the types are fixed.</code></p>
<h2 id="the-inline-strategy">The INLINE strategy</h2>
<p>An obvious suggestion, which GHC implemented for a long time, is to give `dCList` an INLINE pragma. Then it'll inline at every call site, the dictionary will be visible to the selectors, and good things happen.</p>
<p>But it leads to a huge code blow-up in some cases. We call these dictionary functions a lot, often in a nested way, and we know programs for which the INLINE-all-dfuns approach generates gigantic code. (Example: Serge's !DoCon.)</p>
<h2 id="the-out-of-line-a-strategy">The out-of-line (A) strategy</h2>
<p>The INLINE strategy would make sense if `dCList` could be guaranteed small. Suppose the original instance declaration had been like this:  This is exactly what GHC 6.10 now does, behind the scenes. Desugaring just as above, we'd get the following:  Notice that</p>
<p><code>* `dCList` is guaranteed small, and could reasonably be INLINEd</code><br />
<code>  at every call site.  This good because it exposes the dictionary</code><br />
<code>  structure to selectors.</code></p>
<p><code>* `dCList` and `opF_aux` are mutually recursive.  But if we </code><br />
<code>  avoid choosing `dCList` as the loop breaker we can inline</code><br />
<code>  `dCList` into `opF_aux`, and then the `opF` selector</code><br />
<code>  can &quot;see&quot; the dictionary structure, and `opF_aux` simplifies, thus:</code></p>
<p></p>
<p><code>  Good!  Now `opF_aux` is self-recursive as it should be.</code><br />
<code>  The same thing happens with two mutually recursive methods</code></p>
<p><code>* BUT notice that we reconstruct the `(C [a])` dictionary on</code><br />
<code>  each iteration of the loop.  As Ganesh points out in #3073, that</code><br />
<code>  is sometimes bad.</code></p>
<h2 id="the-out-of-line-b-strategy">The out-of-line (B) strategy</h2>
<p>We can avoid reconstructing the dictionary by passing it to `opF_aux`, by recasting latter thus:  Notice the extra `C [a]` in the context of `opF_aux`. (Remember this is all internal to GHC.) Now the same desugaring does this:  The two definitions aren't even recursive. BUT now that `d_as` is an <em>argument</em> of `opF_aux`, the latter can't &quot;see&quot; that it's always a dictionary! Sigh. As a result, the recursion in `opF_aux` always indirects through the (higher order) dictionary argument, using a so-called &quot;unknown&quot; call, which is <em>far</em> less efficient than direct recursion.</p>
<p>Note also that</p>
<p><code>* Typechecking `opF_aux` is a bit fragile; see #3018.  Trouble is that</code><br />
<code>  when a constraint `(C [a])` arises in its RHS there are two ways</code><br />
<code>  of discharging it: by using the argument `d_as` directly, or by</code><br />
<code>  calling `(dCList d_a)`.  As #3018 shows, it's hard to guarantee that</code><br />
<code>  we'll do the former.</code></p>
<h2 id="user-inline-pragmas-and-out-of-line-a">User INLINE pragmas and out-of-line (A)</h2>
<p>There is another difficulty with the out-of-line(A) strategy, that is currently unsolved. Consider something like this:  Then we'll desugar to something like this:  The INLINE on `dCT` is added by the compiler; the INLINE on `opF_aux` is just propagated from the users's INLINE pragma... maybe the RHS is big.</p>
<p>Now the difficulty is that we GHC currently doesn't inline into the RHS of an INLINE function (else you'd get terrible code blowup). So the recursion between `dCT` and `opF_aux` is not broken. One of the two must be chosen as loop breaker, and the simplifier chooses `opF_aux`. Ironcially, therefore the user INLINE pragma has served only to guarantee that it <em>won't</em> be inlined!!</p>
<p>(This issue doesn't arise with out-of-line(B) because (B) doesn't make `dCT` and `opF_aux` mutually recursive.)</p>
<h2 id="summary">Summary</h2>
<p>Here are the current (realistic) options:</p>
<p><code>* Out-of-line(A): GHC 6.10 does this. </code><br />
<code>  * Good: recursive methods become directly mutually-recursive</code><br />
<code>  * Bad: lack of memoisation</code><br />
<code>  * Bad: difficulty with user INLINE pragmas</code></p>
<p><code>* Out-of-line(B)</code><br />
<code>  * Good: memoisation works</code><br />
<code>  * Very bad: recursive methods iterate only via &quot;unknown&quot; calls.</code><br />
<code>  * Good: no difficulty with user INLINE pragmas</code></p>
<p>My current difficulty is that I see no way to get all the good things at once.</p>
<p>PS: see also the comments at the start of `compiler/typecheck/TcInstDcls.lhs`, which cover some of the same ground.</p>
<h1 id="bugs-other-problems">Bugs &amp; Other Problems</h1>
<p>I've moved all known bugs into the trac bug database, the can be found <a href="http://hackage.haskell.org/trac/ghc/query?status=infoneeded&amp;status=merge&amp;status=new&amp;status=patch&amp;component=Compiler+%28LLVM%29&amp;order=priority&amp;col=id&amp;col=summary&amp;col=status&amp;col=type&amp;col=priority&amp;col=milestone&amp;col=component">here</a></p>
<h1 id="compiling-more-than-one-module-at-once">Compiling more than one module at once</h1>
<p>When compiling a single module, we can assume that all of our dependencies have already been compiled, and query the environment as necessary when we need to do things like look up interfaces to find out what the types in our dependencies are. When we compile more than module at once, as in `--make`, things get a bit more complicated:</p>
<p>1. We have to analyze the dependency structure of the program in question, and come up with a plan for how to compile the various modules, and</p>
<p>2. We have an opportunity to cache and reuse information from interface files which we may load from the environment. This is why, for example, `ghc --make` outperforms parallel one-shot compilation on one core.</p>
<p>This discussion is going to omit concerns related to dynamic code loading in GHC (as would be the case in GHCi).</p>
<h2 id="the-overall-driver">The overall driver</h2>
<p>The meat of this logic is in <a href="GhcFile(compiler/main/GhcMake.hs)" class="uri" title="wikilink">GhcFile(compiler/main/GhcMake.hs)</a>, with primary entry point the function `load` (in the case of `--make`, this function is called with `LoadAllTargets`, instructing all target modules to be compiled, which is stored in `hsc_targets`).</p>
<h3 id="dependency-analysis-1">Dependency analysis</h3>
<p>Dependency analysis is carried out by the `depanal` function; the resulting `ModuleGraph` is stored into `hsc_mod_graph`. Essentially, this pass looks at all of the imports of the target modules (`hsc_targets`), and recursively pulls in all of their dependencies (stopping at package boundaries.) The resulting module graph consists of a list of `ModSummary` (defined in <a href="GhcFile(compiler/main/HscTypes.lhs)" class="uri" title="wikilink">GhcFile(compiler/main/HscTypes.lhs)</a>), which record various information about modules prior to compilation (recompilation checking, even), such as their module identity (the current package name plus the module name), whether or not the file is a boot file, where the source file lives. Dependency analysis inside GHC is often referred to as **downsweep**.</p>
<p>ToDo: say something about how hs-boot files are</p>
<p>The dependency analysis is cached (in `hsc_mod_graph`), so later calls to `depanal` can reuse this information. (This is not germane for `--make`, which only calls `depanal` once.) `discardProg` deletes this information entirely, while `invalidateModSummaryCache` simply &quot;touches&quot; the timestamp associated with the file so that we resummarize it.</p>
<p>The result of dependency analysis is topologically sorted in `load` by `topSortModuleGraph`.</p>
<h3 id="recompilation-checking-and-stability">Recompilation checking and stability</h3>
<p>See also the page on [wiki:Commentary/Compiler/RecompilationAvoidance recompilation avoidance].</p>
<p>ToDo: say something about stability; it's per SCC</p>
<h3 id="compilation">Compilation</h3>
<p>Compilation, also known as **upsweep**, walks the module graph in topological order and compiles everything. Depending on whether or not we are doing parallel compilation, this implemented by `upsweep` or by `parUpsweep`. In this section, we'll talk about the sequential upsweep.</p>
<p>The key data structure which we are filling in as we perform compilation is the **home package table** or HPT (`hsc_HPT`, defined in <a href="GhcFile(compiler/main/HscTypes.lhs)" class="uri" title="wikilink">GhcFile(compiler/main/HscTypes.lhs)</a>). As its name suggests, it contains informations from the *home package*, i.e. the package we are currently compiling. Its entries, `HomeModInfo`, contain the sum total knowledge of a module after compilation: both its pre-linking interface `ModIface` as well as the post-linking details `ModDetails`.</p>
<p>We *clear* out the home package table in the session (for `--make`, this was empty anyway), but we pass in the old HPT.</p>
<p>ToDo: talk about how we fix up loops after we finish the loop</p>
<p>Finally, when the module is completely done being compiled, it is registered in the home package table</p>
<p>ToDo: Talk about what happens when we fail while in the middle of compiling a module cycle</p>
<h1 id="eager-promotion">Eager Promotion</h1>
<p>Eager promotion is a technique we use in GHC to improve the performance of generational GC. It is somewhat specific to the characteristics of lazy evaluation, since it takes advantage of the fact that we have some objects that are mutated just once (i.e. thunks).</p>
<p>The key observation is this: when an object P contains a pointer to an object Q in a younger generation, and P is not mutable, then we know that Q cannot be garbage collected until the generation in which P resides is collected. Hence, we might as well promote Q to this generation immediately, rather than [wiki:Commentary/Rts/Storage/GC/Aging aging] it or promoting it to an intermediate generation. Furthermore, if eager promotion is successful, then the object containing the old-to-new pointers will no longer need to be in the [wiki:Commentary/Rts/Storage/GC/RememberedSets remembered set] for the generation it resides in.</p>
<p>We gave some performance results for this technique in <a href="http://www.haskell.org/~simonmar/papers/multicore-ghc.pdf">Runtime Support for Multicore Haskell</a>; the upshot is that it's worth 10% or so.</p>
<p>Eager promotion works like this. To do eager promtion, the scavenger sets the flag `gct-&gt;eager_promotion` (it can leave the flag set when scavenging multiple objects, this is the usual way), and `gct-&gt;evac_gen` is set to the generation to which to eagerly promote objects. The `evacuate` function will try to move each live object into `gct-&gt;evac_gen` or a higher generation if possible, and set `gct-&gt;failed_to_evac` if it fails (see [wiki:Commentary/Rts/Storage/GC/RememberedSets]). It may fail if the target object has already been moved: we can't move an object twice during GC, because there may be other pointers already updated to point to the new location. It may also fail if the object is in a generation that is not being collected during this cycle.</p>
<p>Objects which are repeatedly mutable should not be subject to eager promotion, because the object may be mutated again, so eagerly promoting the objects it points to may lead to retaining garbage unnecessarily. Hence, when we are scavenging a mutable object (see <a href="GhcFile(rts/sm/Scav.c)" class="uri" title="wikilink">GhcFile(rts/sm/Scav.c)</a>), we temporarily turn off `gct-&gt;eager_promotion`.</p>
<h1 id="eager-version-bumping-strategy">Eager Version Bumping Strategy</h1>
<p>Versioning of GHC core/boot libraries adheres to Haskell's <a href="https://wiki.haskell.org/Package_versioning_policy">Package Versioning Policy</a> whose scope is considered to apply to **released artifacts** (and therefore doesn't prescribe when to //actually// perform version increments during development)</p>
<p>However, in the spirit of continuous integration, GHC releases snapshot artifacts, and therefore it becomes important for early testers/evaluators/package-authors to be presented with accurate PVP-adhering versioning, especially for those who want adapt to upcoming API changes in new major GHC releases early (rather than being hit suddenly by a disruptive version-bump-wave occurring at GHC release time).</p>
<p>So while the usual scheme is to update a package version in the VCS right before a release (and reviewing at that point whether a patchlevel, minor or major version bump is mandated by the PVP), for GHC bundled core/boot packages, the **eager version bumping** scheme is preferred, which basically means:</p>
<p></p>
<p>This becomes particularly easy when also maintaining a `changelog` file during development highlighting the changes for releases, as then one easily keeps track of the last released version, as well as becoming aware more easily of minor/major version increment-worthy API changes.</p>
<p>Video: <a href="http://www.youtube.com/watch?v=pN9rhQHcfCo&amp;list=PLBkRCigjPwyeCSD_DFxpd246YIF7_RDDI">Types and Classes</a> (23'53&quot;)</p>
<h1 id="data-types-for-haskell-entities-and">Data types for Haskell entities: , , , , and </h1>
<p>For each kind of Haskell entity (identifier, type variable, type constructor, data constructor, class) GHC has a data type to represent it. Here they are:</p>
<p><code>* </code><strong><code>Type</code> <code>constructors</code></strong><code> are represented by the </code><code> type (</code><a href="GhcFile(compiler/types/TyCon.hs)" title="wikilink"><code>GhcFile(compiler/types/TyCon.hs)</code></a><code>).</code><br />
<code>* </code><strong><code>Classes</code></strong><code> are represented by the </code><code> type (</code><a href="GhcFile(compiler/types/Class.hs)" title="wikilink"><code>GhcFile(compiler/types/Class.hs)</code></a><code>).</code><br />
<code>* </code><strong><code>Data</code> <code>constructors</code></strong><code> are represented by the </code><code> type (</code><a href="GhcFile(compiler/basicTypes/DataCon.hs)" title="wikilink"><code>GhcFile(compiler/basicTypes/DataCon.hs)</code></a><code>).</code><br />
<code>* </code><strong><code>Pattern</code> <code>synonyms</code></strong><code> are represented by the </code><code> type (</code><a href="GhcFile(compiler/basicTypes/PatSyn.hs)" title="wikilink"><code>GhcFile(compiler/basicTypes/PatSyn.hs)</code></a><code>).</code><br />
<code>* </code><strong><code>Term</code> <code>variables</code></strong><code> </code><code> and </code><strong><code>type</code> <code>variables</code></strong><code> </code><code> are both represented by the </code><code> type (</code><a href="GhcFile(compiler/basicTypes/Var.hs)" title="wikilink"><code>GhcFile(compiler/basicTypes/Var.hs)</code></a><code>).</code></p>
<p>All of these entities have a , but that's about all they have in common. However they are sometimes treated uniformly:</p>
<p><code>* A </code><strong><code>`TyThing`</code></strong><code> (</code><a href="GhcFile(compiler/types/TypeRep.hs)" title="wikilink"><code>GhcFile(compiler/types/TypeRep.hs)</code></a><code>) is simply the sum of all four:</code></p>
<p></p>
<p><code>For example, a type environment is a map from </code><code> to </code><code>.  (The fact that a </code><code> tells what name space it belongs to allow, for example, identically named values and types to  sit in a single map.)</code></p>
<p>All these data types are implemented as a big record of information that tells you everything about the entity. For example, a  contains a list of its data constructors; a  contains its type (which mentions its ); a  contains the s of all its method selectors; and an  contains its type (which mentions type constructors and classes).</p>
<p>So you can see that the GHC data structures for entities is a <em>graph</em> not tree: everything points to everything else. This makes it very convenient for the consumer, because there are accessor functions with simple types, such as . But it means that there has to be some tricky almost-circular programming (&quot;knot-tying&quot;) in the type checker, which constructs the entities.</p>
<h2 id="type-variables-and-term-variables">Type variables and term variables</h2>
<p>Type variables and term variables are represented by a single data type, , thus (<a href="GhcFile(compiler/basicTypes/Var.hs)" class="uri" title="wikilink">GhcFile(compiler/basicTypes/Var.hs)</a>):  It's incredibly convenient to use a single data type for both, rather than using one data type for term variables and one for type variables. For example:</p>
<p><code>* Finding the free variables of a term gives a set of variables (both type and term variables): </code><code>.</code><br />
<code>* We only need one lambda constructor in Core: </code><code>.</code></p>
<p>The  type distinguishes the two sorts of variable; indeed, it makes somewhat finer distinctions (<a href="GhcFile(compiler/basicTypes/Var.hs)" class="uri" title="wikilink">GhcFile(compiler/basicTypes/Var.hs)</a>):  Every  has fields  and a . The latter is identical to the  in the former, but is cached in the  for fast comparison.</p>
<p>Here are some per-flavour notes:</p>
<p><code>:: is self explanatory.</code></p>
<p><code>:: is used during type-checking only.  Once type checking is finished, there are no more </code><code>s.</code></p>
<p><code>:: is used for term variables bound </code><em><code>in</code> <code>the</code> <code>module</code> <code>being</code> <code>compiled</code></em><code>.   More specifically, a </code><code> is bound either </code><em><code>within</code></em><code> an expression (lambda, case, local let), or at the top level of the module being compiled.</code><br />
<code>* The </code><code> of a </code><code> may change as the simplifier repeatedly bashes on it.</code><br />
<code>* A </code><code> carries a flag saying whether it's exported. This is useful for knowing whether we can discard it if it is not used.</code></p>
<p></p>
<p><code>:: is used for fixed, immutable, top-level term variables, notably ones that are imported from other modules.  This means that, for example, the optimizer won't change its properties.</code><br />
<code>* Always has an </code><code> or </code><code> [wiki:Commentary/Compiler/NameType Name], and hence has a </code><code> that is globally unique across the whole of a GHC invocation.</code><br />
<code>* Always bound at top level. </code><br />
<code>* The </code><code> of a </code><code> is completely fixed.</code><br />
<code>* All implicit Ids (data constructors, class method selectors, record selectors and the like) are are </code><code>s from birth, even the ones defined in the module being compiled.</code><br />
<code>* When finding the free variables of an expression (</code><code>), we only collect </code><code> and ignore </code><code>.</code></p>
<p>All the value bindings in the module being compiled (whether top level or not) are s until the !CoreTidy phase. In the !CoreTidy phase, all top-level bindings are made into s. This is the point when a  becomes &quot;frozen&quot; and becomes a fixed, immutable .</p>
<h2 id="and-implict-ids"> and implict Ids</h2>
<p>s are further classified by their . This type is defined in <a href="GhcFile(compiler/basicTypes/IdInfo.hs)" class="uri" title="wikilink">GhcFile(compiler/basicTypes/IdInfo.hs)</a>, because it mentions other structured types such as . Unfortunately it is <em>used</em> in Var.hs so there's a hi-boot knot to get it there. Anyway, here's the declaration (elided a little):  Some s are called <strong>implicit s</strong>. These are s that are defined by a declaration of some other entity (not just an ordinary variable binding). For example:</p>
<p><code>* The selectors of a record type</code><br />
<code>* The method selectors of a class</code><br />
<code>* The worker and wrapper Id for a data constructor</code></p>
<p>It's easy to distinguish these Ids, because the  field says what kind of thing it is: .</p>
<h1 id="hc-files-and-the-evil-mangler">HC files and the Evil Mangler</h1>
<p>GHC no longer has an evil mangler.</p>
<h1 id="strictness-analysis-examples">Strictness analysis: examples</h1>
<p>Consider:</p>
<p></p>
<p>We want to make sure to figure out that f's argument is demanded with type L1X(L1X(LMX)) -- that is, it may or may not be demanded, but if it is, it's always applied to two arguments. This shows why  shouldn't just throw away the argument info: in this case, the  expression has a nonstrict demand placed on it, yet we still care about the arguments.</p>
<p>On the other hand, in:  we want to say that if the result of  has demand  placed on it (i.e., not a call demand), the body of  has demand  placed on it, not . So this case needs to be treated differently from the one above.</p>
<h1 id="system-fc-equality-constraints-and-coercions">System FC: equality constraints and coercions</h1>
<p>For many years, GHC's intermediate language was essentially:</p>
<p><code>* System Fw, plus</code><br />
<code>* algebraic data types (including existentials)</code></p>
<p>But that is inadequate to describe GADTs and associated types. So in 2006 we extended GHC to support System FC, which adds</p>
<p><code>* equality constraints and coercions</code></p>
<p>You can find a full description of FC in the paper <a href="http://research.microsoft.com/~simonpj/papers/ext-f">3</a>; note that GHC uses the system described in post-publication Appendix C, not the system in the main body of the paper. The notes that follow sketch the implementation of FC in GHC, but without duplicating the contents of the paper.</p>
<p>A coercion `c`, is a type-level term, with a kind of the form `T1 :=: T2`. (`c :: T1 :=: T2`) is a proof that a term of type `T1` can be coerced to type `T2`. Coercions are classified by a new sort of kind (with the form ). Most of the coercion construction and manipulation functions are found in the  module, <a href="GhcFile(compiler/types/Coercion.hs)" class="uri" title="wikilink">GhcFile(compiler/types/Coercion.hs)</a>.</p>
<p>Coercions appear in Core in the form of  expressions: if `t :: T1` and `c :: T1:=:T2`, then . See [wiki:Commentary/Compiler/CoreSynType].</p>
<h2 id="coercions-and-coercion-kinds">Coercions and Coercion Kinds</h2>
<p>The syntax of coercions extends the syntax of types (and the type `Coercion` is just a synonym for `Type`). By representing coercion evidence on the type level, we can take advantage of the existing erasure mechanism and keep non-termination out of coercion proofs (which is necessary to keep the system sound). The syntax of coercions and types also overlaps a lot. A normal type is evidence for the reflexive coercion, i.e.,  Coercion variables are used to abstract over evidence of type equality, as in </p>
<p>There are also coercion constants that are introduced by the compiler to implement some source language features (newtypes for now, associated types soon and probably more in the future). Coercion constants are represented as `TyCon`s made with the constructor `CoercionTyCon`.</p>
<p>Coercions are type level terms and can have normal type constructors applied to them. The action of type constructors on coercions is much like in a logical relation. So if `c1 :: T1 :=: T2` then</p>
<p></p>
<p>and if `c2 :: S1 :=: S2` then  The sharing of syntax means that a normal type can be looked at as either a type or as coercion evidence, so we use two different kinding relations, one to find type-kinds (implemented in Type as `typeKind :: Type -&gt; Kind`) and one to find coercion-kinds (implemented in Coercion as `coercionKind :: Coercion -&gt; Kind`).</p>
<p>Coercion variables are distinguished from type variables, and non-coercion type variables (just like any normal type) can be used as the reflexive coercion, while coercion variables have a particular coercion kind which need not be reflexive.</p>
<h2 id="gadts">GADTs</h2>
<p>The internal representation of GADTs is as regular algebraic datatypes that carry coercion evidence as arguments. A declaration like  would result in a data constructor with type  This means that (unlike in the previous intermediate language) all data constructor return types have the form `T a1 ... an` where `a1` through `an` are the parameters of the datatype.</p>
<p>However, we also generate wrappers for GADT data constructors which have the expected user-defined type, in this case  Where the 4th and 5th arguments given to `T1` are the reflexive coercions </p>
<h2 id="representation-of-coercion-assumptions">Representation of coercion assumptions</h2>
<p>In most of the compiler, as in the FC paper, coercions are abstracted using `ForAllTy cv ty` where `cv` is a coercion variable, with a kind of the form `PredTy (EqPred T1 T2)`. However, during type inference it is convenient to treat such coercion qualifiers in the same way other class membership or implicit parameter qualifiers are treated. So functions like `tcSplitForAllTy` and `tcSplitPhiTy` and `tcSplitSigmaTy`, treat `ForAllTy cv ty` as if it were `FunTy (PredTy (EqPred T1 T2)) ty` (where `PredTy (EqPred T1 T2)` is the kind of `cv`). Also, several of the `dataCon`XXX functions treat coercion members of the data constructor as if they were dictionary predicates (i.e. they return the `PredTy (EqPred T1 T2)` with the theta).</p>
<h2 id="newtypes-are-coerced-types">Newtypes are coerced types</h2>
<p>The implementation of newtypes has changed to include explicit type coercions in the place of the previously used ad-hoc mechanism. For a newtype declared by  the `NewTyCon` for `T` will contain n`t_co = CoT` where:  This `TyCon` is a `CoercionTyCon`, so it does not have a kind on its own; it basically has its own typing rule for the fully-applied version. If the newtype `T` has k type variables, then `CoT` has arity at most k. In the case that the right hand side is a type application ending with the same type variables as the left hand side, we &quot;eta-contract&quot; the coercion. So if we had  then we would generate the arity 0 coercion `CoS : S :=: []`. The primary reason we do this is to make newtype deriving cleaner. If the coercion cannot be reduced in this fashion, then it has the same arity as the tycon.</p>
<p>In the paper we'd write  and then when we used `CoT` at a particular type, `s`, we'd say  which encodes as `(TyConApp instCoercionTyCon [TyConApp CoT [], s])`</p>
<p>But in GHC we instead make `CoT` into a new piece of type syntax (like `instCoercionTyCon`, `symCoercionTyCon` etc), which must always be saturated, but which encodes as  In the vocabulary of the paper it's as if we had axiom declarations like  The newtype coercion is used to wrap and unwrap newtypes whenever the constructor or case is used in the Haskell source code.</p>
<p>Such coercions are always used when the newtype is recursive and are optional for non-recursive newtypes. Whether or not they are used can be easily changed by altering the function mkNewTyConRhs in iface/BuildTyCl.lhs.</p>
<h2 id="roles">Roles</h2>
<p>Roles specify what nature of equality a coercion is proving. See [wiki:Roles] and RolesImplementation.</p>
<h2 id="simplification">Simplification</h2>
<p><code>* exprIsConApp_maybe</code></p>
<p><code>* simplExpr</code></p>
<h1 id="ghc-commentary-runtime-aspects-of-the-ffi">GHC Commentary: Runtime aspects of the FFI</h1>
<h2 id="foreign-import-wrapper">Foreign Import &quot;wrapper&quot;</h2>
<p>Files <a href="GhcFile(rts/Adjustor.c)" class="uri" title="wikilink">GhcFile(rts/Adjustor.c)</a> <a href="GhcFile(rts/AdjustorAsm.S)" class="uri" title="wikilink">GhcFile(rts/AdjustorAsm.S)</a>.</p>
<p>Occasionally, it is convenient to treat Haskell closures as C function pointers. This is useful, for example, if we want to install Haskell callbacks in an existing C library. This functionality is implemented with the aid of adjustor thunks.</p>
<p>An adjustor thunk is a dynamically allocated code snippet that allows Haskell closures to be viewed as C function pointers.</p>
<p>Stable pointers provide a way for the outside world to get access to, and evaluate, Haskell heap objects, with the RTS providing a small range of ops for doing so. So, assuming we've got a stable pointer in our hand in C, we can jump into the Haskell world and evaluate a callback procedure, say. This works OK in some cases where callbacks are used, but does require the external code to know about stable pointers and how to deal with them. We'd like to hide the Haskell-nature of a callback and have it be invoked just like any other C function pointer.</p>
<p>Enter adjustor thunks. An adjustor thunk is a little piece of code that's generated on-the-fly (one per Haskell closure being exported) that, when entered using some 'universal' calling convention (e.g., the C calling convention on platform X), pushes an implicit stable pointer (to the Haskell callback) before calling another (static) C function stub which takes care of entering the Haskell code via its stable pointer.</p>
<p>An adjustor thunk is allocated on the C heap, and is called from within Haskell just before handing out the function pointer to the Haskell (IO) action. User code should never have to invoke it explicitly.</p>
<p>An adjustor thunk differs from a C function pointer in one respect: when the code is through with it, it has to be freed in order to release Haskell and C resources. Failure to do so will result in memory leaks on both the C and Haskell side.</p>
<hr />
<p>CategoryStub</p>
<h1 id="function-calls">Function Calls</h1>
<p>Source files: <a href="GhcFile(rts/Apply.h)" class="uri" title="wikilink">GhcFile(rts/Apply.h)</a>, <a href="GhcFile(rts/Apply.cmm)" class="uri" title="wikilink">GhcFile(rts/Apply.cmm)</a></p>
<p>Dealing with calls is by far the most complicated bit of the execution model, and hence of the code generator. GHC uses an <em>eval/apply</em> strategy for compiling function calls; all the details of the design are in the paper <a href="http://www.haskell.org/~simonmar/papers/eval-apply.pdf">Making a fast curry: push/enter vs. eval/apply for higher-order languages</a>.</p>
<p>First, we need some terminology:</p>
<p><code> * The </code><strong><code>arity</code></strong><code> of a function is the number of lambdas statically used in [wiki:Commentary/Compiler/StgSynType the lambda-form of its definition].  Note that arity is not deducible from the type.  Example:</code></p>
<p></p>
<p><code>   Here, `f` has arity 1, even though its type suggests it takes two arguments.  The point is that the compiled code for `f` will expect to be passed just one argument, `x`.</code></p>
<p><code> * The </code><strong><code>entry</code> <code>point</code></strong><code> (sometimes called the </code><strong><code>fast</code> <code>entry</code> <code>point</code></strong><code>) of a function of arity N expects its first N  arguments to be passed in accordance with the standard [wiki:Commentary/Rts/HaskellExecution/CallingConvention calling conventions].</code></p>
<p><code> * A </code><strong><code>known</code> <code>call</code></strong><code> is a call of a function whose binding site is statically visible:</code><br />
<code>   * The function is bound at top level in this module; or,</code><br />
<code>   * The function is bound at top level in another module, and optimistion is on, so we can see the details (notably arity) of the function in the module's interface file; or,</code><br />
<code>   * The function is bound by an `let` binding that encloses the call.</code></p>
<p>When compiling a call, there are several cases to consider, which are treated separately.</p>
<p><code> * </code><strong><code>Unknown</code> <code>function</code></strong><code>;  a call in which we do not statically know what the function is.  In that case we must do a &quot;generic apply&quot;.  This is so exciting that it deserves its [wiki:Commentary/Rts/HaskellExecution/FunctionCalls#Genericapply own section].</code></p>
<p><code> * </code><strong><code>Known</code> <code>function,</code> <code>saturated</code> <code>call</code></strong><code>.   The function is applied to exactly the right number of arguments to satisfy its arity.  In that case, we simply load the arguments according to the standard entry convention, and tail-call (jump to) the function's entry point.  On average, about 80% of all calls fall into this category (see the eval/apply paper for measurements).</code></p>
<p><code> * </code><strong><code>Known</code> <code>function,</code> <code>too</code> <code>few</code> <code>arguments</code></strong><code>.  In this case, we want to build a partial application (PAP), and return with a pointer to the PAP in the return register.  Since building a PAP is a complicated business, instead we just behave as for an unknown function call, which will end up calling into the </code><a href="ref(Generic_apply)" title="wikilink"><code>ref(Generic</code> <code>apply)</code></a><code> code, which will build the PAP for us.</code></p>
<p><code> * </code><strong><code>Known</code> <code>function,</code> <code>too</code> <code>many</code> <code>arguments</code></strong><code>.  We want to save the extra arguments on the stack, push a return address, and then behave just like a saturated call.  When the result comes back, we should behave like &quot;unknown call&quot;.  However, to avoid needing to generate code for a new continuation here, the return address that we push on the stack is that of an appropriate </code><a href="ref(Generic_apply)" title="wikilink"><code>ref(Generic</code> <code>apply)</code></a><code> function, which will perform the application of the extra arguments to the (unknown) function returned by the saturated call.</code></p>
<h2 id="generic-apply">Generic apply</h2>
<p>Files: <a href="GhcFile(utils/genapply)" class="uri" title="wikilink">GhcFile(utils/genapply)</a></p>
<p>When compiling a call that has an unknown function, we must generate code to</p>
<p><code> * Evaluate the function</code><br />
<code> * Scrutinise the function value returned to see its arity, and dispatch into the same three cases as in the case of known calls:</code><br />
<code>   * Exactly the right number of arguments: load them into the standard locations and tail-call the function's entry point</code><br />
<code>   * Too few arguments: build a PAP</code><br />
<code>   * Too many arguments: save the excess arguments, and tail call the function as for a saturated cal.</code></p>
<p>All of this takes quite a lot of code, so we pre-generate a whole bunch of generic-apply code sequencues, one for each combination of arguments. This code is generated by the tool <a href="GhcFile(utils/genapply)" class="uri" title="wikilink">GhcFile(utils/genapply)</a>, and the generated code appears in `rts/AutoApply.cmm`.</p>
<p>For example, if we find a call to an unknown function applied to two (boxed) `Int` arguments, load the function and its two arguments as for the standard entry convention and jump to `stg_ap_pp_fast`. This latter code is in `rts/AutoApply.cmm`, generated by the `genapply` tool. The &quot;`pp`&quot; part is the bit that says the code is specialised for two pointer arguments.</p>
<p>In addition to the family of `stg_ap_<pattern>_fast` functions for making calls to unknown functions with various argument patterns, there is a corresponding family of return addresses `stg_ap_<pattern>_info`. The idea is that you can push a continuation that will make a call to the function that is returned to it. For example, to push a continuation that will apply a single pointer argument, we would push the following words on the stack:</p>
<p>|| arg || || `stg_ap_p_info` ||</p>
<h1 id="the-garbage-collector">The Garbage Collector</h1>
<p>GC concepts:</p>
<p><code>* [wiki:Commentary/Rts/Storage/GC/Aging Aging]</code><br />
<code>* [wiki:Commentary/Rts/Storage/GC/Pinned Pinned objects]</code><br />
<code>* [wiki:Commentary/Rts/Storage/GC/Roots Roots]</code><br />
<code>* [wiki:Commentary/Rts/Storage/GC/EagerPromotion Eager promotion]</code><br />
<code>* [wiki:Commentary/Rts/Storage/GC/RememberedSets Remembered sets]</code><br />
<code>* [wiki:Commentary/Rts/Storage/GC/Weak Weak pointers and finalizers]</code><br />
<code>* [wiki:Commentary/Rts/Storage/GC/CAFs CAFs]</code></p>
<p>GC algorithms supported:</p>
<p><code>* [wiki:Commentary/Rts/Storage/GC/Copying Copying GC]</code><br />
<code>* [wiki:Commentary/Rts/Storage/GC/Parallel Parallel GC]</code><br />
<code>* [wiki:Commentary/Rts/Storage/GC/Marking Marking] (for compaction or sweeping)</code><br />
<code>* [wiki:Commentary/Rts/Storage/GC/Compaction Compaction]</code><br />
<code>* [wiki:Commentary/Rts/Storage/GC/Sweeping Sweeping] (for mark-region GC)</code><br />
<code>* [wiki:Commentary/Rts/Storage/GC/Immix Immix] (not supported yet)</code></p>
<h2 id="gc-overview">GC overview</h2>
<p>The GC is designed to be flexible, supporting lots of ways to tune its behaviour. Here's an overview of the techniques we use:</p>
<p><code>* Generational GC, with a runtime-selectable number of generations (`+RTS -G</code><n><code> -RTS`, where `n &gt;= 1`).  Currently it is a</code><br />
<code>  traditional generational collector where each collection collects a particular generation and all younger generations.</code><br />
<code>  Generalizing this such that any subset of generations can be collected is a possible future extension.</code></p>
<p><code>* The heap grows on demand.  This is straightforwardly implemented by basing the whole storage manager on a [wiki:Commentary/Rts/Storage/BlockAlloc block allocator].</code></p>
<p><code>* Aging: objects can be aged within a generation, to avoid premature promotion.  See [wiki:Commentary/Rts/Storage/GC/Aging].</code></p>
<p><code>* The heap collection policy is runtime-tunable.  You select how large a generation gets before it is collected using the `+RTS -F</code><n><code> -RTS` option, where `</code><n><code>` is a factor of the generation's size the last time it was collected.  The default value is 2, that is a generation is allowed to double in size before being collected.</code></p>
<h2 id="gc-data-structures">GC data structures</h2>
<p><a href="GhcFile(includes/rts/storage/GC.h)" class="uri" title="wikilink">GhcFile(includes/rts/storage/GC.h)</a></p>
<h3 id="generation">generation</h3>
<p>The main data structure is `generation`, which contains:</p>
<p><code>`blocks`::</code><br />
<code>  a pointer to a list of blocks</code></p>
<p><code>`large_objects`::</code><br />
<code>  a pointer to a list of blocks containing large objects</code></p>
<p><code>`threads`::</code><br />
<code>  a list of threads in this generation</code></p>
<p><code>`mut_list`::</code><br />
<code>  the [wiki:Commentary/Rts/Storage/GC/RememberedSets remembered set], a list of blocks containing pointers to objects in </code><em><code>this</code></em><code> generation that point to objects in </code><em><code>younger</code></em><code> generations</code></p>
<p>and various other administrative fields (see <a href="GhcFile(includes/rts/storage/GC.h)" class="uri" title="wikilink">GhcFile(includes/rts/storage/GC.h)</a> for the details).</p>
<p>Generations are kept in the array `generations[]`, indexed by the generation number.</p>
<h3 id="nursery">nursery</h3>
<p>A `nursery` is a list of blocks into which the mutator allocates new (small) objects. For reasons of locality, we want to re-use the list of blocks for the nursery after each GC, so we keep the nursery blocks rather than freeing and re-allocating a new nursery after GC.</p>
<p>The struct `nursery` contains only two fields</p>
<p><code>`blocks`::</code><br />
<code>  the list of blocks in this nursery</code><br />
<code>`n_blocks`::</code><br />
<code>  the number of blocks in the above list</code></p>
<p>In the threaded RTS, there is one nursery per Capability, as each Capability allocates independently into its own allocation area. Nurseries are therefore stored in an array `nurseries[]`, indexed by Capability number.</p>
<p>The blocks of the nursery notionally logically to generation 0, although they are not kept on the list `generations[0].blocks`. The reason is that we want to keep the actual nursery blocks separate from any blocks containing live data in generation 0. Generation 0 may contain live data for two reasons:</p>
<p><code>* objects live in the nursery are not promoted to generation 1 immediately, instead they are [wiki:Commentary/Rts/Storage/GC/Aging aged], first being copied to generation 0, and then being promoted to generation 1 in the next GC cycle if they are still alive.</code></p>
<p><code>* If there is only one generation (generation 0), then live objects in generation 0 are retained in generation 0 after a GC.</code></p>
<h1 id="i-know-kung-fu-learning-stg-by-example">I know kung fu: learning STG by example</h1>
<p>The STG machine is an essential part of GHC, the world's leading Haskell compiler. It defines how the Haskell evaluation model should be efficiently implemented on standard hardware. Despite this key role, it is generally poorly understood amongst GHC users. This document aims to provide an overview of the STG machine in its modern, eval/apply-based, pointer-tagged incarnation by a series of simple examples showing how Haskell source code is compiled.</p>
<h2 id="what-is-stg-exactly">What is STG, exactly?</h2>
<p>Haskell code being sucked through GHC has a complex lifecycle. Broadly speaking, it transitions between five representations:</p>
<p></p>
<p>The path from C-- to assembly varies: the three possible backends are C (`-fvia-c`), LLVM (`-fllvm`), and the default backend -- the native code genarator (or NCG), which generates assembly directly from the GHC-internal C-- data type.</p>
<p>STG is a simple functional language, rather like the more famous Core language. It differs in the following main respects:</p>
<p><code>1. In its current incarnation, it isn't typed in the Haskell sense,</code><br />
<code>   though it does know about </code><em><code>representation</code></em><code> types</code><br />
<code>2. It is in administrative normal form (ANF), which is where every</code><br />
<code>   subexpression is given a name</code><br />
<code>3. Every $\lambda$, constructor application, and primitive operator</code><br />
<code>   is $\eta$-expanded</code><br />
<code>4. It is annotated with a ton of information that the code</code><br />
<code>   generator is interested in knowing</code></p>
<p>STG expressions can be one of the following:</p>
<p><code>1. Atoms (i.e. literals and variables)</code><br />
<code>2. `let`-bindings (both recursive and non-recursive) over another</code><br />
<code>   expression, where let-bound things are one of:</code><br />
<code>    * A function value with explicit lambdas</code><br />
<code>    * An unsaturated application</code><br />
<code>    * A constructor applied to atoms</code><br />
<code>    * A thunk (i.e. any expression not fitting into one of the above</code><br />
<code>      categories)</code></p>
<p><code>3. Saturated primitive application of a primitive to variables</code><br />
<code>4. Application of a variable to one or more atoms</code><br />
<code>5. Case deconstruction of an expression, where each branch may also</code><br />
<code>   be an expression</code></p>
<p>The job of the <em>STG machine</em> is to evaluate these expressions in a way which is efficiently implementable on standard hardware. This document will look at how exactly this is achieved by looking at real examples of the C-- code GHC generates for various Haskell expressions.</p>
<p>This document will take a very low-level view of the machine, so if you want to get comfortable with how the STG machine executes at a more abstract level before reading this document, you might want to read the paper <a href="http://research.microsoft.com/en-us/um/people/simonpj/papers/eval-apply/">&quot;How to make a fast curry: push/enter vs. eval/apply&quot;</a>. It presents the STG machine without reference to an explicit stack or registers, but instead as a transition system. This transition system has also been implemented as a Haskell program called <a href="http://hackage.haskell.org/package/ministg">ministg</a> by <a href="http://ww2.cs.mu.oz.au/~bjpop/">Bernie Pope</a>, for those who wish to see it in action on some simple examples.</p>
<h2 id="an-overview-of-the-stg-machine">An overview of the STG machine</h2>
<p>Before we dive in, a note: this document will describe the STG machine as it is implemented on x86-style architectures. I will use the terms &quot;the STG machine&quot; and &quot;the STG machine as implemented on x86 by GHC&quot; interchangeably. The implementation is somewhat different on x64, not least due to the greater number of available registers.</p>
<p>This overview section is rather bare. Readers might be able to fill in any gaps in my explanation by using some of the following sources:</p>
<p><code>* </code><a href="http://hackage.haskell.org/trac/ghc/wiki/Commentary/Rts/HaskellExecution"><code>The</code> <code>Haskell</code> <code>Execution</code> <code>Model</code></a><br />
<code>* </code><a href="http://hackage.haskell.org/trac/ghc/wiki/Commentary/Rts/Storage"><code>Storage</code></a><br />
<code>* </code><a href="http://research.microsoft.com/en-us/um/people/simonpj/Papers/spineless-tagless-gmachine.ps.gz"><code>The</code> <code>Spineless</code> <code>Tagless</code> <code>G-machine</code></a><br />
<code>  - now sadly rather out of date</code><br />
<code>* </code><a href="http://research.microsoft.com/en-us/um/people/simonpj/papers/ptr-tag/ptr-tagging.pdf"><code>Faster</code> <code>laziness</code> <code>through</code> <code>dynamic</code> <code>pointer</code> <code>tagging</code></a></p>
<h3 id="components-of-the-machine">Components of the machine</h3>
<p>In its bare essentials, the STG machine consists of three parts:</p>
<p><code>1. The STG registers:</code><br />
<code>    * There are rather a lot of registers here: more than can be</code><br />
<code>      practicably stored in actual available processor registers on most</code><br />
<code>      architectures.</code><br />
<code>    * To deal with the lack of processor registers, most of the STG </code><br />
<code>      registers are actually kept on the stack in a block of memory</code><br />
<code>      pointed to by a special STG register called the &quot;base register&quot; (or </code><br />
<code>      `BaseReg`). To get or set values of registers which are not kept in</code><br />
<code>      processor registers, the STG machine generates an instruction to</code><br />
<code>      load or store from an address relative to the `BaseReg`.</code><br />
<code>    * The most important four registers are the `BaseReg`, the stack</code><br />
<code>      pointer (`Sp`), the heap pointer (`Hp`), and the general purpose</code><br />
<code>      register `R1` which is used for intermediate values, as well as for </code><br />
<code>      returning evaluated values when unwinding the stack. These are the </code><br />
<code>      four registers which are assigned actual processor registers when</code><br />
<code>      implementing the STG machine on x86.</code><br />
<code>2. The STG stack:</code><br />
<code>    * Stores function arguments and continuations (i.e. the stack</code><br />
<code>      frames which are executed when a function returns)</code><br />
<code>    * Grows downwards in memory</code><br />
<code>    * The top of the stack is pointed to by the STG register `Sp`, and </code><br />
<code>      the maximum available stack pointer is stored in `SpLim`. There is</code><br />
<code>      no frame pointer.</code></p>
<p><code>3. The heap:</code><br />
<code>   * Used to store many different sorts of heap object: notably</code><br />
<code>     functions, thunks and data constructors</code><br />
<code>   * Grows upwards in memory, towards the stack</code><br />
<code>   * All allocation occurs using a bump-allocator: the heap pointer is</code><br />
<code>     simply incremented by the number of bytes desired (subject to to a</code><br />
<code>     check that this does not exhaust available memory). The garbage</code><br />
<code>     collector is responsible for moving objects out of the area of the </code><br />
<code>     heap managed by the bump allocator and into the care of its </code><br />
<code>     generational collector.</code><br />
<code>   * The last address in the bump-allocated part of the heap that has </code><br />
<code>     been used is pointed to by the STG register `Hp`, with `HpLim`</code><br />
<code>     holding the maximum address available for bump-allocation.</code></p>
<h3 id="important-concepts-in-the-machine">Important concepts in the machine</h3>
<p>Some of the key concepts in the STG machine include <em>closures</em>, <em>info tables</em> and <em>entry code</em>. We tackle them in reverse order:</p>
<p><code>Entry code::</code><br />
<code>    The actual machine code that the STG machine will execute upon</code><br />
<code>    &quot;entry&quot;. Entry means different things for different heap objects.</code></p>
<p><code>     * For </code><em><code>thunks</code></em><code>, entry is when the thunk is forced by some demand</code><br />
<code>       for its value, such as a `case` expression scrutinising it</code><br />
<code>     * For </code><em><code>functions</code></em><code>, entry is when the function is applied to as</code><br />
<code>       many arguments as are demanded by the arity recorded in its info</code><br />
<code>       table</code><br />
<code>     * For </code><em><code>continuations</code></em><code>, entry occurs when a value is returned from</code><br />
<code>       a nested call, and hence the need arises to consume the value and</code><br />
<code>       continue evaluation</code></p>
<p><code>Info table::</code><br />
<code>    A block of memory allocated statically, which contains metadata</code><br />
<code>    about a closure. The most important fields for our purposes are the</code><br />
<code>    entry code pointer and the arity information (if this is the info</code><br />
<code>    table for a thunk, function or partial application)</code></p>
<p><code>Closure::</code><br />
<code>    Essentially a heap-allocated pair of the free variables of some</code><br />
<code>    code, and a pointer to its info table (i.e. its info pointer).</code></p>
<p>For an example of how these parts work together, consider the following code</p>
<p></p>
<p>The nested lambda will give rise to all of the above objects.</p>
<p>The closure will store a pointer to `x`'s closure (as it is a free variable of the lambda), along with a pointer to an info table. That info table will contain information relevant to a function value, recording information such as the fact that it has an arity of 1 (i.e. the binding for `y`), and the pointer to the entry code for the function `\y -&gt; y + x` itself. This entry code will implement the addition by combining the closure for the free variable `x` (taken from the closure) with the stack-passed `y` variable's closure.</p>
<p>Upon entry to some code, pointers to closures are made available in `R1`. That is to say, before entry code is jumped to, `R1` is set up to point to the associated closure, so that the entry code can access free variables (if any).</p>
<p>Closures for code which contain no free variables (such as the closure for `True` and `False`, and functions applied to no arguments such as `(:)` and `id`) are allocated statically by the compiler in the same manner as info tables are.</p>
<h3 id="overview-of-execution-model-of-the-machine">Overview of execution model of the machine</h3>
<p>This will be covered in more detail in the examples below, so I will use this section to make some general points.</p>
<p>The goal of the STG machine is to reduce the current expression to a value. When it has done so, it:</p>
<p><code>1. Stores a tagged pointer to evaluated closure in the STG register</code><br />
<code>   `R1`</code><br />
<code>2. Jumps to the entry code of the info table pointed to by the</code><br />
<code>   value at the top of the STG stack</code><br />
<code>    * This may also be called the info table of the </code><em><code>continuation</code></em><code> of</code><br />
<code>      the expression</code></p>
<p>The continuation code is responsible for popping its info pointer (and stack-allocated free variables, if any) from the stack before returning.</p>
<p>Arguments are passed on the stack, and are popped by the callee. Upon a jump to the entry code for a function, there are always precisely as many arguments on the stack as the (statically known) arity of that function, and those arguments will be followed by the info pointer of a continuation.</p>
<h2 id="saturated-application-to-known-functions">Saturated application to known functions</h2>
<p>Handling application in the STG machine is a big topic, and so in this first section we only look at the case of <em>saturated</em> applications to <em>known</em> functions - i.e. those functions that the compiler statically knows information such as the entry code pointer and arity for.</p>
<h3 id="example-1-function-application-with-sufficient-stack-space">Example 1: function application with sufficient stack space</h3>
<p>Application of functions is the bread and butter of the STG machine. Correspondingly, this first Haskell program</p>
<p></p>
<p>compiles to very simple C-- code</p>
<p></p>
<p></p>
<p>The STG machine passes arguments to functions on the STG stack, and a pointer to the stack top is stored in the STG register `Sp`. Furthermore, because GHC currently uses the eval/apply variant of the STG machine, exactly as many arguments as the function expects to receive are guaranteed to present on the stack.</p>
<p>Therefore, upon entry to the `known_app` function, we are guaranteed that the STG stack has a pointer to a closure of type `()` on top of it. In order to call `known_fun`, we just modify the top of the stack to replace that pointer with a pointer to the statically allocated closure for the literal `10`, and then tail-call into the entry code of `known_fun`.</p>
<h3 id="example-2-function-application-that-needs-to-grow-the-stack">Example 2: function application that needs to grow the stack</h3>
<p>This Haskell code is apparently little more complicated than the previous example</p>
<p></p>
<p>however, it generates radically different C-- code:</p>
<p></p>
<p></p>
<p>As before, upon entry the STG stack is guaranteed to have a single closure pointer at its top. However, in order to call into known_fun_2 we need at least two free stack slots at the top for arguments, which means that we have to grow the stack by one word before we can make the call.</p>
<h4 id="checking-for-sufficient-stack-space">Checking for sufficient stack space</h4>
<p>First, we check to see if growing the stack would overflow allocated stack space, by comparing the STG stack pointer register `Sp` with the stack limit register `SpLim`:</p>
<p></p>
<p>(The stack grows downwards, hence the <em>subtraction</em> of 4 from the current `Sp`). If the stack check fails, we branch to `clH`:</p>
<p></p>
<p>This stores the closure of the current function in `R1`, and then jumps into the hand-written garbage collector code to force it to grow the stack. After the stack has been grown, the collector will call back into `Main_knownzuappzu2_entry` by using the information stored in the (statically-allocated) `Main_knownzuappzu2_closure` closure pointed to by `R1`, and the stack check will be run again - hopefully succeeding this time!</p>
<h4 id="making-the-known-call">Making the known call</h4>
<p>Given that the stack check succeeds, it is easy to make the actual call we are after. We simply grow the stack by the required amount, and write the two arguments to `known_fun_2` into the top two stack slots (overwriting our own first argument in the process, of course):</p>
<p></p>
<p>A simple tail call to the new function finishes us off:</p>
<p></p>
<h2 id="example-3-unsaturated-applications-to-known-functions">Example 3: Unsaturated applications to known functions</h2>
<p>Despite describing an undersaturated call, this Haskell code</p>
<p></p>
<p>compiles to straightforward C-- as follows</p>
<p></p>
<p></p>
<p>The reason that there is no special magic to deal with undersaturated applications to known functions is simple: GHC simply gives `known_undersaturated_app` an arity of 2, so by the time we jump to the entry code the stack must already contain any arguments required by `known_fun_2`.</p>
<h2 id="example-4-applications-to-unknown-functions">Example 4: Applications to unknown functions</h2>
<p>We aren't going to tackle oversaturated calls to known functions until we've considered happens to calls to statically-unknown functions. To see what these look like, we are going to use the following Haskell code</p>
<p></p>
<p>Which compiles to this C-- function</p>
<p></p>
<p></p>
<p>Unlike the previous cases we have looked at, we are compiling an application where we don't statically know either the arity or the info pointer of the function being applied. To deal with such cases, the STG machine uses several pre-compiled &quot;generic apply&quot; functions which inspect the info-table for the function in question and decide how the available arguments should be applied to it.</p>
<h3 id="dealing-with-generic-application">Dealing with generic application</h3>
<p>There are three cases the generic apply functions have to deal with:</p>
<p><code>1. The function's arity (recorder in the function closure's info</code><br />
<code>   table) exactly matches the number of arguments available on the</code><br />
<code>   stack</code><br />
<code>    * This is the best case. In this case, the generic apply function</code><br />
<code>      simply makes a tail call into the function's entry code</code></p>
<p><code>2. The function's arity is greater than the number of arguments</code><br />
<code>   available on the stack</code><br />
<code>    * In this case, the generic apply code allocates a PAP (partial</code><br />
<code>      application) closure which closes over both the new arguments and</code><br />
<code>      the function pointer, and returns that value, in the normal STGish</code><br />
<code>      way, to the continuation on the top of the stack</code></p>
<p><code>3. The function's arity is less than the number of arguments</code><br />
<code>   available on the stack</code><br />
<code>    * In this case, a number of arguments matching the arity are pushed</code><br />
<code>      on top of the stack, followed by a continuation which uses another</code><br />
<code>      of the generic apply functions to apply the remaining arguments.</code><br />
<code>      The code for the original function is then entered</code><br />
<code>    * Eventually the code for the continuation is entered and another</code><br />
<code>      generic apply function will be tail-called to deal with the</code><br />
<code>      result</code></p>
<p>Potentially, one generic apply function is required for every &quot;argument pattern&quot;. Some example argument patterns are:</p>
<p></p>
<p>Because the number of patterns is large (actually unbounded, because functions might be of any arity), GHC only generates generic apply functions for enough patterns so that 99.9% of all calls observed in practice have a generic apply function. Generic apply functions for calls of larger arity can be simulated by chaining together several smaller generic apply functions, in a similar manner as when dealing with oversaturated function applications.</p>
<h3 id="making-the-call-to-the-generic-application-code">Making the call to the generic application code</h3>
<p>Let's remind ourselves of the original code:</p>
<p></p>
<p>Knowing about generic apply functions, the call itself is easy to understand. We pop the top of the stack (the function argument) into `R1` and then jump into the generic application code for the case where the stack contains a single pointer argument, which deals with all the cases for `f` described above.</p>
<h2 id="example-5-oversaturated-applications-to-known-functions">Example 5: oversaturated applications to known functions</h2>
<p>This Haskell code</p>
<p></p>
<p>compiles to the following C-- function</p>
<p></p>
<p></p>
<p>As you might see, despite being a call to a known function, this code makes use of the generic apply functions we discussed in the last section. Let's pick the function apart and see how it works.</p>
<p>First, we do the usual stack check. What differs from the last time we saw this check is that we are not only allocating space for arguments on the stack, but also for a <em>continuation</em>. We set up these new stack entries as follows:</p>
<p></p>
<p>i.e. the final stack looks as follows (note that the code overwrites the old pointer to a closure of type ()):</p>
<p></p>
<p>Because `known_fun_2` is of arity 2, when we jump to its entry code, it will only consume the top two arguments from the stack: i.e. the two pointers to `base_GHCziBase_id_closure`. It will then evaluate to some sort of value and transfer control to the entry code for `stg_ap_p_info`.</p>
<p>This is where the magic happens: the entry code for `stg_ap_p_info` will apply the function value that was returned from `known_fun_2` to the (pointer) argument in the &quot;free variable&quot; of its (stack allocated) closure -- and we have arranged that that is `stg_INTLIKE_closure+209`, i.e. the closure for the `Int` literal `10`. This code is shared with the generic application functions for calls to unknown functions, so this will make use of the `stg_ap_p_fast` function we saw before.</p>
<p>Finally, control will be transferred back to the caller for `known_oversat_app`, and all will be well.</p>
<h2 id="example-6-allocation-of-thunks-and-data">Example 6: allocation of thunks and data</h2>
<p>Something that happens all the time in Haskell is allocation. There are three principal types of thing that get allocated: function closures, thunks, and data. These are all treated pretty much the same in the STG machine for the simple reason that they share many common characteristics:</p>
<p><code>* Entry code which the STG machine jumps to, in order to evaluate</code><br />
<code>  them</code><br />
<code>   * Note that for constructors, the entry code is trivial, as they</code><br />
<code>     are always already evaluated! In this case, control will be</code><br />
<code>     transferred directly back to the caller's continuation.</code></p>
<p><code>* Free variables stored in a closure</code><br />
<code>   * For data, these &quot;free variables&quot; will be the values in the fields</code><br />
<code>     of the particular data constructor</code></p>
<p><code>* Info-tables containing various miscellaneous metadata about the</code><br />
<code>  heap object, such as function arity</code></p>
<p>Let us look at how a thunk and a data constructor get allocated in a simple setting:</p>
<p></p>
<p>This compiles into the following C--:</p>
<p></p>
<p></p>
<p>Let's break this function down slowly.</p>
<h3 id="checking-for-sufficient-heap-space">Checking for sufficient heap space</h3>
<p>Any function that needs to allocate memory might find that the heap has been exhausted. If that happens, it needs to call into the garbage collector in order to get the heap cleaned up and (possibly) enlarged.</p>
<p>Hence, the first thing any such function does is check to see if enough memory is available for its purposes:</p>
<p></p>
<p>This is simple enough. The function needs to allocate 20 bytes (the data constructor takes up 2 words, and the thunk will take up 3), so it speculatively increments Hp and then checks the STG registers `Hp` and `HpLim` (the pointer to the top of the available heap space) against each other.</p>
<p>If memory is insufficient (i.e. we have moved `Hp` past the top of the available heap), the code deals with it by setting the `HpAlloc` register to the number of bytes needed and `R1` to the closure for the function in which the heap check failed, before jumping into the hand-written garbage collector code for the cleanup. The garbage collector will resume execution of the code by using the information from `R1`, after it has freed up enough memory.</p>
<p>Side note: I believe that the line setting `R1` is unnecessary here, because `R1` should anyway always be set to the address of the closure when executing the closure entry code. I could be wrong, though.</p>
<h3 id="performing-the-actual-allocation">Performing the actual allocation</h3>
<p>Once the heap check succeeds, we will be able to enter the body of the function proper. Since the `Hp` has already been incremented, we can just construct the new heap objects directly:</p>
<p></p>
<p>So we get something like this:</p>
<p></p>
<p>The bottom two words are the allocated `Just` value, and the three above that correspond to the `x + 1` closure.</p>
<h3 id="returning-an-allocated-value-to-the-caller">Returning an allocated value to the caller</h3>
<p>Now that we have allocated the data we entered the function in order to construct, we need to return it to the caller. This is achieved by the following code:</p>
<p></p>
<p>To return, the STG machine:</p>
<p><code>1. Sets `R1` to the pointer to the result of evaluation</code><br />
<code>2. Pops all the arguments to the function from the stack</code><br />
<code>3. Jumps to the entry code for the continuation. This is always</code><br />
<code>   found at the top of the STG stack, logically below any arguments</code><br />
<code>   that were pushed to make the call.</code></p>
<p>This is indeed exactly what happens here, with two interesting points: pointer tagging, and the double-deference of the stack pointer. These will be discussed in the next two subsections.</p>
<h4 id="pointer-tagging">Pointer tagging</h4>
<p>One exciting feature is that the code setting `R1`, i.e. `R1 = Hp - 2`. This is setting `R1` to point to the `Just`, we just allocated, but simultaneously tagging that pointer with the value 2. The fact that the tag is non-zero indicates to users of the pointer that the thing pointed to is already evaluated. Furthermore, because `Maybe` has only two constructors, we are able to use the pointer tags to record which constructor it evaluated to: in this case, the 2 indicates the `Just` constructor.</p>
<p>It is compulsory to tag pointers before jumping to the address of the continuation entry code: the entry code can and will rely on those tags being present!</p>
<h4 id="tables_next_to_code">`TABLES_NEXT_TO_CODE`</h4>
<p>Because I have compiled GHC without `TABLES_NEXT_TO_CODE`, the entry code for the continuation is found by dereferencing the pointer to the info table we found at the top of the STG stack - i.e. a double-dereference.</p>
<p>The layout of heap objects without `TABLES_NEXT_TO_CODE` is as follows:</p>
<p></p>
<p>With `TABLES_NEXT_TO_CODE` on, the situation looks more like this:</p>
<p></p>
<p>The `TABLES_NEXT_TO_CODE` optimisation removes the need for that second dereference during the return, because the entry code is always right next to the info table. However, it requires special support from the backend for ensuring that data (i.e. the info table) and code are contiguous in memory, so it cannot always be used.</p>
<h2 id="example-7-case-expressions">Example 7: `case` expressions</h2>
<p>Let us now examine how `case` expressions are handled. Compiling the following Haskell</p>
<p></p>
<p>Produces this C-- code</p>
<p></p>
<p></p>
<p>Notice that GHC has generated <em>two</em> functions: `Main_casezuscrut_entry` and `scj_ret` correspond to the code for forcing the argument to the `case`, and for the <em>continuation</em> of the `case` respectively. Let's pick them apart and see how they work!</p>
<h3 id="forcing-the-scrutinee-of-the-case">Forcing the scrutinee of the `case`</h3>
<p>When we first call the `case_scrut` function, its entry code begins executing:</p>
<p></p>
<p>This is a function of arity 1 (i.e. with a single argument), so upon entry the machine state looks like this:</p>
<p></p>
<p>Because this is a top level function, the closure is statically allocated and contains no free variables. However, as discussed previously, the single argument to the function is guaranteed to be present at the top of the stack.</p>
<p>The code starts off by saving this argument (the `x`) temporarily into `R1`:</p>
<p></p>
<p>The next thing the code does is overwrites this argument on the stack with a pointer to the info-table of the continuation code. This is the code that will be invoked after `x` has been evaluated into WHNF, and which will do the test to decide whether to continue as the `Nothing` or as the `Just` branch of the case:</p>
<p></p>
<p>As we saw earlier, any time that the STG machine decides that it has a value in its hand, it will continue evaluation by tail-calling the entry code found by dereferencing the info-table pointer at the top of the stack. So by putting the address of our continuation in here, we ensure that the entry code for `scj_info` is executed after `x` becomes a value.</p>
<p>Now, what we need to do is to start the evaluation of `x`. We could just jump into `x`'s entry code and hope for the best, but thanks to GHC's pointer tagging we can sometimes avoid doing this indirect branch.</p>
<p>So, instead, we test to see if the `x` pointer has a tag. If it is tagged, then we know that it is already evaluated and hence jump directly to the code for the continuation. If it is not tagged, we are forced to make the jump into the entry code for `x`. This choice is embodied by the following code:</p>
<p></p>
<p>Note the test `R1 &amp; 3 != 0`: this reflects the fact that pointer tags are stored in the lower 2 bits of the pointer on 32 bit machines. Another interesting feature is how the `jump` instructions find the entry code: again, we see a deference of the info pointer because `TABLES_NEXT_TO_CODE` is turned off.</p>
<p>As we saw, the `case` scrutinisation code ended with one of two things happening: 1. A direct call into the continuation code `scj_ret` if the scrutinee was already evaluated 2. A call into the entry code for the scrutinee, if the scrutinee was not evaluated (or it <em>was</em> evaluated, but the pointer was somehow not tagged with that information) - Because we pushed `scj_info` onto the STG stack, control will eventually return to `scj_ret` after the evaluation of `x` has finished</p>
<p>It is now time to examine the continuation code to see what happens after `x` becomes a value.</p>
<h3 id="dealing-with-the-forced-scrutinee">Dealing with the forced scrutinee</h3>
<p>The continuation code is a little more complicated:</p>
<p></p>
<p>Whenever the STG machine evaluates to a value it will return the value by jumping to the entry point at the top of the stack. In this case, `R1` is guaranteed to be a (tagged) pointer to the thing that was just evaluated. Because we are scrutinising a `Maybe` type (which has fewer than 4 constructors) the code for the `case` continuation is able to use the tag bits on the returned pointer to decide which of the two branches to take:</p>
<p></p>
<p>If we were scrutinising a data type with more constructors, the tag bits would only tell us that the thing was evaluated, not which constructor it was evaluated to. In this case, we would have to read the constructor tag by dereferencing `R1` and testing the resulting info table pointer against all possibilities.</p>
<p>If the tag was greater than or equal to 2, we go to the `ccv` branch, which deals with what happens if we had a `Just`. In this case, we need to continue by forcing the thunk inside the `Just` and returning that value to our caller, which is what these lines are doing:</p>
<p></p>
<p>To access the thing inside the `Just`, the code assumes that the `R1` pointer is tagged with the 2 that indicates a `Just` constructor, and hence finds the first free variable (stored 4 bytes into the closure) using `I32[R1 + 2]`, which is then saved into `R1`. It pops the address of `scj_info` that was pushed onto the stack in `Main_casezuscrut_entry` by moving `Sp` up 4 bytes (remember that the STG stack grows downwards) and then untags and jumps into the entry code for the `R1` thunk, using the same double-dereference pattern discussed earlier.</p>
<p>There seems to be a small missed opportunity here: the code could check the pointer tag on `R1`, and then return directly if it is set. I imagine that this isn't being done in order to reduce possible code bloat.</p>
<h2 id="example-8-thunks-and-thunk-update">Example 8: thunks and thunk update</h2>
<p>You might be wondering how the `x + 1` thunk we saw allocated in a previous section will behave when it is actually forced. To remind you, the thunk we saw was constructed by the following Haskell code:</p>
<p></p>
<p>So how does the `x + 1` thunk work? An excellent question! Let's take a look at the C-- for its entry code and find out:</p>
<p></p>
<p></p>
<p>The original Haskell code read `x + 1`, but GHC has inlined the actual code for the addition operation on `Int`s, which looks something like:</p>
<p></p>
<p>The second pattern match (to get `b`) has been performed statically by GHC, obtaining the machine literal 1, which shows up directly in the generated code. Therefore, the code only need to evaluate and case-decompose the unknown free variable `x` of our closure, to get the `a` argument to `plusInt`.</p>
<h3 id="thunk-entry-point">Thunk entry point</h3>
<p>This evaluation is what is being done by the thunk entry code `slk_entry`. Ignoring the stack check, the C-- begins thusly:</p>
<p></p>
<p>Remembering that upon entry to the thunk entry code, `R1` points to the thunk's closure, the new stack looks as follows:</p>
<p></p>
<p>The C-- statement `R1 = I32[R1 + 8]` is pulling out the pointer to the free variable of the thunk (which was set up in `Main_buildzudata_entry`) into `R1`.</p>
<p>Finally, the entry code evaluates that free variable (checking the tag bits of the pointer first, as usual):</p>
<p></p>
<p>Because we put `soN_info` at the top of the stack, when evaluation of `x` is complete the STG machine will continue by executing the `soN_ret` code.</p>
<p>The most interesting feature of this code is the extra stuff that has been pushed onto the stack below `soN_ret`: an info pointer called `stg_upd_frame_info`, and a pointer to the thunk currently being evaluated.</p>
<p>This is all part of the STG machine's thunk update mechanism. When the `soN_ret` continuation returns, it will transfer control <em>not</em> to the code forcing the thunk, but to some code which overwrites the contents of the current thunk closure with a closure representing an &quot;indirection&quot;. The entry code for such an indirection closure is trivial: it immediately returns a pointer to the thing that was returned from the `soN_ret` continuation in `R1`.</p>
<p>These indirections are the mechanism which ensures that the STG machine never repeats the work of evaluating a thunk more than once: after the first evaluation, any code forcing the thunk jumps into the indirection entry code rather than `slk_entry`.</p>
<p>That being said, let us look at how the continuation responsible for actually finding the value of `x + 1` works:</p>
<h3 id="continuation-of-the-thunk">Continuation of the thunk</h3>
<p>Upon entry to the continuation code, we have the evaluated `x` in `R1`: it now needs to do the addition and allocate a `I#` constructor to hold the result of the addition. Because of the allocation, `soN_ret` begins with a heap check. Ignoring that check, we have the following code:</p>
<p></p>
<p>This is mostly standard stuff. Because the `R1` pointer is guaranteed tagged, and there is only one possible constructor, the tag must be 1 and so the `Int#` value inside the `Int` is pulled out using `I32[R1 + 3]`. This is then put into a newly heap-allocated `I#` constructor, which is returned in `R1` after we pop the `soN_info` pointer from the stack.</p>
<p>The only interesting point is where we return to: rather than dereference `Sp` to find the info pointer at the top of the STG stack, GHC has generated code that takes advantage of the fact that the `Sp` is guaranteed to point to `stg_upd_frame_info`. This avoids one pointer dereference.</p>
<h2 id="conclusion">Conclusion</h2>
<p>This document has left much of the detail of how STG is implemented out: notable omissions include CAFs, and the precise behaviour of the garbage collector. Nonetheless, my hope is that it has helped you to gain some more insight into the weird and wonderful way the Haskell evaluation model is implemented.</p>
<h1 id="support-for-generic-programming">Support for generic programming</h1>
<p><a href="PageOutline" class="uri" title="wikilink">PageOutline</a></p>
<p>GHC includes a new (in 2010) mechanism to let you write generic functions. It is described in paper <a href="http://www.dreixel.net/research/pdf/gdmh_nocolor.pdf">A generic deriving mechanism for Haskell</a>. This page sketches the specifics of the implementation; we assume you have read the paper. The <a href="http://www.haskell.org/haskellwiki/Generics">HaskellWiki page</a> gives a more general overview.</p>
<p>This mechanism replaces the <a href="http://www.haskell.org/ghc/docs/6.12.2/html/users_guide/generic-classes.html">previous generic classes implementation</a>. What we describe until the &quot;Kind polymorphic overhaul&quot; section is implemented and released in GHC 7.2.1.</p>
<h2 id="status">Status</h2>
<p>Use <strong>Keyword</strong> = `Generics` to ensure that a ticket ends up on this auto-generated list</p>
<p>Open Tickets: <a href="TicketQuery(status=infoneeded,status=new" title="wikilink">patch|infoneeded,keywords=~Generics)</a></p>
<p>Closed Tickets: <a href="TicketQuery(status=infoneeded,status=closed,keywords=~Generics)" class="uri" title="wikilink">TicketQuery(status=infoneeded,status=closed,keywords=~Generics)</a></p>
<h2 id="main-components">Main components</h2>
<p><code>* `TcDeriv.tcDeriving` now allows deriving `Generic` instances.</code></p>
<p><code>* The representation types and core functionality of the library live on `GHC.Generics` (on the `ghc-prim` package).</code></p>
<p><code>* Many names have been added as known in `prelude/PrelNames`</code></p>
<p><code>* Most of the code generation is handled by `types/Generics`</code></p>
<h2 id="things-that-have-been-removed">Things that have been removed</h2>
<p><code>* All of the </code><a href="http://www.haskell.org/ghc/docs/6.12.2/html/users_guide/generic-classes.html"><code>generic</code> <code>classes</code> <code>stuff</code></a><code>. In particular, the following have been removed:</code><br />
<code>  * `hasGenerics` field from `TyCon`;</code><br />
<code>  * `HsNumTy` constructor from `HsType`;</code><br />
<code>  * `TypePat` constructor from `Pat`.</code></p>
<p><code>* The `-XGenerics` flag is now deprecated.</code></p>
<h2 id="what-already-works">What already works</h2>
<p><code>* `Generic` and `Generic1` instances can be derived when `-XDeriveGeneric` is enabled.</code></p>
<p><code>* The `default` keyword can used for generic default method signatures when `-XDefaultSignatures` is enabled.</code></p>
<p><code>* Generic defaults are properly instantiated when giving an instance without defining the generic default method.</code></p>
<p><code>* Base types like `[]`, `Maybe`, tuples, come with Generic instances.</code></p>
<h2 id="testing">Testing</h2>
<p><code>* Tests are available under the `generics` directory of the testsuite.</code></p>
<h1 id="kind-polymorphic-overhaul">Kind polymorphic overhaul</h1>
<p>With the new `-XPolyKinds` functionality we can make the support for generic programming better typed. The basic idea is to define the universe codes (`M1`, `:+:`, etc.) as constructors of a datatype. Promotion then lifts these constructors to types, which we can use as before, only that now we have them all classified under a new kind. The overhaul of the main module is explained below; for easier comparison with the current approach, names are kept the same whenever possible.</p>
<h2 id="generic-representation-universe">Generic representation universe</h2>
<p>`m` is the only real parameter here. `f` and `x` are there because we can't write kinds directly, since `Universe` is also a datatype (even if we're only interested in its promoted version). So we pass `f` and `x` only to set them to `* -&gt; *` and `*`, respectively, in `Interprt`. `m` is different: it stands for the kind of metadata representation types, and we really want to be polymorphic over that, since each user datatype will introduce a new metadata kind. </p>
<h2 id="universe-interpretation">Universe interpretation</h2>
<p>As promised, we set `f` to `* -&gt; *` and `x` to `*`. Unfortunately we don't have [GhcKinds#Explicitkindvariables explicit kind variable annotations] yet, so we cannot leave `m` polymorphic! So this code doesn't compile: </p>
<h3 id="names">Names</h3>
<p>As an aside, note that we have to come up with names like `UU` and `KK` for the `Universe` even though we really just wanted to use `U1` and `K1`, like before. Then we would have a type and a constructor with the same name, but that's ok. However, `Universe` defines both a type (with constructors) and a kind (with types). So if we were to use `U1` in the `Universe` constructors, then we could no longer use that name in the `Interprt` constructors. It's a bit annoying, because we are never really interested in the type `Universe` and its constructors: we're only interested in its promoted variant. This is a slight annoyance of automatic promotion: when you define a &quot;singleton type&quot; (like our GADT `Interprt` for `Universe`) you cannot reuse the constructor names.</p>
<h2 id="metadata-representation">Metadata representation</h2>
<p> There's more of these, but they don't add any new concerns.</p>
<h2 id="conversion-between-user-datatypes-and-generic-representation">Conversion between user datatypes and generic representation</h2>
<p>We now get a more precise kind for `Rep`:</p>
<p></p>
<h2 id="example-generic-function-fmap-kind--">Example generic function: `fmap` (kind `* -&gt; *`)</h2>
<p>User-visible class, exported: </p>
<p>Defined by the generic programmer, not exported: </p>
<p>Note that previously `Functor` and `GFunctor` had exactly the same types. Now we can make clear what the difference between them is.</p>
<h2 id="example-generic-function-show-kind-uses-metadata">Example generic function: `show` (kind `*`, uses metadata)</h2>
<p>User-visible class, exported: </p>
<p>Defined by the generic programmer, not exported: </p>
<p>The other cases do not add any further complexity.</p>
<h2 id="example-datatype-encoding-lists-derived-by-the-compiler">Example datatype encoding: lists (derived by the compiler)</h2>
<p></p>
<p>Note that we use only one datatype; more correct would be to use 3, one for `DList`, another for the constructors, and yet another for the selectors (or maybe even n datatypes for the selectors, one for each constructor?) But we don't do that because `Universe` is polymorphic only over `m`, so a single metadata representation type. If we want a more fine-grained distinction then we would need more parameters in `Universe`, and also to split the `MM` case. </p>
<h3 id="digression">Digression</h3>
<p>Even better would be to index the metadata representation types over the type they refer to. Something like:  But now we are basically asking for promotion of data families, since we want to use promoted `DList`. Also, the case for `MM` in `Universe` would then be something like:  But I'm not entirely sure about this.</p>
<h2 id="ghc-8.0-and-later">GHC 8.0 and later</h2>
<h3 id="type-level-metadata-encoding">Type-level metadata encoding</h3>
<p>Because what we've described so far is rather backwards-incompatible, we wanted to at least try to improve the encoding of metadata, which was currently rather clunky prior to GHC 8.0 (giving rise to lots of empty, compiler-generated datatypes and respective instances). We can accomplished that by changing `M1` to keep the meta-information <em>at the type level</em>: </p>
<p>Why did we need to add `FixityI`? Because `Fixity` does not promote. Yet, we wanted to expose `Fixity` to the user, not `FixityI`. Note that the meta-data classes remained mostly unchanged (aside from some enhancements to <a href="https://ghc.haskell.org/trac/ghc/ticket/10030">Datatype</a> and <a href="https://ghc.haskell.org/trac/ghc/ticket/10716">Selector</a>): </p>
<p>But now, using the magic of singletons, we give <em>one single instance</em> for each of these classes, instead of having to instantiate them each time a user derives `Generic`: </p>
<p>Naturally, we require singletons for `Bool`, `Maybe`, `FixityI`, `Associativity`, `SourceUnpackedness`, `SourceStrictness`, and `DecidedStrictness`, but that is one time boilerplate code, and is not visible for the user. (In particular, this is where we encode that the demotion of (the kind) `FixityI` is (the type) `Fixity`.)</p>
<p>I believe this change is almost fully backwards-compatible, and lets us simplify the code for `deriving Generic` in GHC. Furthermore, I suspect it will be useful to writers of generic functions, who can now match at the type-level on things such as whether a constructor is a record or not.</p>
<p>I say &quot;almost fully backwards-compatible&quot; because handwritten `Generic` instances might break with this change. But we've never recommended doing this, and I think users who do this are more than aware that they shouldn't rely on it working across different versions of GHC.</p>
<h4 id="example-1">Example</h4>
<p>Before GHC 8.0, the following declaration:</p>
<p></p>
<p>Would have generated all of this:</p>
<p></p>
<p>But on GHC 8.0 and later, this is all that is generated (assuming it was compiled with no strictness optimizations):</p>
<p></p>
<p>Not bad!</p>
<h3 id="strictness">Strictness</h3>
<p>The `Selector` class now looks like this:</p>
<p></p>
<p>This design draws much inspiration from the way Template Haskell handles strictness as of GHC 8.0 (see <a href="https://ghc.haskell.org/trac/ghc/ticket/10697">here</a> for what motivated the change). We make a distinction between the <em>source</em> strictness annotations and the strictness GHC actually <em>decides</em> during compilation. To illustrate the difference, consider the following data type:</p>
<p></p>
<p>If we were to encode the source unpackedness and strictness of each of `T`'s fields, they were be `SourceUnpack`/`SourceStrict`, `NoSourceUnpackedness`/`SourceStrict`, and `NoSourceUnpackedness`/`NoSourceStrictness`, no matter what. Source unpackedness/strictness is a purely syntactic property.</p>
<p>The strictness that the user writes, however, may be different from the strictness that GHC decides during compilation. For instance, if we were to compile `T` with no optimizations, the decided strictness of each field would be `DecidedStrict`, `DecidedStrict`, and `DecidedLazy`. If we enabled `-O2`, however, they would be `DecidedUnpack`, `DecidedStrict`, and `DecidedLazy`.</p>
<p>Things become even more interesting when `-XStrict` and `-O2` are enabled. Then the strictness that GHC would decided is `DecidedUnpack`, `DecidedStrict`, and `DecidedStrict`. And if you enable `-XStrict`, `-O2`, <em>and</em> `-funbox-strict-fields`, then the decided strictness is `DecidedUnpack`, `DecidedUnpack`, and `DecidedUnpack`.</p>
<p>The variety of possible `DecidedStrictness` combinations demonstrates that strictness is more just annotation</p>
<h2 id="source-tree-layout">Source Tree Layout</h2>
<p>An overview of the source tree may be found [wiki:Commentary/SourceTree here].</p>
<h2 id="build-system-basics">Build System Basics</h2>
<p>Detailed information about the build system may be found [wiki:Building here]; what follows is a quick overview, highlighting the areas where GHC's build system diverges substantially from the way  is used in most other projects.</p>
<p>Most projects keep the parts of their build machinery in files called  found in many/most subdirectories of the source tree. GHC uses the filename  instead; you'll find a file with this name in quite a number of subdirectories.</p>
<p>Other build system files are in  and .</p>
<h2 id="coding-style">Coding Style</h2>
<p>The [wiki:WorkingConventions Coding style guidelines] may be found on the wiki.</p>
<h1 id="the-ghc-commentary-ghci">The GHC Commentary: GHCi</h1>
<p>This isn't a coherent description of how GHCi works, sorry. What it is (currently) is a dumping ground for various bits of info pertaining to GHCi, which ought to be recorded somewhere.</p>
<h2 id="debugging-the-interpreter">Debugging the interpreter</h2>
<p>The usual symptom is that some expression / program crashes when running on the interpreter (commonly), or gets wierd results (rarely). Unfortunately, finding out what the problem really is has proven to be extremely difficult. In retrospect it may be argued a design flaw that GHC's implementation of the STG execution mechanism provides only the weakest of support for automated internal consistency checks. This makes it hard to debug.</p>
<p>Execution failures in the interactive system can be due to problems with the bytecode interpreter, problems with the bytecode generator, or problems elsewhere. From the bugs seen so far, the bytecode generator is often the culprit, with the interpreter usually being correct.</p>
<p>Here are some tips for tracking down interactive nonsense:</p>
<p><code>* Find the smallest source fragment which causes the problem.</code></p>
<p><code>* Using an RTS compiled with `-DDEBUG`, run with `+RTS -Di` to get a listing in great detail from the interpreter. Note that the listing is so voluminous that this is impractical unless you have been diligent in the previous step.</code></p>
<p><code>* At least in principle, using the trace and a bit of GDB poking around at the time of death (See also [wiki:Debugging]), you can figure out what the problem is. In practice you quickly get depressed at the hopelessness of ever making sense of the mass of details. Well, I do, anyway.</code></p>
<p><code>* `+RTS -Di` tries hard to print useful descriptions of what's on the stack, and often succeeds. However, it has no way to map addresses to names in code/data loaded by our runtime linker. So the C function `ghci_enquire` is provided. Given an address, it searches the loaded symbol tables for symbols close to that address. You can run it from inside GDB:</code></p>
<p></p>
<p><code>In this case the enquired-about address is `PrelBase_ZMZN_static_entry`. If no symbols are close to the given addr, nothing is printed. Not a great mechanism, but better than nothing.</code></p>
<p><code>* We have had various problems in the past due to the bytecode generator (compiler/ghci/ByteCodeGen.lhs) being confused about the true set of free variables of an expression. The compilation scheme for `let`s applies the BCO for the RHS of the `let` to its free variables, so if the free-var annotation is wrong or misleading, you end up with code which has wrong stack offsets, which is usually fatal.</code></p>
<p><code>* Following the traces is often problematic because execution hops back and forth between the interpreter, which is traced, and compiled code, which you can't see. Particularly annoying is when the stack looks OK in the interpreter, then compiled code runs for a while, and later we arrive back in the interpreter, with the stack corrupted, and usually in a completely different place from where we left off.</code></p>
<p><code>If this is biting you baaaad, it may be worth copying sources for the compiled functions causing the problem, into your interpreted module, in the hope that you stay in the interpreter more of the time.</code></p>
<p><code>* There are various commented-out pieces of code in Interpreter.c which can be used to get the stack sanity-checked after every entry, and even after after every bytecode instruction executed. Note that some bytecodes (`PUSH_UBX`) leave the stack in an unwalkable state, so the `do_print_stack` local variable is used to suppress the stack walk after them. </code></p>
<h2 id="useful-stuff-to-know-about-the-interpreter">Useful stuff to know about the interpreter</h2>
<p>The code generation scheme is straightforward (naive, in fact). `-ddump-bcos` prints each BCO along with the Core it was generated from, which is very handy.</p>
<p><code>* Simple `let`s are compiled in-line. For the general case, `let v = E in ...`, the expression `E` is compiled into a new BCO which takes as args its free variables, and `v` is bound to `AP(the new BCO, free vars of E)`.</code></p>
<p><code>* `case`s as usual, become: push the return continuation, enter the scrutinee. There is some magic to make all combinations of compiled/interpreted calls and returns work, described below. In the interpreted case, all `case` alts are compiled into a single big return BCO, which commences with instructions implementing a switch tree. </code></p>
<h3 id="stack-management">Stack management</h3>
<p>There isn't any attempt to stub the stack, minimise its growth, or generally remove unused pointers ahead of time. This is really due to laziness on my part, although it does have the minor advantage that doing something cleverer would almost certainly increase the number of bytecodes that would have to be executed. Of course we `SLIDE` out redundant stuff, to get the stack back to the sequel depth, before returning a HNF, but that's all. As usual this is probably a cause of major space leaks.</p>
<h3 id="building-constructors">Building constructors</h3>
<p>Constructors are built on the stack and then dumped into the heap with a single `PACK` instruction, which simply copies the top N words of the stack verbatim into the heap, adds an info table, and zaps N words from the stack. The constructor args are pushed onto the stack one at a time. One upshot of this is that unboxed values get pushed untaggedly onto the stack (via `PUSH_UBX`), because that's how they will be in the heap. That in turn means that the stack is not always walkable at arbitrary points in BCO execution, although naturally it is whenever GC might occur.</p>
<p>Function closures created by the interpreter use the AP-node (tagged) format, so although their fields are similarly constructed on the stack, there is never a stack walkability problem.</p>
<h3 id="perspective">Perspective</h3>
<p>I designed the bytecode mechanism with the experience of both STG hugs and Classic Hugs in mind. The latter has an small set of bytecodes, a small interpreter loop, and runs amazingly fast considering the cruddy code it has to interpret. The former had a large interpretative loop with many different opcodes, including multiple minor variants of the same thing, which made it difficult to optimise and maintain, yet it performed more or less comparably with Classic Hugs.</p>
<p>My design aims were therefore to minimise the interpreter's complexity whilst maximising performance. This means reducing the number of opcodes implemented, whilst reducing the number of insns despatched. In particular, very few (TODO: How many? Which?) opcodes which deal with tags. STG Hugs had dozens of opcodes for dealing with tagged data. Finally, the number of insns executed is reduced a little by merging multiple pushes, giving `PUSH_LL` and `PUSH_LLL`. These opcode pairings were determined by using the opcode-pair frequency profiling stuff which is ifdef-d out in Interpreter.c. These significantly improve performance without having much effect on the ugliness or complexity of the interpreter.</p>
<p>Overall, the interpreter design is something which turned out well, and I was pleased with it. Unfortunately I cannot say the same of the bytecode generator.</p>
<h2 id="case-returns-between-interpreted-and-compiled-code">case returns between interpreted and compiled code</h2>
<p>Variants of the following scheme have been drifting around in GHC RTS documentation for several years. Since what follows is actually what is implemented, I guess it supersedes all other documentation. Beware; the following may make your brain melt. In all the pictures below, the stack grows downwards.</p>
<h3 id="returning-to-interpreted-code.">Returning to interpreted code.</h3>
<p>Interpreted returns employ a set of polymorphic return infotables. Each element in the set corresponds to one of the possible return registers (R1, D1, F1) that compiled code will place the returned value in. In fact this is a bit misleading, since R1 can be used to return either a pointer or an int, and we need to distinguish these cases. So, supposing the set of return registers is {R1p, R1n, D1, F1}, there would be four corresponding infotables, stg_ctoi_ret_R1p_info, etc. In the pictures below we call them stg_ctoi_ret_REP_info.</p>
<p>These return itbls are polymorphic, meaning that all 8 vectored return codes and the direct return code are identical.</p>
<p>Before the scrutinee is entered, the stack is arranged like this: </p>
<p>On entry, the interpreted contination BCO expects the stack to look like this: </p>
<p>A machine code return will park the returned value in R1/F1/D1, and enter the itbl on the top of the stack. Since it's our magic itbl, this pushes the returned value onto the stack, which is where the interpreter expects to find it. It then pushes the BCO (again) and yields. The scheduler removes the BCO from the top, and enters it, so that the continuation is interpreted with the stack as shown above.</p>
<p>An interpreted return will create the value to return at the top of the stack. It then examines the return itbl, which must be immediately underneath the return value, to see if it is one of the magic stg_ctoi_ret_REP_info set. Since this is so, it knows it is returning to an interpreted contination. It therefore simply enters the BCO which it assumes it immediately underneath the itbl on the stack.</p>
<h3 id="returning-to-compiled-code.">Returning to compiled code.</h3>
<p>Before the scrutinee is entered, the stack is arranged like this: </p>
<p>The scrutinee value is then entered. The case continuation(s) expect the stack to look the same, with the returned HNF in a suitable return register, R1, D1, F1 etc.</p>
<p>A machine code return knows whether it is doing a vectored or direct return, and, if the former, which vector element it is. So, for a direct return we jump to `Sp[0]`, and for a vectored return, jump to `((CodePtr*)(Sp[0]))[ - ITBL_LENGTH - vector number ]`. This is (of course) the scheme that compiled code has been using all along.</p>
<p>An interpreted return will, as described just above, have examined the itbl immediately beneath the return value it has just pushed, and found it not to be one of the ret_REP_ctoi_info set, so it knows this must be a return to machine code. It needs to pop the return value, currently on the stack, into R1/F1/D1, and jump through the info table. Unfortunately the first part cannot be accomplished directly since we are not in Haskellised-C world.</p>
<p>We therefore employ a second family of magic infotables, indexed, like the first, on the return representation, and therefore with names of the form stg_itoc_ret_REP_info. (Note: itoc; the previous bunch were ctoi). This is pushed onto the stack (note, tagged values have their tag zapped), giving: </p>
<p>We then return to the scheduler, asking it to enter the itbl at t.o.s. When entered, stg_itoc_ret_REP_info removes itself from the stack, pops the return value into the relevant return register, and returns to the itbl to which we were trying to return in the first place.</p>
<p>Amazingly enough, this stuff all actually works! Well, mostly ...</p>
<h2 id="unboxed-tuples-a-right-royal-spanner-in-the-works">Unboxed tuples: a Right Royal Spanner In The Works</h2>
<p>The above scheme depends crucially on having magic infotables stg_{itoc,ctoi}_ret_REP_info for each return representation REP. It unfortunately fails miserably in the face of unboxed tuple returns, because the set of required tables would be infinite; this despite the fact that for any given unboxed tuple return type, the scheme could be made to work fine.</p>
<p>This is a serious problem, because it prevents interpreted code from doing IO-typed returns, since IO t is implemented as `(# t, RealWorld# #)` or thereabouts. This restriction in turn rules out FFI stuff in the interpreter. Not good.</p>
<p>Although we have no way to make general unboxed tuples work, we can at least make IO-types work using the following ultra-kludgey observation: `RealWorld#` doesn't really exist and so has zero size, in compiled code. In turn this means that a type of the form `(# t, RealWorld# #)` has the same representation as plain t does. So the bytecode generator, whilst rejecting code with general unboxed tuple returns, recognises and accepts this special case. Which means that IO-typed stuff works in the interpreter. Just.</p>
<p>If anyone asks, I will claim I was out of radio contact, on a 6-month walking holiday to the south pole, at the time this was ... er ... dreamt up.</p>
<h1 id="porting-ghc-using-llvm-backend">Porting GHC using LLVM backend</h1>
<p>This document is kind of short porting roadmap which serves as a high-level overview for porters of GHC who decided to use LLVM instead of implementing new NCG for their target platform. Please have [wiki:Commentary/Compiler/Backends/LLVM/Design Design &amp; Implementation] at hand since this contains more in-depth information. The list of steps needed for new GHC/LLVM port is:</p>
<p><strong>(1)</strong> Make sure GHC unregisterised build is working on your target platform (using the C backend). This guide isn't intended for porting GHC to a completely unsupported platform. If the platform in question doesn't have a GHC unregisterised build then follow the [wiki:Building/Porting GHC Porting Guide] first.</p>
<p><strong>(2)</strong> Now try to compile some very simple programs such as 'hello world' or simpler using the GHC you just built. Try with the C backend First to make sure everything is working. Then try with the LLVM backend. If the llvm backend built programs are failing find out why. This is done using a combination of things such as the error message you get when the program fails, [wiki:Debugging/CompiledCode tracing the execution with GDB] and also just comparing the assembly code produced by the C backend to what LLVM produces. This last method is often the easiest and you can occasionally use techniques like doing doing a 'binary search' for the bug by merging the assembly produced by the C backend and LLVM backend.</p>
<p><strong>(3)</strong> When the programs you throw at the LLVM backend are running, try running the GHC testsuite. First run it against the C backend to get a baseline, then run it against the LLVM backend. Fix any failures that are LLVM backend specific.</p>
<p><strong>(4)</strong> If the testsuite is passing, now try to build GHC itself using the LLVM backend. This is a very tough test. When working though its a good proof that the LLVM backend is working well on your platform.</p>
<p><strong>(5)</strong> Now you have LLVM working in unregistered mode, so the next thing is to implement the GHC calling convention in LLVM that is used by GHC's LLVM backend. This should then allow you to get the LLVM backend working in registered mode but with (TABLES_NEXT_TO_CODE = NO in your build.mk). Majority of this step involves hacking inside the LLVM code. Usually lib/Target/<your target platform name> is the best way to start. Also you might study what David Terei did for <a href="http://lists.cs.uiuc.edu/pipermail/llvmdev/2010-March/030031.html">x86 support</a> and his <a href="http://lists.cs.uiuc.edu/pipermail/llvmdev/attachments/20100307/714e5c37/attachment-0001.obj">patch itself</a> to get an idea what's really needed.</p>
<p><strong>(6)</strong> Once <strong>(5)</strong> is working you have it all running except TABLES_NEXT_TO_CODE. So change that to Yes in your build.mk and get that working. This will probably involve changing the mangler used by LLVM to work on the platform you are targeting.</p>
<h2 id="registerised-mode">Registerised Mode</h2>
<p>Here is an expanded version of what needs to be done in step 5 and 6 to get a registerised port of LLVM working:</p>
<p>1. GHC in registerised mode stores some of its virtual registers in real hardware registers for performance. You will need to decide on a mapping of GHC's virtual registers to hardware registers. So how many registers you want to map and which virtual registers to store and where. GHC's design for this on X86 is basically to use as many hardware registers as it can and to store the more frequently cessed virtual registers like the stack pointer in callee saved registers rather than caller saved registers. You can find the mappings that GHC currently uses for supported architectures in 'includes/stg/MachRegs.h'.</p>
<p>2. You will need to implement a custom calling convention for LLVM for your platform that supports passing arguments using the register map you decided on. You can see the calling convention I have created for X86 in the llvm source file 'lib/Target/X86/X86CallingConvention.td'.</p>
<p>3. Get GHC's build system running on your platform in registerised mode.</p>
<p>4. Add new inline assembly code for your platform to ghc's RTS. See files like 'rts/StgCRun.c' that include assembly code for the architectures GHC supports. This is the main place as its where the boundary between the RTS and haskell code is but I'm sure there are definitely other places that will need to be changed. Just grep the source code to find existing assembly and add code for your platform appropriately.</p>
<p>5. Will need to change a few things in LLVM code gen.</p>
<p>5.1 'compiler/llvmGen/LlvmCodeGen/Ppr.hs' defines a platform specific string that is included in all generated llvm code. Add one for your platform. This string specifies the datalayout parameters for the platform (e.g pointer size, word size..). If you don't include one llvm should still work but wont optimise as aggressively.</p>
<p>5.2 'compiler/llvmGen/LlvmCodeGen/CodeGen.hs' has some platform specific code on how write barriers should be handled.</p>
<p>6. Probably some stuff elsewhere in ghc that needs to be changed (most likely in the main/ subfolder which is where most the compiler driver lives or in codegen/ which is the Cmm code generator).</p>
<p>7. This is just what I know needs to be done, I'm sure there is many small pieces missing although they should all fall into one of the above categories. In the end just trial and error your way to success.</p>
<p><a href="PageOutline" class="uri" title="wikilink">PageOutline</a></p>
<h1 id="packages-in-ghc">Packages in GHC</h1>
<p>This page summarises our current proposal for packages in GHC. (See also [wiki:Commentary/Packages/PackageNamespacesProposal an extended proposal] to make namespaces first-class. The two proposals are mutually exclusive.)</p>
<h2 id="the-problem">The problem</h2>
<p>A vexed question in the current design of Haskell is the issue of whether a single program can contain two modules with the same name. In Haskell 98 that is absolutely ruled out. As a result, packages are fundamentally non-modular: to avoid collisions <em>every</em> module in <em>every</em> package written by <em>anyone</em> must have different module names. That's like saying that every function must have different local variables, and is a serious loss of modularity.</p>
<p>GHC 6.6 makes a significant step forward by lifting this restriction. However it leaves an open question, which is what this page is about.</p>
<h2 id="assumptions">Assumptions</h2>
<p>Before we start, note that we take for granted the following</p>
<p><code>* </code><strong><code>Each</code> <code>package</code> <code>has</code> <code>a</code> <code>globally-unique</code> <code>name</code></strong><code>, organised by some social process.  This assumption is deeply built into Cabal, and lots of things would need to change if it wasn't met.</code></p>
<p><code>* </code><strong><code>Module</code> <code>names</code> <code>describe</code> <em><code>purpose</code></em> <code>(what</code> <code>it's</code> <code>for,</code> <code>e.g.</code> <code>),</code> <code>whereas</code> <code>package</code> <code>names</code> <code>describe</code> <em><code>provenance</code></em> <code>(where</code> <code>it</code> <code>comes</code> <code>from,</code> <code>e.g.</code> <code>)</code></strong><code>.  We should not mix these two up, and that is a good reason for not combining package and module names into a single grand name.  One quite frequently wants to globally change provenance but not purpose (e.g. compile my program with a new version of package &quot;foo&quot;), without running through all the source files to change the import statements.</code></p>
<p><code>* </code><strong><code>New:</code> <code>a</code> <code>module</code> <code>name</code> <code>must</code> <code>be</code> <code>unique</code> <code>within</code> <code>its</code> <code>package</code> <code>(only)</code></strong><code>.   That is, a single program can use two modules with the same module name, provided they come from different packages.  This is new in GHC 6.6.  </code></p>
<p>For all this to work, GHC must incorporate the package name (and version) into the names of entities the package defines. That means that when compiling a module M you must say what package it is part of:  Then C.o will contain symbols like &quot;&quot; etc. In effect, the &quot;original name&quot; of a function  in module  of package  is .</p>
<h2 id="the-open-question">The open question</h2>
<p>The remaining question is this: <strong>When you say , from what package does A.B.C come?</strong>. Three alternatives are under consideration:</p>
<p><code> * Plan A (GHC's current story)</code><br />
<code> * Plan B: grafting.  An enhancement of plan A; see [wiki:Commentary/Packages/PackageMountingProposal Frederik Eaton's proposal]</code><br />
<code> * Plan C: optionally specify the package in the import.  An alternative to (B), described in a [wiki:Commentary/Packages/PackageImportsProposal separate page].</code></p>
<hr />
<h2 id="plan-a-ghcs-current-story">Plan A: GHC's current story</h2>
<p>GHC already has a fairly elaborate scheme (perhaps too elaborate; <a href="http://www.haskell.org/ghc/dist/current/docs/users_guide/packages.html">documentation here</a>) for deciding what package you mean when you say &quot;import A.B.C&quot;:</p>
<p><code>* For a start, it only looks in </code><em><code>installed</code></em><code> packages.  </code><br />
<code>* Even for installed packages, the package may or may not be </code><em><code>exposed</code></em><code> by default (reasoning: you may want old versions of package X to be installed, but not in scope by default).  </code><br />
<code>* Then, you can use the </code><code> flag to hide an otherwise-exposed package, and the </code><code> flag to expose an otherwise-hidden package.</code></p>
<p>So, you can expose package P1 when compiling module M (say), and expose P2 when compiling module N by manipulating these flags. Then M and N could both import module A.B.C, which would come from P1 and P2 respectively. But:</p>
<p><code>* What if you wanted to import A.B.C from P1 and A.B.C from P2 into the </code><em><code>same</code></em><code> module?</code><br />
<code>* What if you want to only replace </code><em><code>parts</code></em><code> of P1 (e.g., you want to use an updated version of a module in </code><code>)?</code><br />
<code>* Compiling different modules with different flags in a way that affects the </code><em><code>semantics</code></em><code> (rather than, say, the optimisation level) seems undesirable.</code><br />
<code>* To support </code><code> in this situation we'd need to allow </code><code> flags in the per-module </code><code> pragmas, which isn't currently supported.  (</code><code> already gathers those options together for the link step.)  </code><em><code>This</code> <code>is</code> <code>not</code> <code>yet</code> <code>implemented,</code> <code>but</code> <code>it</code> <code>is</code> <code>close</code> <code>to</code> <code>being</code> <code>implemented.</code></em></p>
<p>If we did implement the &quot;`-package` in `OPTIONS` pragma&quot; fix, then is is not clear how pressing the need is for anything more. It's still impossible to import M from P1, and M from P2, into the same module. But how often will that happen?</p>
<hr />
<h2 id="plan-b-package-mounting">Plan B: package mounting</h2>
<p>This proposal is described by a [wiki:Commentary/Packages/PackageMountingProposal separate page].</p>
<hr />
<h2 id="plan-c-mention-the-package-in-the-import">Plan C: mention the package in the import</h2>
<p>This proposal is described by a [wiki:Commentary/Packages/PackageImportsProposal separate page].</p>
<p>This wiki discusses how bringing <a href="https://nixos.org/nix/">Nix</a>-style package management facilities to cabal can solve various cabal problems and help in effective mitigation of cabal hell. It also contains the goals and implementation plan for the GSoC project. It is based on a <a href="http://www.well-typed.com/blog/2015/01/how-we-might-abolish-cabal-hell-part-2/">blog post by Duncan Coutts</a>.</p>
<h1 id="problems">Problems</h1>
<h2 id="breaking-re-installations">Breaking re-installations</h2>
<p><a href="Image(http://www.well-typed.com/blog/aux/images/cabal-hell/install-example1.png)" class="uri" title="wikilink">Image(http://www.well-typed.com/blog/aux/images/cabal-hell/install-example1.png)</a></p>
<p>There are situations where Cabal's chosen solution would involve reinstalling an existing version of a package but built with different dependencies. In this example, after installing app-1.1, app-1.0 and other-0.1 will be broken. The root of the problem is having to delete or mutate package instances when installing new packages. This is due to the limitation of only being able to have one instance of a package version installed at once.</p>
<h2 id="type-errors-when-using-packages-together">Type errors when using packages together</h2>
<p><a href="Image(http://www.well-typed.com/blog/aux/images/cabal-hell/install-example2.png)" class="uri" title="wikilink">Image(http://www.well-typed.com/blog/aux/images/cabal-hell/install-example2.png)</a></p>
<p>The second, orthogonal, problem is that it is possible to install two packages and then load them both in GHCi and find that you get type errors when composing things defined in the two different packages. Effectively you cannot use these two particular installed packages together. The fundamental problem is that developers expect to be able to use combinations of their installed packages together, but the package tools do not enforce consistency of the developer's environment.</p>
<h1 id="goals">Goals</h1>
<ul>
<li>Fix breaking re-installs (Persistent package store)</li>
<li>Implement garbage collection to free unreachable packages</li>
<li>Enable sharing of packages between sandbox</li>
<li>Enforce development environment consistency (Giving error earlier and better)</li>
<li>Implement package manager tools in cabal-install(cabal upgrade and cabal remove)</li>
</ul>
<h1 id="implementation-plan">Implementation Plan</h1>
<h2 id="persistent-package-store">Persistent package store</h2>
<p>A <a href="https://github.com/ghc/ghc/commit/dd3a7245d4d557b9e19bfa53b0fb2733c6fd4f88">patch</a> has been pushed for ghc-7.11 that allows multiple instance of the same package to be installed. So the remaining work is in cabal tool for never modifying installed packages. I have written a <a href="https://github.com/fugyk/cabal/commit/45ec5edbaada1fd063c67d6109e69efa0e732e6a">patch to make cabal (almost)non-destructive</a>. This patch makes all the changes to Package database non-destructive if Installed Package ID is different. To make it fully non-mutable, Thomas Tuegel suggested to</p>
<ul>
<li>change installed package IDs to be computed by hash of the sdist tarball.</li>
<li>Enforce that a package is never overwritten by taking out a lock when updating the database.(before building the package)</li>
</ul>
<p>It will have additional benefit that package will not be built again if same source has already been built earlier, thus saving time.</p>
<h2 id="views">Views</h2>
<p>Views will the subset of packages of package store whose modules can be imported. Views will be present as various *.view file in <Package DB location>/views like default.view. The view file contains list of installed package IDs. There will exist a default view which contains packages installed by cabal install outside sandbox. If a package name is installed two times, default view will contain the instance of package which is installed at last. Views' packages will also act like GC roots.</p>
<p>To facilitate views, ghc-pkg will need some new commands:</p>
<ul>
<li>Create a view / Symlink a view</li>
<li>Delete a view</li>
<li>List all views</li>
<li>Modify a view</li>
<li>Add a package</li>
<li>Remove a package</li>
</ul>
<p>Sandbox will be a view. Cabal needs to set view when using sandbox. It also needs the ability to make a view and also add a package to the view. Packages can be shared between views. View path will be passed to ghc using -package-env. The view file that sandbox creates lies in the same directory and is symlinked from the package database view file for allowing GC. It will have a benefit that when we just delete the sandbox directory without deleting the view, GC can free that sandbox package.</p>
<p>It looks similar to nix development environment but has some differences. nix environments are like everything that is visible. It is kind of like imported packages with dependency closure. nix needs to make directories visible, while here we already have one more layer. Here we only need exposed package and ghc can make complete environment already. Views are just exposed packages of the environment. So, dependency of the package need not be in the view that the package is in. The problem that we are trying to solve with views is consistent way of managing packages and sandboxes, allowing packages to be shared between sandboxes and its packages being used as GC root.</p>
<p>Summary of design details</p>
<p>When installing a package outside sandbox</p>
<ul>
<li>Package is added to default view / modified in default view</li>
</ul>
<p>Making a sandbox</p>
<ul>
<li>A view is made in the directory</li>
<li>The new view is symlinked from the database</li>
<li>Packages that are installed are added to that view. Sandboxes cannot affect any other things outside sandbox.</li>
</ul>
<p>Within sandbox</p>
<ul>
<li>All the cabal commands pass view name to ghc and ghc will use relevant package</li>
</ul>
<h2 id="consistent-developer-environment">Consistent developer environment</h2>
<p>It will require additional constraint to check that there is no other instance of the same package or its dependencies is in the environment(packages from which we have imported the modules with their dependency closure) when we are importing the module from a package. It also needs to be checked when cabal is configuring the package, that a package do not directly or indirectly depends on two version of same package. If it is violated it needs to give out an error.</p>
<h2 id="garbage-collection">Garbage collection</h2>
<p>This will firstly involve determining the root packages and package list. Root packages are the packages which are in some view. Then we find list of all packages in the database. As there will be single database after implementing views, we don't need to call it for every sandbox database. Then we need to do mark-sweep and find which package are not in the reachable package list and select it for garbage collection. Then the selected packages will be deleted from the package store and also unregistered from database with ghc-pkg.</p>
<h2 id="cabal-remove">cabal remove</h2>
<p>With everything implemented above, it is just removing a package from default view. If package is unreachable it can be freed from disk by GC. It is guaranteed to not break any package except the package that is removed.</p>
<h2 id="cabal-upgrade">cabal upgrade</h2>
<p>cabal upgrade is just installing every package that is present in default view that has update available.</p>
<h2 id="current-status-1">Current Status</h2>
<p>It is possible to install multiple instances of the same package version with my forks of cabal and ghc. Quite a few problems remain.</p>
<p>See also [wiki:Commentary/Packages/MultiInstances]</p>
<h3 id="unique-install-location">Unique Install Location</h3>
<p>When specifying the install location there is a new variable $unique available. It is resolved to a random number by cabal-install during configuring. The default libsubdir for cabal-install should be &quot;$packageid-$unique&quot; for example &quot;mtl-2.1.2-1222131559&quot;. Cabal the library does not understand $unique so multiple instances of the same package version installed via &quot;runhaskell Setup.hs install&quot; are still problematic.</p>
<h3 id="ghc-pkg">ghc-pkg</h3>
<p>ghc-pkg never removes registered packages when registering a new one. Even if a new package with the same `InstalledPackageId` as an existing package is registered. Or if a new package that points to the same install directory is registered. `ghc-pkg` should probably check this and issue a warning.</p>
<h3 id="adhoc-dependency-resolution">Adhoc dependency resolution</h3>
<p>A new field `timestamp` was added to `InstalledPackageInfo`. It is set by Cabal the library when registering a package. It is used by Cabal the library, GHC and cabal-install to choose between different instances of the same package version.</p>
<h3 id="detect-whether-an-overwrite-happens-and-warn-about-it">Detect whether an overwrite happens and warn about it</h3>
<p>Currently cabal-install still warns about dangerous reinstalls and requires `--force-reinstalls` when it is sure a reinstall would happen. The correct behaviour here would be to detect if a reinstall causes overwriting (because of a version of ghc-pkg that does this) and warn only in this case. In this implementation reinstalls are not dangerous anymore.</p>
<h3 id="communicate-the-installedpackageid-back-to-cabal-install">Communicate the `InstalledPackageId` back to cabal-install</h3>
<p>An `InstallPlan` contains installed packages as well as packages to be installed and dependencies between those. We want to specify all of these dependencies with an `InstalledPackageId`. Unfortunately the `InstalledPackageId` is determined after installation and therefore not available for not yet installed packages. After installation it would have to be somehow communicated back to cabal-install. The current workaround is to only specify those packages that were previously installed with an `InstalledPackageId` and trust on Cabal picking the instance that was most recently (during execution of this install plan) installed for the other ones.</p>
<h3 id="garbage-collection-1">Garbage Collection</h3>
<p>A garbage collection should offer the removal of a certain package specified by `InstalledPackageId`, the removal of broken packages and the removal of probably unnecessary packages. A package is unnecessary if all packages that depend on it are unnecessary (this includes the case that no package depends on it) and it is not the most recently installed instance for its version. All of this should be accompanied by a lot of &quot;are you sure&quot; questioning.</p>
<h3 id="about-shadowing">About Shadowing</h3>
<p>GHC has the concept of shadowing. It was introduced as far as i understand (correct me please) because when combining the global and the user package databases you could end up with two instances of the same package version. The instance in the user database was supposed to shadow the one in the global database. Now that there are multiple instances of the same package version even in one package database this concepts needs to be rethought. This is non-trivial because flags asking for a package version as well as flags requiring a certain instance need to be taken into account.</p>
<h3 id="about-unique-identifier">About Unique Identifier</h3>
<p>Currently a big random number is created by cabal-install during configuration and passed to Cabal to be appended to the `InstalledPackageId` before registering. The reason is that the `InstalledPackageId` still contains the ABI hash which is only known after compilation. I personally would like the `InstalledPackageId` to be the name of the package, the version and a big random number. This could be determined before compilation, used as the `libsubdir` and baked into `package_Paths.hs`. Since it would be determined by cabal-install it would also make communicating the InstalledPackageId back to cabal-install after an installation unnecessary. The problem is that the `InstalledPackageId` would not be deterministic anymore.</p>
<h2 id="original-plan">Original Plan</h2>
<p>Cabal and GHC do not support multiple instances of the same package version installed at the same time. If a second instance of a package version is installed it is overwritten on the file system as well as in the `PackageDB`. This causes packages that depended upon the overwritten instance to break. The idea is to never overwrite an installed package. As already discussed in <a href="http://hackage.haskell.org/trac/ghc/wiki/Commentary/Packages/MultiInstances">4</a> the following changes need to be made:</p>
<p><code>* Cabal should install packages to a location that does not just depend on name and version,</code><br />
<code>* `ghc-pkg` should always add instances to the `PackageDB` and never overwrite them,</code><br />
<code>* `ghc --make`, `ghci`, and the configure phase of Cabal should select suitable instances according to some rule of thumb (similar to the current resolution technique),</code><br />
<code>* we want to be able to make more fine-grained distinctions between package instances than currently possible, for example by distinguishing different build flavours or &quot;ways&quot; (profiling, etc.)</code><br />
<code>* `cabal-install` should still find an `InstallPlan`, and still avoid unnecessarily rebuilding packages whenever it makes sense</code><br />
<code>* some form of garbage collection should be offered to have a chance to reduce the amount of installed packages</code></p>
<h2 id="hashes-and-identifiers">Hashes and identifiers</h2>
<p>There are three identifiers:</p>
<p><code>* `XXXX`: the identifier appended to the installation directory so that installed packages do not clash with each other</code><br />
<code>* `YYYY`: the `InstalledPackageId`, which is an identifier used to uniquely identify a package in the package database.</code><br />
<code>* `ZZZZ`: the ABI hash derived by GHC after compiling the package</code></p>
<p>The current situation:</p>
<p><code>* `XXXX`: is empty, which is bad (two instances of a package install in the same place)</code><br />
<code>* `YYYY`: is currently equal to `ZZZZ`, which is bad because we need to make more distinctions:</code><br />
<code>  * we need to distinguish between two packages that have identical ABIs but different behaviour (e.g. a bug was fixed)</code><br />
<code>  * we need to distinguish between two instances of a package that are compiled against different dependencies, or with different options, or compiled in a different way (profiling, dynamic)  </code></p>
<p>Some notes:</p>
<p><code>* `XXXX` must be decided </code><em><code>before</code></em><code> we begin compiling, because we have to generate the `Paths_P.hs` file that is compiled along with the package, whereas `ZZZZ` is only available </code><em><code>after</code></em><code> we have compiled the package.</code><br />
<code>* `ZZZZ` is not uniquely determined by the compilation inputs (see #4012), although in the future we hope it will be</code><br />
<code>* It is desirable that when two packages have identical `YYYY` values, then they are compatible, even if they were built on separate systems.  Note that this is not guaranteed even if `YYYY` is a deterministic function of the compilation inputs, because `ZZZZ` is non-deterministic (previous point).  Hence `YYYY` must be dependent on `ZZZZ`. </code><br />
<code>* It is desirable that `YYYY` be as deterministic as possible, i.e. we would rather not use a GUID, but `YYYY` should be determined by the compilation inputs and `ZZZZ`.  We know that `ZZZZ` is currently not deterministic, but in the future it will be, and at that point `YYYY` will become deterministic too, in the meantime `YYYY` should be no less deterministic than `ZZZZ`.</code></p>
<p>Our proposal:</p>
<p><code>* We define a new </code><em><code>Cabal</code> <code>Hash</code></em><code> that hashes the compilation inputs (the `LocalBuildInfo` and the contents of the source files)</code><br />
<code>* `XXXX` is a GUID.</code><br />
<code>  * Why not use the </code><em><code>Cabal</code> <code>Hash</code></em><code>?  We could, but then there could conceivably be a clash. (Andres - please expand this point, I have forgotten the full rationale).</code><br />
<code>* `YYYY` is the combination of the </code><em><code>Cabal</code> <code>Hash</code></em><code> and `ZZZZ` (concatenated)</code><br />
<code>* `ZZZZ` is recorded in the package database as a new field `abi-hash`.</code><br />
<code>  * When two packages have identical `ZZZZ`s then they are interface-compatible, and the user might in the future want to change a particular dependency to use a different package but the the same `ZZZZ`.  We do not want to make this change automatically, because even when two packages have identical `ZZZZ`s, they may have different behaviour (e.g. bugfixes).</code></p>
<h2 id="install-location-of-installed-cabal-packages">Install location of installed Cabal packages</h2>
<p>Currently the library part of packages is installed to `$prefix/lib/$pkgid/$compiler`. For example the `GLUT` package of version 2.3.0.0 when compiled with GHC 7.4.1 when installed globally lands in `/usr/local/lib/GLUT-2.3.0.0/ghc-7.4.1/`. This is the default path. It is completely customizable by the user. In order to allow multiple instances of this package to coexist we need to change the install location to a path that is unique for each instance. Several ways to accomplish this have been discussed:</p>
<h3 id="hash">Hash</h3>
<p>Use a hash to uniquely identify package instances and make the hash part of both the InstalledPackageId and the installation path.</p>
<p>The ABI hash currently being used by GHC is not suitable for unique identification of a package, because it is nondeterministic and not necessarily unique. In contrast, the proposed Cabal hash should be based on all the information needed to build a package.</p>
<p>This approach requires that we know the hash prior to building the package, because there is a data directory (per default under $prefix/share/$pkgid/) that is baked into Paths_foo.hs in preparation of the build process.</p>
<h3 id="unique-number">Unique number</h3>
<p>Use a unique number as part of the installation path.</p>
<p>A unique number could be the number of packages installed, or the number of instances of this package version already installed, or a random number. It is important that the numbers are guaranteed to be unique system-wide, so the counter-based approaches are somewhat tricky.</p>
<p>The advantage over using a hash is that this approach should be very simple to implement. On the other hand, identifying installed packages (see below) could possibly become more difficult, and migrating packages to other systems is only possible if the chance of collisions is reasonably low (for example, if random numbers are being used).</p>
<p><code> 1. The unique number is also part of the installed package id.</code></p>
<p><code> 2. We can use another unique identifier (for example, a Cabal hash) to identify installed packages. In this case, that identifier would be allowed to depend on the output of a package build.</code></p>
<h2 id="ghc-pkg-1">`ghc-pkg`</h2>
<p>`ghc-pkg` currently identifies each package by means of an `InstalledPackageId`. At the moment, this id has to be unique per package DB and is thereby limiting the amount of package instances that can be installed in a single package DB at one point in time.</p>
<p>In the future, we want the `InstalledPackageId` to still uniquely identify installed packages, but in addition to be unique among all package instances that could possibly be installed on a system. There's still the option that one InstalledPackageId occurs in several package DBs at the same time, but in this case, the associated packages should really be completely interchangeable. [If we want to be strict about this, we'd have to include the ABI hash in the `InstalledPackageId`.]</p>
<p>Even though, as discussed above, the ABI hash is not suitable for use as the `InstalledPackageId` given these changed requirements, we will need to keep the ABI hash as an essential piece of information for ghc itself.</p>
<p>`ghc-pkg` is responsible for storing all information we have about installed packages. Depending on design decisions about the solver and the Cabal hash, further information may be required in `ghc-pkg`'s description format (see below).</p>
<p>The following fields will be added to the description format:</p>
<p>A field <em>Way</em> of type `[String]`. It tracks the way in which the package was compiled. It is a subset of `{v,d,p}`. &quot;v&quot; means vanilla, &quot;d&quot; means dynamic linking and &quot;p&quot; means profiling. Other ways may be added later.</p>
<p>A `timestamp` of the time when the package was installed (or built?). It is used by GHC and Cabal to put a preference on the latest package of a certain version.</p>
<p>A currently empty but extensible set of fields starting with &quot;x-cabal-...&quot;. `ghc-pkg` ignores them when parsing. During the resolution phase `cabal-install` might use them to decide compatibility between packages.</p>
<p>A field abi-hash that contains the ABI hash because it is no longer stored implicitly as part of the `InstalledPackageId`.</p>
<h2 id="simplistic-dependency-resolution">Simplistic dependency resolution</h2>
<p>The best tool for determining suitable package instances to use as build inputs is `cabal-install`. However, in practice there will be many situations where users will probably not have the full `cabal-install` functionality available:</p>
<p><code> 1. invoking GHCi from the command line,</code><br />
<code> 2. invoking GHC directly from the command line,</code><br />
<code> 3. invoking the configure phase of Cabal (without using `cabal-install`).</code></p>
<p>In these cases, we have to come up with a suitable selection of package instances, and the only info we have available are the package DBs plus potential command line flags. Cabal will additionally take into account the local constraints of the package it is being invoked for, whereas GHC will only consider command-line flags, but not modules it has been invoked with.</p>
<p>Currently if GHC is invoked by the user it does some adhoc form of dependency resolution. The most common case of this is using ghci. If there are multiple instances of the same package in the `PackageDBStack` the policy used to select a single one prefers DBs higher in the stack. It then prefers packages with a higher version. Once we allow package instances with the same version within a single package DB, we need to refine the algorithm. Options are:</p>
<p><code>* pick a random / unspecified instances</code><br />
<code>* use the time of installation</code><br />
<code>* user-specified priorities</code><br />
<code>* use the order in the `PackageDB`</code><br />
<code>* look at the transitive closure of dependencies and their versions</code><br />
<code>* build a complex solver into GHC</code></p>
<p>Picking a random version is a last resort. A combination of installation time and priorities seems rather feasible. It makes conflicts unlikely, and allows to persistently change the priorities of installed packages. Using the order in the package DB is difficult if directories are being used as DBs. Looking at the transitive closure of dependencies makes it hard to define a total ordering of package instances. Adding a complex solver is unattractive unless we find a way to reuse `cabal-install`'s functionality within GHC, but probably we do not want to tie the two projects together in this way.</p>
<h2 id="build-flavours">Build flavours</h2>
<p>Once we distinguish several package instances with the same version, we have a design decision how precise we want that distinction to be.</p>
<p>The minimal approach would be to just take the transitive dependencies into account. However, we might also want to include additional information about builds such as Cabal flag settings, compiler options, profiling, documentation, build tool versions, external (OS) dependencies, and more.</p>
<p>These differences have to be tracked. The two options we discuss are to store information in the `ghc-pkg` format, or to incorporate them in a Cabal hash (which is then stored). Both options can be combined.</p>
<h3 id="the-cabal-hash">The Cabal hash</h3>
<p>[A few notes about where to find suitable information in the source code:]</p>
<p>A build configuration consists of the following:</p>
<p>The Cabal hashes of all the package instances that are actually used for compilation. This is the environment. It is available in the `installedPkgs` field of `LocalBuildInfo` which is available in every step after configuration. It can also be extracted from an `InstallPlan` after dependency resolution.</p>
<p>The compiler, its version and its arguments and the tools and their version and their arguments. Available from LocalBuildInfo also. More specifically: `compiler`, `withPrograms`, `withVanillaLib`, `withProfLib`, `withSharedLib`, `withDynExe`, `withProfExe`, `withOptimization`, `withGHCiLib`, `splitObjs`, `stripExes`. And a lot more. [Like what?]</p>
<p>The source code. This is necessary because if the source code changes the result of compilation changes. For released packages I would assume that the version number uniquely identifies the source code. A hash of the source code should be available from hackage to avoid downloading the source code. For an unreleased package we need to find all the source files that are needed for building it. Including non-haskell source files. One way is to ask a source tarball to be built as if the package was released and then hash all the sources included in that.</p>
<p>OS dependencies are not taken into account because i think it would be very hard.</p>
<h3 id="released-and-unreleased-packages">Released and Unreleased packages</h3>
<p>If we cabal install a package that is released on hackage we call this a <strong>clean install</strong>. If we cabal install an unreleased package we call this a <strong>dirty install</strong>. Clean installs are mainly used to bring a package into scope for ghci and to install applications. While they can be used to satisfy dependencies this is discouraged. For released packages the set of source files needed for compilation is known. For unreleased packages this is currently not the case.</p>
<h2 id="dependency-resolution-in-cabal-install">Dependency resolution in cabal-install</h2>
<p>There are two general options for communicating knowledge about build flavors to the solver:</p>
<p><code> 1. </code><strong><code>the</code> <code>direct</code> <code>way</code></strong><code>: i.e., all info is available to ghc-pkg and can be communicated back to Cabal and therefore the solver can figure out if a particular package is suitable to use or not, in advance;</code></p>
<p><code> 2. </code><strong><code>the</code> <code>agnostic</code> <code>way</code></strong><code>: this is based on the idea that the solver at first doesn't consider installed packages at all. It'll just do resolution on the source packages available. Then, taking all build parameters into account, Cabal hashes will be computed, which can then be compared to hashes of installed packages.</code></p>
<p>Reusing installed packages instead of rebuilding them is then an optimization of the install plan.</p>
<p>The agnostic way does not require `ghc-pkg` to be directly aware of all the build parameters, as long as the hash computation is robust</p>
<p>The options are to support either both by putting all info into `InstalledPackageInfo` or to support only the second option by just putting a hash into `InstalledPackageInfo`. The disadvantage of supporting both is that `InstalledPackageInfo` would have to change more often. This could be fixed by explicitly making the `InstalledPackageInfo` format extensible in a backwards-compatible way.</p>
<p>The advantages of having all info available, independently of the solver algorihm, are that the info might be useful for other tools and user feedback.</p>
<p>Possible disadvantages of the agnostic approach could be that is is a rather significant change and can probably not be supported in a similar way for other Haskell implementation. Also, in the direct approach, we could in principle allow more complex compatibility rules, such as allowing non-profiling libraries to depend on profiling libraries.</p>
<p>Also, even if we go for the agnostic approach, we still have to be able to handle packages such as base or ghc-prim which are in general not even available in source form.</p>
<p>On the other hand, the agnostic approach might lead to more predictable and reproducible solver results across many different systems.</p>
<h2 id="garbage-collection-2">Garbage Collection</h2>
<p>The proposed changes will likely lead to a dramatic increase of the number of installed package instances on most systems. This is particularly relevant for package developers who will conduct lots of dirty builds that lead to new instances being installed all the time.</p>
<p>It should therefore be possible to have a garbage collection to remove unneeded packages. However, it is not possible for Cabal to see all potential reverse dependencies of a package, so automatic garbage collection would be extremely unsafe.</p>
<p>Options are to either offer an interactive process where packages that look unused are suggested for removal, or to integrate with a sandbox mechanism. If, for example, dirty builds are usually installed into a separate package DB, that package DB could just be removed completely by a user from time to time.</p>
<p>The garbage collection functionality is part of cabal-install not of ghc-pkg. As a first approximation gc does not remove files only unregisters packages from the `PackageDB`.</p>
<h2 id="currently-open-design-decisions">Currently open design decisions</h2>
<h3 id="installedpackageid-and-install-path">`InstalledPackageId` and install path</h3>
<p>Options for uniquely identifying `InstalledPackageId`:</p>
<p><code>  * Cabal hash only</code><br />
<code>  * Cabal + ABI hash (truly unique)</code><br />
<code>  * random number</code></p>
<p>Options for identifying install path:</p>
<p><code>  * Cabal hash</code><br />
<code>  * random number</code></p>
<p>ABI hash cannot be in install path because it's only available after build.</p>
<h3 id="handling-of-dirty-builds">Handling of dirty builds</h3>
<p>How should hash computation work for dirty builds?</p>
<p><code>  * Use a random number even if we otherwise use hashes</code><br />
<code>  * Hash the complete build directory</code><br />
<code>  * Attempt to make a clean (sdist-like) copy or linked copy of the sources and hash and build from that.</code><br />
<code>  * Use the Cabal file to determine the files that would end up in an sdist and hash those directly without copying.</code></p>
<p>The third option has the advantage(?) that the build is more guaranteed to use only files actually mentioned in the Cabal file.</p>
<h3 id="build-flavours-1">Build flavours</h3>
<p>To what degree should we distinguish package instances?</p>
<p><code>  * Only package versions transitively</code><br />
<code>  * Ways and Cabal flags</code><br />
<code>  * Everything Haskell-specific info that we can query</code><br />
<code>  * Even non-Haskell-specific inputs such as OS dependencies</code></p>
<h3 id="installedpackageinfo-and-solver-algorithm">`InstalledPackageInfo` and solver algorithm</h3>
<p>Options for `InstalledPackageInfo`:</p>
<p><code>  * Only add Cabal hash.</code><br />
<code>  * Add (nearly) all information, but in an extensible format.</code><br />
<code>  * Add all information in a way that `ghc-pkg` itself can use it.</code></p>
<p>[These aren't necessarily mutually exclusive.]</p>
<p>Options for the solver:</p>
<p><code>  * Direct (see above): requires a certain amount of info in the `InstalledPackageInfo`.</code></p>
<p><code>  * Agnostic (except for builtin packages): could be done with only the Cabal hash in `InstalledPackageInfo`.</code></p>
<h3 id="simplistic-dependency-resolution-1">Simplistic dependency resolution</h3>
<p>Options (in order of preference):</p>
<p><code>* use the time of installation</code><br />
<code>* user-specified priorities</code><br />
<code>* pick a random / unspecified instances</code><br />
<code>* (build a complex solver into GHC)</code></p>
<p>A combination of the first two seems possible and useful.</p>
<h2 id="related-topics">Related topics</h2>
<p>In the following, we discuss some other issues which are related to the multi-instance problem, but not necessarily directly relevant in order to produce an implementation.</p>
<h3 id="separating-storage-and-selection-of-packages">Separating storage and selection of packages</h3>
<p>Currently the two concepts of storing package instances (cabal store) and selecting package instances for building (environment) are conflated into a `PackageDB`. Sandboxes are used as a workaround to create multiple different environments. But they also create multiple places to store installed packages. The disadvantages of this are disk usage, compilation time and one might lose the overview. Also if the multi-instance restriction is not lifted sandboxes will eventually suffer from the same unintended breakage of packages as non-sandboxed `PackageDB`s. There should be a separation between the set of all installed packages called the cabal store and a subset of these called an environment. While the cabal store can contain multiple instances of the same package version an environment needs to be consistent. An environment is consistent if for every package version it contains only one instance of that package version.</p>
<h3 id="first-class-environments">First class environments</h3>
<p>It would be nice if we had some explicit notion of an environment.</p>
<h2 id="questions-to-remember">Questions to remember</h2>
<p>Should the cabal version be part of the hash?</p>
<p>Does the hash contain characters conflicting under windows?</p>
<p>What about builtin packages like ghc-prim, base, rts and so on?</p>
<p>Inplace Registration?</p>
<p>Who has assumptions about the directory layout of installed packages?</p>
<p>Executables?</p>
<p>Haddock?</p>
<p>Installation Planner?</p>
<p>Custom Builds and BuildHooks?</p>
<p>Other Compilers, backwards compatibility?</p>
<p>What is ComponentLocalBuildInfo for?</p>
<h1 id="the-haskell-execution-model">The Haskell Execution Model</h1>
<p>The [wiki:Commentary/Compiler/StgSynType STG language] has a clear <em>operational</em> model, as well as having a declarative lambda-calculus reading. The business of the [wiki:Commentary/Compiler/CodeGen code generator] is to translate the STG program into `C--`, and thence to machine code, but that is mere detail. From the STG program you should be able to understand:</p>
<p><code> * What functions are in the compiled program, and what their entry and return conventions are</code><br />
<code> * What heap objects are allocated, when, and what their layout is</code></p>
<p>GHC uses an eval/apply execution model, described in the paper <a href="http://research.microsoft.com/%7Esimonpj/papers/eval-apply">How to make a fast curry: push/enter vs eval/apply</a>. This paper is well worth reading if you are interested in this section.</p>
<p>Contents:</p>
<p><code>* [wiki:Commentary/Rts/HaskellExecution/Registers Registers]</code><br />
<code>* [wiki:Commentary/Rts/HaskellExecution/FunctionCalls Function Calls]</code><br />
<code>* [wiki:Commentary/Rts/HaskellExecution/CallingConvention Call and Return Conventions]</code><br />
<code>* [wiki:Commentary/Rts/HaskellExecution/HeapChecks Heap and Stack checks]</code><br />
<code>* [wiki:Commentary/Rts/HaskellExecution/Updates Updates]</code><br />
<code>* [wiki:Commentary/Rts/HaskellExecution/PointerTagging Pointer Tagging]</code></p>
<h1 id="heap_alloced">HEAP_ALLOCED</h1>
<p>This page is about the `HEAP_ALLOCED()` macro/function in the runtime system. See #8199 which is about getting rid of `HEAP_ALLOCED`.</p>
<p></p>
<p>It is defined in `rts/sm/MBlock.h`. The purpose of `HEAP_ALLOCED()` is to return true if the given address is part of the dynamically-allocated heap, and false otherwise. Its primary use is in the Garbage Collector: when examining a pointer, we need to get to the block descriptor for that object. Static objects don't have block descriptors, because they live in static code space, so we need to establish whether the pointer is into the dynamic heap first, hence `HEAP_ALLOCED()`.</p>
<p>On a 32-bit machine, `HEAP_ALLOCED()` is implemented with a 4096-entry byte-map, one byte per megabyte of the address space (the dynamic heap is allocated in units of aligned megabytes).</p>
<p>On a 64-bit machine, it's a bit more difficult. The current method (GHC 6.10.1 and earlier) uses a cache, with a 4096-entry map and a 32-bit tag. If the upper 32 bits of the pointer match the tag, we look up in the map, otherwise we back off to a slow method that searches a list of mappings (bug #2934 is about the lack of thread-safety in the slow path here). This arrangement works fine for small heaps, but is pessimal for large (multi-GB) heaps, or heaps that are scattered around the address space.</p>
<h2 id="speeding-up-heap_alloced">Speeding up `HEAP_ALLOCED()`</h2>
<p>We should consider how to speed up `HEAP_ALLOCED()` for large heaps on 64-bit machines. This involves some kind of cache arrangement - the memory map is like a page table, and we want a cache that gives us quick access to commonly accessed parts of that map.</p>
<p><a href="attachment:faster-heap-alloced.patch.gz">5</a> implements one such scheme. Measurements show that it slows down GC by about 20% for small heaps (hence it wasn't committed), though it would probably speed up GC on large heaps.</p>
<h2 id="eliminating-heap_alloced-completely">Eliminating `HEAP_ALLOCED` completely</h2>
<p>Can we eliminate `HEAP_ALLOCED` altogether? We must arrange that all closure pointers have a valid block descriptor.</p>
<h3 id="method-1-put-static-closures-in-an-aligned-section">Method 1: put static closures in an aligned section</h3>
<p>ELF sections can be arbitrarily aligned. So we could put all our static closures in a special section, align the section to 1MB, and arrange that there is space at the beginning of the section for the block descriptors.</p>
<p>This almost works (see <a href="attachment:eliminate-heap-alloced.patch.gz">6</a>), but sadly fails for shared libraries: the system dynamic linker doesn't honour section-alignment requests larger than a page, it seems. Here is a simple test program which shows the problem on Linux:</p>
<p></p>
<p></p>
<p>Compare static linking and dynamic linking:</p>
<p></p>
<h3 id="method-2-copy-static-closures-into-a-special-area-at-startup">Method 2: copy static closures into a special area at startup</h3>
<p>We could arrange that we access all static closures via indirections, and then at startup time we copy all the static closures into a special area with block descriptors.</p>
<p>Disadvantages:</p>
<p><code>* references to static objects go through another indirection. (This includes all of the RTS code!)</code><br />
<code>  * when doing dynamic linking, references to static objects in another package</code><br />
<code>    already go through an indirection and we could arrange that only one indirection is required.</code><br />
<code>  * References to static closures from the the fields of a static constructor would not incur the extra indirection,</code><br />
<code>    only direct references to static closures from code.</code><br />
<code>  * we currently reference the static closure of a function from the heap-check-fail code, but in fact</code><br />
<code>    we only really need to pass the info pointer.</code></p>
<p>Advantages</p>
<p><code>* we get to fix up all the tag bits in static closure pointers</code><br />
<code>* we get to eliminate HEAP_ALLOCED, speeding up GC and removing complexity</code><br />
<code>* CAFs might get a bit simpler, since they are already indirections into the heap</code></p>
<h1 id="heap-and-stack-checks">Heap and Stack checks</h1>
<p>Source files: <a href="GhcFile(rts/HeapStackCheck.cmm)" class="uri" title="wikilink">GhcFile(rts/HeapStackCheck.cmm)</a></p>
<p>When allocating a heap object, we bump `Hp` and compare to `HpLim`. If the test fails we branch to ???. Usually this code tests an interrupt flag (to see if execution should be brought tidily to a halt); grabs the next block of allocation space; makes `Hp` point to it and `HpLim` to its end; and returns. If there are no more allocation-space blocks, garbage collection is triggered.</p>
<hr />
<p>CategoryStub</p>
<p><a href="PageOutline" class="uri" title="wikilink">PageOutline</a></p>
<h1 id="ghc-commentary-the-layout-of-heap-objects">GHC Commentary: The Layout of Heap Objects</h1>
<h2 id="terminology">Terminology</h2>
<p><code>* A </code><em><code>lifted</code></em><code> type is one that contains bottom (_|_), conversely an </code><em><code>unlifted</code></em><code> type does not contain _|_.</code><br />
<code>  For example, </code><code> is lifted, but </code><code> is unlifted.</code></p>
<p><code>* A </code><em><code>boxed</code></em><code> type is represented by a pointer to an object in the heap, an </code><em><code>unboxed</code></em><code> object is represented by a value.</code><br />
<code>  For example, </code><code> is boxed, but </code><code> is unboxed.</code></p>
<p>The representation of _|_ must be a pointer: it is an object that when evaluated throws an exception or enters an infinite loop. Therefore, only boxed types may be lifted.</p>
<p>There are boxed unlifted types: eg. . If you have a value of type , it definitely points to a heap object with type  (see below), rather than an unevaluated thunk.</p>
<p>Unboxed tuples  are both unlifted and unboxed. They are represented by multiple values passed in registers or on the stack, according to the [wiki:Commentary/Rts/HaskellExecution return convention].</p>
<p>Unlifted types cannot currently be used to represent terminating functions: an unlifted type on the right of an arrow is implicitly lifted to include `_|_`.</p>
<hr />
<h2 id="heap-objects">Heap Objects</h2>
<p>All heap objects have the same basic layout, embodied by the type  in [source:includes/rts/storage/Closures.h Closures.h]. The diagram below shows the layout of a heap object:</p>
<p><a href="Image(heap-object.png)" class="uri" title="wikilink">Image(heap-object.png)</a></p>
<p>A heap object always begins with a <em>header</em>, defined by  in [source:includes/rts/storage/Closures.h Closures.h]:</p>
<p></p>
<p>The most important part of the header is the <em>info pointer</em>, which points to the info table for the closure. In the default build, this is all the header contains, so a header is normally just one word. In other builds, the header may contain extra information: eg. in a profiling build it also contains information about who built the closure.</p>
<p>Most of the runtime is insensitive to the size of ; that is, we are careful not to hardcode the offset to the payload anywhere, instead we use C struct indexing or . This makes it easy to extend  with new fields if we need to.</p>
<p>The compiler also needs to know the layout of heap objects, and the way this information is plumbed into the compiler from the C headers in the runtime is described here: [wiki:Commentary/Compiler/CodeGen#Storagemanagerrepresentations].</p>
<hr />
<h2 id="info-tables">Info Tables</h2>
<p>The info table contains all the information that the runtime needs to know about the closure. The layout of info tables is defined by  in [source:includes/rts/storage/InfoTables.h InfoTables.h]. The basic info table layout looks like this:</p>
<p><a href="Image(basic-itbl.png)" class="uri" title="wikilink">Image(basic-itbl.png)</a></p>
<p>Where:</p>
<p><code>* The </code><em><code>closure</code> <code>type</code></em><code> is a constant describing the kind of closure this is (function, thunk, constructor etc.).  All</code><br />
<code>  the closure types are defined in [source:includes/rts/storage/ClosureTypes.h ClosureTypes.h], and many of them have corresponding C struct</code><br />
<code>  definitions in [source:includes/rts/storage/Closures.h Closures.h].</code></p>
<p><code>* The </code><em><code>SRT</code> <code>bitmap</code></em><code> field is used to support [wiki:Commentary/Rts/Storage/GC/CAFs garbage collection of CAFs].</code></p>
<p><code>* The </code><em><code>layout</code></em><code> field describes the layout of the payload for the garbage collector, and is described in more</code><br />
<code>  detail in </code><a href="ref(Types_of_Payload_Layout)" title="wikilink"><code>ref(Types</code> <code>of</code> <code>Payload</code> <code>Layout)</code></a><code> below.</code></p>
<p><code>* The </code><em><code>entry</code> <code>code</code></em><code> for the closure is usually the code that will </code><em><code>evaluate</code></em><code> the closure.  There is one exception:</code><br />
<code>  for functions, the entry code will apply the function to the arguments given in registers or on the stack, according</code><br />
<code>  to the calling convention.  The entry code assumes all the arguments are present - to apply a function to fewer arguments</code><br />
<code>  or to apply an unknown function, the [wiki:Commentary/Rts/HaskellExecution/FunctionCalls#Genericapply generic apply functions] must</code><br />
<code>  be used.</code></p>
<p>Some types of object add more fields to the end of the info table, notably functions, return addresses, and thunks.</p>
<p>Space in info tables is a premium: adding a word to the standard info table structure increases binary sizes by 5-10%.</p>
<h3 id="section-5"></h3>
<p>Note that the info table is followed immediately by the entry code, rather than the code being at the end of an indirect pointer. This both reduces the size of the info table and eliminates one indirection when jumping to the entry code.</p>
<p>GHC can generate code that uses the indirect pointer instead; the  turns on the optimised layout. Generally  is turned off when compiling unregisterised.</p>
<p>When  is off, info tables get another field, , which points to the entry code. In a generated object file, each symbol  representing an info table will have an associated symbol  pointing to the entry code (in , the entry symbol is omitted to keep the size of symbol tables down).</p>
<hr />
<h2 id="types-of-payload-layout">Types of Payload Layout</h2>
<p>The GC needs to know two things about the payload of a heap object: how many words it contains, and which of those words are pointers. There are two basic kinds of layout for the payload: <em>pointers-first</em> and <em>bitmap</em>. Which of these kinds of layout is being used is a property of the <em>closure type</em>, so the GC first checks the closure type to determine how to interpret the layout field of the info table.</p>
<h3 id="pointers-first-layout">Pointers-first layout</h3>
<p>The payload consists of zero or more pointers followed by zero or more non-pointers. This is the most common layout: constructors, functions and thunks use this layout. The layout field contains two half-word-sized fields:</p>
<p><code> * Number of pointers</code><br />
<code> * Number of non-pointers</code></p>
<h3 id="bitmap-layout">Bitmap layout</h3>
<p>The payload consists of a mixture of pointers and non-pointers, described by a bitmap. There are two kinds of bitmap:</p>
<p><strong>Small bitmaps.</strong> A small bitmap fits into a single word (the layout word of the info table), and looks like this:</p>
<p>|| Size (bits 0-4) || Bitmap (bits 5-31) ||</p>
<p>(for a 64-bit word size, the size is given 6 bits instead of 5).</p>
<p>The size field gives the size of the payload, and each bit of the bitmap is 1 if the corresponding word of payload contains a pointer to a live object.</p>
<p>The macros , , and  in [source:includes/rts/storage/InfoTables.h InfoTables.h] provide ways to conveniently operate on small bitmaps.</p>
<p><strong>Large bitmaps.</strong> If the size of the stack frame is larger than the 27 words that a small bitmap can describe, then the fallback mechanism is the large bitmap. A large bitmap is a separate structure, containing a single word size and a multi-word bitmap: see  in [source:includes/rts/storage/InfoTables.h InfoTables.h].</p>
<hr />
<h2 id="dynamic-vs.-static-objects">Dynamic vs. Static objects</h2>
<p>Objects fall into two categories:</p>
<p><code>* </code><em><code>dynamic</code></em><code> objects reside in the heap, and may be moved by the garbage collector.</code></p>
<p><code>* </code><em><code>static</code></em><code> objects reside in the compiled object code.  They are never moved, because pointers to such objects are</code><br />
<code>  scattered through the object code, and only the linker knows where.</code></p>
<p>To find out whether a particular object is dynamic or static, use the [wiki:Commentary/Rts/Storage/HeapAlloced HEAP_ALLOCED()] macro, from [source:rts/sm/HeapAlloc.h]. This macro works by consulting a bitmap (or structured bitmap) that tells for each [wiki:Commentary/Rts/Storage#Structureofblocks megablock] of memory whether it is part of the dynamic heap or not.</p>
<h3 id="dynamic-objects">Dynamic objects</h3>
<p>Dynamic objects have a minimum size, because every object must be big enough to be overwritten by a forwarding pointer (<a href="ref(Forwarding_Pointers)" title="wikilink">ref(Forwarding Pointers)</a>) during GC. The minimum size of the payload is given by  in [source:includes/rts/Constants.h].</p>
<h3 id="static-objects">Static objects</h3>
<p>All static objects have closure types ending in , eg.  for static data constructors.</p>
<p>Static objects have an additional field, called the <em>static link field</em>. The static link field is used by the GC to link all the static objects in a list, and so that it can tell whether it has visited a particular static object or not - the GC needs to traverse all the static objects in order to [wiki:Commentary/Rts/CAFs garbage collect CAFs].</p>
<p>The static link field resides after the normal payload, so that the static variant of an object has compatible layout with the dynamic variant. To access the static link field of a closure, use the  macro from [source:includes/rts/storage/ClosureMacros.h].</p>
<hr />
<h2 id="types-of-object">Types of object</h2>
<h3 id="data-constructors">Data Constructors</h3>
<p>All data constructors have pointers-first layout:</p>
<p>|| Header || Pointers... || Non-pointers... ||</p>
<p>Data constructor closure types:</p>
<p><code>* </code><code>: a vanilla, dynamically allocated constructor</code><br />
<code>* </code><code>: a constructor whose layout is encoded in the closure type (eg. </code><code> has one pointer</code><br />
<code>  and zero non-pointers.  Having these closure types speeds up GC a little for common layouts.</code><br />
<code>* </code><code>: a statically allocated constructor.</code><br />
<code>* </code><code>: TODO: Needs documentation</code></p>
<p>The entry code for a constructor returns immediately to the topmost stack frame, because the data constructor is already in WHNF. The return convention may be vectored or non-vectored, depending on the type (see [wiki:Commentary/Rts/HaskellExecution/CallingConvention]).</p>
<p>Symbols related to a data constructor X:</p>
<p><code>* X_</code><code>: info table for a dynamic instance of X</code><br />
<code>* X_</code><code>: info table for a static instance of X</code><br />
<code>* X_</code><code>: the </code><em><code>wrapper</code></em><code> for X (a function, equivalent to the</code><br />
<code>  curried function </code><code> in Haskell, see</code><br />
<code>  [wiki:Commentary/Compiler/EntityTypes]).  </code><br />
<code>* X_</code><code>: static closure for X's wrapper</code></p>
<h3 id="function-closures">Function Closures</h3>
<p>A function closure represents a Haskell function. For example:  Here,  would be represented by a static function closure (see below), and  a dynamic function closure. Every function in the Haskell program generates a new info table and entry code, and top-level functions additionally generate a static closure.</p>
<p>All function closures have pointers-first layout:</p>
<p>|| Header || Pointers... || Non-pointers... ||</p>
<p>The payload of the function closure contains the free variables of the function: in the example above, a closure for  would have a payload containing a pointer to .</p>
<p>Function closure types:</p>
<p><code>* </code><code>: a vanilla, dynamically allocated function</code><br />
<code>* </code><code>: same, specialised for layout (see constructors above)</code><br />
<code>* </code><code>: a static (top-level) function closure</code></p>
<p>Symbols related to a function :</p>
<p><code>* </code><code>: f's info table and code</code><br />
<code>* </code><code>: f's static closure, if f is a top-level function.</code><br />
<code>  The static closure has no payload, because there are no free</code><br />
<code>  variables of a top-level function.  It does have a static link</code><br />
<code>  field, though.</code></p>
<h3 id="thunks">Thunks</h3>
<p>A thunk represents an expression that is not obviously in head normal form. For example, consider the following top-level definitions:  Here the right-hand sides of  and  are both thunks; the former is static while the latter is dynamic.</p>
<p>Thunks have pointers-first layout:</p>
<p>|| Header || (empty) || Pointers... || Non-pointers... ||</p>
<p>As for function closures, the payload contains the free variables of the expression. A thunk differs from a function closure in that it can be [wiki:Commentary/Rts/HaskellExecution#Updates updated].</p>
<p>There are several forms of thunk:</p>
<p><code>* </code><code>, </code><code>: vanilla, dynamically allocated</code><br />
<code>  thunks.  Dynamic thunks are overwritten with normal indirections</code><br />
<code>  </code><code> when evaluated.</code></p>
<p><code>* </code><code>: a static thunk is also known as a ''constant</code><br />
<code>  applicative form'', or </code><em><code>CAF</code></em><code>.  Static thunks are overwritten with</code><br />
<code>  static indirections (</code><code>).</code></p>
<p>The only label associated with a thunk is its info table:</p>
<p><code>* </code><code> is f's info table.</code></p>
<p>The empty padding is to allow thunk update code to overwrite the target of an indirection without clobbering any of the saved free variables. This means we can do thunk update without synchronization, which is a big deal.</p>
<h3 id="selector-thunks">Selector thunks</h3>
<p> is a (dynamically allocated) thunk whose entry code performs a simple selection operation from a data constructor drawn from a single-constructor type. For example, the thunk  is a selector thunk. A selector thunk is laid out like this:</p>
<p>|| Header || Selectee pointer ||</p>
<p>The `layout` word contains the byte offset of the desired word in the selectee. Note that this is different from all other thunks.</p>
<p>The garbage collector &quot;peeks&quot; at the selectee's tag (in its info table). If it is evaluated, then it goes ahead and does the selection, and then behaves just as if the selector thunk was an indirection to the selected field. If it is not evaluated, it treats the selector thunk like any other thunk of that shape.</p>
<p>This technique comes from the Phil Wadler paper <a href="http://homepages.inf.ed.ac.uk/wadler/topics/garbage-collection.html">Fixing some space leaks with a garbage collector</a>, and later Christina von Dorrien who called it &quot;Stingy Evaluation&quot;.</p>
<p>There is a fixed set of pre-compiled selector thunks built into the RTS, representing offsets from 0 to , see [source:rts/StgStdThunks.cmm]. The info tables are labelled  where  is the offset. Non-updating versions are also built in, with info tables labelled .</p>
<p>These thunks exist in order to prevent a space leak. For example, if y is a thunk that has been evaluated, and y is unreachable, but x is reachable, the risk is that x keeps both the a and b components of y live. By making the selector thunk a special case, we make it possible to reclaim the memory associated with b. (The situation is further complicated when selector thunks point to other selector thunks; the garbage collector sees all, knows all.)</p>
<h3 id="partial-applications">Partial applications</h3>
<p>Partial applications are tricky beasts.</p>
<p>A partial application, closure type , represents a function applied to too few arguments. Partial applications are only built by the [wiki:Commentary/Rts/HaskellExecution/FunctionCalls#Genericapply generic apply functions] in [source:rts/Apply.cmm].</p>
<p>|| Header || Arity || No. of words || Function closure || Payload... ||</p>
<p>Where:</p>
<p><code>* </code><em><code>Arity</code></em><code> is the arity of the PAP.  For example, a function with</code><br />
<code>  arity 3 applied to 1 argument would leave a PAP with arity 2.</code></p>
<p><code>* </code><em><code>No.</code> <code>of</code> <code>words</code></em><code> refers to the size of the payload in words.</code></p>
<p><code>* </code><em><code>Function</code> <code>closure</code></em><code> is the function to which the arguments are</code><br />
<code>  applied.  Note that this is always a pointer to one of the</code><br />
<code>  </code><code> family, never a </code><code>.  If a </code><code> is applied</code><br />
<code>  to more arguments to give a new </code><code>, the arguments from</code><br />
<code>  the original </code><code> are copied to the new one.</code></p>
<p><code>* The payload is the sequence of arguments already applied to</code><br />
<code>  this function.  The pointerhood of these words are described</code><br />
<code>  by the function's bitmap (see </code><code> in </code><br />
<code>  [source:rts/sm/Scav.c] for an example of traversing a PAP).</code></p>
<p>There is just one standard form of PAP. There is just one info table too, called . A PAP should never be entered, so its entry code causes a failure. PAPs are applied by the generic apply functions in .</p>
<h3 id="generic-application">Generic application</h3>
<p>An  object is very similar to a , and has identical layout:</p>
<p>|| Header || Arity || No. of words || Function closure || Payload... ||</p>
<p>The difference is that an  is not necessarily in WHNF. It is a thunk that represents the application of the specified function to the given arguments.</p>
<p>The arity field is always zero (it wouldn't help to omit this field, because it is only half a word anyway).</p>
<p> closures are used mostly by the byte-code interpreter, so that it only needs a single form of thunk object. Interpreted thunks are always represented by the application of a  to its free variables.</p>
<h3 id="stack-application">Stack application</h3>
<p>An  is a special kind of object:</p>
<p>|| Header || Size || Closure || Payload... ||</p>
<p>It represents computation of a thunk that was suspended midway through evaluation. In order to continue the computation, copy the payload onto the stack (the payload was originally the stack of the suspended computation), and enter the closure.</p>
<p>Since the payload is a chunk of stack, the GC can use its normal stack-walking code to traverse it.</p>
<p> closures are built by  in [source:rts/RaiseAsync.c] when an [wiki:Commentary/Rts/AsyncExceptions asynchronous exception] is raised. It's fairly typical for the end of an AP_STACK's payload to have another AP_STACK: you'll get one per update frame.</p>
<h3 id="indirections">Indirections</h3>
<p>Indirection closures just point to other closures. They are introduced when a thunk is updated to point to its value. The entry code for all indirections simply enters the closure it points to.</p>
<p>The basic layout of an indirection is simply</p>
<p>|| Header || Target closure ||</p>
<p>There are several variants of indirection:</p>
<p><code>* </code><code>: is the vanilla, dynamically-allocated indirection.</code><br />
<code>  It is removed by the garbage collector.  An </code><code> only exists in the youngest generation.  </code><br />
<code>  The update code (</code><code> and friends) checks whether the updatee is in the youngest</code><br />
<code>  generation before deciding which kind of indirection to use.</code><br />
<code>* </code><code>: a static indirection, arises when we update a </code><code>.  A new </code><br />
<code>  is placed on the mutable list when it is created (see </code><code> in [source:rts/sm/Storage.c]).</code></p>
<h3 id="byte-code-objects">Byte-code objects</h3>
<p></p>
<h3 id="black-holes">Black holes</h3>
<p>, </p>
<p>Black holes represent thunks which are under evaluation by another thread (that thread is said to have claimed the thunk). Attempting to evaluate a black hole causes a thread to block until the thread who claimed the thunk either finishes evaluating the thunk or dies. You can read more about black holes in the paper 'Haskell on a Shared-Memory Multiprocessor'. Black holes have the same layout as indirections.</p>
<p>|| Header || Target closure ||</p>
<p>Sometimes black holes are just ordinary indirection. Check `stg_BLACKHOLE_info` for the final word: if the indirectee has no tag, then we assume that it is the TSO that has claimed the thunk; if the indirectee is tagged, then it is just a normal indirection. (EZY: I think this optimization is to avoid having to do two memory writes on thunk update; we don't bother updating the header, only the target.)</p>
<p>When eager blackholing is enabled, the black hole that is written is not a true black hole, but an eager black hole. True black holes are synchronized, and guarantee that only one black hole is claimed (this property is used to implement non-dupable unsafePerformIO). Eager black holes are not synchronized; eager black hole are converted into true black holes in ThreadPaused.c. Incidentally, this facility is also used to convert update frames to black holes; this is important for eliminating a space leak caused by the thunk under evaluation retaining too much data (overwriting it with a black hole frees up variable.)</p>
<h3 id="arrays">Arrays</h3>
<p>, , , , </p>
<p>Non-pointer arrays are straightforward:</p>
<p>||| Header ||| Bytes ||| Array payload |||</p>
<p>Arrays with pointers are a little more complicated, they include a card table, which is used by the GC to know what segments of the array to traverse as roots (the card table is modified by the GC write barrier):</p>
<p>||| Header ||| Ptrs ||| Size ||| Array payload + card table |||</p>
<p>You can access the card table by using `mutArrPtrsCard(array, element index)`, which gives you the address of the card for that index.</p>
<h3 id="mvars">MVars</h3>
<p></p>
<p>MVars have a queue of the TSOs blocking on them along with their value:</p>
<p>|| Header || Head of queue || Tail of queue || Value ||</p>
<p>An MVar can be in several states. It can be empty (in which case the value is actually just a `stg_END_TSO_QUEUE_closure`) or it can be full. When it is full, the queue of TSOs are those waiting to put; when it is empty, the queue of TSOs are those waiting to read and take (with readers first). Like many mutable objects, MVars have CLEAN and DIRTY headers to avoid reapplying a write barrier when an MVar is already dirty.</p>
<h3 id="weak-pointers">Weak pointers</h3>
<p></p>
<h3 id="stable-names">Stable Names</h3>
<p></p>
<h3 id="thread-state-objects">Thread State Objects</h3>
<p>Closure type  is a Thread State Object. It represents the complete state of a thread, including its stack.</p>
<p>TSOs are ordinary objects that live in the heap, so we can use the existing allocation and garbage collection machinery to manage them. This gives us one important benefit: the garbage collector can detect when a blocked thread is unreachable, and hence can never become runnable again. When this happens, we can notify the thread by sending it the  exception.</p>
<p>GHC keeps divides stacks into stack chunks, with logic to handle stack underflow and overflow: <a href="http://hackage.haskell.org/trac/ghc/blog/stack-chunks" class="uri">http://hackage.haskell.org/trac/ghc/blog/stack-chunks</a></p>
<p>The TSO structure contains several fields. For full details see [source:includes/rts/storage/TSO.h]. Some of the more important fields are:</p>
<p><code>* </code><em><code>link</code></em><code>: field for linking TSOs together in a list.  For example, the threads blocked on an </code><code> are kept in</code><br />
<code>  a queue threaded through the link field of each TSO.</code><br />
<code>* </code><em><code>global_link</code></em><code>: links all TSOs together; the head of this list is </code><code> in [source:rts/Schedule.c].</code><br />
<code>* </code><em><code>what_next</code></em><code>: how to resume execution of this thread.  The valid values are:</code><br />
<code>  * </code><code>: continue by returning to the top stack frame.</code><br />
<code>  * </code><code>: continue by interpreting the BCO on top of the stack.</code><br />
<code>  * </code><code>: this thread has received an exception which was not caught.</code><br />
<code>  * </code><code>: this thread ran out of stack and has been relocated to a larger TSO; the link field points</code><br />
<code>    to its new location.</code><br />
<code>  * </code><code>: this thread has finished and can be garbage collected when it is unreachable.</code><br />
<code>* </code><em><code>why_blocked</code></em><code>: for a blocked thread, indicates why the thread is blocked.  See [source:includes/rts/Constants.h] for</code><br />
<code>  the list of possible values.</code><br />
<code>* </code><em><code>block_info</code></em><code>: for a blocked thread, gives more information about the reason for blockage, eg. when blocked on an</code><br />
<code>   MVar, block_info will point to the MVar.</code><br />
<code>* </code><em><code>bound</code></em><code>: pointer to a [wiki:Commentary/Rts/Scheduler#Task Task] if this thread is bound</code><br />
<code>* </code><em><code>cap</code></em><code>: the [wiki:Commentary/Rts/Scheduler#Capabilities Capability] on which this thread resides.</code></p>
<h3 id="stm-objects">STM objects</h3>
<p>These object types are used by [wiki:Commentary/Rts/STM STM]: , , , .</p>
<h3 id="forwarding-pointers">Forwarding Pointers</h3>
<p>Forwarding pointers appear temporarily during [wiki:Commentary/Rts/Storage/GC garbage collection]. A forwarding pointer points to the new location for an object that has been moved by the garbage collector. It is represented by replacing the info pointer for the closure with a pointer to the new location, with the least significant bit set to 1 to distinguish a forwarding pointer from an info pointer.</p>
<h2 id="how-to-add-new-heap-objects">How to add new heap objects</h2>
<p>There are two This page is a stub.</p>
<h2 id="change-history">Change History</h2>
<p><code>* </code><em><code>History</code> <code>of</code> <code>when</code> <code>Hoopl</code> <code>was</code> <code>integrated</code> <code>into</code> <code>a</code> <code>GHC</code> <code>back</code> <code>end</code></em></p>
<p><code>* After the publication of the Hoopl paper, a contributor (sorry I have forgotten who) did quite a bit to integrate the supply of </code><code>s into Hoopl.  (Time? Person?)</code></p>
<p><code>* </code><em><code>Note</code> <code>that</code> <code>the</code> <code>new</code> <code>code</code> <code>generator</code> <code>appears</code> <code>about</code> <code>10x</code> <code>slower</code> <code>than</code> <code>the</code> <code>old.</code> <code>Slowdown</code> <code>attributed</code> <code>to</code> <code>Hoopl</code> <code>dataflow.</code></em><code>   See </code><a href="https://plus.google.com/107890464054636586545/posts/dBbewpRfw6R"><code>Google</code> <code>Plus</code> <code>post</code> <code>by</code> <code>Simon</code> <code>Marlow</code></a><code>.</code></p>
<p><code>* Fixed-point algorithm rewritten to reduce duplicate computation.  (Simon Marlow in late 2011.  Also Edward Yang in spring 2011.) Is there any more? I suggest looking at traces in the simple cases.</code></p>
<p><code>* Change in representation of blocks, Simon Marlow, late 2011.  (Details?)  Performance difference almost too small to be measurable, but Simon M likes the new rep anyway.</code></p>
<h2 id="speculation-and-commentary">Speculation and Commentary</h2>
<p><code>* Simon PJ had questions about &quot;optimization fuel&quot; from the beginning.  Norman maintains that optimization fuel is an invaluable debugging aid, but that in a production compiler, one would like it to be turned off.   At some point we had abstracted over the </code><code> so that we could make a &quot;zero&quot; fuel monad that did nothing and cost nothing.  As of January 2012, Norman doesn't know what the state of that plan is or whether GHC's optimiser can actually eliminate the overheads.</code></p>
<p><code>* Unlike Fuel, a supply of </code><code>s was believed to be an absolute necessity: an optimiser must be able to rewrite blocks, and in the general case, it must be able to introduce new blocks.  It was believed that the only way to do this consistent with GHC was to plumb in a Uniq supply.   </code><em><code>Query</code></em><code>: was this integrated with Fuel somehow?</code></p>
<p><code>* The published version of Hoopl passes an explicit dictionary that contains all the dataflow facts for all the labels.   Earlier versions of Hoopl kept this information in a monad.  It's not known whether the change has implications for performance, but it is probably easier to manage the speculative rewriting without a monad.</code></p>
<p><code>* Norman has always been uneasy about the dictionaries passed to the </code><code> function.  He conjectures that most blocks have a small number of outedges, and probably not that many inedges either (case expressions and the Adams optimisation notwithstanding).  He wonders if instead of some kind of trie structure with worst-case logarithmic performance, we might not be better off with a simple association list---especially because it is common to simply join </code><em><code>all</code></em><code> facts flowing into a block.   </code><strong><code>Query:</code> <code>Is</code> <code>there</code> <code>a</code> <code>way</code> <code>to</code> <code>measure</code> <code>the</code> <code>costs</code> <code>of</code> <code>using</code> <code>dictionaries</code> <code>in</code> <code>this</code> <code>fashion?</code> <code>Cost</code> <code>centers,</code> <code>perhaps?</code></strong></p>
<p><code>* There was a Google Plus thread in which CPS was criticized (by Jan Maessen, I think).  The original authors had many big fights, and one of them was about CPS.  At some point Norman drafted a dataflow analyser that was very aggressively CPS.  Simon PJ found the extensive CPS difficult to read.  Norman doesn't remember the eventual outcome.   Is it possible that the CPS is causing the allocation of too many function closures?   Could the CPS be rewritten, perhaps by a different way of nesting functions, to eliminate the need to allocate closures in the inner loop?  Johan Tibell tried optimizing postorder_dfs, but was put off by the CPS style of code. (We speculate that caching the result of toposort may help.)</code></p>
<p><code>* Another important thing to keep in mind is that some of the existing passes used by GHC may be implemented inefficiently (of no fault of Hoopl itself.) For example, the rewrite assignments pass takes around 15% of the entire compilation time; we believe this is because it has to rewrite the entire graph into a new representation before doing any transformations, and then rewrite it back to the original. Optimizations here (for example, storing the information in an external map as opposed to the AST itself) would probably would help a lot.</code></p>
<h2 id="record-of-performance-improvements-made-to-the-hoopl-library-starting-january-2012">Record of performance improvements made to the Hoopl library starting January 2012</h2>
<h1 id="haskell-program-coverage">Haskell Program Coverage</h1>
<p>This page describes the Haskell Program Coverage implementation inside GHC. Background information can be found in the paper <a href="http://www.ittc.ku.edu/~andygill/papers/Hpc07.pdf">Haskell Program Coverage</a> by Andy Gill and Colin Runciman, and the Haskell wiki page <a href="https://wiki.haskell.org/Haskell_program_coverage">Haskell program coverage</a>.</p>
<p>The basic idea is this</p>
<p><code>* For each (sub)expression in the Haskell Syntax, write the (sub)expression in a    </code><br />
<code>  `HsTick`</code><br />
<code>* Each `HsTick` has a module local index number.</code><br />
<code>* There is a table (The Mix data structure) that maps this index number to original source location.</code><br />
<code>* Each `HsTick` is mapped in the Desugar pass with: </code></p>
<p></p>
<p><code>* This tick is a special type of `Id`, a `TickOpId` which takes no core-level argument, but has two pre-applied arguments; the module name and the module-local tick number.</code><br />
<code>  * We store both module name and tick number to allow this Id to be passed (inlined) inside other modules.</code><br />
<code>  * This `Id` has type </code><strong><code>State#</code> <code>World#</code></strong><br />
<code>* The core simplifier must not remove this case, but it can move it.</code><br />
<code>  * The do-not-remove is enforced via the ... function in ....</code><br />
<code>  * The semantics are tick if-and-when-and-as you enter the `DEFAULT` case. But a chain of consecutive ticks can be executed in any order.</code><br />
<code>* The !CoreToStg Pass translates the ticks into `StgTick`</code></p>
<p></p>
<p><code>* The `Cmm` code generator translates `StgTick` to a 64 bit increment.</code></p>
<p>Other details</p>
<p><code>* A executable startup time, we perform a depth first traversal some module</code><br />
<code>  specific code, gathering a list of all Hpc registered modules, and the</code><br />
<code>  module specific tick table. </code><br />
<code>* There is one table per module, so we can link the increment statically,</code><br />
<code>  without needing to know the global tick number.</code><br />
<code>* The module Hpc.c in the RTS handles all the reading of these table.</code><br />
<code>* At startup, if a .tix file is found, Hpc.c checks that this is the same</code><br />
<code>  binary as generated the .tix file, and if so, pre-loads all the tick counts</code><br />
<code>  in the module specific locations.</code><br />
<code>* (I am looking for a good way of checking the binaries for sameness)</code><br />
<code>* At shutdown, we write back out the .tix files, from the module-local tables.</code></p>
<h3 id="binary-tick-boxes">Binary Tick Boxes</h3>
<p>There is also the concept of a binary tick box. This is a syntactical boolean, like a guard or conditional for an if. We use tick boxes to record the result of the boolean, to check for coverage over True and False.</p>
<p><code>* Each `HsBinaryTick` is mapped in the Desugar pass with: </code></p>
<p></p>
<ul>
<li>After desugaring, there is no longer any special code for binary tick box.</li>
</ul>
<h2 id="machine-generated-haskell">Machine Generated Haskell</h2>
<p>Sometimes, Haskell is the target language - for example, Happy and Alex. In this case, you want to be able to check for coverage of your <strong>original</strong> program. So we have a new pragma.</p>
<p></p>
<p>This means that the expression was obtained from the given file and locations. This might be code included verbatim (for example the actions in Happy), or be generated from a specification from this location.</p>
<h1 id="compiling-one-module-hscmain">Compiling one module: !HscMain</h1>
<p>Here we are going to look at the compilation of a single module. There is a picture that goes with this description, which appears at the bottom of this page, but you'll probably find it easier to open [wiki:Commentary/Compiler/HscPipe this link] in another window, so you can see it at the same time as reading the text.</p>
<p>You can also watch a <strong>video</strong> of Simon Peyton-Jones explaining the compilation pipeline here: <a href="http://www.youtube.com/watch?v=Upm_kYMgI_c&amp;list=PLBkRCigjPwyeCSD_DFxpd246YIF7_RDDI">Compiler Pipeline II</a> (10'16&quot;)</p>
<p>Look at the picture first. The yellow boxes are compiler passes, while the blue stuff on the left gives the data type that moves from one phase to the next. The entire pipeline for a single module is run by a module called !HscMain (<a href="GhcFile(compiler/main/HscMain.hs)" class="uri" title="wikilink">GhcFile(compiler/main/HscMain.hs)</a>). Each data type's representation can be dumped for further inspection using a `-ddump-*` flag. (Consider also using `-ddump-to-file`: some of the dump outputs can be large!) Here are the steps it goes through:</p>
<p><code>* The </code><strong><code>Front</code> <code>End</code></strong><code> processes the program in the [wiki:Commentary/Compiler/HsSynType big HsSyn type]. </code><code> is parameterised over the types of the term variables it contains.  The first three passes (the front end) of the compiler work like this:</code><a href="BR" title="wikilink"><code>BR</code></a><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  * The </code><strong><code>[wiki:Commentary/Compiler/Parser</code> <code>Parser]</code></strong><code> produces </code><code> parameterised by </code><strong><code>[wiki:Commentary/Compiler/RdrNameType</code> <code>RdrName]</code></strong><code>.  To a first approximation, a </code><code> is just a string. (`-ddump-parsed`) </code><a href="BR" title="wikilink"><code>BR</code></a><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  * The </code><strong><code>[wiki:Commentary/Compiler/Renamer</code> <code>Renamer]</code></strong><code> transforms this to </code><code> parameterised by </code><strong><code>[wiki:Commentary/Compiler/NameType</code> <code>Name]</code></strong><code>.  To a first appoximation, a </code><code> is a string plus a </code><code> (number) that uniquely identifies it.  In particular, the renamer associates each identifier with its binding instance and ensures that all occurrences which associate to the same binding instance share a single </code><code>. (`-ddump-rn`)  </code><a href="BR" title="wikilink"><code>BR</code></a><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  * The </code><strong><code>[wiki:Commentary/Compiler/TypeChecker</code> <code>Typechecker]</code></strong><code> transforms this further, to </code><code> parameterised by </code><strong><code>[wiki:Commentary/Compiler/EntityTypes</code> <code>Id]</code></strong><code>.  To a first approximation, an </code><code> is a </code><code> plus a type. In addition, the type-checker converts class declarations to </code><code>es, and type declarations to </code><code>s and </code><code>s.  And of course, the type-checker deals in </code><code>s and </code><code>s. The [wiki:Commentary/Compiler/EntityTypes data types for these entities] (</code><code>, </code><code>, </code><code>, </code><code>, </code><code>) are pervasive throughout the rest of the compiler. (`-ddump-tc`)</code></p>
<p><code>These three passes can all discover programmer errors, which are sorted and reported to the user.</code><br />
<br />
<code>* The </code><strong><code>Desugarer</code></strong><code> (</code><a href="GhcFile(compiler/deSugar/Desugar.hs)" title="wikilink"><code>GhcFile(compiler/deSugar/Desugar.hs)</code></a><code>) converts from the massive </code><code> type to [wiki:Commentary/Compiler/CoreSynType GHC's intermediate language, CoreSyn].  This Core-language data type is unusually tiny: just eight constructors.) (`-ddump-ds`)</code><a href="BR" title="wikilink"><code>BR</code></a><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  Generally speaking, the desugarer produces few user errors or warnings. But it does produce </code><em><code>some</code></em><code>.  In particular, (a) pattern-match overlap warnings are produced here; and (b) when desugaring Template Haskell code quotations, the desugarer may find that `THSyntax` is not expressive enough.  In that case, we must produce an error (</code><a href="GhcFile(compiler/deSugar/DsMeta.hs)" title="wikilink"><code>GhcFile(compiler/deSugar/DsMeta.hs)</code></a><code>).</code><a href="BR" title="wikilink"><code>BR</code></a><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  This late desugaring is somewhat unusual.  It is much more common to desugar the program before typechecking, or renaming, because that presents the renamer and typechecker with a much smaller language to deal with.  However, GHC's organisation means that</code><br />
<code>   * error messages can display precisely the syntax that the user wrote; and </code><br />
<code>   * desugaring is not required to preserve type-inference properties.</code><a href="BR" title="wikilink"><code>BR</code></a><a href="BR" title="wikilink"><code>BR</code></a></p>
<p><code>* The </code><strong><code>!SimplCore</code></strong><code> pass (</code><a href="GhcFile(compiler/simplCore/SimplCore.hs)" title="wikilink"><code>GhcFile(compiler/simplCore/SimplCore.hs)</code></a><code>) is a bunch of Core-to-Core passes that optimise the program; see </code><a href="http://research.microsoft.com/%7Esimonpj/Papers/comp-by-trans-scp.ps.gz"><code>A</code> <code>transformation-based</code> <code>optimiser</code> <code>for</code> <code>Haskell</code> <code>(SCP'98)</code></a><code> for a more-or-less accurate overview.  See [wiki:Commentary/Compiler/Core2CorePipeline] for an overview of the Core-to-Core optimisation pipeline. The main passes are:</code><a href="BR" title="wikilink"><code>BR</code></a><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>   * The </code><strong><code>Simplifier</code></strong><code>, which applies lots of small, local optimisations to the program.  The simplifier is big and complicated, because it implements a </code><em><code>lot</code></em><code> of transformations; and tries to make them cascade nicely.  The transformation-based optimiser paper gives lots of details, but two other papers are particularly relevant: </code><a href="http://research.microsoft.com/%7Esimonpj/Papers/inlining/index.htm"><code>Secrets</code> <code>of</code> <code>the</code> <code>Glasgow</code> <code>Haskell</code> <code>Compiler</code> <code>inliner</code> <code>(JFP'02)</code></a><code> and </code><a href="http://research.microsoft.com/%7Esimonpj/Papers/rules.htm"><code>Playing</code> <code>by</code> <code>the</code> <code>rules:</code> <code>rewriting</code> <code>as</code> <code>a</code> <code>practical</code> <code>optimisation</code> <code>technique</code> <code>in</code> <code>GHC</code> <code>(Haskell</code> <code>workshop</code> <code>2001)</code></a><code>.  (`-ddump-simpl`)</code><a href="BR" title="wikilink"><code>BR</code></a><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>   * The </code><strong><code>float-out</code></strong><code> and </code><strong><code>float-in</code></strong><code> transformations, which move let-bindings outwards and inwards respectively.  See </code><a href="http://research.microsoft.com/%7Esimonpj/papers/float.ps.gz"><code>Let-floating:</code> <code>moving</code> <code>bindings</code> <code>to</code> <code>give</code> <code>faster</code> <code>programs</code> <code>(ICFP</code> <code>'96)</code></a><code>.</code><a href="BR" title="wikilink"><code>BR</code></a><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>   * The </code><strong><code>strictness</code> <code>analyser</code></strong><code>.  This actually comprises two passes: the </code><strong><code>analyser</code></strong><code> itself and the </code><strong><code>worker/wrapper</code></strong><code> transformation that uses the results of the analysis to transform the program. (Further described in [wiki:Commentary/Compiler/Demand Demand analysis].) The same analyser also does </code><a href="http://research.microsoft.com/%7Esimonpj/Papers/cpr/index.htm"><code>Constructed</code> <code>Product</code> <code>Result</code> <code>analysis</code></a><code> and </code><a href="http://research.microsoft.com/en-us/um/people/simonpj/papers/usage-types/cardinality-extended.pdf"><code>Cardinality</code> <code>analysis</code></a><code>. (`-ddump-stranal`)</code><a href="BR" title="wikilink"><code>BR</code></a><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>   * The </code><strong><code>liberate-case</code></strong><code> transformation.</code><a href="BR" title="wikilink"><code>BR</code></a><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>   * The </code><strong><code>constructor-specialialisation</code></strong><code> transformation.</code><a href="BR" title="wikilink"><code>BR</code></a><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>   * The </code><strong><code>common</code> <code>sub-expression</code> <code>eliminiation</code></strong><code> (CSE) transformation. (`-ddump-cse`)</code></p>
<p><code>* Then the </code><strong><code>!CoreTidy</code> <code>pass</code></strong><code> gets the code into a form in which it can be imported into subsequent modules (when using </code><code>) and/or put into an interface file.  </code><br />
<br />
<code>It makes a difference whether or not you are using `-O` at this stage.  With `-O` (or rather, with `-fomit-interface-pragmas` which is a consequence of `-O`), the tidied program (produced by `tidyProgram`) has unfoldings for Ids, and RULES.  Without `-O` the unfoldings and RULES are omitted from the tidied program.  And that, in turn, affects the interface file generated subsequently.</code></p>
<p><code>There are good notes at the top of the file </code><a href="GhcFile(compiler/main/TidyPgm.hs)" title="wikilink"><code>GhcFile(compiler/main/TidyPgm.hs)</code></a><code>; the main function is </code><code>, documented as &quot;Plan B&quot; (&quot;Plan A&quot; is a simplified tidy pass that is run when we have only typechecked, but haven't run the desugarer or simplifier).</code></p>
<p><code> * At this point, the data flow forks.  First, the tidied program is dumped into an interface file.  This part happens in two stages:</code><br />
<code>   * It is </code><strong><code>converted</code> <code>to</code> </strong><code> (defined in </code><a href="GhcFile(compiler/iface/IfaceSyn.hs)" title="wikilink"><code>GhcFile(compiler/iface/IfaceSyn.hs)</code></a><code> and </code><a href="GhcFile(compiler/iface/IfaceType.hs)" title="wikilink"><code>GhcFile(compiler/iface/IfaceType.hs)</code></a><code>).</code><br />
<code>   * The </code><code> is </code><strong><code>serialised</code> <code>into</code> <code>a</code> <code>binary</code> <code>output</code> <code>file</code></strong><code> (</code><a href="GhcFile(compiler/iface/BinIface.hs)" title="wikilink"><code>GhcFile(compiler/iface/BinIface.hs)</code></a><code>).</code><br />
<code> The serialisation does (pretty much) nothing except serialise.  All the intelligence is in the `Core`-to-`IfaceSyn` conversion; or, rather, in the reverse of that step.</code></p>
<p><code> * The same, tidied Core program is now fed to the Back End.  First there is a two-stage conversion from </code><code> to [wiki:Commentary/Compiler/StgSynType GHC's intermediate language, StgSyn].</code><br />
<code>   * The first step is called </code><strong><code>!CorePrep</code></strong><code>, a Core-to-Core pass that puts the program into A-normal form (ANF).  In ANF, the argument of every application is a variable or literal; more complicated arguments are let-bound.  Actually `CorePrep` does quite a bit more: there is a detailed list at the top of the file </code><a href="GhcFile(compiler/coreSyn/CorePrep.hs)" title="wikilink"><code>GhcFile(compiler/coreSyn/CorePrep.hs)</code></a><code>.</code><br />
<code>   * The second step, </code><strong><code>!CoreToStg</code></strong><code>, moves to the </code><code> data type (</code><a href="GhcFile(compiler/stgSyn/CoreToStg.hs)" title="wikilink"><code>GhcFile(compiler/stgSyn/CoreToStg.hs)</code></a><code>).  The output of !CorePrep is carefully arranged to exactly match what </code><code> allows (notably ANF), so there is very little work to do. However, </code><code> is decorated with lots of redundant information (free variables, let-no-escape indicators), which is generated on-the-fly by </code><code>.</code></p>
<p><code> * Next, the </code><strong><code>[wiki:Commentary/Compiler/CodeGen</code> <code>Code</code> <code>Generator]</code></strong><code> converts the STG program to a </code><code> program.  The code generator is a Big Mother, and lives in directory </code><a href="GhcFile(compiler/codeGen)" title="wikilink"><code>GhcFile(compiler/codeGen)</code></a><code>  </code></p>
<p><code> * Now the path forks again:</code><br />
<code>   * If we are generating GHC's stylised C code, we can just pretty-print the </code><code> code as stylised C (</code><a href="GhcFile(compiler/cmm/PprC.hs)" title="wikilink"><code>GhcFile(compiler/cmm/PprC.hs)</code></a><code>)</code><br />
<code>   * If we are generating native code, we invoke the native code generator.  This is another Big Mother (</code><a href="GhcFile(compiler/nativeGen)" title="wikilink"><code>GhcFile(compiler/nativeGen)</code></a><code>).</code><br />
<code>   * If we are generating LLVM code, we invoke the LLVM code generator. This is a reasonably simple code generator (</code><a href="GhcFile(compiler/llvmGen)" title="wikilink"><code>GhcFile(compiler/llvmGen)</code></a><code>).</code></p>
<h1 id="the-diagram">The Diagram</h1>
<p>This diagram is also located [wiki:Commentary/Compiler/HscPipe here], so that you can open it in a separate window.</p>
<p><a href="Image(Commentary/Compiler/HscPipe:HscPipe2.png)" class="uri" title="wikilink">Image(Commentary/Compiler/HscPipe:HscPipe2.png)</a></p>
<h1 id="picture-of-the-main-compiler-pipeline">Picture of the main compiler pipeline</h1>
<p>See [wiki:Commentary/Compiler compiling one module] for the commentary on this diagram.</p>
<p><a href="Image(HscPipe2.png)" class="uri" title="wikilink">Image(HscPipe2.png)</a></p>
<p><a href="PageOutline" class="uri" title="wikilink">PageOutline</a></p>
<p>Video: <a href="http://www.youtube.com/watch?v=lw7kbUvAmK4&amp;list=PLBkRCigjPwyeCSD_DFxpd246YIF7_RDDI">Abstract Syntax Types</a> (1hr03')</p>
<h1 id="the-types">The  types</h1>
<p>The program is initially parsed into &quot;<strong></strong>&quot;, a collection of data types that describe the full abstract syntax of Haskell.  is a pretty big collection of types: there are 52 data types at last count. Many are pretty trivial, but a few have a lot of constructors ( has 40).  represents Haskell in its full glory, complete with all syntactic sugar.</p>
<p>The  modules live in the <a href="GhcFile(compiler/hsSyn)" class="uri" title="wikilink">GhcFile(compiler/hsSyn)</a> directory. Each module declares a related group of declarations, <em>and</em> gives their pretty-printer.</p>
<p><code>* </code><a href="GhcFile(compiler/hsSyn/HsSyn.hs)" title="wikilink"><code>GhcFile(compiler/hsSyn/HsSyn.hs)</code></a><code>: the root module.  It exports everything you need, and it's generally what you should import.</code><br />
<code>* </code><a href="GhcFile(compiler/hsSyn/HsBinds.hs)" title="wikilink"><code>GhcFile(compiler/hsSyn/HsBinds.hs)</code></a><code>: bindings.</code><br />
<code>* </code><a href="GhcFile(compiler/hsSyn/HsImpExp.hs)" title="wikilink"><code>GhcFile(compiler/hsSyn/HsImpExp.hs)</code></a><code>: imports and exports.</code><br />
<code>* </code><a href="GhcFile(compiler/hsSyn/HsDecls.hs)" title="wikilink"><code>GhcFile(compiler/hsSyn/HsDecls.hs)</code></a><code>: top-level declarations.</code><br />
<code>* </code><a href="GhcFile(compiler/hsSyn/HsExpr.hs)" title="wikilink"><code>GhcFile(compiler/hsSyn/HsExpr.hs)</code></a><code>: expressions, match expressions, comprehensions.</code><br />
<code>* </code><a href="GhcFile(compiler/hsSyn/HsLit.hs)" title="wikilink"><code>GhcFile(compiler/hsSyn/HsLit.hs)</code></a><code>: literals.</code><br />
<code>* </code><a href="GhcFile(compiler/hsSyn/HsPat.hs)" title="wikilink"><code>GhcFile(compiler/hsSyn/HsPat.hs)</code></a><code>: patterns.</code><br />
<code>* </code><a href="GhcFile(compiler/hsSyn/HsTypes.hs)" title="wikilink"><code>GhcFile(compiler/hsSyn/HsTypes.hs)</code></a><code>: types.</code><br />
<code>* </code><a href="GhcFile(compiler/hsSyn/HsUtils.hs)" title="wikilink"><code>GhcFile(compiler/hsSyn/HsUtils.hs)</code></a><code>: utility functions (no data types).</code></p>
<p>There is significant mutual recursion between modules, and hence a couple of  files. Look at [wiki:ModuleDependencies] to see the dependencies.</p>
<h2 id="decorating-hssyn-with-type-information">Decorating `HsSyn` with type information</h2>
<p>The type checker adds type information to the syntax tree, otherwise leaving it as undisturbed as possible. This is done in two ways:</p>
<p><code> * Some constructors have a field of type </code><code>, which is just a synonym for </code><code>. For example:</code></p>
<p></p>
<p><code> An </code><code> represents the explicit list construct in Haskell (e.g. &quot;</code><code>&quot;). The parser fills the </code><code> field with an error thunk </code><code>; and the renamer does not touch it.  The typechecker figures out the type, and fills in the value.  So until the type checker, we cannot examine or print the </code><code> fields.</code></p>
<p><code> The error thunks mean that we can't conveniently pretty-print the `PostTcType` fields, because the pretty-printer would poke the error thunks when run on pre-typchecked code.  We could have defined `PostTcType` to be `Maybe Type`, but that would have meant unwrapping lots of `Just` constructors, which is messy.  It would be nicer to parameterise `HsSyn` over the `PostTcType` fields.  Thus:</code></p>
<p></p>
<p><code> This would be a Good Thing to do.</code></p>
<p><code> * In a few cases, the typechecker moves from one constructor to another.  Example:</code></p>
<p></p>
<p><code> The parser and renamer use </code><code>; the typechecker generates a </code><code>. This naming convention is used consistently.</code></p>
<p><code> * There are a few constructors added by type checker (rather than replacing an input constructor), particularly:</code><br />
<code>   * </code><code>, in the </code><code> type.</code><br />
<code>   * </code><code>, in the </code><code> type.</code><br />
<code> These are invariably to do with type abstraction and application, since Haskell source is implicitly generalized and instantiated, whereas GHC's intermediate form is explicitly generalized and instantiated.</code></p>
<h2 id="source-locations">Source Locations</h2>
<p>`HsSyn` makes heavy use of the `Located` type (<a href="GhcFile(compiler/basicTypes/SrcLoc.hs)" class="uri" title="wikilink">GhcFile(compiler/basicTypes/SrcLoc.hs)</a>):  A `Located t` is just a pair of a `SrcSpan` (which describes the source location of `t`) and a syntax tree `t`. The module `SrcLoc` defines two other types:</p>
<p><code> * `SrcLoc` specifies a particular source location: (filename, line number, character position)</code><br />
<code> * `SrcSpan` specifes a range of source locations: (filename, start line number and character position, end line number and character position)</code></p>
<p>More details in <a href="GhcFile(compiler/basicTypes/SrcLoc.hs)" class="uri" title="wikilink">GhcFile(compiler/basicTypes/SrcLoc.hs)</a>.</p>
<p>Naming convention within the code: &quot;`LHs`&quot; means located Haskell, e.g. </p>
<h1 id="interface-files">Interface files</h1>
<p>An <strong>interface file</strong> supports separate compilation by recording the information gained by compiling  in its interface file . Morally speaking, the interface file  is part of the object file ; it's like a super symbol-table for .</p>
<p>Interface files are kept in binary, GHC-specific format. The format of these files changes with each GHC release, but not with patch-level releases. The contents of the interface file is, however, completely independent of the back end you are using (`-fviaC`, `-fasm`, `-fcmm` etc).</p>
<p>Although interface files are kept in binary format, you can print them in human-readable form using the command:  This textual format is not particularly designed for machine parsing. Doing so might be possible, but if you want to read GHC interface files you are almost certainly better off using the [wiki:Commentary/Compiler/API GHC API] to do so. If you are wondering how some particular language feature is represented in the interface file, this command is really useful! Cross-reference its output with the `Outputable` instance defined in <a href="GhcFile(compiler/iface/LoadIface.hs)" class="uri" title="wikilink">GhcFile(compiler/iface/LoadIface.hs)</a></p>
<p>Here are some of the things stored in an interface file </p>
<p><code>* The version of GHC used to compile the module, as well as the compilation way and other knick-knacks</code><br />
<code>* A list of what </code><code> exports.</code><br />
<code>* The types of exported functions, definition of exported types, and so on.</code><br />
<code>* Version information, used to drive the [wiki:Commentary/Compiler/RecompilationAvoidance recompilation checker].</code><br />
<code>* The strictness, arity, and unfolding of exported functions.  This is crucial for cross-module optimisation; but it is only included when you compile with </code><code>.</code></p>
<p>The contents of an interface file is the result of serialising the <strong></strong> family of data types. The data types are in <a href="GhcFile(compiler/iface/IfaceSyn.lhs)" class="uri" title="wikilink">GhcFile(compiler/iface/IfaceSyn.lhs)</a> and <a href="GhcFile(compiler/iface/IfaceType.lhs)" class="uri" title="wikilink">GhcFile(compiler/iface/IfaceType.lhs)</a>; the binary serialisation code is in <a href="GhcFile(compiler/iface/BinIface.hs)" class="uri" title="wikilink">GhcFile(compiler/iface/BinIface.hs)</a>. The definition of a module interface is the <strong></strong> data type in <a href="GhcFile(compiler/main/HscTypes.lhs)" class="uri" title="wikilink">GhcFile(compiler/main/HscTypes.lhs)</a>.</p>
<p>Details of some of the types involved in GHC's representation of Modules and Interface files can be found [wiki:Commentary/Compiler/ModuleTypes here].</p>
<h2 id="when-is-an-interface-file-loaded">When is an interface file loaded?</h2>
<p>The act of loading an interface file can cause various parts of the compiler to behave differently; for instance, a type class instance will only be used if the interface file which defines it was loaded. Additionally, GHC tries to avoid loading interface files if it can avoid it, since every loaded interface file requires going to the file system and parsing the result.</p>
<p>The big situations when we load an interface file:</p>
<ul>
<li>When you import it (either explicitly using an `import`, or implicitly, e.g. through `-fimplicit-import-qualified` in GHCi; `loadSrcInterface`)</li>
<li>When we need to get the type for an identifier (`loadInterface` in `importDecl`)</li>
<li>When it is listed as an orphan of an imported module (`loadModuleInterfaces &quot;Loading orphan modules&quot;`)</li>
</ul>
<p>We also load interface files in some more obscure situations:</p>
<ul>
<li>When it is used as the backing implementation of a signature (`loadSysInterface` in `tcRnSignature`)</li>
<li>When we look up its family instances (`loadSysInterface` in `getFamInsts`)</li>
<li>When its information or safety (`getModuleInterface` in `hscGetSafe`)</li>
<li>When we an identifier is explicitly used (including a use from Template Haskell), we load the interface to check if the identifier is deprecated (`loadInterfaceForName` in `warnIfDeprecated`/`loadInterfaceforName` in `rn_bracket`)</li>
<li>Recompilation checking (`needInterface` in `checkModUsage`)</li>
<li>When we need the fixity for an identifier (`loadInterfaceForName` in `lookupFixityRn`)</li>
<li>When we reify a module for Template Haskell (`loadInterfaceForModule` in `reifyModule`)</li>
<li>When we use a wired-in type constructor, since otherwise the interface file would not be loaded because the compiler already has the type for the identifier. (`Loading instances for wired-in things`)</li>
<li>When `-XParallelArrays` or `-fvectorise` are specified for DPH (`loadModule` in `initDs`)</li>
<li>When we load a plugin (`DynamicLoading`)</li>
<li>To check consistency against the `hi-boot` of a module</li>
<li>To check the old interface file for recompilation avoidance</li>
</ul>
<h1 id="immix-garbage-collector">Immix Garbage Collector</h1>
<p>In a <a href="http://socghop.appspot.com/gsoc/student_project/show/google/gsoc2010/haskell/t127230760695">Google Summer of Code project</a>, <a href="http://wiki.debian.org/MarcoSilva">marcot</a> started an implementation of the Immix Garbage Collector in GHC. It's not in a state where it can be included in GHC yet, but it's functional, don't have known bugs and gets better results than the default GC in the <a href="http://www.dcs.gla.ac.uk/fp/software/ghc/nofib.html">nofib</a> suite. On the other hand, it gets worse results than the default GC for the nofib/gc suite. The implementation was reported on these blog posts: <a href="http://marcotmarcot.wordpress.com/2010/05/17/google-summer-of-code-weekly-report-1/">1</a> <a href="http://marcotmarcot.wordpress.com/2010/05/31/summer-of-code-weekly-report-3/">3</a> <a href="http://marcotmarcot.wordpress.com/2010/06/04/summer-of-code-weekly-report-4/">4</a> <a href="http://marcotmarcot.wordpress.com/2010/06/15/summer-of-code-weekly-report-5/">5</a> <a href="http://marcotmarcot.wordpress.com/2010/06/18/immix-on-ghc-summer-of-code-weekly-report-6/">6</a> <a href="http://marcotmarcot.wordpress.com/2010/06/29/immix-on-ghc-summer-of-code-weekly-report-7/">7</a> <a href="http://marcotmarcot.wordpress.com/2010/07/05/immix-on-ghc-summer-of-code-weekly-report-8/">8</a> <a href="http://marcotmarcot.wordpress.com/2010/07/07/immix-on-ghc-summer-of-code-weekly-report-9/">9</a> <a href="http://marcotmarcot.wordpress.com/2010/07/21/immix-on-ghc-summer-of-code-weekly-report-10/">10</a> <a href="http://marcotmarcot.wordpress.com/2010/08/10/immix-on-ghc-summer-of-code-report-11/">11</a> <a href="http://marcotmarcot.wordpress.com/2010/08/13/immix-on-ghc-summer-of-code-report-12-debconf-debian-day-bh/">12</a></p>
<h1 id="the-patches">The patches</h1>
<p>There are <a href="http://people.debian.org/~marcot/immix/">some patches available</a>.</p>
<h2 id="the-main-patch">The main patch</h2>
<p><code>* </code><a href="http://people.debian.org/~marcot/immix/immix.patch"><code>Generated</code> <code>with</code> <code>darcs</code> <code>diff</code> <code>-u</code></a><br />
<code>* </code><a href="http://people.debian.org/~marcot/immix/immix.dpatch"><code>Darcs</code> <code>bundle</code></a></p>
<p>This patch includes the basic implementation of Immix. It's tested, and has no known bugs. In <a href="http://people.debian.org/~marcot/immix/log.tar.gz">the measurements</a>, it has shown these results:</p>
<p>|| || <strong>Runtime</strong> || <strong>Memory used</strong> || || <strong>nofib</strong> || -2.9% || -1.7% || || <strong>nofib/gc</strong> || +4.3% || +1.2% ||</p>
<p>Currently, it overwrites the [wiki:Commentary/Rts/Storage/GC/Sweeping mark/sweep algorithm]. It uses the same mark bits as [wiki:Commentary/Rts/Storage/GC/Marking mark/compact and mark/sweep], but consider these bits in groups of 32 or 64, depending on the architecture used, which are called lines. It creates a list of free lines for each <a href="http://hackage.haskell.org/trac/ghc/wiki/Commentary/Rts/Storage/GC/Aging">generation</a>, and allocates on them when possible.</p>
<p>As only the first part of each object in memory is marked in the [wiki:Commentary/Rts/Storage/GC/Marking bitmap], it skips the first free line for each group of subsequent lines, because it's possible that an object that starts in the previous line is using part of it. Also, it doesn't deal with [wiki:Commentary/Rts/Storage/BlockAlloc blocks] that objects bigger than the size of a line, called medium sized objects, marked with `BF_MEDIUM`.</p>
<p>The mark stack is used to ensure that the objects allocated on lines get scavenged.</p>
<h2 id="line-before-inscreasing-block-size">Line before inscreasing block size</h2>
<p><code>* </code><a href="http://people.debian.org/~marcot/immix/order.patch"><code>Generated</code> <code>with</code> <code>darcs</code> <code>diff</code> <code>-u</code></a><br />
<code>* </code><a href="http://people.debian.org/~marcot/immix/order.dpatch"><code>Darcs</code> <code>bundle</code></a></p>
<p>Before the implementation of Immix, the code in todo_block_full did the following:</p>
<p><code>1. Try to increase the block size.</code><br />
<code>2. If it could not be increased, get a new block.</code></p>
<p>With Immix, it turned to:</p>
<p><code>1. If we were allocating in a block, try to increase the block size.</code><br />
<code>2. If it could not be increased, search for a line.</code><br />
<code>3. If there're no free lines, get a new block.</code></p>
<p>Another possibility for it is:</p>
<p><code>1. Search for a line.</code><br />
<code>2. If there are no free lines </code><strong><code>and</code></strong><code> we were allocating in a block, try to increase the block.</code><br />
<code>3. If it could not be increased, get a new block.</code></p>
<p>Basically, this swaps 1 and 2, making it prefer allocating on lines than increasing the block size. In the measurements done so far, it has not shown significative improvements over the way the code is now, so I'll keep it here to benchmark again when another thing changes, like:</p>
<h2 id="allocate-in-lines-in-minor-gcs">Allocate in lines in minor GCs</h2>
<p><code>* </code><a href="http://people.debian.org/~marcot/immix/minor.patch"><code>Generated</code> <code>with</code> <code>darcs</code> <code>diff</code> <code>-u</code></a><br />
<code>* </code><a href="http://people.debian.org/~marcot/immix/minor.dpatch"><code>Darcs</code> <code>bundle</code></a></p>
<p>This small patch makes it possible to allocate on lines during minor GCs, removing the check about being in a major GC for the search for lines and for the creating of the mark stack. Maybe it shouldn't be so small, because it's not working. The code is being debugged, and possibly there will be a fix soon.</p>
<h2 id="remove-partial-list">Remove partial list</h2>
<p>With the allocation on lines, it's possible not to allocate on partially full blocks. By making all blocks full (with possibly free lines), there'll be no need to use the list of partial blocks. This is not done yet.</p>
<h1 id="to-do">To do</h1>
<p><code>* Make it faster and use less memory than the default GC for all benchmarks</code><br />
<code>* Correct &quot;Allocate in lines in minor GCs&quot;</code><br />
<code>* Implement and bechmark &quot;Remove partial lists&quot;</code></p>
<p><a href="PageOutline" class="uri" title="wikilink">PageOutline</a></p>
<h1 id="ghc-source-tree-roadmap-includes">GHC Source Tree Roadmap: includes/</h1>
<p>This directory contains C header files that are included in a GHC distribution. The headers fall into several categories.</p>
<h2 id="external-apis">External APIs</h2>
<p>These are header files that define an external API to the RTS that can be used by client code. These interfaces are intended to be relatively stable:</p>
<p><code>[source:includes/HsFFI.h HsFFI.h]::</code><br />
<code> The external FFI api, as required by the FFI spec</code></p>
<p><code>[source:includes/RtsAPI.h RtsAPI.h]::</code><br />
<code> The API for calling into the RTS.  Used by the implementation</code><br />
<code> of `foreign export` calls, but may also be used by external</code><br />
<code> clients.</code></p>
<p><code>[source:includes/Rts.h Rts.h]::</code><br />
<code> This header file defines everything that is visible</code><br />
<code> externally to the RTS.  It includes `Stg.h` and everything</code><br />
<code> in the `rts` subdirectory.</code></p>
<h2 id="derived-constants">Derived Constants</h2>
<p>The canonical definition of certain structures are in C header files. For example, the layout of closures and info tables are defined in the headers [source:includes/rts/storage/Closures.h Closures.h] and [source:includes/rts/storage/InfoTables.h InfoTables.h] respectivesly. How do we get the information about the layout of these structures to the parts of the system that are not written in C, such as the compiler itself, or the C-- code in the RTS?</p>
<p>Our solution is the Haskell program in [source:utils/deriveConstants/DeriveConstants.hs]. It determines the sizes and fields offsets from the C header files by invoking the C compiler for the target platform, and then looking at the resulting object file (we can't <em>run</em> the code generated by the target C compiler, because this is the host platform).</p>
<p>The !DeriveConstants program generates a few header files, notably `includes/dist-derivedconstants/header/DerivedConstants.h`, which contains C `#define`s for each of the derived constants; this file is used by C-- code in the RTS. It also generates a few files of Haskell code which are included into GHC itself, in the `DynFlags` module.</p>
<h2 id="used-when-compiling-via-c">Used when compiling via C</h2>
<p>These header files are `#included` into the `.hc` file generated by GHC when it compiles Haskell code to C. They are also `#included` by `Rts.h`, so the definitions from these files are shared by the RTS code.</p>
<p>These days the amount of stuff included this way is kept to a minimum. In particular, there are no function prototypes: all calls to C functions from `.hc` files are given types at the call site.</p>
<p><code>[source:includes/Stg.h Stg.h]::</code><br />
<code> The top of the hierarchy is `Stg.h`, which includes everything</code><br />
<code> required by `.hc` code.  Most files `#included` by `Stg.h` are in the</code><br />
<code> `stg` subdirectory.</code></p>
<p><code>[source:includes/ghcconfig.h ghcconfig.h]::</code><br />
<code> Configuration info derived by the `configure` script.</code><br />
<code>[source:includes/MachDeps.h MachDeps.h]::</code><br />
<code> Sizes of various basic types (should be in the `stg` subdirectory,</code><br />
<code> but left here for backwards-compatibility reasons).</code><br />
<code>[source:includes/stg/DLL.h stg/DLL.h]::</code><br />
<code> Stuff related to Windows DLLs.</code><br />
<code>[source:includes/stg/MachRegs.h stg/MachRegs.h]::</code><br />
<code> Global register assignments for this processor.</code><br />
<code>[source:includes/stg/MiscClosures.h stg/MiscClosures.h]::</code><br />
<code> Declarations for closures &amp; info tables built-in to the RTS</code><br />
<code>[source:includes/stg/Regs.h stg/Regs.h]::</code><br />
<code> &quot;registers&quot; in the virtual machine.</code><br />
<code>[source:includes/stg/SMP.h stg/SMP.h]::</code><br />
<code> Atomic memory operations for SMP support</code><br />
<code>[source:includes/stg/Ticky.h stg/Ticky.h]::</code><br />
<code> Declarations for ticky-ticky counters</code><br />
<code>[source:includes/stg/Types.h stg/Types.h]::</code><br />
<code> Basic types specific to the virtual machine (eg. `StgWord`).</code></p>
<h2 id="the-rts-external-apis">The RTS external APIs</h2>
<p>The header [source:includes/Rts.h Rts.h] includes all the headers below the `rts` subdirectory, which together define the RTS external API. Virtually all RTS code `#includes` `Rts.h`.</p>
<p>The rts header files are divided into a few directories:</p>
<p><code>* [source:includes/rts includes/rts]: Most of</code><br />
<code>  the external RTS APIs, in separate header files per-subsystem</code></p>
<p><code>* [source:includes/rts/storage includes/rts/storage]: Definitions of the layout of heap and stack</code><br />
<code>  objects, info tables, structures that define memory areas managed</code><br />
<code>  by the GC, and memory management APIs.</code></p>
<p><code>* [source:includes/rts/prof includes/rts/prof]:</code><br />
<code>  Interfaces and definitions for profiling.</code></p>
<h2 id="included-into-c---.cmm-code">Included into C-- (`.cmm`) code</h2>
<p><code>[source:includes/Cmm.h Cmm.h]::</code><br />
<code> included into `.cmm` source only; provides useful macros for writing</code><br />
<code> low-level C-- code for GHC.</code></p>
<p><a href="PageOutline" class="uri" title="wikilink">PageOutline</a></p>
<h1 id="installing-using-the-llvm-back-end">Installing &amp; Using the LLVM Back-end</h1>
<h2 id="installing">Installing</h2>
<p>The LLVM backend is now included in GHC HEAD. Just grab the git HEAD version of GHC and build it. The backend now also supports all modes that GHC can be built in so you shouldn't need to change your build.mk file either.</p>
<p>For instructions on building GHC go <a href="http://hackage.haskell.org/trac/ghc/wiki/Building">here</a></p>
<h2 id="llvm-support">LLVM Support</h2>
<p>The LLVM backend only supports LLVM version <strong>2.7</strong> or later. Problems with LLVM &gt;= 2.9 and GHC 7.0.3 currently exist (see bug #5103). GHC 7.2 and later works fine with LLVM &gt;= 2.9.</p>
<p>Simply install GHC and make sure the various llvm tools (opt, llc) are available on your path.</p>
<h2 id="using">Using</h2>
<p>Once built you can check that you have the LLVM backend GHC will support these extra options:</p>
<p><code> * </code><em><code>-fllvm</code></em><code> - Compile code using the llvm backend</code><br />
<code> * </code><em><code>-pgmlo</code></em><code> - The program to use as the llvm optimiser</code><br />
<code> * </code><em><code>-pgmlc</code></em><code> - The program to use as the llvm compiler</code><br />
<code> * </code><em><code>-optlo</code></em><code> - Extra options to pass to the llvm optimiser</code><br />
<code> * </code><em><code>-optlc</code></em><code> - Extra options to pass to the llvm compiler</code><br />
<code> * </code><em><code>-ddump-llvm</code></em><code> - Dumps the llvm IR while compiling</code><br />
<code> * </code><em><code>-keep-llvm-files</code></em><code> - Keep a copy of the llvm intermediate file around</code></p>
<h2 id="supported-platforms-correctness">Supported Platforms &amp; Correctness</h2>
<p><code>* Linux x86-32/x86-64: Currently well supported. The back-end can pass the test suite and build a working version of GHC (bootstrap test).</code><br />
<code>* Windows x86-32: Currently well supported. The back-end can pass the test suite and build a working version of GHC (bootstrap test).</code><br />
<code>* Mac OS X 10.5/10.6 (x86-32/x86-64): Currently well supported. The back-end can pass the test suite and bootstrap GHC. OS X has caused a lot more problems then Linux or Windows and does a few things slightly differently then them. It is quite stable these days though.</code><br />
<code>* ARM: Work is currently progressing to fully support GHC using the LLVM backend on ARM. You can see a blog with info about this </code><a href="http://ghcarm.wordpress.com/"><code>here</code></a><code>.</code><br />
<code>* Other platforms haven't been tested at all.</code></p>
<h2 id="shared-libraries">Shared Libraries</h2>
<p>Shared libraries are supported on Linux x64 and Mac OSX x64. Other platforms aren't supported.</p>
<h2 id="performance">Performance</h2>
<p>(All done on linux/x86-32)</p>
<p>A quick summary of the results are that for the 'nofib' benchmark suite, the LLVM code generator was 3.8% slower than the NCG (the C code generator was 6.9% slower than the NCG). The DPH project includes a benchmark suite which I (David Terei) also ran and for this type of code using the LLVM back-end shortened the runtime by an average of 25% compared to the NCG. Also, while not included in my thesis paper as I ran out of time, I did do some benchmarking with the 'nobench' benchmark suite. It gave performance ratios for the back-ends of around:</p>
<p>||NCG || 1.11|| ||C || 1.05|| ||LLVM || 1.14||</p>
<p>A nice demonstration of the improvements the LLVM back-end can bring to some code though can be see at <a href="http://donsbot.wordpress.com/2010/02/21/smoking-fast-haskell-code-using-ghcs-new-llvm-codegen/" class="uri">http://donsbot.wordpress.com/2010/02/21/smoking-fast-haskell-code-using-ghcs-new-llvm-codegen/</a></p>
<p><a href="PageOutline" class="uri" title="wikilink">PageOutline</a></p>
<h1 id="ghc-commentary-librariesinteger">GHC Commentary: Libraries/Integer</h1>
<p>GHC is set up to allow different implementations of the `Integer` type to be chosen at build time.</p>
<h2 id="selecting-an-integer-implementation">Selecting an Integer implementation</h2>
<p>You can select which implementation of Integer is used by defining `INTEGER_LIBRARY` in `mk/build.mk`. This tells the build system to build the library in `libraries/$(INTEGER_LIBRARY)`, and the `cIntegerLibrary` and `cIntegerLibraryType` values in `Config.hs` are defined accordingly.</p>
<p>The default value is `integer-gmp`, which uses the <a href="http://gmplib.org/">GNU Multiple Precision Arithmetic Library (GMP)</a> to define the Integer type and its operations.</p>
<p>The other implementation currently available is `integer-simple`, which uses a simple (but slow, for larger Integers) pure Haskell implementation.</p>
<h2 id="the-integer-interface">The Integer interface</h2>
<p>All Integer implementations should export the same set of types and functions from `GHC.Integer` (within whatever `integer` package you are using). These exports are used by the `base` package However, all of these types and functions must actually be defined in `GHC.Integer.Type`, so that GHC knows where to find them. Specifically, the interface is this:</p>
<p></p>
<h2 id="how-integer-is-handled-inside-ghc">How Integer is handled inside GHC</h2>
<p><code>* </code><strong><code>Front</code> <code>end</code></strong><code>.  Integers are represented using the `HsInteger` constructor of `HsLit` for the early phases of compilation (e.g. type checking)</code></p>
<p><code>* </code><strong><code>Core</code></strong><code>.  In `Core` representation, an integer literal is represented by the `LitInteger` constructor of the `Literal` type. </code></p>
<p></p>
<p><code>While `Integer`s aren't &quot;machine literals&quot; like the other `Core` `Literal` constructors, it is more convenient when writing constant folding RULES to pretend that they are literals rather than having to understand their concrete representation. (Especially as the concrete representation varies from package to package.) We also carry around a `Type`, representing the `Integer` type, in the constructor, as we need access to it in a few functions (e.g. `literalType`).</code></p>
<p><code>* </code><strong><code>Constant</code> <code>folding</code></strong><code>.  There are many constant-folding optimisations for `Integer` expressed as built-in rules in </code><a href="GhcFile(compiler/prelude/PrelRules.lhs)" title="wikilink"><code>GhcFile(compiler/prelude/PrelRules.lhs)</code></a><code>; look at `builtinIntegerRules`.  All of the types and functions in the `Integer` interface have built-in names, e.g. `plusIntegerName`, defined in </code><a href="GhcFile(compiler/prelude/PrelNames.lhs)" title="wikilink"><code>GhcFile(compiler/prelude/PrelNames.lhs)</code></a><code> and included in `basicKnownKeyNames`. This allows us to match on all of the functions in `builtinIntegerRules` in </code><a href="GhcFile(compiler/prelude/PrelRules.lhs)" title="wikilink"><code>GhcFile(compiler/prelude/PrelRules.lhs)</code></a><code>, so we can constant-fold Integer expressions. An important thing about constant folding of Integer divisions is that they depend on inlining. Here's a fragment of `Integral Integer` instance definition from `libraries/base/GHC/Real.lhs`:</code></p>
<p></p>
<p><code>Constant folding rules for divisions are defined for `quotInteger` and other division functions from `integer-gmp` library. If `quot` was not inlined constant folding rules would not fire. The rules would also not fire if call to `quotInteger` was inlined, but this does not happen because it is marked with NOINLINE pragma - see below.</code></p>
<p><code>* </code><strong><code>Converting</code> <code>between</code> <code>Int</code> <code>and</code> <code>Integer</code></strong><code>.  It's quite commonly the case that, after some inlining, we get something like `integerToInt (intToInteger i)`, which converts an `Int` to an `Integer` and back.  This </code><em><code>must</code></em><code> optimise away (see #5767).  We do this by requiring that the `integer` package exposes</code></p>
<p></p>
<p><code>Now we can define `intToInteger` (or, more precisely, the `toInteger` method of the `Integral Int` instance in `GHC.Real` ) thus</code></p>
<p></p>
<p><code>And we have a RULE for `integerToInt (smallInteger i)`.</code></p>
<p><code>* </code><strong><code>Representing</code> <code>integers</code></strong><code>.  We stick to the `LitInteger` representation (which hides the concrete representation) as late as possible in the compiler.   In particular, it's important that the `LitInteger` representation is used in unfoldings in interface files, so that constant folding can happen on expressions that get inlined.  </code></p>
<p><code>We finally convert `LitInteger` to a proper core representation of Integer in </code><a href="GhcFile(compiler/coreSyn/CorePrep.lhs)" title="wikilink"><code>GhcFile(compiler/coreSyn/CorePrep.lhs)</code></a><code>, which looks up the Id for `mkInteger` and uses it to build an expression like `mkInteger True [123, 456]` (where the `Bool` represents the sign, and the list of `Int`s are 31 bit chunks of the absolute value from lowest to highest).</code></p>
<p><code>However, there is a special case for `Integer`s that are within the range of `Int` when the `integer-gmp` implementation is being used; in that case, we use the `S#` constructor (via `integerGmpSDataCon` in </code><a href="GhcFile(compiler/prelude/TysWiredIn.lhs)" title="wikilink"><code>GhcFile(compiler/prelude/TysWiredIn.lhs)</code></a><code>) to break the abstraction and directly create the datastructure.</code></p>
<p><code>* </code><strong><code>Don't</code> <code>inline</code> <code>integer</code> <code>functions</code></strong><code>.  Most of the functions in the Integer implementation in the `integer` package are marked `NOINLINE`. For example in `integer-gmp` we have</code></p>
<p></p>
<p><code>Not only is this a big function to inline, but inlining it typically does no good because the representation of literals is abstact, so no pattern-matching cancellation happens.  And even if you have `(a+b+c)`, the conditionals mean that no cancellation happens, or you get an exponential code explosion!</code></p>
<h1 id="an-integrated-code-generator-for-ghc">An Integrated Code Generator for GHC</h1>
<p>We propose reworking GHC's back end into an <strong>Integrated Code Generator</strong>, which will widen the interface between machine-independent and machine-dependent parts of the back end. We wish to <strong>dissolve the barrier</strong> between the current machine-independent transformations (CPS conversion, stack layout, etc) and the native-code generators (instruction selection, calling conventions, register allocation -- including spilling to the C stack, etc). The goal is instead to have a code generator that <strong>integrates both machine-independent and machine-dependent components</strong>, which will interact through wide but well-specified interfaces. From this refactoring we expect the following benefits:</p>
<p><code>* </code><strong><code>The</code> <code>back</code> <code>end</code> <code>will</code> <code>be</code> <code>simpler</code> <code>overall</code></strong><code>, primarily because the</code><br />
<code>  refactoring will reduce or eliminate duplication of code</code></p>
<p><code>* </code><strong><code>Complexity</code> <code>will</code> <code>be</code> <code>isolated</code></strong><code> in two modules with well-defined</code><br />
<code>  interfaces: a dataflow engine and a register allocator</code></p>
<p><code>* </code><strong><code>GHC</code> <code>will</code> <code>generate</code> <code>better</code> <code>machine</code> <code>code</code></strong><code>, primarily because</code><br />
<code>  important decisions about register usage will be made at a later</code><br />
<code>  stage of translation and will exploit knowledge of the actual</code><br />
<code>  target machine. </code></p>
<h2 id="design-elements">Design elements</h2>
<p>The important elements of our design are as follows:</p>
<p><code> 0. Build two big hammers, and hit as many nails as possible.  (The big hammers are the </code><strong><code>dataflow</code> <code>optimization</code> <code>engine</code></strong><code> and a </code><strong><code>coalescing</code> <code>register</code> <code>allocator.</code></strong><code> For more on their uses, see our [wiki:Commentary/Compiler/IntegratedCodeGen#Designphilosophy design philosophy].)  The hammer itself may be big and complicated, but </code><strong><code>using</code> <code>a</code> <code>big</code> <code>hammer</code> <code>should</code> <code>be</code> <code>easy</code></strong><code> and should give easily predictable results.</code><br />
<code> 0. Load all back ends into every instance of the compiler, and </code><strong><code>treat</code> <code>every</code> <code>compilation</code> <code>as</code> <code>a</code> <code>cross-compilation.</code></strong><code>  Despite having been used in production compilers for at least twenty years, this technique is still seen as somewhat unorthodox, but it removes many </code><code>s and saves significant complexity at compiler-configuration time. Removing </code><code>s also mitigates problems with  validating the compiler under different build configurations.</code></p>
<h2 id="design-philosophy">Design philosophy</h2>
<p>State-of-the art dataflow optimization and register allocation both require complex implementations. We live with this complexity because <strong>creating new clients is easy.</strong></p>
<p><code>* </code><strong><code>Dataflow</code> <code>optimization:</code></strong><code> We can define a new</code><br />
<code>  optimization simply by defining a lattice of dataflow facts (akin</code><br />
<code>  to a specialized logic) and then writing the dataflow-transfer</code><br />
<code>  functions found in compiler textbooks.   Handing these functions to</code><br />
<code>  the dataflow engine produces a new optimization that is not only</code><br />
<code>  useful on its own, but that can easily be composed with other</code><br />
<code>  optimizations to create an integrated &quot;superoptimization&quot; that is</code><br />
<code>  strictly more powerful than any sequence of individual optimizations,</code><br />
<code>  no matter how many times they are re-run.</code><br />
<code>  The dataflow engine is based on </code><br />
<code>  </code><a href="http://citeseer.ist.psu.edu/old/lerner01composing.html"><code>(Lerner,</code> <code>Grove,</code> <code>and</code> <code>Chambers</code> <code>2002)</code></a><code>;</code><br />
<code>  you can find a functional implementation of the dataflow engine presented in</code><br />
<code>  </code><a href="http://www.cs.tufts.edu/~nr/pubs/zipcfg-abstract.html"><code>(Ramsey</code> <code>and</code> <code>Dias</code> <code>2005)</code></a><code>.</code></p>
<p><code>* </code><strong><code>Coalescing</code> <code>register</code> <code>allocator:</code></strong><code> The back end can use fresh temporaries and register-register moves</code><br />
<code>  with abandon, knowing that a state-of-the-art register allocator</code><br />
<code>  will eliminate almost all move instructions.</code></p>
<p><code>* </code><strong><code>Back</code> <code>ends:</code></strong><code> Our ultimate goal is to make adding a new back end easy as well.</code><br />
<code>  In the long run, we wish to apply John Dias's dissertation work to GHC.</code><br />
<code>  In the short run, however, we</code><br />
<code>  think it more sensible to represent each target-machine instruction</code><br />
<code>  set with an algebraic datatype.  We propose to use type classes to</code><br />
<code>  define common functions such as identifying the registers read and</code><br />
<code>  written by each instruction.</code></p>
<h2 id="proposed-compilation-pipeline">Proposed compilation pipeline</h2>
<p><code>0. Convert from </code><code> to an control flow graph </code><code>:</code><br />
<code>0. Instruction selection:</code><br />
<code>0. Optimise:</code><br />
<code>0. Proc-point analysis, and transformation</code><br />
<code>0. Register allocation</code><br />
<code>0. Stack layout</code><br />
<code>0. Tidy up</code></p>
<h3 id="convert-from-stg-to-control-flow-graph">Convert from STG to control flow graph</h3>
<p>Convert from  to an control flow graph  (<a href="GhcFile(compiler/cmm/ZipCfg.hs)" class="uri" title="wikilink">GhcFile(compiler/cmm/ZipCfg.hs)</a>, <a href="GhcFile(compiler/cmm/ZipCfgCmmRep.hs)" class="uri" title="wikilink">GhcFile(compiler/cmm/ZipCfgCmmRep.hs)</a>). This step is Simon PJ's &quot;new code generator&quot; from September 2007. This conversion may introduce new variables, stack slots, and compile-time constants. </p>
<p><code>  * Implements calling conventions for call, jump, and return instructions: all parameter passing is turned into data-movement instructions (register-to-register move, load, or store), and stack-pointer adjustments are inserted. After this point, calls, returns, and jumps are just control-transfer instructions -- the parameter passing has been compiled away.  </code><br />
<code>  * How do we refer to locations on the stack when we haven't laid it out yet? The compiler names a stack slot using the idea of a &quot;late compile-time constant,&quot; which is just a symbolic constant that will be replaced with an actual stack offset when the stack layout is chosen.One departure from the old code generator is that </code><strong><code>we</code> <code>do</code> <code>not</code> <code>build</code> <code>a</code>  <code>abstract-syntax</code> <code>tree;</code></strong><code> instead we go straight to a control-flow graph.</code></p>
<p>In practice, we first generate an &quot;abstract control flow graph&quot;, `CmmAGraph`, which makes the business of generating fresh `BlockId`s more convenient, and convert that to a `CmmGraph`. The former is convenient for <em>construction</em> but cannot be analysed; the latter is concrete, and can be analyzed, transformed, and optimized.</p>
<h3 id="instruction-selection">Instruction selection</h3>
<p>Instruction selection: each   and  node in the control-flow graph is replaced with a new graph in which the nodes are machine instructions.  The  type represents computational machine instructions; the  type represents control-transfer instructions. The choice of representation is up to the author of the back end, but for continuity with the existing native code generators, we expect to begin by using algebraic data types inspired by the existing definitions in <a href="GhcFile(compiler/nativeGen/MachInstrs.hs)" class="uri" title="wikilink">GhcFile(compiler/nativeGen/MachInstrs.hs)</a>.</p>
<p>Note that the graph still contains:</p>
<p><code>* </code><strong><code>Variables</code></strong><code> (ie local register that are not yet mapped to particular machine registers)</code><br />
<code>* </code><strong><code>Stack-slot</code> <code>addressing</code> <code>modes</code></strong><code>, which include late-bound compile-time constants, such as the offset in the frame of the a variable spill location, or !BlockId stack-top-on-entry.</code></p>
<p>The invariant is that each node could be done by one machine instruction, provided each `LocalReg` maps to a (suitable) physical register; and an instruction involving a stack-slot can cope with (Sp+n).</p>
<p>An <strong>extremely important distinction</strong> from the existing code is that we plan to eliminate  and instead provide multiple datatypes, e.g., in , , , and so on.</p>
<p>Similarly, we expect a an instruction selector for <em>each</em> back end, so for example, we might have a transformation that maps  (with variables, stack slots, and compile-time constants)  (with variables, stack slots, and compile-time constants).</p>
<p>We expect to <strong>abstract away from the details of these representations</strong> by borrowing some abstractions from <a href="http://www.eecs.harvard.edu/hube/software/nci/overview.html">Machine SUIF</a>. In the longer term we would like to support RTL-based representations such as are used in gcc, vpo and Quick C--. What this means is that `I386.Middle` (etc) is an abstract type, an instance of type class that supports the functions that the rest of the pipeline needs. For example:  This allows us to <strong>make code improvements machine-independent</strong>, by using machine-dependent functions to capture the semantics of instructions. Figuring out precisely what the interface should be is a key step. For example, to support copy propagation we might want an operation  Similarly, to support peephole optimsation (eg transform `x = y+2; p = bits32[x]` to `p = bits32[y+2]`) we might want something like  The `substExprs` operation returns a `Just` iff a substitution took place.</p>
<p>Interfaces like these would require the machine-specific abstract type `i` to contain enough information to reconstruct a `LocalReg` or `CmmExpr`. Later one, we'll need to construct SRTs too, so we must continue to track pointer-hood.</p>
<p>One possible implementation for `I386` or `Sparc` would be to use a generic RTL representation, together with a recogniser to maintain the machine invariant. Our initial idea, though, is that is an implementation choice. It's still possible that a machine-independent optimisation could take advantage of the representation being an RTL. For example, we could provide a function in the `Instr` class  which is particularly cheap for architectures that do use `RTL` as the representation type.</p>
<h3 id="optimisation">Optimisation</h3>
<p>Optimise the code.  (with variables, stack slots, and compile-time constants)  (with variables, stack slots, and compile-time constants), such as</p>
<p><code>  * Branch chain elimination.</code><br />
<code>  * Remove unreachable blocks (dead code).</code><br />
<code>  * Constant propagation.</code><br />
<code>  * Copy propagation.</code><br />
<code>  * Lazy code motion (hoisting, sinking, partial redundancy elimination).</code><br />
<code>  * Block concatenation.  branch to K; and this is the only use of K.  </code><br />
<code>  * Common Block Elimination (like CSE). This essentially implements the Adams optimisation, we believe.</code><br />
<code>  * Consider (sometime): block duplication.  branch to K; and K is a short block.  Branch chain elimination is just a special case of this.</code><br />
<code>  * Peephole optimisation.  The difficulty of implementing a good peephole optimizer varies greatly with the representation of instructions.  We propose to postpone serious work on peephole optimization until we have a back end capable of representing machine instructions as RTLs, which makes peephole optimization trivial.</code></p>
<h3 id="proc-point-analysis">Proc-point analysis</h3>
<p> Both input and output still have variables and stack-slot addressing modes.</p>
<p><code> * Proc points are found, and the appropriate control-transfer instructions are inserted.</code><br />
<code> * Why so early(before register allocation, stack layout)? Depending on the back end (think of C as the worst case), the proc-point analysis might have to satisfy some horrible calling convention. We want to make these requirements explicit before we get to the register allocator.  We also want to </code><strong><code>exploit</code> <code>the</code> <code>register</code> <code>allocator</code></strong><code> to make the best possible decisions about </code><em><code>which</code> <code>live</code> <code>variables</code> <code>(if</code> <code>any)</code> <code>should</code> <code>be</code> <code>in</code> <code>registers</code> <code>at</code> <code>a</code> <code>proc</code> <code>point</code></em><code>.</code></p>
<h3 id="register-allocation">Register allocation</h3>
<p>Register allocation replaces variable references with machine register and stack slots. This may introduce spills and reloads (to account for register shortage), which which is why we may get new stack-slot references.</p>
<p>That is, register allocation takes  (with variables, stack slots)  (with stack slots only). No more variables!</p>
<p>We no longer need to spill to the C stack, because we have fully allocated everything to machine registers.</p>
<h3 id="stack-layout">Stack layout</h3>
<p>Stack Layout:  (with stack slots, and compile-time constants) </p>
<p><code> * Choose a stack layout.</code><br />
<code> * Replace references to stack slots with addresses on the stack.</code><br />
<code> * Replace compile-time constants with offsets into the stack.</code></p>
<p>No more stack-slot references.</p>
<h3 id="tidy-up">Tidy up</h3>
<p><code>0. Proc-point splitting: </code><code> </code><br />
<code> * Each proc point gets its own procedure.</code><br />
<code>0. Code layout: </code><br />
<code> * A reverse postorder depth-first traversal simultaneously converts the graph to sequential code and converts each instruction into an assembly-code string: </code><strong><code>Assembly</code> <code>code</code> <code>ahoy</code></strong><code>!</code></p>
<h2 id="machine-dependence">Machine-dependence</h2>
<p>A key property of the design is that the scopes of machine-dependent code and machine-dependent static types are limited as much as possible:</p>
<p><code> 0. The representation of machine instructions may be machine-dependent (algebraic data type), or we may use a machine-independent representation that satisfies a machine-dependent dynamic invariant (RTLs).   The back end should be designed in such a way that most passes don't know the difference; we intend to borrow heavily from Machine SUIF.  To define the interface used to conceal the difference, Machine SUIF uses C++ classes; we will use Haskell's type classes.</code><br />
<code> 0. Instruction selection is necessarily machine-dependent, and moreover, it must know the representation of machine instructions</code><br />
<code> 0. Most of the optimizer need not know the representation of machine instructions.</code><br />
<code> 0. Other passes, including register allocation, stack layout, and so on, should be completely machine-independent.</code><br />
<code> 0. RTLs are not a new representation; they are a trivial extension of existing </code><code> representations.</code></p>
<h1 id="ghc-commentary-the-byte-code-interpreter-and-dynamic-linker">GHC Commentary: The byte-code interpreter and dynamic linker</h1>
<h2 id="linker">Linker</h2>
<p>The linker lives in `rts/Linker.c` and is responsible for handling runtime loading of code into a Haskell process. This is something of a big blob of unpleasant code, and see DynamicGhcPrograms for information about efforts to reduce our dependence on this linker.</p>
<p>Nevertheless, GHC's linker certainly adds functionality, and this has been enough to earn its keep (for now). In particular, the linker knows how to **relocate static libraries** (e.g. `.o` and `.a` libraries). This is a pretty rare feature to find: ordinarily, libraries that are to be loaded at runtime are compiled as position independent code (-fPIC), which allows the same physical code pages to be shared between processes, reducing physical memory usage. At runtime, GHC rewrites the relocations, meaning that the resulting page cannot be shared across processes, but that the result is just as efficient as if the code had been statically linked to begin with.</p>
<p>Implementation of the linker cuts three axes: object file format (ELF, Mach-O, PEi386), operating system (Linux, MingW, Darwin, etc), and architecture (i386, x86_64, powerpc, arm), and there are corresponding sets of macros for fiddling with each (`OBJFORMAT_*`, `*_HOST_OS` and `*_HOST_ARCH`). Are large part of the unpleasantness of the current linker is the fact that all of these different concerns are jumbled in one file; refactoring these out to separate files would be a very nice service.</p>
<p>(write more here)</p>
<h2 id="bytecode-interpreter">Bytecode Interpreter</h2>
<hr />
<p>CategoryStub</p>
<h1 id="the-io-manager">The I/O Manager</h1>
<p>This page describes the internals of the I/O manager, the latest version of which can be found in <a href="http://hackage.haskell.org/packages/archive/base/latest/doc/html/GHC-Event.html">GHC.Event</a>. The I/O manager's job is to to provide a blocking I/O API to the user without forcing the RTS to create one operating system thread per Haskell thread. We here focus on the <em>threaded</em> RTS on non-Windows platforms.</p>
<p>ezyang: <strong>WARNING: some of this information may be out of date</strong></p>
<p>The RTS keeps a global list of pending events, unsuprising called `pendingEvents`, containing a elements of the following data type:</p>
<p></p>
<p>When a thread wants to read from a file descriptor `fd` it calls `threadWaitRead` which in turn calls `waitForReadEvent`.</p>
<p></p>
<p>`waitForReadEvent` creates a new `MVar`, adds it to `pendingEvents` and finally blocks on it. `pendingEvents` gets read by the I/O manager thread which runs the event loop, in GHC called `service_loop`. It roughly performs these steps:</p>
<p><code>1. Pick up new I/O requests from `pendingRequests` and set the variable to the empty list.</code><br />
<code>2. Create data structures appropriate for calling `select`.</code><br />
<code>3. For each `Read` request in `pendingEvents` check if the file descriptor is in the ready set returned by `select`. If so perform a `putMVar` on the `MVar` associated with that request to wake up the blocked thread.</code><br />
<code>4. Repeat from step 1.</code></p>
<h1 id="key-data-types">Key data types</h1>
<p>The key to understanding GHC is to understand its key data types. There are pages describing many of them here (please add new pages!). The diagram below shows their inter-dependencies.</p>
<p><code>* [wiki:Commentary/Compiler/HsSynType The source language: HsSyn] </code><br />
<code>* [wiki:Commentary/Compiler/RdrNameType RdrNames, Modules, and OccNames]</code><br />
<code>* [wiki:Commentary/Compiler/ModuleTypes ModIface, ModDetails, ModGuts]</code><br />
<code>* [wiki:Commentary/Compiler/Unique Uniques]: Not drawn in the diagram, because nearly everything depends on Uniques.</code><br />
<code>* [wiki:Commentary/Compiler/NameType Names]</code><br />
<code>* [wiki:Commentary/Compiler/EntityTypes Entities]: variables, type constructors, data constructors, and classes.</code><br />
<code>* Types: [wiki:Commentary/Compiler/TypeType Type and Kind], [wiki:Commentary/Compiler/FC equality types and coercions]</code><br />
<code>* [wiki:Commentary/Compiler/CoreSynType The core language]</code><br />
<code>* [wiki:Commentary/Compiler/StgSynType The STG language]</code><br />
<code>* [wiki:Commentary/Compiler/CmmType The Cmm language]</code><br />
<code>* [wiki:Commentary/Compiler/BackEndTypes Back end types]</code></p>
<p><a href="Image(types.png)" class="uri" title="wikilink">Image(types.png)</a></p>
<h1 id="kinds">Kinds</h1>
<p>Kinds classify types. So for example:  The base kinds are these:</p>
<p><code>* &quot;`*`&quot; is the kind of boxed values. Things like `Int` and `Maybe Float` have kind `*`.</code><br />
<code>* &quot;`#`&quot; is the kind of unboxed values. Things like `Int#` have kind `#`.</code><br />
<code>* With the advent of [wiki:GhcKinds data type promotion and kind polymorphism] we can have a lot more kinds.</code></p>
<p>(Unboxed tuples used to have a distinct kind, but in 2012 we combined unboxed tuples with other unboxed values in a single kind &quot;`#`&quot;.)</p>
<h2 id="representing-kinds">Representing kinds</h2>
<p>Kinds are represented by the data type `Type` (see [wiki:Commentary/Compiler/TypeType]):  Basic kinds are represented using type constructors, e.g. the kind `*` is represented as  where `liftedTypeKindTyCon` is a built-in `PrimTyCon`. The arrow type constructor is used as the arrow kind constructor, e.g. the kind `* -&gt; *` is represented internally as  It's easy to extract the kind of a type, or the sort of a kind:  The &quot;sort&quot; of a kind is always one of the sorts: `TY` (for kinds that classify normal types) or `CO` (for kinds that classify coercion evidence). The coercion kind, `T1 :=: T2`, is represented by `PredTy (EqPred T1 T2)`.</p>
<h2 id="kind-subtyping">Kind subtyping</h2>
<p>There is a small amount of sub-typing in kinds. Suppose you see `(t1 -&gt; t2)`. What kind must `t1` and `t2` have? It could be `*` or `#`. So we have a single kind `OpenKind`, which is a super-kind of both, with this simple lattice:</p>
<p><a href="Image(https://docs.google.com/drawings/pub?id=1M5yBP8iAWTgqdI3oG1UNnYihVlipnvvk2vLInAFxtNM&amp;w=359&amp;h=229)" class="uri" title="wikilink">Image(https://docs.google.com/drawings/pub?id=1M5yBP8iAWTgqdI3oG1UNnYihVlipnvvk2vLInAFxtNM&amp;w=359&amp;h=229)</a></p>
<p>(You can edit this picture <a href="https://docs.google.com/drawings/d/1M5yBP8iAWTgqdI3oG1UNnYihVlipnvvk2vLInAFxtNM/edit?hl=en_GB">here</a>.)</p>
<h1 id="linearity">Linearity</h1>
<p>The solution is to distinguish call demands from product demands. Consider again:  The demands placed on  by the first and second call get bothed together to yield . But this is incorrect. Consider:  Here, the demands placed on  by the body of  and by the call to  in the -body get bothed together: . Note that this is the same as the demand placed on  above, yet we want to distinguish between the two situations, because in the first example, the inner lambda in 's rhs is only called once.</p>
<p>The solution is to treat call demands and product demands differently, and to define the  function for call demands to have the same behavior as . Then in the first example,  has demand  placed on it, and in the second, . This is what we want; now, if  has demand  placed on it, that implies  is always called with two arguments.</p>
<p>Why does this make sense? Consider what it means if we see an example like:  (where  is lazy in , and  is strict in  and ).  is used both with demand  (in the call to  and with demand  (in the call to ). This means it's perfectly same to strictly evaluate , so when we both together the two demands, we should get . On the other hand, if a function is <em>called</em> once with one argument and once with two, we don't want to treat it as a function that's always called with two arguments; we're only interested in functions that are <em>always</em> called with <em>n</em> arguments for a given <em>n</em>. Hence, both should behave the same way as lub for call demands.</p>
<h1 id="ticky">Ticky</h1>
<p>(NB out-of-date, but maybe historically useful; cf [wiki:Debugging/TickyTicky])</p>
<p>The following code inserts extra fields into closures when ticky is enabled (and so had to be commented out):  in <a href="GhcFile(compiler/codeGen/CgTicky.hs)" class="uri" title="wikilink">GhcFile(compiler/codeGen/CgTicky.hs)</a>.</p>
<p>Other relevant functions:  in <a href="GhcFile(compiler/codeGen/CgTicky.hs)" class="uri" title="wikilink">GhcFile(compiler/codeGen/CgTicky.hs)</a> (called by  in <a href="GhcFile(compiler/codeGen/CgClosure.lhs)" class="uri" title="wikilink">GhcFile(compiler/codeGen/CgClosure.lhs)</a>).</p>
<p>Argh! I spent days tracking down this bug:  in <a href="GhcFile(compiler/cmm/CLabel.hs)" class="uri" title="wikilink">GhcFile(compiler/cmm/CLabel.hs)</a> needs to return  for labels of type  (i.e., labels for ticky counters.) By default, it was returning , which caused the ticky counter labels to get declared with the wrong type in the generated C, which caused C compiler errors.</p>
<h2 id="declarations-for-ticky-counters">Declarations for ticky counters</h2>
<p> spits out C declarations that look like this:  Here,  is actually an  (this type is declared in <a href="GhcFile(includes/StgTicky.h)" class="uri" title="wikilink">GhcFile(includes/StgTicky.h)</a>). The counters get used by  in <a href="GhcFile(rts/Ticky.c)" class="uri" title="wikilink">GhcFile(rts/Ticky.c)</a>, which prints out the ticky reports. The counter fields are accessed using offsets defined in <a href="GhcFile(includes/GHCConstants.h)" class="uri" title="wikilink">GhcFile(includes/GHCConstants.h)</a> (), which in turn get generated from <a href="GhcFile(includes/mkDerivedConstants.c)" class="uri" title="wikilink">GhcFile(includes/mkDerivedConstants.c)</a> (change it and then run  in .</p>
<p><s>Note that the first 3 fields of the counters are 16-bit ints and so the generated ticky-counter registration code has to reflect that (I fixed a bug where the first field was getting treated as a 32-bit int.)</s> I modified the  type so that all fields are s, because it seems that the code generator can't cope with anything else anyway (i.e., in the declaration above,  is an array of s, even though the C type declaration implies that some fields are halfwords.)</p>
<p>In  in <a href="GhcFile(compiler/codeGen/CgClosure.lhs)" class="uri" title="wikilink">GhcFile(compiler/codeGen/CgClosure.lhs)</a>, &quot;eager blackholing&quot; was getting employed in the case where ticky was turned on; this was causing programs to  when they wouldn't with ticky disabled, so I turned that off.</p>
<h1 id="strictness-and-let-floating">Strictness and let-floating</h1>
<p>We run into the following problem in the  nofib benchmark: suppose we have:  where  doesn't depend on . Demand analysis says that  has a strict demand placed on it. Later,  gets floated to the top level because it doesn't depend on  (in reality it's more complicated because in this case  probably would have gotten floated out before demand analysis, but bear with me).  still has a strict demand signature, which a top-level binding isn't allowed to have. Currently this manifests itself as an assertion failure in <a href="GhcFile(compiler/simplCore/SimplEnv.lhs)" class="uri" title="wikilink">GhcFile(compiler/simplCore/SimplEnv.lhs)</a>.</p>
<p>There are two possible easy solutions: don't float out bindings for strict things, or &quot;both&quot; the demand for a binder with Lazy when its binding gets floated out. The question is, is it better to do the let-floating and lose the strictness into or to evaluate something strictly but lose sharing?</p>
<h1 id="coercions">Coercions</h1>
<p>When we run into an expression like  that we're placing demand  on, we analyze  to get , then check whether the depth of  is equal to the depth of  or not. This is necessary because we might be casting a function to a non-function type. So, if  and  have equal depth, we return  as is; if 's arity is less, we drop the appropriate number of args from ; if 's arity is less, we add the appropriate number of dummy argument demands to it.</p>
<h1 id="warn-arity">WARN: arity /</h1>
<p>dmdTypeDepth rhs_dmd_ty &amp;&amp; not (exprIsTrivial rhs) =</p>
<p>This warning was happening for (at least) two reasons: - lambdas with a strict non-call demand placed on them were being handled wrong (see the first two examples in [wiki:Commentary/Compiler/StrictnessAnalysis/Examples]) - coercions were being handled wrong, resulting in a demand type with depth 0 being assigned to an rhs consisting of a cast from/to a function type</p>
<h1 id="explaining-demand-transformers">Explaining demand transformers</h1>
<p>For those who, like me, are a little slow, this example might go in section 5.1 of the paper:</p>
<p>(a):  (b): </p>
<p>In both (a) and (b), 's rhs places a strict demand on . So if we see:  with a strict demand placed on it, it wouldn't be sound to look at 's demand signature and say that  places a strict demand on  under  -- because we don't know whether  is like (a) or like (b). This is why when we see a partial application of , we discard all of the argument information in 's demand type.</p>
<h1 id="nofib-stuff">Nofib stuff</h1>
<p>I've had weird problems with the  and  commands under MSYS but I think it's just when running nofib. At some point I wrote down:</p>
<p>TIME needs to be  not </p>
<p>and</p>
<p>MSYS  does not work, use cygwin </p>
<p>but of *course* I no longer remember what I meant. <a href="PageOutline" class="uri" title="wikilink">PageOutline</a></p>
<h1 id="ghc-commentary-libraries">GHC Commentary: Libraries</h1>
<p>All GHC build trees contain a set of libraries, called the <strong>Boot Packages</strong>. These are the libraries that GHC's source code imports. Obviously you need the boot packages to build GHC at all. The boot packages are those packages in the file [source:packages] that have a `-` in the &quot;tag&quot; column.</p>
<p>The repository structure of a GHC source tree is described in [wiki:Repositories].</p>
<p>You can see exactly which versions of what packages GHC depends on by looking in [source:compiler/ghc.cabal.in]. The versions of the boot packages (including the `base` library) associated with each GHC release are tabulated in <a href="wiki:Commentary/Libraries/VersionHistory" title="wikilink">GHC Boot Library Version History</a>.</p>
<h1 id="building-packages-that-ghc-doesnt-depend-on">Building packages that GHC doesn't depend on</h1>
<p>You can make the build system build extra packages, on which GHC doesn't strictly depend, by adding them to the `$(TOP)/packages` file, with an `extra` tag. Then set `BUILD_EXTRA_PKGS=YES` in your `mk/build.mk` file.</p>
<p>It should be exceptional, but you can make the build system provide per-package compiler flags, by adding some definitions in `$(TOP)/ghc.mk`, just below the comment </p>
<hr />
<h1 id="classifying-boot-packages">Classifying boot packages</h1>
<p>A <strong>boot package</strong> is, by definition, a package that can be built by GHC's build system.</p>
<p>Boot packages can be classified in four different ways:</p>
<p><code> * Required vs optional</code><br />
<code> * Wired-in vs independent</code><br />
<code> * Zero-boot vs not zero-boot</code><br />
<code> * Installed vs not installed</code></p>
<p>These distinctions are described in the following sub-sections.</p>
<h2 id="required-or-optional">Required or optional</h2>
<p>Most boot packages <strong>required</strong> to build `ghc-stage2`, or one of the supporting utilities such as `ghc-pkg`, `hsc2hs`, etc.</p>
<p>However a few are <strong>optional</strong>, and are built only</p>
<p><code>* To ensure that they do indeed build cleanly; they are stress tests of GHC.  E.g. `dph`</code><br />
<code>* Because they are used in regression tests</code></p>
<h2 id="coupling-to-ghc">Coupling to GHC</h2>
<p>An important classification of the boot packages is as follows:</p>
<p><code>* </code><strong><code>Wired</code> <code>in</code> <code>packages</code></strong><code> are totally specific to GHC.  See the list in `compiler/main/Packages.lhs` function `findWiredInPackages`, and c.f. [wiki:Commentary/Compiler/Packages]. At the moment these are:</code><br />
<code>  * `ghc-prim`</code><br />
<code>  * `integer-gmp`, `integer-simple`</code><br />
<code>  * `base`</code><br />
<code>  * `template-haskell`</code><br />
<code>  * `dph`</code></p>
<p><code>* </code><strong><code>Independent</code></strong><code> packages are loosely coupled to GHC, and often maintained by others.  Most boot packages are independent; e.g. `containers`, `binary`, `haskeline` and so on.  </code></p>
<p>Independent libraries may have a master repository somewhere separate from the GHC repositories. Whenever we release GHC, we ensure that the installed boot libraries (i.e. that come with GHC) that are also independent are precisely sync'd with a particular released version of that library.</p>
<h2 id="zero-boot-packages">Zero-boot packages</h2>
<p>Since GHC's source code imports the boot packages, <em>even the bootstrap compiler must have the boot packages available</em>. (Or, more precisely, all the types and values that are imported must be available from some package in the bootstrap compiler; the exact set of packages does not need to be identical.)</p>
<p>For the most part we simply assume that the bootstrap compiler already has the boot packages installed. The <strong>Zero-boot Packages</strong> are a set of packages for which this assumption does not hold. Two reasons dominate:</p>
<p><code>* For certain fast-moving boot packages (notably `Cabal`), we don't want to rely on the user having installed a bang-up-to-date version of the package.</code><br />
<code>* The only packages that we can &quot;assume that the bootstrap compiler already has&quot; are those packages that come with GHC itself; i.e. the installed boot packages.  So non-installed boot packages are also zero-boot packages.  Example: `bin-package-db` or `hoopl`.</code></p>
<p>So we begin the entire build process by installing the zero-boot packages in the bootstrap compiler. (This installation is purely local to the build tree.) This is done in `ghc.mk` by setting `PACKAGES_STAGE0` to the list of zero-boot packages; indeed this is the only way in which zero-boot packages are identified in the build system.</p>
<p>As time goes on, a Zero-boot package may become an ordinary boot package, because the bootstrap compiler is expected to have (a sufficiently up to date) version of the package already. Remember that we support bootstrapping with two previous versions of GHC.</p>
<p>To find out which packages are currently zero-boot packages, do the following in a GHC build: </p>
<p>Some Zero-boot packages are <strong>maintained by other people</strong>. In order to avoid GHC being exposed to day-by-day changes in these packages, we maintain a &quot;lagging&quot; Git repository for each that we occasionally sync with the master repository. We never push patches to lagging repository; rather we push to the master (in discussion with the package maintainer), and pull the patches into the lagging repo. The current Zero-boot packages of this kind are:</p>
<p><code>* `Cabal`: we frequently update Cabal and GHC in sync</code><br />
<code>* `binary` (renamed to `ghc-binary` in the 6.12 branch): required by `bin-package-db`.</code></p>
<p>Other Zero-boot packages are <strong>maintained by us</strong>. There is just one Git repo for each, the master. When we make a GHC release, we simultaneously tag and release each of these packages. They are:</p>
<p><code>* `hpc`</code><br />
<code>* `extensible-exceptions`: this is a shim that provides an API to older versions of GHC that is compatible with what the current `base` package now exports.  So, unusually, `extensible-exceptions` is a zero-boot package, but not a boot package.</code><br />
<code>* `bin-package-db`: a GHC-specific package that provides binary serialisation of the package database, use by `ghc-pkg` and GHC itself.</code></p>
<h2 id="installation">Installation</h2>
<p>When we build a distribution of GHC, it includes at least some libraries, otherwise it would be utterly useless. Since GHC is part of the Haskell Platform, any library that is installed with GHC is necessarily part of the Haskell Platform, so we have to be a bit careful what we include.</p>
<p>Alas, since the `ghc` package (implementing the GHC API) is certainly an installed package, all the packages on which it depends must also be installed, and hence willy-nilly become part of the Haskell Platform. In practice that means that almost all the Boot Packages are installed. In some cases that is unfortunate. For example, we currently have a special version of the `binary` library, which we don't really expect Haskell users to use; in this case, we call it `ghc-binary`, and informally discourage its use.</p>
<p>Currently the Boot Packages that are not installed are `haskeline`, `mtl`, and `terminfo`; these are needed to build the GHC front-end, but not to build the `ghc` <em>package</em>.</p>
<p><strong>QUESTION</strong>: where in the build system is the list of installed packages defined?</p>
<hr />
<h1 id="boot-packages-dependencies">Boot packages dependencies</h1>
<p><code>* At the root of the hierarchy we have </code><strong><code>`ghc-prim`</code></strong><code>. As the name implies, this package contains the most primitive types and functions. It only contains a handful of modules, including `GHC.Prim` (which contains `Int#`, `+#`, etc) and `GHC.Bool`, containing the `Bool` datatype.  See &quot;WARNING: pattern matching&quot; below.</code></p>
<p><code>* Above `ghc-prim` are the packages</code><br />
<code>  * `integer-gmp`</code><br />
<code>  * `integer-simple`</code><br />
<code>The two have the same interface, and only one of the two is used. (When we want to be vague about which one, we call it `integer-impl`.)  They provide a definition of the `Integer` type (on top of the C `gmp` library, or in plain Haskell, respectively). Which functionality is provided in `ghc-prim` is mostly driven by what functionality the `integer-impl` packages need. By default `integer-gmp` is used; to use `integer-simple` define `INTEGER_LIBRARY=integer-simple` in `mk/build.mk`.</code></p>
<p><code>  See &quot;WARNING: pattern matching&quot; below.</code></p>
<p><code>* Next is the </code><strong><code>`base`</code></strong><code> package. This contains a large number of modules, many of which are in one big cyclic import knot, mostly due to the `Exception` type.</code></p>
<p><code>* On top of base are a number of other, more specialised packages, whose purpose is generally clear from their name. If not, you can get more detail from the descriptions in their Cabal files.  The up-to-date list of packages can be found in the file [source:packages].</code></p>
<p>The `haskell98`, `old-time`, `old-locale` and `random` packages are mostly only needed for Haskell 98 support, although `dph` currently uses `random` too.</p>
<h2 id="warning-pattern-matching-in-ghc-prim-integer-simple-and-integer-gmp">WARNING: Pattern matching in `ghc-prim`, `integer-simple`, and `integer-gmp`</h2>
<p>Note that `ghc-prim` and `integer-impl` are below the dependency chain from Exception (in `base`), which means they must not raise generate code to raise an exception (it's not enough that this code will never run). One particularly subtle case of GHC exception-raising code is in the case of (complete!) pattern matches. Consider the unboxed form of Integers, which has the constructor S# or J#.</p>
<p></p>
<p>GHC will incorrectly generate core that pattern matches against the second argument twice, the second match being a partial one with (dead) exception raising code. When compiled with optimizations, the dead code is eliminated. However, this breaks with -O0, thus:  The fix is to explicitly spell out the constructor in the second and third line, so that GHC does not generate calls to `patError`:</p>
<p></p>
<h1 id="repositories">Repositories</h1>
<p>The list of repository locations has moved to [wiki:Repositories].</p>
<h1 id="the-llvm-backend">The LLVM backend</h1>
<p>David Terei wrote a new code generator for GHC which targets the LLVM compiler infrastructure. Most of the work was done as part of an honours thesis at the University of New South Wales under the supervision of Manuel Chakravarty. It was merged into GHC Head around May of 2010 and has been included in GHC since the 7.0 release.</p>
<p>Documentation:</p>
<p><code>* [wiki:Commentary/Compiler/Backends/LLVM/Installing Installing &amp; Using]</code><br />
<code>* [wiki:Commentary/Compiler/Backends/LLVM/Design Design &amp; Implementation]</code><br />
<code>* [wiki:Commentary/Compiler/Backends/LLVM/Mangler LLVM Mangler]</code><br />
<code>* [wiki:Commentary/Compiler/Backends/LLVM/DevelopmentNotes Bugs &amp; Other Problems]</code><br />
<code>* [wiki:Commentary/Compiler/Backends/LLVM/GHC_LLVMPorting Porting GHC/LLVM to another platform]</code></p>
<p>Work in Progress:</p>
<p><code>* [wiki:SIMD SIMD instructions and LLVM]</code><br />
<code>* [wiki:Commentary/Compiler/Backends/LLVM/Alias Improving Alias Analysis]</code></p>
<p>Future Ideas:</p>
<p><code>* [wiki:Commentary/Compiler/Backends/LLVM/WIP ToDo List of Sorts]</code><br />
<code>* [wiki:Commentary/Compiler/Backends/LLVM/ReplacingNCG Replacing the Native Code Generator]</code><br />
<code>* </code><a href="http://dterei.blogspot.com/2011/09/ghc-project-for-all.html"><code>David</code> <code>Terei</code> <code>blog</code> <code>post</code> <code>of</code> <code>LLVM-related</code> <code>projects</code></a></p>
<p>Other information:</p>
<p><code>* The </code><a href="http://www.cse.unsw.edu.au/~pls/thesis/davidt-thesis.pdf"><code>thesis</code> <code>paper</code></a><code> which offers a detailed performance evaluation, as well as the motivation and design of the back-end.</code><br />
<code>* </code><a href="http://blog.llvm.org/2010/05/glasgow-haskell-compiler-and-llvm.html"><code>Blog</code> <code>post</code></a><code> on the LLVM blog about the backend.</code><br />
<code>* A more recent </code><a href="http://www.cse.unsw.edu.au/~chak/papers/TC10.html"><code>paper</code></a><code> submitted to the Haskell Symposium '10, gives updated design overview and performance numbers.</code></p>
<h1 id="loopification">Loopification</h1>
<p>Loopification is a C-- optimisation pass that turns tail recursion into proper loops.</p>
<p>Here is a summary of relevant links and tickets</p>
<ul>
<li><a href="http://research.microsoft.com/en-us/um/people/simonpj/tmp/wos-diss-draft.pdf">Krzysztof Wos's project</a> in which he reports great performance improvements by turning tail recursion into loops in C--.</li>
</ul>
<ul>
<li>Tickets:</li>
</ul>
<p><code> * #8285</code><br />
<code> * #8793, #11372; see comment 15 of #8793) etc, where it seems that we are missing loopification for a simple IO function</code><br />
<code> * #8585 concerned getting the loop to start </code><em><code>after</code></em><code> the stack check</code></p>
<h1 id="llvm-mangler">LLVM Mangler</h1>
<p>The LLVM backend sadly includes a 'mangler'. This is a Haskell written program (well pass of GHC) that runs on the assembly code generated by the LLVM compiler. We do this as there are a few issues with communicating to LLVM exactly what we want generated as object code and so, for now, it is easiest to post-process the assembly.</p>
<p>Long term we ideally would submit patches to LLVM and get rid of the mangler. The work required to do that may be quite high and the patches needed potentially fairly specific to GHC. So no one has done that yet.</p>
<p>Below are the issues that the LLVM Mangler addresses in the assembly code.</p>
<h2 id="tables_next_to_code-tntc">TABLES_NEXT_TO_CODE (TNTC)</h2>
<p>TODO</p>
<h2 id="stack-alignment">Stack Alignment</h2>
<p>LLVM requires that the C stack be properly aligned for spills. One Win32 the stack is 4-byte aligned, which is not enough for SSE spills, and even on x64 platforms the stack is only 16-byte aligned, which is not enough for AVX spills. When the stack is not properly aligned for spills, LLVM generates prologue/epilogue code that fiddles with the base pointer, which GHC uses as its stack pointer, and disables tail call optimization. Both are very bad. Therefore we currently tell LLVM to always assume the stack is properly aligned and then rewrite all aligned SSE/AVX move instructions to their unaligned counterparts inside the mangler.</p>
<h2 id="simd-avx">SIMD / AVX</h2>
<h1 id="migrating-old-commentary">Migrating Old Commentary</h1>
<p>Below you will find a table with a line for each section of the <a href="http://darcs.haskell.org/ghc/docs/comm/">old commentary</a>. Please replace <em>unknown</em> with <strong>done</strong> if you believe that the wiki commentary completely captures <em>all</em> of the information in that section of the old commentary, and that there is no longer any reason for people to read that section of the commentary.</p>
<h2 id="before-the-show-begins">Before the Show Begins</h2>
<p>||Feedback||<strong>done</strong>|| ||Other Sources of Wisdom||<strong>done</strong>||</p>
<h2 id="genesis">Genesis</h2>
<p>||Outline of the Genesis||<strong>done</strong>|| ||Mindboggling Makefiles||<strong>done</strong>|| ||GHC's Marvellous Module Structure||<strong>done</strong>||</p>
<h2 id="the-beast-dissected">The Beast Dissected</h2>
<p>||Coding style used in the compiler||<strong>done</strong>|| ||The Glorious Driver||Sections 1 &amp; 2 <strong>done</strong>, <em>Other sections mostly outdated</em>|| ||Primitives and the Prelude||<em>unknown</em>|| ||Just Syntax||<em>unknown</em>|| ||The Basics||<em>unknown</em>|| ||Modules, !ModuleNames and Packages||<em>unknown</em>|| ||The truth about names: Names and !OccNames||<em>unknown</em>|| ||The Real Story about Variables, Ids, !TyVars, and the like||<em>unknown</em>|| ||Data types and constructors||<em>unknown</em>|| ||The Glorious Renamer||<em>unknown</em>|| ||Hybrid Types||<em>unknown</em>|| ||Checking Types||<em>unknown</em>|| ||Sugar Free: From Haskell To Core||<em>unknown</em>|| ||The Mighty Simplifier||<em>unknown</em>|| ||The Evil Mangler||<strong>done</strong>|| ||Alien Functions||<em>unknown</em>|| ||You Got Control: The STG-language||<em>unknown</em>|| ||The Native Code Generator||<em>unknown</em>|| ||GHCi||<em>unknown</em>|| ||Implementation of foreign export||<em>unknown</em>|| ||Compiling and running the Main module||<em>unknown</em>||</p>
<h2 id="rts-libraries">RTS &amp; Libraries</h2>
<p>||Coding Style Guidelines||<strong>done</strong>|| ||Spineless Tagless C||<em>unknown</em>|| ||Primitives||<em>unknown</em>|| ||Prelude Foundations||<em>unknown</em>|| ||Cunning Prelude Code||<em>unknown</em>|| ||On why we have !ForeignPtr||<em>unknown</em>|| ||Non-blocking I/O for Win32||<em>unknown</em>|| ||Supporting multi-threaded interoperation||<em>unknown</em>||</p>
<h2 id="extensions-or-making-a-complicated-system-more-complicated">Extensions, or Making a Complicated System More Complicated</h2>
<p>||Template Haskell||<em>unknown</em>|| ||Parallel Arrays||<em>unknown</em>||</p>
<h1 id="the-marvellous-module-structure-of-ghc">The Marvellous Module Structure of GHC</h1>
<p><code>* </code><strong><code>See</code> <code>also:</code> <code>[ModuleDependencies/Hierarchical</code> <code>Proposal</code> <code>for</code> <code>hierarchical</code> <code>module</code> <code>structure]</code></strong></p>
<p><code>* </code><strong><code>NOTE:</code></strong><code> Possibly outdated.</code></p>
<p>GHC is built out of about 245 Haskell modules. It can be quite tricky to figure out what the module dependency graph looks like. It can be important, too, because loops in the module dependency graph need to be broken carefully using .hi-boot interface files.</p>
<p>This section of the commentary documents the subtlest part of the module dependency graph, namely the part near the bottom.</p>
<p><code>* The list is given in compilation order: that is, module near the top are more primitive, and are compiled earlier.</code><br />
<code>* Each module is listed together with its most critical dependencies in parentheses; that is, the dependencies that prevent it being compiled earlier.</code><br />
<code>* Modules in the same bullet don't depend on each other.</code><br />
<code>* Loops are documented by a dependency such as &quot;loop Type.Type&quot;. This means tha the module imports Type.Type, but module Type has not yet been compiled, so the import comes from Type.hi-boot. </code></p>
<h2 id="compilation-order-is-as-follows">Compilation order is as follows:</h2>
<p><code>* First comes a layer of modules that have few interdependencies, and which implement very basic data types:</code><br />
<code>  * Util</code><br />
<code>  * !OccName</code><br />
<code>  * Pretty</code><br />
<code>  * Outputable</code><br />
<code>  * !StringBuffer</code><br />
<code>  * !ListSetOps</code><br />
<code>  * Maybes</code><br />
<code>  * etc </code></p>
<p><code>* Now comes the main subtle layer, involving types, classes, type constructors identifiers, expressions, rules, and their operations.</code><br />
<code>  * Name, !PrimRep</code><br />
<code>  * !PrelNames</code><br />
<code>  * Var (Name, loop !IdInfo.!IdInfo, loop Type.Type, loop Type.Kind)</code><br />
<code>  * !VarEnv, !VarSet, !ThinAir</code><br />
<code>  * Class (loop !TyCon.!TyCon, loop Type.Type)</code><br />
<code>  * !TyCon (loop Type.Type, loop !DataCon.!DataCon, loop Generics.!GenInfo)</code><br />
<code>  * !TypeRep (loop !DataCon.!DataCon, loop Subst.substTyWith)</code><br />
<code>  * Type (loop !PprType.pprType, loop Subst.substTyWith)</code><br />
<code>  * !FieldLabel (Type), !TysPrim (Type)</code><br />
<code>  * Literal (!TysPrim, !PprType), !DataCon (loop !PprType, loop Subst.substTyWith, !FieldLabel.!FieldLabel)</code><br />
<code>  * !TysWiredIn (loop !MkId.mkDataConIds)</code><br />
<code>  * !TcType ( lots of !TysWiredIn stuff)</code><br />
<code>  * !PprType ( lots of !TcType stuff )</code><br />
<code>  * !PrimOp (!PprType, !TysWiredIn)</code><br />
<code>  * !CoreSyn [does not import Id]</code><br />
<code>  * !IdInfo (!CoreSyn.Unfolding, !CoreSyn.!CoreRules)</code><br />
<code>  * Id (lots from !IdInfo)</code><br />
<code>  * CoreFVs, !PprCore</code><br />
<code>  * !CoreUtils (!PprCore.pprCoreExpr, CoreFVs.exprFreeVars, !CoreSyn.isEvaldUnfolding !CoreSyn.maybeUnfoldingTemplate)</code><br />
<code>  * !CoreLint ( !CoreUtils ), !OccurAnal (!CoreUtils.exprIsTrivial), !CoreTidy (!CoreUtils.exprArity )</code><br />
<code>  * !CoreUnfold (!OccurAnal.occurAnalyseGlobalExpr)</code><br />
<code>  * Subst (!CoreUnfold.Unfolding, CoreFVs), Generics (!CoreUnfold.mkTopUnfolding), Rules (!CoreUnfold.Unfolding, !PprCore.pprTidyIdRules)</code><br />
<code>  * !MkId (!CoreUnfold.mkUnfolding, Subst, Rules.addRule)</code><br />
<code>  * !PrelInfo (!MkId), !HscTypes ( Rules.!RuleBase ) </code></p>
<p><code>* That is the end of the infrastructure. Now we get the main layer of modules that perform useful work.</code><br />
<code>  * !CoreTidy (!HscTypes.!PersistentCompilerState) </code></p>
<h2 id="typechecker-stuff">Typechecker stuff</h2>
<p><code>* !TcType</code><br />
<code>* !TcEvidence( !TcType )</code><br />
<code>* TcMType( !TcEvidence )</code><br />
<code>* !TcUnify( TcMType )</code><br />
<code>* TcSMonad( TcMType )</code><br />
<code>* !TcSimplify( TcSMonad )</code><br />
<code>* !TcValidity( !TcSimplify.simplifyTop, !TcUnify.tcSubType )</code><br />
<code>* !TcHsType( !TcValidity.checkValidType, !TcValidity.checkValidInstance )</code></p>
<h2 id="hssyn-stuff">!HsSyn stuff</h2>
<p><code>* !HsPat.hs-boot</code><br />
<code>* !HsExpr.hs-boot (loop !HsPat.LPat)</code><br />
<code>* !HsTypes (loop !HsExpr.!HsSplice)</code><br />
<code>* !HsBinds (!HsTypes.LHsType, loop !HsPat.LPat, !HsExpr.pprFunBind and others) !HsLit (!HsTypes.!SyntaxName)</code><br />
<code>* !HsPat (!HsBinds, !HsLit) !HsDecls (!HsBinds)</code><br />
<code>* !HsExpr (!HsDecls, !HsPat) </code></p>
<h2 id="library-stuff-base-package">Library stuff: base package</h2>
<p><code>* GHC.Base</code><br />
<code>* Data.Tuple (GHC.Base), GHC.Ptr (GHC.Base)</code><br />
<code>* GHC.Enum (Data.Tuple)</code><br />
<code>* GHC.Show (GHC.Enum)</code><br />
<code>* GHC.Num (GHC.Show)</code><br />
<code>* GHC.ST (GHC.Num), GHC.Real (GHC.Num)</code><br />
<code>* GHC.Arr (GHC.ST) GHC.STRef (GHC.ST)</code><br />
<code>* GHC.!IOBase (GHC.Arr)</code><br />
<code>* Data.Bits (GHC.Real)</code><br />
<code>* Data.!HashTable (Data.Bits, Control.Monad)</code><br />
<code>* Data.Typeable (GHC.IOBase, Data.!HashTable)</code><br />
<code>* GHC.Weak (Data.Typeable, GHC.IOBase) </code></p>
<h2 id="high-level-dependency-graph">High-level Dependency Graph</h2>
<p>Dark red edges indicate that only one module in one group depends on a module in the other group. Dark green means 11 or more dependencies. Arrows point from the importing module to the imported module.</p>
<p><a href="Image(dep5.png)" class="uri" title="wikilink">Image(dep5.png)</a></p>
<h1 id="module-types">Module Types</h1>
<p>Here we attempt to describe some of the main data structures involved in GHC's representation and handling of Haksell modules. GHC uses a number of different data types to represent modules, for efficiency (some types load less information) and categorising how other modules relate to the one being compiled. Most these types are defined in <a href="GhcFile(compiler/main/HscTypes.lhs)" class="uri" title="wikilink">GhcFile(compiler/main/HscTypes.lhs)</a>.</p>
<h2 id="module">Module</h2>
<p>Location: <a href="GhcFile(compiler/basicTypes/Module.lhs)" class="uri" title="wikilink">GhcFile(compiler/basicTypes/Module.lhs)</a></p>
<p>The <strong>Module</strong> data type is simply an identifier of a module; its fully qualified name.</p>
<p></p>
<h2 id="modiface">!ModIface</h2>
<p>Location: <a href="GhcFile(compiler/main/HscTypes.lhs)" class="uri" title="wikilink">GhcFile(compiler/main/HscTypes.lhs)</a></p>
<p>The <strong>!ModIface</strong> data type is one of the fullest representations of a module. It is a complete representation of a modules interface file (<strong>.hi</strong>). It is this data structure that is serialised to produce a modules <strong>.hi</strong> file.</p>
<h2 id="moddetails">!ModDetails</h2>
<p>Location: <a href="GhcFile(compiler/main/HscTypes.lhs)" class="uri" title="wikilink">GhcFile(compiler/main/HscTypes.lhs)</a></p>
<p><strong>!ModDetails</strong> is essentially a cache for information in the <strong>!ModIface</strong> for home modules only. It stores information about a module after linking has taken place. <strong>!ModIface</strong> stores information about a module before linking. Information stored in a <strong>!ModDetails</strong> is created from a <strong>!ModIface</strong>, typically during type checking.</p>
<h3 id="modguts">!ModGuts</h3>
<p>Location: <a href="GhcFile(compiler/main/HscTypes.lhs)" class="uri" title="wikilink">GhcFile(compiler/main/HscTypes.lhs)</a></p>
<p>A <strong>!ModGuts</strong> is carried through the compiler, accumulating stuff as it goes. There is only one <strong>!ModGuts</strong> at any time, the one for the module being compiled right now. Once it is compiled, a <strong>!ModIface</strong> and <strong>!ModDetails</strong> are extracted and the <strong>!ModGuts</strong> is discarded.</p>
<h2 id="modsummary">!ModSummary</h2>
<p>Location: <a href="GhcFile(compiler/main/HscTypes.lhs)" class="uri" title="wikilink">GhcFile(compiler/main/HscTypes.lhs)</a></p>
<p>A <strong>!ModSummary</strong> stores a summary of a module that is suitable for recompilation checking. A <strong>!ModSummary</strong> is a node in the compilation manager's dependency graph.</p>
<h2 id="homemodinfo">!HomeModInfo</h2>
<p>Location: <a href="GhcFile(compiler/main/HscTypes.lhs)" class="uri" title="wikilink">GhcFile(compiler/main/HscTypes.lhs)</a></p>
<p>A <strong>!HomeModInfo</strong> stores information about a module in the package being compiled. It simply stores for the <strong>!ModIface</strong>, <strong>!ModDetails</strong> and linkage information about a single module.</p>
<h2 id="homepackagetable">!HomePackageTable</h2>
<p>Location: <a href="GhcFile(compiler/main/HscTypes.lhs)" class="uri" title="wikilink">GhcFile(compiler/main/HscTypes.lhs)</a></p>
<p>The home package table describes already-compiled home-package modules, /excluding/ the module we are compiling right now.</p>
<h2 id="externalpackagestate">!ExternalPackageState</h2>
<p>Location: <a href="GhcFile(compiler/main/HscTypes.lhs)" class="uri" title="wikilink">GhcFile(compiler/main/HscTypes.lhs)</a></p>
<p>Stores information about other packages that we have pulled in while compiling the current module.</p>
<h1 id="multi-instance-packages">Multi-instance packages</h1>
<p>This page is about how to change the package system to allow multiple instances of a package to be installed at the same time. There are two reasons we want to be able to do this:</p>
<p><code>* To be able to track the different &quot;ways&quot; in which a package is available: e.g. profiling, dynamic.  At the moment, the package database doesn't track this information, with the result that the user has to reinstall packages with `--enable-profiling` on a trial-and-error basis in order to get profiling support for packages they have already installed.</code><br />
<code>  The same holds, in principle, for different flag settings or other configuration variations of a package.</code></p>
<p><code>* To make installing new packages more robust.  When installing a new package, we sometimes need to upgrade packages that are already installed to new versions, which may require recompiling other packages against the new version.  For example, if we have P1 installed, Q1 depends on P (any version), and we need to install R that depends on both P2 and Q1.  We need to build P2, rebuild Q1 against P2, and finally build R against P2 and the new Q1.  We would like to do this without removing P1 or the old Q1 from the package database, because other packages may be depending on the old Q1, and we don't want to break those packages (which is what currently happens with GHC 7.0).</code></p>
<p>See also</p>
<p><code>* [wiki:Commentary/Packages Commentary pages about packages]</code><br />
<code>* Philipp Schuster's GSoC project </code><a href="http://www.google-melange.com/gsoc/proposal/review/google/gsoc2012/phischu/1"><code>proposal</code> <code>(DEAD)</code></a><code>, </code><a href="http://www.google-melange.com/gsoc/project/google/gsoc2012/phischu/19001"><code>GSoC</code> <code>project</code> <code>page</code> <code>(DEAD)</code></a><code>,  [wiki:Commentary/GSoCMultipleInstances Trac wiki page], </code><a href="https://github.com/phischu/cabal"><code>git</code> <code>repo</code></a><code>, and </code><a href="https://www.youtube.com/watch?v=h4QmkyN28Qs"><code>video</code></a><code>.</code><br />
<code>* </code><a href="http://coldwa.st/e/blog/2013-08-20-Cabal-sandbox.html"><code>Mikhail's</code> <code>post</code></a><code> about Cabal sandboxes. </code><br />
<code>* Mailing list stuff </code><a href="http://comments.gmane.org/gmane.comp.lang.haskell.ghc.devel/443"><code>here</code></a><code> and </code><a href="http://markmail.org/message/4qvegvx32lhlo66g#query:+page:1+mid:bwdgykv4g2hzqg5t+state:results"><code>here</code></a><code>.</code></p>
<h2 id="todo-list">!ToDo list</h2>
<p><code>* ghc-pkg: do not overwrite previous instances in the package DB</code><br />
<code>  * but we need to think about the case where we overwrite an existing package on the file system and re-register. This will happen with local (or in-place) package registration that occurs when building a bunch of related components. In this case the tool should know it's doing that and unregister the old instance first (though reliably tracking that state may be tricky, since users can make clean etc). We should check make it a checked error to re-register in the same filesystem location with new package id, without unregistering the old one first. Perhaps we can identify some key file.</code></p>
<p><code>* GHC: discard conflicting instances during its shadowing phase</code><br />
<code>  * SDM: GHC will currently do *something* here, but it might end up with a result that the user didn't want/expect.  One way to improve things is to prioritise packages that were installed more recently.</code><br />
<code>  * Andres suggests that GHC should be much cleverer, and look at the actual dependencies of the modules being compiled before deciding which packages to enable.  This would almost certainly result in more things working and possibly less surprising behaviour sometimes, but Simon thinks that (a) it is too hard, (b) if users need this, they should use Cabal and its dependency resolver, which will do a good job, (c) you can often resolve problems by adding `-package X`, and (d) eventually we will want a system where users manage separate sessions, so they can set up an environment in which the packages they want are available.  This has a lot in common with `cabal-dev` and sandboxes, so the mechanisms (and concepts) should be shared. (kosmikus: perhaps an alternative is to force the user to make an active decision in case of conflicts, i.e., to create a sandbox that exposes a consistent package set).</code></p>
<p><code>* GHC: allow specifying a package instance in the -package flags</code><br />
<code>  * SDM: already done (-package-id flag)</code><br />
<code>  * DC: already used by Cabal</code></p>
<p><code>* Cabal: allow specifying a package instance when doing Setup.hs configure</code><br />
<code>  * DC: currently only == version constraints can be used, not installed package id. Shouldn't be too hard to add however.</code><br />
<code>  * JT: Done according to DC.</code></p>
<p><code>* instances of packages must install in a different location</code><br />
<code>  * install directory includes hash?</code><br />
<code>  * SDM: not done yet.  One problem is that we don't know the hash until the package is built, but we need to know the install locations earlier because we bake them into `Paths_foo.hs`.</code><br />
<code>  * Simon and Andres discussed that one option is to let Cabal compute its own hash. However, then we'd have two hashes to deal with. Only using the Cabal-computed hash isn't an option either according to Simon, because apparently GHC's ABI hash computation is non-deterministic, so we might end up with situations where Cabal's hash is stable, but GHC computes an ABI-incompatible version. This is somewhat worrying ... </code><br />
<code>  * Duncan thinks that we should store both a package identity and a package ABI hash. Currently we form the package id from the name, version and ABI hash. We should store the ABI hash separately anyway because eventually we will want to know it, to know which packages are ABI compatible. So Cabal can compute a package Id in advance, however is sensible, and the ABI hash is calculated as now, after the build. The installation directory follows the package Id.</code></p>
<p><code>* Cabal: will the dependency solver work correctly in the presence of multiple package instances?</code><br />
<code>  * Andres claims it will using the new solver. (There is now no point in updating the old solver, though it'd be technically possible.) A little bit more detail: the modular solver has no concept of shadowing, only of preference. So if several instances are provided by one or more package DBs, they'll all be valid choices.</code></p>
<p><code>* ghc-pkg cleanup: remove old/unused instances of packages</code><br />
<code>  * how can we tell when something is unnecessary? This is actually rather hard because unlike Nix we do not track every random executable that the user compiles.</code></p>
<h2 id="next-step-dealing-with-ways">Next step: dealing with ways</h2>
<p><code>* Add the &quot;way&quot; to InstalledPackageInfo, include the way in the hash</code></p>
<p><code>* GHC: slice the package DB during startup according to the correct way</code></p>
<p><code>* Cabal: fix up the dep resolver (kosmikus: anything still needed there?)</code></p>
<p><code>* Cabal: ways? (this would be really easy, if we could get more information about installed packages back from ghc-pkg)</code></p>
<p><code>* To handle flags and other config, add two new fields to InstalledPackageInfo: `install-agent: {agent-id}` which identifies cabal/rpm/etc and then `configuration: {free text}`. The interpretation of the configuration string depends on the installation agent, and need be known only to that agent. This way, agents can see if it was them that installed a package, and so they should know how to interpret the config string. For cabal this would include config flags etc. It should make it possible to reproduce a package, e.g. if we have to rebuild for some reason, or to get the profiling equiv of a normal instance.</code></p>
<h1 id="the-type-1">The  type</h1>
<p>Every entity (type constructor, class, identifier, type variable) has a . The Name type is pervasive in GHC, and is defined in <a href="GhcFile(compiler/basicTypes/Name.hs)" class="uri" title="wikilink">GhcFile(compiler/basicTypes/Name.hs)</a>. Here is what a  looks like, though it is private to the Name module: </p>
<p><code>* The </code><code> field says what sort of name this is: see </code><strong><code>[wiki:Commentary/Compiler/NameType#TheNameSortofaName</code> <code>NameSort]</code></strong><code> below. </code><br />
<code>* The </code><code> field gives the &quot;occurrence name&quot;, or </code><strong><code>[wiki:Commentary/Compiler/RdrNameType#TheOccNametype</code> <code>OccName]</code></strong><code>, of the Name.</code><br />
<code>* The </code><code> field allows fast tests for equality of Names. </code><br />
<code>* The </code><code> field gives some indication of where the name was bound. </code></p>
<h2 id="the-of-a-name">The  of a Name</h2>
<p>There are four flavours of Name: </p>
<p><code>, </code><code>::     </code><br />
<code>   An </code><code> </code><code> has only an occurrence name. Distinct </code><code> </code><code> may have the same occurrence name; the </code><code> distinguishes them.  </code></p>
<p><code>There is only a tiny difference between </code><code> and </code><code>; the former simply remembers that the name was originally written by the programmer, which helps when generating error messages.</code></p>
<p><code>:: </code><br />
<code>   An </code><code> </code><code> has a globally-unique (module, occurrence name) pair, namely the original name of the entity, that describes where the thing was originally defined. So for example, if we have </code></p>
<p></p>
<p><code>   then in module </code><code>, the function </code><code> has an External Name </code><code>.</code></p>
<p><code> During any invocation of GHC, each (module, occurrence-name) gets one, and only one, </code><code>, stored in the </code><code> field of the </code><code>.  This association remains fixed even when GHC finishes one module and starts to compile another.  This association between (module, occurrence-name) pairs and the corresponding </code><code> (with its </code><code> field) is maintained by the Name Cache.</code></p>
<p><code> </code><code>::</code><br />
<code>   A </code><code> </code><code> is a special sort of </code><code> </code><code>, one that is completely known to the compiler (e.g. the </code><code> type constructor).  See [wiki:Commentary/Compiler/WiredIn].</code></p>
<p><code> The </code><code> field is just a boolean yes/no flag that identifies entities that are denoted by built-in syntax, such as </code><code> for the empty list.  These </code><code> aren't &quot;in scope&quot; as such, and we occasionally need to know that.</code></p>
<h2 id="entities-and">Entities and </h2>
<p>Here are the sorts of Name an entity can have:</p>
<p><code>* Class: always has an </code><code> Name. </code></p>
<p><code>* !TyCon: always has an </code><code> or </code><code> Name. </code></p>
<p><code>* !TyVar: can have </code><code>, or </code><code> Names; the former are ones arise from instantiating programmer-written type signatures.</code></p>
<p><code>* Ids: can have </code><code>, </code><code>, or </code><code> Names. </code><br />
<code>   * Before !CoreTidy, the Ids that were defined at top level in the original source program get </code><code> Names, whereas extra top-level bindings generated (say) by the type checker get </code><code> Names. This distinction is occasionally useful for filtering diagnostic output; e.g. for </code><code>. </code><br />
<code>   * After !CoreTidy: An Id with an </code><code> Name will generate symbols that appear as external symbols in the object file. An Id with an </code><code> Name cannot be referenced from outside the module, and so generates a local symbol in the object file. The !CoreTidy pass makes the decision about which names should be External and which Internal. </code></p>
<h1 id="native-code-generator-ncg">Native Code Generator (NCG)</h1>
<p>For other information related to this page, see:</p>
<p><code>* [wiki:BackEndNotes] for optimisation ideas regarding the current NCG</code><br />
<code>* [wiki:Commentary/Compiler/CmmType The Cmm language] (the NCG code works from Haskell's implementation of C-- and many optimisations in the NCG relate to Cmm)</code><br />
<code>* [wiki:Commentary/Compiler/Backends/NCG/RegisterAllocator The register allocator].</code></p>
<p>On some platforms (currently x86 and x86_64, with possibly bitrotted support for PowerPC and Sparc), GHC can generate assembly code directly. The NCG is enabled by default on supported platforms.</p>
<p>The NCG has always been something of a second-class citizen inside GHC, an unloved child, rather. This means that its integration into the compiler as a whole is rather clumsy, which brings some problems described below. That apart, the NCG proper is fairly cleanly designed, as target-independent as it reasonably can be, and so should not be difficult to retarget.</p>
<p>NOTE! The native code generator was largely rewritten as part of the C-- backend changes, around May 2004. Unfortunately the rest of this document still refers to the old version, and was written with relation to the CVS head as of end-Jan 2002. Some of it is relevant, some of it isn't.</p>
<h3 id="files-parts">Files, Parts</h3>
<p>After GHC has produced [wiki:Commentary/Compiler/CmmType Cmm] (use -ddump-cmm or -ddump-opt-cmm to view), the Native Code Generator (NCG) transforms Cmm into architecture-specific assembly code. The NCG is located in <a href="GhcFile(compiler/nativeGen)" class="uri" title="wikilink">GhcFile(compiler/nativeGen)</a> and is separated into eight modules:</p>
<p><code>* </code><a href="GhcFile(compiler/nativeGen/AsmCodeGen.lhs)" title="wikilink"><code>GhcFile(compiler/nativeGen/AsmCodeGen.lhs)</code></a><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  top-level module for the NCG, imported by </code><a href="GhcFile(compiler/main/CodeOutput.lhs)" title="wikilink"><code>GhcFile(compiler/main/CodeOutput.lhs)</code></a><code>; also defines the Monad for optimising generic Cmm code, </code><a href="BR" title="wikilink"><code>BR</code></a><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>* </code><a href="GhcFile(compiler/nativeGen/MachCodeGen.hs)" title="wikilink"><code>GhcFile(compiler/nativeGen/MachCodeGen.hs)</code></a><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  generates architecture-specific instructions (a Haskell-representation of assembler) from Cmm code</code><a href="BR" title="wikilink"><code>BR</code></a><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>* </code><a href="GhcFile(compiler/nativeGen/MachInstrs.hs)" title="wikilink"><code>GhcFile(compiler/nativeGen/MachInstrs.hs)</code></a><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  contains data definitions and some functions (comparison, size, simple conversions) for machine instructions, mostly carried out through the </code><code> data type, defined here</code><a href="BR" title="wikilink"><code>BR</code></a><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>* </code><a href="GhcFile(compiler/nativeGen/NCGMonad.hs)" title="wikilink"><code>GhcFile(compiler/nativeGen/NCGMonad.hs)</code></a><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  defines the the main monad in the NCG: the Native code Machine instruction Monad, </code><code>, and related functions.  </code><em><code>Note:</code> <code>the</code> <code>NCG</code> <code>switches</code> <code>between</code> <code>two</code> <code>monads</code> <code>at</code> <code>times,</code> <code>especially</code> <code>in</code> <code>:</code>  <code>and</code> <code>the</code>  <code>Monad</code> <code>used</code> <code>throughout</code> <code>the</code> <code>compiler.</code></em><a href="BR" title="wikilink"><code>BR</code></a><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>* </code><a href="GhcFile(compiler/nativeGen/PIC.hs)" title="wikilink"><code>GhcFile(compiler/nativeGen/PIC.hs)</code></a><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  handles generation of position independent code and issues related to dynamic linking in the NCG; related to many other modules outside the NCG that handle symbol import, export and references, including </code><code>, </code><code>, </code><code> and the RTS, and the Mangler</code><a href="BR" title="wikilink"><code>BR</code></a><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>* </code><a href="GhcFile(compiler/nativeGen/PprMach.hs)" title="wikilink"><code>GhcFile(compiler/nativeGen/PprMach.hs)</code></a><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  Pretty prints machine instructions (</code><code>) to assembler code (currently readable by GNU's </code><code>), with some small modifications, especially for comparing and adding floating point numbers on x86 architectures</code><a href="BR" title="wikilink"><code>BR</code></a><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>* </code><a href="GhcFile(compiler/nativeGen/RegAllocInfo.hs)" title="wikilink"><code>GhcFile(compiler/nativeGen/RegAllocInfo.hs)</code></a><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  defines the main register information function, </code><code>, which takes a set of real and virtual registers and returns the actual registers used by a particular </code><code>; register allocation is in AT&amp;T syntax order (source, destination), in an internal function, </code><code>; defines the </code><code> data type</code><a href="BR" title="wikilink"><code>BR</code></a><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>* </code><a href="GhcFile(compiler/nativeGen/RegisterAlloc.hs)" title="wikilink"><code>GhcFile(compiler/nativeGen/RegisterAlloc.hs)</code></a><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  one of the most complicated modules in the NCG, </code><code> manages the allocation of registers for each </code><em><code>basic</code> <code>block</code></em><code> of Haskell-abstracted assembler code: management involves </code><em><code>liveness</code></em><code> analysis, allocation or deletion of temporary registers, </code><em><code>spilling</code></em><code> temporary values to the </code><em><code>spill</code> <code>stack</code></em><code> (memory) and many optimisations.  ''See [wiki:Commentary/Compiler/CmmType The Cmm language] for the definition of a </code><em><code>basic</code> <code>block</code></em><code> (in Haskell, </code><em></em><code>).''</code></p>
<p>and one header file:</p>
<p><code>* </code><a href="GhcFile(compiler/nativeGen/NCG.h)" title="wikilink"><code>GhcFile(compiler/nativeGen/NCG.h)</code></a><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  defines macros used to separate architecture-specific code in the Haskell NCG files; since GHC currently only generates machine code for the architecture on which it was compiled (GHC is not currently a cross-compiler), the Haskell NCG files become considerably smaller after preprocessing; ideally all architecture-specific code would reside in separate files and GHC would have them available to support cross-compiler capabilities.</code></p>
<p>The NCG has <strong>machine-independent</strong> and <strong>machine-dependent</strong> parts.</p>
<p>The <strong>machine-independent</strong> parts relate to generic operations, especially optimisations, on Cmm code. The main machine-independent parts begin with <em>Cmm blocks.</em> (A <em>Cmm block</em> is a compilation unit of Cmm code, a file. See [wiki:Commentary/Compiler/CmmType The Cmm language] for a discussion of what a <em>Cmm block</em> is but note that <em>Cmm</em> is a type synonym for .) A machine-specific (assembler) instruction is represented as a . The machine-independent NCG parts:</p>
<p><code>1. optimise each Cmm block by reordering its basic blocks from the original order (the </code><code> order from the </code><code>) to minimise the number of branches between basic blocks, in other words, by maximising fallthrough of execution from one basic block to the next.</code><a href="BR" title="wikilink"><code>BR</code></a><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>1. lazily convert each Cmm block to abstract machine instructions (</code><code>) operating on an infinite number of registers--since the NCG Haskell files only contain instructions for the host computer on which GHC was compiled, these </code><code> are machine-specific; and,</code><a href="BR" title="wikilink"><code>BR</code></a><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>1. lazily allocate real registers for each basic block, based on the number of available registers on the target (currently, only the host) machine; for example, 32 integer and 32 floating-point registers on the PowerPC architecture.  The NCG does not currently have support for SIMD registers such as the vector registers for Altivec or any variation of SSE.</code><a href="BR" title="wikilink"><code>BR</code></a><em><code>Note</code></em><code>: if a basic block simultaneously requires more registers than are available on the target machine and the temporary variable needs to be used (would sill be </code><em><code>live</code></em><code>) after the current instruction, it will be moved (</code><em><code>spilled</code></em><code>) into memory.</code></p>
<p>The <strong>machine-dependent</strong> parts:</p>
<p><code>1. define the abstract (Haskell) assembler </code><code> for the target (host) machine and convert every Cmm block into it;</code><br />
<code>1. define, manage and allocate the real registers available on the target system;</code><br />
<code>1. pretty-print the Haskell-assembler to GNU AS (GAS) assembler code</code></p>
<h2 id="overview-3">Overview</h2>
<p>The top-level code generator function is  The returned `SDoc` is for debugging, so is empty unless you specify `-ddump-stix`. The `Pretty.Doc` bit is the final assembly code. Translation involves three main phases, the first and third of which are target-independent.</p>
<h4 id="translation-into-the-stix-representation">Translation into the Stix representation</h4>
<p>Stix is a simple tree-like RTL-style language, in which you can mention:</p>
<p><code>* An infinite number of temporary, virtual registers.</code><br />
<code>* The STG &quot;magic&quot; registers (`MagicId`), such as the heap and stack pointers.</code><br />
<code>* Literals and low-level machine ops (`MachOp`).</code><br />
<code>* Simple address computations.</code><br />
<code>* Reads and writes of: memory, virtual regs, and various STG regs.</code><br />
<code>* Labels and `if ... goto ...` style control-flow. </code></p>
<p>Stix has two main associated types:</p>
<p><code>* `StixStmt` -- trees executed for their side effects: assignments, control transfers, and auxiliary junk such as segment changes and literal data.</code><br />
<code>* `StixExpr` -- trees which denote a value. </code></p>
<p>Translation into Stix is almost completely target-independent. Needed dependencies are knowledge of word size and endianness, used when generating code to do deal with half-word fields in info tables. This could be abstracted out easily enough. Also, the Stix translation needs to know which `MagicId`s map to registers on the given target, and which are stored in offsets from `BaseReg`.</p>
<p>After initial Stix generation, the trees are cleaned up with constant-folding and a little copy-propagation (&quot;Stix inlining&quot;, as the code misleadingly calls it). We take the opportunity to translate `MagicId`s which are stored in memory on the given target, into suitable memory references. Those which are stored in registers are left alone. There is also a half-hearted attempt to lift literal strings to the top level in cases where nested strings have been observed to give incorrect code in the past.</p>
<p>Primitive machine-level operations will already be phrased in terms of `MachOp`s in the presented Abstract C, and these are passed through unchanged. We comment only that the `MachOp`s have been chosen so as to be easy to implement on all targets, and their meaning is intended to be unambiguous, and the same on all targets, regardless of word size or endianness.</p>
<p><strong>A note on `MagicId`s</strong>. Those which are assigned to registers on the current target are left unmodified. Those which are not are stored in memory as offsets from `BaseReg` (which is assumed to permanently have the value (`&amp;MainCapability.r`)), so the constant folder calculates the offsets and inserts suitable loads/stores. One complication is that not all archs have `BaseReg` itself in a register, so for those (sparc), we instead generate the address as an offset from the static symbol `MainCapability`, since the register table lives in there.</p>
<p>Finally, `BaseReg` does occasionally itself get mentioned in Stix expression trees, and in this case what is denoted is precisely (`&amp;MainCapability.r`), not, as in all other cases, the value of memory at some offset from the start of the register table. Since what it denotes is an r-value and not an l-value, assigning `BaseReg` is meaningless, so the machinery checks to ensure this never happens. All these details are taken into account by the constant folder.</p>
<h4 id="instruction-selection-1">Instruction selection</h4>
<p>This is the only majorly target-specific phase. It turns Stix statements and expressions into sequences of `Instr`, a data type which is different for each architecture. Instr, unsurprisingly, has various supporting types, such as `Reg`, `Operand`, `Imm`, etc. The generated instructions may refer to specific machine registers, or to arbitrary virtual registers, either those created within the instruction selector, or those mentioned in the Stix passed to it.</p>
<p>The instruction selectors live in `MachCode.lhs`. The core functions, for each target, are:</p>
<p></p>
<p>The insn selectors use the &quot;maximal munch&quot; algorithm. The bizarrely-misnamed `getRegister` translates expressions. A simplified version of its type is:  That is: it (monadically) turns a StixExpr into a sequence of instructions, and a register, with the meaning that after executing the (possibly empty) sequence of instructions, the (possibly virtual) register will hold the resulting value. The real situation is complicated by the presence of fixed registers, and is detailed below.</p>
<p>Maximal munch is a greedy algorithm and is known not to give globally optimal code sequences, but it is good enough, and fast and simple. Early incarnations of the NCG used something more sophisticated, but that is long gone now.</p>
<p>Similarly, `getAmode` translates a value, intended to denote an address, into a sequence of insns leading up to a (processor-specific) addressing mode. This stuff could be done using the general `getRegister` selector, but would necessarily generate poorer code, because the calculated address would be forced into a register, which might be unnecessary if it could partially or wholly be calculated using an addressing mode.</p>
<p>Finally, `assignMem_IntCode` and `assignReg_IntCode` create instruction sequences to calculate a value and store it in the given register, or at the given address. Because these guys translate a statement, not a value, they just return a sequence of insns and no associated register. Floating-point and 64-bit integer assignments have analogous selectors.</p>
<p>Apart from the complexities of fixed vs floating registers, discussed below, the instruction selector is as simple as it can be. It looks long and scary but detailed examination reveals it to be fairly straightforward.</p>
<h4 id="register-allocation-1">Register allocation</h4>
<p>The register allocator, `AsmRegAlloc.lhs` takes sequences of Instrs which mention a mixture of real and virtual registers, and returns a modified sequence referring only to real ones. It is gloriously and entirely target-independent. Well, not exactly true. Instead it regards `Instr` (instructions) and `Reg` (virtual and real registers) as abstract types, to which it has the following interface:  `insnFuture` is used to (re)construct the graph of all possible control transfers between the insns to be allocated. `regUsage` returns the sets of registers read and written by an instruction. And `patchRegs` is used to apply the allocator's final decision on virtual-to-real reg mapping to an instruction.</p>
<p>Clearly these 3 fns have to be written anew for each architecture. They are defined in `RegAllocInfo.lhs`. Think twice, no, thrice, before modifying them: making false claims about insn behaviour will lead to hard-to-find register allocation errors.</p>
<p>`AsmRegAlloc.lhs` contains detailed comments about how the allocator works. Here is a summary. The head honcho  takes a list of instructions and a list of real registers available for allocation, and maps as many of the virtual regs in the input into real ones as it can. The returned `Bool` indicates whether or not it was successful. If so, that's the end of it. If not, the caller of `allocUsingTheseRegs` will attempt spilling. More of that later. What `allocUsingTheseRegs` does is:</p>
<p><code>* Implicitly number each instruction by its position in the input list.</code><br />
<code>* Using `insnFuture`, create the set of all flow edges -- possible control transfers -- within this set of insns.</code><br />
<code>* Using `regUsage` and iterating around the flow graph from the previous step, calculate, for each virtual register, the set of flow edges on which it is live.</code><br />
<code>* Make a real-register committment map, which gives the set of edges for which each real register is committed (in use). These sets are initially empty. For each virtual register, attempt to find a real register whose current committment does not intersect that of the virtual register -- ie, is uncommitted on all edges that the virtual reg is live. If successful, this means the vreg can be assigned to the realreg, so add the vreg's set to the realreg's committment.</code><br />
<code>* If all the vregs were assigned to a realreg, use `patchInstr` to apply the mapping to the insns themselves. </code></p>
<h3 id="spilling">Spilling</h3>
<p>If `allocUsingTheseRegs` fails, a baroque mechanism comes into play. We now know that much simpler schemes are available to do the same thing and give better results. Anyways:</p>
<p>The logic above `allocUsingTheseRegs`, in `doGeneralAlloc` and `runRegAllocate`, observe that allocation has failed with some set R of real registers. So they apply `runRegAllocate` a second time to the code, but remove (typically) two registers from R before doing so. This naturally fails too, but returns a partially-allocated sequence. `doGeneralAlloc` then inserts spill code into the sequence, and finally re-runs `allocUsingTheseRegs`, but supplying the original, unadulterated R. This is guaranteed to succeed since the two registers previously removed from R are sufficient to allocate all the spill/restore instructions added.</p>
<p>Because x86 is very short of registers, and in the worst case needs three removed from R, a softly-softly approach is used. `doGeneralAlloc` first tries with zero regs removed from R, then if that fails one, then two, etc. This means `allocUsingTheseRegs` may get run several times before a successful arrangement is arrived at. `findReservedRegs` cooks up the sets of spill registers to try with.</p>
<p>The resulting machinery is complicated and the generated spill code is appalling. The saving grace is that spills are very rare so it doesn't matter much. I did not invent this -- I inherited it.</p>
<h3 id="dealing-with-common-cases-fast">Dealing with common cases fast</h3>
<p>The entire reg-alloc mechanism described so far is general and correct, but expensive overkill for many simple code blocks. So to begin with we use `doSimpleAlloc`, which attempts to do something simple. It exploits the observation that if the total number of virtual registers does not exceed the number of real ones available, we can simply dole out a new realreg each time we see mention of a new vreg, with no regard for control flow. `doSimpleAlloc` therefore attempts this in a single pass over the code. It gives up if it runs out of real regs or sees any condition which renders the above observation invalid (fixed reg uses, for example).</p>
<p>This clever hack handles the majority of code blocks quickly. It was copied from the previous reg-allocator (the Mattson/Partain/Marlow/Gill one).</p>
<h2 id="complications-observations-and-possible-improvements">Complications, observations, and possible improvements</h2>
<h3 id="real-vs-virtual-registers-in-the-instruction-selectors">Real vs virtual registers in the instruction selectors</h3>
<p>The instruction selectors for expression trees, namely `getRegister`, are complicated by the fact that some expressions can only be computed into a specific register, whereas the majority can be computed into any register. We take x86 as an example, but the problem applies to all archs.</p>
<p>Terminology: `rreg` means real register, a real machine register. `vreg` means one of an infinite set of virtual registers. The type `Reg` is the sum of `rreg` and `vreg`. The instruction selector generates sequences with unconstrained use of vregs, leaving the register allocator to map them all into rregs.</p>
<p>Now, where was I ? Oh yes. We return to the type of `getRegister`, which despite its name, selects instructions to compute the value of an expression tree. </p>
<p>At first this looks eminently reasonable (apart from the stupid name). `getRegister`, and nobody else, knows whether or not a given expression has to be computed into a fixed rreg or can be computed into any rreg or vreg. In the first case, it returns `Fixed` and indicates which rreg the result is in. In the second case it defers committing to any specific target register by returning a function from `Reg` to `InstrBlock`, and the caller can specify the target reg as it sees fit.</p>
<p>Unfortunately, that forces `getRegister`'s callers (usually itself) to use a clumsy and confusing idiom in the common case where they do not care what register the result winds up in. The reason is that although a value might be computed into a fixed rreg, we are forbidden (on pain of segmentation fault :) from subsequently modifying the fixed reg. This and other rules are record in &quot;Rules of the game&quot; inside `MachCode.lhs`.</p>
<p>Why can't fixed registers be modified post-hoc? Consider a simple expression like `Hp+1`. Since the heap pointer `Hp` is definitely in a fixed register, call it R, `getRegister` on subterm `Hp` will simply return Fixed with an empty sequence and R. But we can't just emit an increment instruction for R, because that trashes `Hp`; instead we first have to copy it into a fresh vreg and increment that.</p>
<p>With all that in mind, consider now writing a `getRegister` clause for terms of the form `(1 + E)`. Contrived, yes, but illustrates the matter. First we do `getRegister` on `E`. Now we are forced to examine what comes back.  This seems unreasonably cumbersome, yet the instruction selector is full of such idioms. A good example of the complexities induced by this scheme is shown by `trivialCode` for x86 in `MachCode.lhs`. This deals with general integer dyadic operations on x86 and has numerous cases. It was difficult to get right.</p>
<p>An alternative suggestion is to simplify the type of `getRegister` to this:  and then we could safely write  which is about as straightforward as you could hope for. Unfortunately, it requires `getRegister` to insert moves of values which naturally compute into an rreg, into a vreg. Consider:  On x86 the ccall result is returned in rreg `%eax`. The resulting sequence, prior to register allocation, would be:  If, as is likely, `%eax` is not held live beyond this point for any other purpose, the move into a fresh register is pointless; we'd have been better off leaving the value in `%eax` as long as possible.</p>
<p>The simplified `getRegister` story is attractive. It would clean up the instruction selectors significantly and make it simpler to write new ones. The only drawback is that it generates redundant register moves. I suggest that eliminating these should be the job of the register allocator. Indeed:</p>
<p><code>* There has been some work on this already (&quot;Iterated register coalescing&quot; ?), so this isn't a new idea.</code></p>
<p><code>* You could argue that the existing scheme inappropriately blurs the boundary between the instruction selector and the register allocator. The instruction selector should .. well .. just select instructions, without having to futz around worrying about what kind of registers subtrees get generated into. Register allocation should be </code><em><code>entirely</code></em><code> the domain of the register allocator, with the proviso that it should endeavour to allocate registers so as to minimise the number of non-redundant reg-reg moves in the final output. </code></p>
<h2 id="selecting-insns-for-64-bit-valuesloadsstores-on-32-bit-platforms">Selecting insns for 64-bit values/loads/stores on 32-bit platforms</h2>
<p>Note that this stuff doesn't apply on 64-bit archs, since the `getRegister` mechanism applies there. The relevant functions are: </p>
<p>`iselExpr64` is the 64-bit, plausibly-named analogue of `getRegister`, and `ChildCode64` is the analogue of `Register`. The aim here was to generate working 64 bit code as simply as possible. To this end, I used the simplified `getRegister` scheme described above, in which iselExpr64generates its results into two vregs which can always safely be modified afterwards.</p>
<p>Virtual registers are, unsurprisingly, distinguished by their `Unique`s. There is a small difficulty in how to know what the vreg for the upper 32 bits of a value is, given the vreg for the lower 32 bits. The simple solution adopted is to say that any low-32 vreg may also have a hi-32 counterpart which shares the same unique, but is otherwise regarded as a separate entity. `getHiVRegFromLo` gets one from the other.  Apart from that, 64-bit code generation is really simple. The sparc and x86 versions are almost copy-n-pastes of each other, with minor adjustments for endianness. The generated code isn't wonderful but is certainly acceptable, and it works.</p>
<h2 id="shortcomings-and-inefficiencies-in-the-register-allocator">Shortcomings and inefficiencies in the register allocator</h2>
<h3 id="redundant-reconstruction-of-the-control-flow-graph">Redundant reconstruction of the control flow graph</h3>
<p>The allocator goes to considerable computational expense to construct all the flow edges in the group of instructions it's allocating for, by using the `insnFuture` function in the `Instr` pseudo-abstract type.</p>
<p>This is really silly, because all that information is present at the abstract C stage, but is thrown away in the translation to Stix. So a good thing to do is to modify that translation to produce a directed graph of Stix straight-line code blocks, and to preserve that structure through the insn selector, so the allocator can see it.</p>
<p>This would eliminate the fragile, hacky, arch-specific `insnFuture` mechanism, and probably make the whole compiler run measurably faster. Register allocation is a fair chunk of the time of non-optimising compilation (10% or more), and reconstructing the flow graph is an expensive part of reg-alloc. It would probably accelerate the vreg liveness computation too.</p>
<h3 id="really-ridiculous-method-for-doing-spilling">Really ridiculous method for doing spilling</h3>
<p>This is a more ambitious suggestion, but ... reg-alloc should be reimplemented, using the scheme described in &quot;Quality and speed in linear-scan register allocation.&quot; (Traub?) For straight-line code blocks, this gives an elegant one-pass algorithm for assigning registers and creating the minimal necessary spill code, without the need for reserving spill registers ahead of time.</p>
<p>I tried it in Rigr, replacing the previous spiller which used the current GHC scheme described above, and it cut the number of spill loads and stores by a factor of eight. Not to mention being simpler, easier to understand and very fast.</p>
<p>The Traub paper also describes how to extend their method to multiple basic blocks, which will be needed for GHC. It comes down to reconciling multiple vreg-to-rreg mappings at points where control flow merges.</p>
<h3 id="redundant-move-support-for-revised-instruction-selector-suggestion">Redundant-move support for revised instruction selector suggestion</h3>
<p>As mentioned above, simplifying the instruction selector will require the register allocator to try and allocate source and destination vregs to the same rreg in reg-reg moves, so as to make as many as possible go away. Without that, the revised insn selector would generate worse code than at present. I know this stuff has been done but know nothing about it. The Linear-scan reg-alloc paper mentioned above does indeed mention a bit about it in the context of single basic blocks, but I don't know if that's sufficient.</p>
<h2 id="x86-arcana-that-you-should-know-about">x86 arcana that you should know about</h2>
<p>The main difficulty with x86 is that many instructions have fixed register constraints, which can occasionally make reg-alloc fail completely. And the FPU doesn't have the flat register model which the reg-alloc abstraction (implicitly) assumes.</p>
<p>Our strategy is: do a good job for the common small subset, that is integer loads, stores, address calculations, basic ALU ops (+, -, and, or, xor), and jumps. That covers the vast majority of executed insns. And indeed we do do a good job, with a loss of less than 2% compared with gcc.</p>
<p>Initially we tried to handle integer instructions with awkward register constraints (mul, div, shifts by non-constant amounts) via various jigglings of the spiller et al. This never worked robustly, and putting platform-specific tweaks in the generic infrastructure is a big No-No. (Not quite true; shifts by a non-constant amount are still done by a giant kludge, and should be moved into this new framework.)</p>
<p>Fortunately, all such insns are rare. So the current scheme is to pretend that they don't have any such constraints. This fiction is carried all the way through the register allocator. When the insn finally comes to be printed, we emit a sequence which copies the operands through memory (`%esp`-relative), satisfying the constraints of the real instruction. This localises the gruesomeness to just one place. Here, for example, is the code generated for integer divison of `%esi` by `%ecx`:  This is not quite as appalling as it seems, if you consider that the division itself typically takes 16+ cycles, whereas the rest of the insns probably go through in about 1 cycle each.</p>
<p>This trick is taken to extremes for FP operations.</p>
<p>All notions of the x86 FP stack and its insns have been removed. Instead, we pretend, to the instruction selector and register allocator, that x86 has six floating point registers, `%fake0` .. `%fake5`, which can be used in the usual flat manner. We further claim that x86 has floating point instructions very similar to SPARC and Alpha, that is, a simple 3-operand register-register arrangement. Code generation and register allocation proceed on this basis.</p>
<p>When we come to print out the final assembly, our convenient fiction is converted to dismal reality. Each fake instruction is independently converted to a series of real x86 instructions. `%fake0` .. `%fake5` are mapped to `%st(0)` .. `%st(5)`. To do reg-reg arithmetic operations, the two operands are pushed onto the top of the FP stack, the operation done, and the result copied back into the relevant register. When one of the operands is also the destination, we emit a slightly less scummy translation. There are only six `%fake` registers because 2 are needed for the translation, and x86 has 8 in total.</p>
<p>The translation is inefficient but is simple and it works. A cleverer translation would handle a sequence of insns, simulating the FP stack contents, would not impose a fixed mapping from `%fake` to `%st` regs, and hopefully could avoid most of the redundant reg-reg moves of the current translation.</p>
<p>There are, however, two unforeseen bad side effects:</p>
<p><code>* This doesn't work properly, because it doesn't observe the normal conventions for x86 FP code generation. It turns out that each of the 8 elements in the x86 FP register stack has a tag bit which indicates whether or not that register is notionally in use or not. If you do a FPU operation which happens to read a tagged-as-empty register, you get an x87 FPU (stack invalid) exception, which is normally handled by the FPU without passing it to the OS: the program keeps going, but the resulting FP values are garbage. The OS can ask for the FPU to pass it FP stack-invalid exceptions, but it usually doesn't.</code></p>
<p><code>Anyways: inside NCG created x86 FP code this all works fine. However, the NCG's fiction of a flat register set does not operate the x87 register stack in the required stack-like way. When control returns to a gcc-generated world, the stack tag bits soon cause stack exceptions, and thus garbage results.</code></p>
<p><code>The only fix I could think of -- and it is horrible -- is to clear all the tag bits just before the next STG-level entry, in chunks of code which use FP insns. `i386_insert_ffrees` inserts the relevant `ffree` insns into such code blocks. It depends critically on `is_G_instr` to detect such blocks.</code></p>
<p><code>* It's very difficult to read the generated assembly and reason about it when debugging, because there's so much clutter. We print the fake insns as comments in the output, and that helps a bit. </code></p>
<h2 id="generating-code-for-ccalls">Generating code for ccalls</h2>
<p>For reasons I don't really understand, the instruction selectors for generating calls to C (genCCall) have proven surprisingly difficult to get right, and soaked up a lot of debugging time. As a result, I have once again opted for schemes which are simple and not too difficult to argue as correct, even if they don't generate excellent code.</p>
<p>The sparc ccall generator in particular forces all arguments into temporary virtual registers before moving them to the final out-registers (`%o0` .. `%o5`). This creates some unnecessary reg-reg moves. The reason is explained in a comment in the code.</p>
<h2 id="duplicate-implementation-for-many-stg-macros">Duplicate implementation for many STG macros</h2>
<p>This has been discussed at length already. It has caused a couple of nasty bugs due to subtle untracked divergence in the macro translations. The macro-expander really should be pushed up into the Abstract C phase, so the problem can't happen.</p>
<p>Doing so would have the added benefit that the NCG could be used to compile more &quot;ways&quot; -- well, at least the 'p' profiling way.</p>
<h2 id="how-to-debug-the-ncg-without-losing-your-sanityhaircool">How to debug the NCG without losing your sanity/hair/cool</h2>
<p>Last, but definitely not least ...</p>
<p>The usual syndrome is that some program, when compiled via C, works, but not when compiled via the NCG. Usually the problem is fairly simple to fix, once you find the specific code block which has been mistranslated. But the latter can be nearly impossible, since most modules generate at least hundreds and often thousands of them.</p>
<p>My solution: cheat.</p>
<p>Because the via-C and native routes diverge only late in the day, it is not difficult to construct a 1-1 correspondence between basic blocks on the two routes. So, if the program works via C but not on the NCG, do the following:</p>
<p><code>* Recompile `AsmCodeGen.lhs` in the afflicted compiler with `-DDEBUG_NCG`, so that it inserts `___ncg_debug_markers` into the assembly it emits.</code><br />
<code>* Using a binary search on modules, find the module which is causing the problem.</code><br />
<code>* Compile that module to assembly code, with identical flags, twice, once via C and once via NCG. Call the outputs `ModuleName.s-gcc` and `ModuleName.s-nat`. Check that the latter does indeed have `___ncg_debug_markers` in it; otherwise the next steps fail.</code><br />
<code>* Build (with a working compiler) the program `utils/debugNCG/diff_gcc_nat`.</code><br />
<code>* Run: `diff_gcc_nat ModuleName.s`. This will construct the 1-1 correspondence, and emits on stdout a cppable assembly output. Place this in a file -- I always call it synth.S. Note, the capital S is important; otherwise it won't get cpp'd. You can feed this file directly to ghc and it will automatically get cpp'd; you don't have to do so yourself.</code><br />
<code>* By messing with the `#define`s at the top of `synth.S`, do a binary search to find the incorrect block. Keep a careful record of where you are in the search; it is easy to get confused. Remember also that multiple blocks may be wrong, which also confuses matters. Finally, I usually start off by re-checking that I can build the executable with all the `#define`s set to 0 and then all to 1. This ensures you won't get halfway through the search and then get stuck due to some snafu with gcc-specific literals. Usually I set `UNMATCHED_GCC` to 1 all the time, and this bit should contain only literal data. `UNMATCHED_NAT` should be empty. </code></p>
<p>`diff_gcc_nat` was known to work correctly last time I used it, in December 01, for both x86 and sparc. If it doesn't work, due to changes in assembly syntax, or whatever, make it work. The investment is well worth it. Searching for the incorrect block(s) any other way is a total time waster.</p>
<h2 id="historical-page-1">Historical page</h2>
<p>This page describes state of the new code generator sometime back in 2008. It is completely outdated and is here only for historical reasons. See [wiki:Commentary/Compiler/CodeGen Code Generator] page for a description of current code generator.</p>
<h1 id="overview-of-modules-in-the-new-code-generator">Overview of modules in the new code generator</h1>
<p>This page gives an overview of the new code generator, including discussion of:</p>
<p><code>* the [wiki:Commentary/Compiler/NewCodeGenModules#ThenewCmmdatatype new Cmm type]</code><br />
<code>* the [wiki:Commentary/Compiler/NewCodeGenModules#Modulestructureofthenewcodegenerator module structure of the new code generator]</code></p>
<p>See also [wiki:Commentary/Compiler/NewCodeGenPipeline the description of the new code generation pipeline].</p>
<h2 id="the-new-cmm-data-type">The new Cmm data type</h2>
<p>There is a new Cmm data type:</p>
<p><code>* </code><a href="GhcFile(compiler/cmm/ZipCfg.hs)" title="wikilink"><code>GhcFile(compiler/cmm/ZipCfg.hs)</code></a><code> contains a generic zipper-based control-flow graph data type.  It is generic in the sense that it's polymorphic in the type of </code><strong><code>middle</code> <code>nodes</code></strong><code> and </code><strong><code>last</code> <code>nodes</code></strong><code> of a block.  (Middle nodes don't do control transfers; last nodes only do control transfers.)  There are extensive notes at the start of the module.</code><a href="BR" title="wikilink"><code>BR</code></a><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>The key types it defines are:</code><br />
<code>  * Block identifiers: `BlockId`, `BlockEnv`, `BlockSet`</code><br />
<code>  * Control-flow blocks: `Block`</code><br />
<code>  * Control-flow graphs: `Graph`</code><a href="BR" title="wikilink"><code>BR</code></a><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>* </code><strong><code>`ZipDataFlow`</code></strong><code> contains a generic framework for solving dataflow problems over `ZipCfg`. It allows you to define a new optimization simply by defining a lattice of dataflow facts (akin to a specialized logic) and then writing the dataflow-transfer functions found in compiler textbooks. Handing these functions to the dataflow engine produces a new optimization that is not only useful on its own, but that can easily be composed with other optimizations to create an integrated &quot;superoptimization&quot; that is strictly more powerful than any sequence of individual optimizations, no matter how many times they are re-run.  The dataflow engine is based on </code><a href="http://citeseer.ist.psu.edu/old/lerner01composing.html"><code>(Lerner,</code> <code>Grove,</code> <code>and</code> <code>Chambers</code> <code>2002)</code></a><code>; you can find a functional implementation of the dataflow engine presented in </code><a href="http://www.cs.tufts.edu/~nr/pubs/zipcfg-abstract.html"><code>(Ramsey</code> <code>and</code> <code>Dias</code> <code>2005)</code></a><code>.</code><a href="BR" title="wikilink"><code>BR</code></a><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>* </code><strong><a href="GhcFile(compiler/cmm/ZipCfgCmmRep.hs)" title="wikilink"><code>GhcFile(compiler/cmm/ZipCfgCmmRep.hs)</code></a></strong><code> instantiates `ZipCfg` for Cmm, by defining types `Middle` and `Last` and using these to instantiate the polymorphic fields of `ZipCfg`.  It also defines a bunch of smart constructor (`mkJump`, `mkAssign`, `mkCmmIfThenElse` etc) which make it easy to build `CmmGraph`.</code><a href="BR" title="wikilink"><code>BR</code></a><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>* </code><strong><code>`CmmExpr`</code></strong><code> contains the data types for Cmm expressions, registers, and the like. Here is a fuller description of these types is at [wiki:Commentary/Compiler/BackEndTypes]. It does not depend on the dataflow framework at all.  </code></p>
<h2 id="module-structure-of-the-new-code-generator">Module structure of the new code generator</h2>
<p>The new code generator has a fair number of modules, which can be split into three groups:</p>
<p><code>* basic datatypes and infrastructure</code><br />
<code>* analyses and transformations</code><br />
<code>* linking the pipeline</code></p>
<p>All the modules mentioned are in the `cmm/` directory, unless otherwise indicated.</p>
<h3 id="basic-datatypes-and-infrastructure">Basic datatypes and infrastructure</h3>
<p>Ubiquitous types:</p>
<p><code>* `CLabel` (`CLabel`): All sorts of goo for making and manipulating labels.</code></p>
<p><code>* `BlockId` (`BlockId`, `BlockEnv`, `BlockSet`):</code><br />
<code>  The type of a basic-block id, along with sets and finite maps.</code></p>
<p><code>* `CmmExpr` (`CmmType`, `LocalReg`, `GlobalReg`, `Area`, `CmmExpr`):</code><br />
<code>  Lots of type definitions: for Cmm types (bit width, GC ptr, float, etc),</code><br />
<code>  registers, stack areas, and Cmm expressions.</code></p>
<p><code>* `Cmm` (`GenCmm`, `CmmInfo`, `CmmInfoTable`):</code><br />
<code>  More type definitions: the parameterized top-level Cmm type (`GenCmm`),</code><br />
<code>  along with the type definitions for info tables.</code></p>
<p>Control-flow graphs:</p>
<p><code>* `ZipCfg` (`Graph`, `LGraph`, `Block`):</code><br />
<code>  Describes a zipper-like representation for true basic-block</code><br />
<code>  control-flow graphs.  A block has a single entry point,</code><br />
<code>  which is a always a label, followed by zero or mode 'middle</code><br />
<code>  nodes', each of which represents an uninterruptible</code><br />
<code>  single-entry, single-exit computation, then finally a 'last</code><br />
<code>  node', which may have zero or more successors.</code><br />
<code>  `ZipCFG` is polymorphic in the type of middle and last nodes.</code><br />
<code>* `ZipCfgCmmRep` (`Middle`, `Last`, `CmmGraph`)</code><br />
<code>  Types to instantiate `ZipCfg` for C--: middle and last nodes,</code><br />
<code>  and a bunch of abbreviations of types in `ZipCfg` and `Cmm`.</code></p>
<p><code>* `MkZipCfg` (`AGraph`, `mkLabel`, `mkMiddle`, `mkBranch`)</code><br />
<code>  Smart constructors for control-flow graphs (and the constructors have</code><br />
<code>  non-monadic types).</code><br />
<code>  Like `ZipCfg`, `MkZipCfg` is polymorphic in the types of middle and last nodes.</code><br />
<code>* `MkZipCfgCmm` (`mkNop`, `mkAssign`, `mkStore`, `mkCall`, ...)</code><br />
<code>  Smart constructors for creating middle and last nodes in</code><br />
<code>  control-flow graphs (and the constructors have non-monadic types).</code></p>
<p>Calling conventions:</p>
<p><code>* `CmmInfo` (`cmmToRawCmm`, `mkBareInfoTable`):</code><br />
<code>  Converts Cmm code to &quot;raw&quot; Cmm.  What this means is: convert a `CmmInfo` data structure describing the info table for each `CmmProc` to a `[CmmStatic]`. </code><br />
<code>  `mkBareInfoTable` is the workhorse that produces the `[CmmStatic]`.  It is also used to produce the info table required for safe foreign calls (a middle node).</code><br />
<code>* `CmmCallConv` (`ArgumentFormat`, `assignArgumentsPos`):</code><br />
<code>  Implements Cmm calling conventions: given arguments and a calling convention,</code><br />
<code>  this module decides where to put the arguments.</code><br />
<code>  (JD: Crufty. Lots of old code in here, needs cleanup.)</code></p>
<p>Dataflow analysis:</p>
<p><code>* `CmmTx` (`Tx`, `TxRes`):</code><br />
<code>  A simple monad for tracking when a transformation has</code><br />
<code>  occurred (something has changed).</code><br />
<code>  Used by the dataflow analysis to keep track of when the graph is rewritten.</code></p>
<p><code>* `OptimizationFuel` (`OptimizationFuel`, `FuelMonad`, `maybeRewriteWithFuel`)</code><br />
<code>  We can use a measure of &quot;fuel&quot; to limit the number of rewrites performed</code><br />
<code>  by a transformation. This module defines a monad for tracking (and limiting)</code><br />
<code>  fuel use.</code><br />
<code>  (JD: Largely untested.)</code></p>
<p><code>* `DFMonad` (`DataflowLattice`, `DataflowAnalysis`, `runDFM`):</code><br />
<code>  Defines the type of a dataflow lattice and an analysis.</code><br />
<code>  Defines the monad used by the dataflow framework.</code><br />
<code>  The monad keeps track of dataflow facts, along with fuel,</code><br />
<code>  and it can provide unique id's.</code><br />
<code>  All in support of the dataflow module.</code></p>
<p><code>* `ZipDataflow` (`ForwardTransfers`, `BackwardTransfers`, `ForwardRewrites`, `BackwardRewrites`,</code><br />
<code>  `zdfSolveFrom`, `zdfRewriteFrom`, etc)</code><br />
<code>  This module implements the Lerner/Grove/Chambers dataflow analysis frameword.</code><br />
<code>  Given the definitions of a lattice and dataflow transfer/rewrite functions,</code><br />
<code>  this module provides all the work of running the dataflow analysis and transformation.</code><br />
<code>  A number of the phases of the back end rely on this code,</code><br />
<code>  and hopefully more optimizations will target it in the future.</code></p>
<p>And a few basic utilities:</p>
<p><code>* `CmmZipUtil`: (JD: Unused, I believe, but probably should be used in a few places.)</code><br />
<code>  A few utility functions for manipulating a zipcfg.</code><br />
<code>* `PprC`:   Prettyprinting to generate C code.</code><br />
<code>* `PprCmm`: Prettyprinting the C-- code.</code><br />
<code>* `PprCmmZ`: (JD: Unused, I believe.)</code><br />
<code>  Prettyprinting functions related to `ZipCfg` and `ZipCfgCmm`.</code></p>
<h3 id="analyses-and-transformations">Analyses and transformations</h3>
<p><code>* `CmmLint` (`cmmLint`, `cmmLintTop`):</code><br />
<code>  Some sanity checking on the old Cmm graphs.</code><br />
<code>  Not sure how effective this is.</code><br />
<code>* `CmmLiveZ` (`CmmLive`, `livelattice`, `cmmLivenessZ`):</code><br />
<code>  Liveness analysis for registers (uses dataflow framework).</code><br />
<code>* `CmmProcPointZ` (`ProcPointSet`, `callProcPoints`, `minimalProcPointSet`,</code><br />
<code>  `procPointAnalysis`, `splitAtProcPoints`)</code><br />
<code>  A proc point is a block in a control-flow graph that must be the</code><br />
<code>  entry point of a new procedure when we generate C code.</code><br />
<code>  For example, successors of calls and joinpoints that follow calls</code><br />
<code>  are procpoints.</code><br />
<code>  This module provides the analyses to find procpoints, as well as</code><br />
<code>  the transformation to split the procedure into pieces.</code><br />
<code>  The procpoint analysis doesn't use the dataflow framework,</code><br />
<code>  but it really should - dominators are the way forward.</code><br />
<code>* `CmmSpillReload` (`DualLive`, `dualLiveLattice`, `dualLiveness`,</code><br />
<code>  `dualLivenessWithInsertion`, `insertLateReloads`,</code><br />
<code>  `removeDeadAssignmentsAndReloads`):</code><br />
<code>  Inserts spills and reloads to establish the invariant that</code><br />
<code>  at a safe call, there are no live variables in registers.</code><br />
<code>* `CmmCommonBlockElimZ` (`elimCommonBlocks`):</code><br />
<code>  Find blocks in the CFG that are identical; merge them.</code><br />
<code>* `CmmContFlowOpt` (`branchChainElimZ`, `removeUnreachableBlocksZ`,</code><br />
<code>  `runCmmOpts`):</code><br />
<code>  Branch-chain elimination and elimination of unreachable code.</code><br />
<code>* `CmmStackLayout` (`SlotEnv`, `liveSlotAnal`, `manifestSP`, `stubSlotsOnDeath`):</code><br />
<code>  The live-slot analysis discovers which stack slots are live</code><br />
<code>  at each basic block.</code><br />
<code>  We use the results for two purposes:</code><br />
<code>  stack layout (manifestSP) and info tables (in !CmmBuildInfoTables).</code><br />
<code>  The function `stubSlotsOnDeath' is used as a debugging pass:</code><br />
<code>  it stubs each stack slot when it dies, hopefully causing bad</code><br />
<code>  programs to fail faster.</code><br />
<code>* `CmmBuildInfoTables` (`CAFEnv`, `cafAnal`, `lowerSafeForeignCalls`,</code><br />
<code>  `setInfoTableSRT`, `setInfoTableStackMap`):</code><br />
<code>  This module is responsible for building info tables.</code><br />
<code>  Specifically, it builds the maps of live variables (stack maps)</code><br />
<code>  and SRTs.</code><br />
<code>  It also has code to lower safe foreign calls into a sequence</code><br />
<code>  that makes them safe (but suspending and resuming threads very carefully).</code><br />
<code>  (JD: The latter function probably shouldn't be here.)</code></p>
<h3 id="linking-the-pipeline">Linking the pipeline</h3>
<p><code>* `CmmCvt`: Converts between `Cmm` and `ZipCfgCmm` representations.</code><br />
<code>  (JD: The Zip -&gt; Cmm path definitely works; haven't tried the</code><br />
<code>  other in a long time -- there's no reason to use it with</code><br />
<code>  the new Stg -&gt; Cmm path).</code><br />
<code>* `CmmCPSZ`: Links the phases of the back end in sequence, along with</code><br />
<code>  some possible debugging output.</code></p>
<h3 id="dead-code">Dead code</h3>
<p><code>* `CmmCPSGen`, `CmmCPS` (Michael Adams), `CmmBrokenBlock`, `CmmLive`, `CmmPprCmmZ`, `StackColor`, `StackPlacements`</code></p>
<h2 id="historical-page-2">Historical page</h2>
<p>This page stores historical information about Cmm Pipeline in the new code generator. This description has been updated and is maintained on the [wiki:Commentary/Compiler/CodeGen Code Generator] page. This page has also historical notes about Adams optimisation. That optimisation is also described in Note [sharing continuations] in <a href="GhcFile(compiler/codeGen/StgCmmMonad.hs)" class="uri" title="wikilink">GhcFile(compiler/codeGen/StgCmmMonad.hs)</a> and probably deserves its own wiki page.</p>
<h1 id="design-of-the-new-code-generator">Design of the new code generator</h1>
<p>This page contains notes about the design of the new code generator. See also: [wiki:Commentary/Compiler/NewCodeGenModules overview of the module structure in the new code generator].</p>
<h2 id="overview-4">Overview</h2>
<p>Code generation now has three stages:</p>
<p><code> 1. Convert STG to Cmm, with implicit stack implicit, and native Cmm calls.</code><br />
<code> 2. Optimise the Cmm, and CPS-convert it to have an explicit stack, and no native calls.</code><br />
<code>    This part of the pipeline is stitched together in `cmm/CmmPipeline.hs`.</code><br />
<code> 3. Feed the CPS-converted Cmm to the existing, unmodified native code generators.</code></p>
<p>Ultimately our plan is to expand the capability of the new pipeline so that it does native code generation too, and we can ultimately discard the existing code generators. The design of this stage is here: [wiki:Commentary/Compiler/IntegratedCodeGen]</p>
<h2 id="the-cmm-pipeline">The Cmm pipeline</h2>
<p>The first two steps are described in more detail here:</p>
<p><code>* </code><strong><code>Code</code> <code>generator</code></strong><code> converts STG to `CmmGraph`.  Implemented in `StgCmm*` modules (in directory `codeGen`). </code><br />
<code>  * `Cmm.CmmGraph` is pretty much a Hoopl graph of `CmmNode.CmmNode` nodes. Control transfer instructions are always the last node of a basic block.</code><br />
<code>  * Parameter passing is made explicit; the calling convention depends on the target architecture.  The key function is `CmmCallConv.assignArgumentsPos`. </code><br />
<code>    * Parameters are passed in virtual registers R1, R2 etc. [These map 1-1 to real registers.] </code><br />
<code>    * Overflow parameters are passed on the stack using explicit memory stores, to locations described abstractly using the [wiki:Commentary/Compiler/StackAreas </code><em><code>Stack</code> <code>Area</code></em><code> abstraction.].   </code><br />
<code>    * Making the calling convention explicit includes an explicit store instruction of the return address, which is stored explicitly on the stack in the same way as overflow parameters. This is done (obscurely) in `MkGraph.mkCall`.</code></p>
<p><code>* </code><strong><code>Simple</code> <code>control</code> <code>flow</code> <code>optimisation</code></strong><code>, implemented in `CmmContFlowOpt`.  It's called both at the beginning and end of the pipeline.</code><br />
<code>  * Branch chain elimination.</code><br />
<code>  * Remove unreachable blocks.</code><br />
<code>  * Block concatenation.  branch to K; and this is the only use of K.  </code></p>
<p><code>* </code><strong><code>More</code> <code>control</code> <code>flow</code> <code>optimisations</code></strong><code>.</code><br />
<code>  * Common Block Elimination (like CSE). This essentially implements the Adams optimisation, we believe.</code><br />
<code>  * Consider (sometime): block duplication.  branch to K; and K is a short block.  Branch chain elimination is just a special case of this.</code></p>
<p><code>* </code><strong><code>Proc-point</code> <code>analysis</code></strong><code> and </code><strong><code>transformation</code></strong><code>, implemented in `CmmProcPoint`. The transformation part adds a function prologue to the front of each proc-point, following a standard entry convention.</code><br />
<code>   * The analysis produces a set of `BlockId` that should become proc-points</code><br />
<code>   * The transformation inserts a function prologue at the start of each proc-point, and a function epilogue just before each branch to a proc-point.</code></p>
<p><code>* </code><strong><code>(OUTDATED</code> <code>-</code> <code>!CmmSpillReload</code> <code>does</code> <code>not</code> <code>exist</code> <code>any</code> <code>more)</code></strong><code> </code><strong><code>Add</code> <code>spill/reload</code></strong><code>, implemented in `CmmSpillReload`, to spill live C-- variables before a call and reload them afterwards.  The spill and reload instructions are simply memory stores and loads respectively, using symbolic stack offsets (see [wiki:Commentary/Compiler/StackAreas#Layingoutthestack stack layout]).  For example, a spill of variable 'x' would look like `Ptr32[SS(x)] = x`.</code><br />
<code>  * `dualLivenessWithInsertion` does two things:</code><br />
<code>    * Spills at the definition of any variable that is subequently live across a call (uses a backward analysis)</code><br />
<code>    * Adds a reload at each return (or proc) point</code><br />
<code>  At this point, no (`LocalReg`) variables are live across a call.</code><br />
<code>  * TODO: avoid  `f();g()` turning into `spill x; f(); reload x; spill x; g(); reload x`.</code></p>
<p><code>* </code><strong><code>(OUTDATED</code> <code>-</code> <code>!CmmRewriteAssignments</code> <code>is</code> <code>not</code> <code>used</code> <code>any</code> <code>more)</code></strong><code> </code><strong><code>Rewrite</code> <code>assignments</code></strong><code> (assignments to local regs, that is, not stores). </code><br />
<code>  * Convert graph to annotated graph whose nodes are `CmmRewriteAssignments.WithRegUsage`.  Specifically, `CmmAssign` is decorated with a flag `RegUsage` saying whether it is used once or many times.</code><br />
<code>  * Sink or inline assignments nearer their use points</code><br />
<code>  * Do constant mach-op folding. This is done in this phase, because folded mach-ops can be inlined, and inlining exposes opportunities for mach-op folding.</code></p>
<p><code>* </code><strong><code>Remove</code> <code>dead</code> <code>assignments</code> <code>and</code> <code>stores</code></strong><code>, implemented in `CmmLive`, removes assignments to dead variables and things like ``a = a`` or ``I32[Hp] = I32[Hp]``. The latter may more appropriately be done in a general optimization pass, as it doesn't take advantage of liveness information.</code></p>
<p><code>* </code><strong><code>Figure</code> <code>out</code> <code>the</code> <code>stack</code> <code>layout</code></strong><code>, implemented in `CmmStackLayout`.</code><br />
<code>  * Each variable 'x', and each proc-point label 'K', has an associated </code><em><code>Area</code></em><code>, written SS(x) and SS(k) resp, that names a contiguous portion of the stack frame.  </code><br />
<code>  * The stack layout pass produces a mapping of: </code><em><code>(`Area`</code> <code>-&gt;</code> <code>`StackOffset`)</code></em><code>. For more detail, see [wiki:Commentary/Compiler/StackAreas#Layingoutthestack the description of stack layout.]</code><br />
<code>  * A `StackOffset` is the byte offset of a stack slot from the old end (high address) of the frame.  It doesn't vary as the physical stack pointer moves.</code></p>
<p><code>* </code><strong><code>Manifest</code> <code>the</code> <code>stack</code> <code>pointer</code></strong><code>, implemented in `CmmStackLayout`.  Once the stack layout mapping has been determined, a second pass walks over the graph, making the stack pointer, `Sp` explicit. Before this pass, there is no `Sp` at all.  After this, `Sp` is completely manifest.</code><br />
<code>  * replacing references to `Areas` with offsets from `Sp`.</code><br />
<code>  * adding adjustments to `Sp`.</code><br />
<br />
<code>* </code><strong><code>Split</code> <code>into</code> <code>multiple</code> <code>!CmmProcs</code></strong><code>, implemented in `CmmProcPointZ`.  At this point we build an info-table for each of the !CmmProcs, including SRTs.  Done on the basis of the live local variables (by now mapped to stack slots) and live CAF statics.</code><br />
<code>  * `LastCall` and `LastReturn` nodes are replaced by `Jump`s.</code></p>
<p><code>* </code><strong><code>Build</code> <code>info</code> <code>tables</code></strong><code>, implemented in `CmmBuildInfoTables`..  </code><br />
<code>  * Find each safe `MidForeignCall` node, &quot;lowers&quot; it into the suspend/call/resume sequence (see `Note [Foreign calls]` in `CmmNode.hs`.), and build an info table for them.</code><br />
<code>  * Convert the `CmmInfo` for each `CmmProc` into a `[CmmStatic]`, using the live variable information computed just before &quot;Figure out stack layout&quot;.  </code></p>
<h3 id="branches-to-continuations-and-the-adams-optimisation">Branches to continuations and the &quot;Adams optimisation&quot;</h3>
<p>A GC block for a heap check after a call should only take one or two instructions. However the natural code:  The label M is the head of the call-gc-and-try-again loop. If we do this, we'll generate two info tables, one for L and one for K.</p>
<p>We can do better like this: </p>
<p>Now the  call has the same return signature as  and can use the same continuation, thus:  Now we can coalesce the uniquely-used block M into L, thus:  (A call followed by a  thus gets optimized down to just the call.)</p>
<p>Now things are good. Simple common block elimination (CBE) will common up K and L, so both calls share the same info table.</p>
<h2 id="runtime-system">Runtime system</h2>
<p><code>* </code><strong><code>Garbage</code> <code>collector</code> <code>entry</code> <code>points</code></strong><code>: see `Note [Heap checks]` in `StgCmmHeapery`.</code></p>
<p><code>* </code><strong><code>PAPs</code></strong><br />
<br />
<code>* </code><strong><code>Update</code> <code>frames</code></strong><code> and </code><strong><code>exception</code> <code>handling</code></strong><code>.  Also STM frames.</code></p>
<p><code>* </code><strong><code>Primitives</code></strong><code> can be rewritten:</code><br />
<code>  * Use parameters</code><br />
<code>  * In a few cases, use native calls (notably eval)</code></p>
<h1 id="note-historical-page">NOTE: Historical page</h1>
<p>This page is here for historical reasons. Most of the issues described here are now fixed (2 Aug 2012), and the new code generator produces code approximately as good as the old code generator. Any remaining issues will be made into tickets as necessary. See [wiki:Commentary/Compiler/CodeGen Code Generator] page for an up-to-date description of the current code generator.</p>
<h1 id="stupidity-in-the-new-code-generator">Stupidity in the New Code Generator</h1>
<p>Presently compiling using the new code generator results in a fairly sizable performance hit, because the new code generator produces sub-optimal (and sometimes absolutely terrible code.) There are <a href="http://darcs.haskell.org/ghc/compiler/cmm/cmm-notes">a lot of ideas for how to make things better</a>; the idea for this wiki page is to document all of the stupid things the new code generator is doing, to later be correlated with specific refactorings and fixes that will hopefully eliminate classes of these stupid things. The hope here is to develop a sense for what the most endemic problems with the newly generated code is.</p>
<h2 id="cantankerous-comparisons">Cantankerous Comparisons</h2>
<p>FIXED in newcg branch, 15/2/2012</p>
<p>In `cgrun065` we have</p>
<p></p>
<p>Which compiles to the nice STG code</p>
<p></p>
<p>But the comparison is compiled into stupid code:</p>
<p></p>
<p>etc.</p>
<p>We're actually converting to a `Bool` and then doing an algebraic case! This is a StgCmm issue, not a pipeline issue.</p>
<h2 id="dead-stackheap-checks">Dead stack/heap checks</h2>
<p>FIXED in newcg branch, but in an ad-hoc way (the stack allocator does it). We probably want to do this as part of a more general optimisation pass.</p>
<p>See in `cgrun065`</p>
<p></p>
<h2 id="instruction-reordering">Instruction reordering</h2>
<p>NEW. We should be able to reorder instructions in order to decrease register pressure. Here's an example from 3586.hs</p>
<p></p>
<p>R1 and Sp probably don't clobber each other, so we ought to use _cPY twice in quick succession. Fortunately stg_IND_STATIC_info is a constant so in this case the optimization doesn't help to much, but in other cases it might make sense. TODO Find better example</p>
<h2 id="stack-space-overuse">Stack space overuse</h2>
<p>FIXED in the newcg branch. (stack layout algorithm redesigned)</p>
<p>CONFIRMED. `T1969.hs` demonstrates this:</p>
<p></p>
<p>The call area for the jump in cbG is using an extra word on the stack, but in fact Sp + 0 at the entry of the function immediately becomes dead after the assignment, so we ought to be able to save some space in our layout. Simon Marlow suggests we distinguish between the return address and the old call area; however, since this can also happen for the return parameters from call areas, we need a more general scheme.</p>
<p>After I discussed this with SPJ, we've decided that we need to teach the stack layout how to handle partial conflicts. There is a complication here, in that if we do this naively, the interference graph will blow up (since, rather than conflicting call areas, we now have conflicting words of call areas.) Simon suggested that we bound the amount of conflicts we track: either up to 3 or conflict with everything (in which case we just place the area as far down as necessary rather than try to be clever.) I plan on doing this once I understand the current layout code...</p>
<h2 id="double-temp-use-means-no-inlinining">Double temp-use means no inlinining?</h2>
<p>CONFIRMED. Here's a curious piece of code that fails to get inlined (from `cc004`):</p>
<p></p>
<p>Why is that? Because the temp gets reused later on:</p>
<p></p>
<p>In this case, we want more aggressive inlining because there are too many temps and they're going to have to get spilled to the stack anyway. IS THAT TRUE? For comparison's sake, the old codegen doesn't appear to do any rewriting, because it just reuses the call area.</p>
<h2 id="stupid-spills">Stupid spills</h2>
<p>CONFIRMED. If something is already in memory, why do we have to spill it again?</p>
<p></p>
<p>Well, it's because the spiller isn't clever enough:</p>
<p></p>
<p>Ick! The old codegen was much better...</p>
<p></p>
<p>The trouble is that the spiller doesn't know that the old call area is also valid game for locations that variables can live in. So, the solution is to rewrite the spiller to know about existing incoming memory locations. Make sure that this information gets to the stack layout engine when we do partial layouts (it should automatically notice, but double check!)</p>
<h2 id="noppy-proc-points">Noppy proc-points</h2>
<p>CONFIRMED. Consider</p>
<p></p>
<p></p>
<p>We generate an extra proc-point for ``cmM``, where in theory we ought to be able to stick the subsequent ``stg_ap_pp_fast`` onto the stack as another return point.</p>
<h2 id="lots-of-temporary-variables">Lots of temporary variables</h2>
<p>WONTFIX. Lots of temporary variables (these can tickle other issues when the temporaries are long-lived, but otherwise would be optimized away). You can at least eliminate some of them by looking at the output of `-ddump-opt-cmm`, which utilizes some basic temporary inlining when used with the native backend `-fasm`, but this doesn't currently apply to the GCC or LLVM backends.</p>
<p>~~At least one major culprit for this is `allocDynClosure`, described in Note `Return a LocalReg`; this pins down the value of the `CmmExpr` to be something for one particular time, but for a vast majority of use-cases the expression is used immediately afterwards. Actually, this is mostly my patches fault, because the extra rewrite means that the inline pass is broken.~~ Fixed in latest version of the pass; we don't quite manage to inline enough but there's only one extra temporary.</p>
<p>Another cause of all of these temporary variables is that the new code generator immediately assigns any variables that were on the stack to temporaries immediately upon entry to a function. This is on purpose. The idea is we optimize these temporary variables away.</p>
<h2 id="double-proc-points">Double proc points</h2>
<p>FIXED in newcg branch.</p>
<p>Given a simple case expression</p>
<p></p>
<p>we generate *two* proc points, not one.</p>
<p></p>
<p>Both `cbE` and `cbW` are going to become proc points.</p>
<p>To avoid it we should generate code that re-uses `cbE` as the destination for the first `if`; that is, we need to load up the registers as if we were returning from the call. This needs some refactoring in the code generator.</p>
<h2 id="rewriting-stacks">Rewriting stacks</h2>
<p>FIXED. `3586.hs` emits the following code:</p>
<p></p>
<p>We see that these temporary variables are being repeatedly rewritten to the stack, even when there are no changes.</p>
<p>Since these areas on the stack are all old call areas, one way to fix this is to inline all of the memory references. However, this has certain undesirable properties for other code, so we need to be a little more clever. The key thing to notice is that these accesses are only used once per control flow path, in which case sinking the loads down and then inlining them should be OK (it will increase code size but not execution time.) However, the other difficulty is that the CmmOpt inliner, as it stands, won't inline things that look like this because although the variable is only used once in different branches, the same name is used, so it can't distinguish between the temporaries with mutually exclusive live ranges. Building a more clever inliner with Hoopl is also a bit tricky, because inlining is a forward analysis/transformation, but usage counting is a backwards analysis.</p>
<p>This looks fixed with the patch from April 14.</p>
<h2 id="spilling-hpsp">Spilling Hp/Sp</h2>
<p>FIXED. `3586.hs` emits the following code:</p>
<p></p>
<p>We see `Hp - 4` being allocated to a temp, and then consequently being spilled to the stack even though `newCAF` definitely will not change `Hp`, so we could have floated the expression down.</p>
<p>This seems to happen whenever there's a `newCAF` ccall.</p>
<p>We also seem to reload these values multiple times.</p>
<p></p>
<p>~~We need to not spill across certain foreign calls, but for which calls this is OK for is unclear.~~ Variables stay live across all unsafe foreign calls (foreign calls in the middle), except for the obvious cases (the return registers), so no spilling should happen at all. The liveness analysis is too conservative.</p>
<p>This is not fixed in the April 14 version of the patch... we still need to fix the liveness analysis? I thought I fixed that... that's because the transform did extra spilling for CmmUnsafeForeignCalls. Removed that code, and now it's fixed.</p>
<h2 id="up-and-down">Up and Down</h2>
<p>FIXED. A frequent pattern is the stack pointer being bumped up and then back down again, for no particular reason.</p>
<p></p>
<p>This is mentioned at the very top of `cmm-notes`. This was a bug in the stack layout code that I have fixed.</p>
<h2 id="sp-is-generally-stupid">Sp is generally stupid</h2>
<p>FIXED. Here is an optimized C-- sample from `arr016.hs`.</p>
<p></p>
<p>Compare with the old code:</p>
<p></p>
<p>You can see the up and down behavior here, but that's been fixed, so ignore it for now. (Update the C--!) The unfixed problem is this (some of the other problems were already addressed): we do an unnecessary stack check on entry to this function. We should eliminate the stack check (and by dead code analysis, the GC call) in such cases.</p>
<p>This pattern essentially happens for every function, since we always assign incoming parameters to temporary variables before doing anything.</p>
<h2 id="heap-and-r1-aliasing">Heap and R1 aliasing</h2>
<p>FIXED. Values on the heap and values from R1 don't necessarily clobber each other. allocDynClosure seems like a pretty safe bet they don't. But is this true in general? ANSWER: Memory writes with Hp are always new allocations, so they don't clobber anything.</p>
<p></p>
<h2 id="historical-page-3">Historical page</h2>
<p>This page stores notes about progress of work on the &quot;new&quot; code generator. This page is here for historical reasons. See [wiki:Commentary/Compiler/CodeGen Code Generator] page for an up-to-date description of the current code generator.</p>
<h1 id="ghcs-glorious-new-code-generator">GHC's glorious new code generator</h1>
<p>This page summarises work that Norman Ramsey, Simon M, Simon PJ, and John Dias are doing on re-architecting GHC's back end. Here is the state of play; see also [wiki:Commentary/Compiler/Backends/LLVM work on the LLVM back end].</p>
<p><code>* Bug list (code-gen related bugs that we may be able to fix):</code><br />
<code>  * #1498 (avoid redundant heap check on the fast path)</code><br />
<code>  * #3552 (unreachable code)</code><br />
<code>  * #3462 (a feature)</code><br />
<code>  * #2249</code><br />
<code>  * #2253</code><br />
<code>  * #2289</code><br />
<code>  * #7219 (reinstate constant-prop)</code><br />
<code>  * #7213 (massive array)</code></p>
<p><code>* (Sept 12) New code generator is live.  Here's the [wiki:Commentary/Compiler/NewCodeGen/Cleanup page listing clean-up tasks] that we can now do.</code></p>
<p><code>* Simon M added a [blog:newcg-update Blog Post] about the new code generator status</code></p>
<p><code>* Link to </code><a href="http://research.microsoft.com/en-us/um/people/simonpj/tmp/wos-diss-draft.pdf"><code>Krzysztof</code> <code>Wos's</code> <code>project</code></a><code>, in which he reports great performance improvements by turning tail recursion into loops in C--.</code></p>
<p><code>* Norman added a [wiki:Commentary/Compiler/HooplPerformance Hoopl performance page]</code></p>
<p><code>* Edward Yang has a wiki page that describes shortcomings of the code generated by the new pipeline: [wiki:Commentary/Compiler/NewCodeGenStupidity]</code></p>
<p><code>* John D has built a complete new codegen pipeline, running alongside the old one, enabled by `-fuse-new-codegen`. It is described here: [wiki:Commentary/Compiler/NewCodeGenPipeline].  It uses a new representation for `Cmm`, mostly with &quot;Z&quot; in the name.  (Let's call the original Cmm `OldCmm` and this new one `CmmZ`.)  It has a new conversion STG-&gt;CmmZ, and then sequence of passes that optimise and cps-convert the Cmm.  Finally, it is converted back to the old Cmm so that it can flow to the old code generators.</code></p>
<p><code>* Compiling through the new pipeline passes all tests and GHC is bootstrappable.</code></p>
<p><code>* Separately, we have developed yet another, and still better, Cmm representation, the subject of an upcoming ICFP 2010 submission.  It uses phantom types and GADTs to add very useful open/closed invariants.  This isn't in GHC at all yet.  I'll call it `CmmGADT` for easy reference.</code></p>
<p>Generally we want to keep old and new pipelines working simultaneously, so that we can switch only when we are sure the new stuff works. Next steps in this grand plan are:</p>
<p><code>* Check the impact on compilation time of the new route.</code></p>
<p><code>* Finalise `CmmGADT` and make the new pipeline use it.</code></p>
<p><code>* Make the Cmm parser (which parses `.cmm` files from the RTS) produce `CmmGADT`, and push that down the new pipeline.</code></p>
<p><code>* Implement the many refactorings and improvements to the new pipeline described in </code><a href="http://darcs.haskell.org/ghc/compiler/cmm/cmm-notes"><code>http://darcs.haskell.org/ghc/compiler/cmm/cmm-notes</code></a><code>. See also: [wiki:Commentary/Compiler/NewCodeGenStupidity]</code></p>
<p><code>* Instead of converting new Cmm to old Cmm, make the downstream code generators consume `CmmGADT`, and convert old Cmm to `CmmGADT`.</code></p>
<p>Longer term</p>
<p><code>* Expand the capability of the new pipeline so that it does native code generation too, and we can ultimately discard the existing code generators.  The design of this stage is here: [wiki:Commentary/Compiler/IntegratedCodeGen]</code></p>
<h2 id="workflow-for-the-new-code-generator-and-hoopl">Workflow for the new code generator and Hoopl</h2>
<p>We have the following repositories:</p>
<p><code>* HEAD: the main GHC git repo. `http://darcs.haskell.org/ghc.git`</code></p>
<p><code>* !HooplMaster: the master Hoopl Git repository.</code><br />
<code>  </code><a href="BR" title="wikilink"><code>BR</code></a><code> </code><strong><code>Location</code></strong><code>: `http://ghc.cs.tufts.edu/hoopl/hoopl.git/`</code><br />
<code>  </code><a href="BR" title="wikilink"><code>BR</code></a><code> (Physical location: `linux.cs.tufts.edu:/r/ghc/www/hoopl/hoopl.git`)</code></p>
<p><code>* !HooplLag: a Git repo that is guaranteed to work with GHC HEAD.    It is</code><br />
<code>  not automatically updated by pushes to !HooplMaster.  Instead a manual</code><br />
<code>  process (below) updates it; hence &quot;lag&quot;.</code><br />
<code>  </code><a href="BR" title="wikilink"><code>BR</code></a><code> </code><strong><code>Location</code></strong><code>: `http://darcs.haskell.org/packages/hoopl.git`.</code></p>
<p>Normal GHC developers, who are uninterested in Hoopl, ignore all this. If they download HEAD including all submodules, they'll get !HooplLag, which is always guaranteed to work with HEAD.</p>
<p>Developers who work on GHC and also need to modify Hoopl need to ensure their changes end up in both repositories.</p>
<p><code>* In your hoopl directory in your development tree, add !HooplMaster as a remote and update your reference there. </code><br />
<code>* Hack away in the development tree.</code><br />
<code>* Record Hoopl commits.</code><br />
<code>* Run validate in the development tree</code><br />
<code>* Push the commits in hoopl to the !HooplMaster Git repo</code><br />
<code>* Wait for the mirrors to update (the impatient can run `/srv/darcs/do_mirrors` on darcs.haskell.org)</code><br />
<code>* Push the commits in hoopl to the !HooplLag Git repo (probably the origin remote)</code></p>
<h2 id="status-report-april-2011">Status report April 2011</h2>
<p>Term</p>
<h1 id="old-code-generator-prior-to-ghc-7.8">Old Code Generator (prior to GHC 7.8)</h1>
<p>Material below describes old code generator that was used up to GHC 7.6 and was retired in 2012. This page is not maintained and is here only for historical purposes. See [wiki:Commentary/Compiler/CodeGen Code generator] page for an up to date description of the current code generator.</p>
<h2 id="storage-manager-representations">Storage manager representations</h2>
<p>See [wiki:Commentary/Rts/Storage The Storage Manager] for the [wiki:Commentary/Rts/Storage/Stack Layout of the stack].</p>
<p>The code generator needs to know the layout of heap objects, because it generates code that accesses and constructs those heap objects. The runtime also needs to know about the layout of heap objects, because it contains the garbage collector. How can we share the definition of storage layout such that the code generator and the runtime both have access to it, and so that we don't have to keep two independent definitions in sync?</p>
<p>Currently we solve the problem this way:</p>
<p><code>* C types representing heap objects are defined in the C header files, see for example </code><a href="GhcFile(includes/rts/storage/Closures.h)" title="wikilink"><code>GhcFile(includes/rts/storage/Closures.h)</code></a><code>.</code></p>
<p><code>* A C program, </code><a href="GhcFile(includes/mkDerivedConstants.c)" title="wikilink"><code>GhcFile(includes/mkDerivedConstants.c)</code></a><code>,  `#includes` the runtime headers.</code><br />
<code>  This program is built and run when you type `make` or `make boot` in `includes/`.  It is</code><br />
<code>  run twice: once to generate `includes/DerivedConstants.h`, and again to generate </code><br />
<code>  `includes/GHCConstants.h`.</code></p>
<p><code>* The file `DerivedConstants.h` contains lots of `#defines` like this:</code></p>
<p></p>
<p><code>  which says that the offset to the why_blocked field of an `StgTSO` is 18 bytes.  This file</code><br />
<code>  is `#included` into </code><a href="GhcFile(includes/Cmm.h)" title="wikilink"><code>GhcFile(includes/Cmm.h)</code></a><code>, so these offests are available to the</code><br />
<code>  [wiki:Commentary/Rts/Cmm hand-written .cmm files].</code></p>
<p><code>* The file `GHCConstants.h` contains similar definitions:</code></p>
<p></p>
<p><code> This time the definitions are in Haskell syntax, and this file is `#included` directly into</code><br />
<code> </code><a href="GhcFile(compiler/main/Constants.lhs)" title="wikilink"><code>GhcFile(compiler/main/Constants.lhs)</code></a><code>.  This is the way that these offsets are made</code><br />
<code> available to GHC's code generator.</code></p>
<h2 id="generated-cmm-naming-convention">Generated Cmm Naming Convention</h2>
<p>See <a href="GhcFile(compiler/cmm/CLabel.hs)" class="uri" title="wikilink">GhcFile(compiler/cmm/CLabel.hs)</a></p>
<p>Labels generated by the code generator are of the form  where  is  for external names and  for internal names.  is one of the following:</p>
<p><code> info::                   Info table</code><br />
<code> srt::                    Static reference table</code><br />
<code> srtd::                   Static reference table descriptor</code><br />
<code> entry::                  Entry code (function, closure)</code><br />
<code> slow::                   Slow entry code (if any)</code><br />
<code> ret::                    Direct return address    </code><br />
<code> vtbl::                   Vector table</code><br />
<code> </code><em><code>n</code></em><code>_alt::              Case alternative (tag </code><em><code>n</code></em><code>)</code><br />
<code> dflt::                   Default case alternative</code><br />
<code> btm::                    Large bitmap vector</code><br />
<code> closure::                Static closure</code><br />
<code> con_entry::              Dynamic Constructor entry code</code><br />
<code> con_info::               Dynamic Constructor info table</code><br />
<code> static_entry::           Static Constructor entry code</code><br />
<code> static_info::            Static Constructor info table</code><br />
<code> sel_info::               Selector info table</code><br />
<code> sel_entry::              Selector entry code</code><br />
<code> cc::                     Cost centre</code><br />
<code> ccs::                    Cost centre stack</code></p>
<p>Many of these distinctions are only for documentation reasons. For example, _ret is only distinguished from _entry to make it easy to tell whether a code fragment is a return point or a closure/function entry.</p>
<h2 id="modules">Modules</h2>
<h3 id="section-6"></h3>
<p>Top level, only exports .</p>
<p>Called from  for each module that needs to be converted from Stg to Cmm.</p>
<p>For each such module  does three things:</p>
<p><code>* </code><code> for the </code><br />
<code>* </code><code> for the </code><code> (These are constructors not constructor calls).</code><br />
<code>* </code><code> for the module</code></p>
<p> generates several boilerplate initialization functions that:</p>
<p><code>* regiser the module,</code><br />
<code>* creates an Hpc table,</code><br />
<code>* setup its profiling info (</code><code>, code coverage info </code><code>), and</code><br />
<code>* calls the initialization functions of the modules it imports.</code></p>
<p>If neither SCC profiling or HPC are used, then the initialization code short circuits to return.</p>
<p>If the module has already been initialized, the initialization function just returns.</p>
<p>The  and  modules get special treatment.</p>
<p> is a small wrapper around  which in turn disptaches to:</p>
<p><code>* </code><code> for </code><br />
<code>  (these are bindings of constructor applications not constructors themselves) and</code><br />
<code>* </code><code> for </code><code>.</code></p>
<p> and  are located in  and  which are the primary modules called by .</p>
<h3 id="section-7"></h3>
<p>TODO</p>
<h3 id="section-8"></h3>
<p>TODO</p>
<h3 id="section-9"></h3>
<p>The monad that most of codeGen operates inside</p>
<p><code>* Reader</code><br />
<code>* State</code><br />
<code>* (could be Writer?)</code><br />
<code>* fork</code><br />
<code>* flatten</code></p>
<h3 id="section-10"></h3>
<p>Called by  and .</p>
<p>Since everything in STG is an expression, almost everything branches off from here.</p>
<p>This module exports only one function , which for the most part just dispatches to other functions to handle each specific constructor in .</p>
<p>Here are the core functions that each constructor is disptached to (though some may have little helper functions called in addition to the core function):</p>
<p><code>:: Calls to </code><code> in </code><br />
<code>:: Calls to </code><code> in </code><br />
<code>::</code><br />
<code>  Calls to </code><code> in </code><br />
<code>   and </code><code> in </code><br />
<code>::</code><br />
<code>  Is a bit more complicated see below.</code><br />
<code>:: Calls to </code><code> in </code><br />
<code>:: Calls to </code><code> in </code><br />
<code>::</code><br />
<code>  Calls to </code><code> in </code><code>, but with a little bit of wrapping</code><br />
<code>  by </code><code> and </code><code>.</code><br />
<code>:: Calls to  </code><code> in </code><br />
<code>:: Calls to </code><code> in </code><br />
<code>::</code><br />
<code>  Does not have a case because it is only for </code><code>'s work.</code></p>
<p>Some of these cases call to functions defined in . This is because they need a little bit of wrapping and processing before calling out to their main worker function.</p>
<p><code>::</code><br />
<code>* For </code><code> calls out to </code><code> in </code><code>.</code><br />
<code>* For </code><code> calls out to </code><code>.</code><br />
<code>  In turn, </code><code> calls out to </code><code> for selectors and thunks,</code><br />
<code>  and calls out to </code><code> in the default case.</code><br />
<code>  Both these are defined in </code><code>.</code></p>
<p><code>::</code><br />
<code>* Wraps a call to </code><code> with </code><br />
<code>  depending on whether it is called on a recursive or a non-recursive binding.</code><br />
<code>  In turn </code><code> wraps </code><br />
<code>  defined in </code><code>.</code></p>
<p> has a number of sub-cases.</p>
<p><code>* </code><br />
<code>* </code><code> of a !TagToEnumOp</code><br />
<code>* </code><code> that is primOpOutOfLine</code><br />
<code>* </code><code> that returns Void</code><br />
<code>* </code><code> that returns a single primitive</code><br />
<code>* </code><code> that returns an unboxed tuple</code><br />
<code>* </code><code> that returns an enumeration type</code></p>
<p>(It appears that non-foreign-call, inline [wiki:Commentary/PrimOps PrimOps] are not allowed to return complex data types (e.g. a |Maybe|), but this fact needs to be verified.)</p>
<p>Each of these cases centers around one of these three core calls:</p>
<p><code>* </code><code> in </code><br />
<code>* </code><code> in </code><br />
<code>* </code><code> in </code></p>
<p>There is also a little bit of argument and return marshelling with the following functions</p>
<p><code>Argument marshelling::</code><br />
<code>  </code><code>, </code><br />
<code>Return marshelling::</code><br />
<code>  </code><code>, </code><code>, </code><br />
<code>Performing the return::</code><br />
<code>  </code><code>, </code><code>,</code><br />
<code>  </code><code>, </code></p>
<p>In summary the modules that get called in order to handle a specific expression case are:</p>
<h4 id="also-called-for-top-level-bindings-by">Also called for top level bindings by </h4>
<p><code>:: for </code><code> and the </code><code> part of </code><br />
<code>:: for the </code><code> part of </code></p>
<h4 id="core-code-generation">Core code generation</h4>
<p><code>:: for </code><code>, </code><code>, and </code><br />
<code>:: for </code><br />
<code>:: for </code><br />
<code>:: for </code></p>
<h4 id="profiling-and-code-coverage-related">Profiling and Code coverage related</h4>
<p><code>:: for </code><br />
<code>:: for </code></p>
<h4 id="utility-modules-that-happen-to-have-the-functions-for-code-generation">Utility modules that happen to have the functions for code generation</h4>
<p><code>:: for </code><br />
<code>:: for </code></p>
<p>Note that the first two are the same modules that are called for top level bindings by , and the last two are really utility modules, but they happen to have the functions needed for those code generation cases.</p>
<h3 id="memory-and-register-management">Memory and Register Management</h3>
<p><code>::</code><br />
<code>  Module for </code><code> which maps variable names</code><br />
<code>  to all the volitile or stable locations where they are stored</code><br />
<code>  (e.g. register, stack slot, computed from other expressions, etc.)</code><br />
<code>  Provides the </code><code>, </code><code> and </code><code> functions</code><br />
<code>  for adding, modifying and looking up bindings.</code></p>
<p><code>::</code><br />
<code>  Mostly utility functions for allocating and freeing stack slots.</code><br />
<code>  But also has things on setting up update frames.</code></p>
<p><code>::</code><br />
<code>  Functions for allocating objects that appear on the heap such as closures and constructors.</code><br />
<code>  Also includes code for stack and heap checks and </code><code>.</code></p>
<h3 id="function-calls-and-parameter-passing">Function Calls and Parameter Passing</h3>
<p>(Note: these will largely go away once CPS conversion is fully implemented.)</p>
<p><code>, </code><code>, </code><code>::</code><br />
<code>  Handle different types of calls.</code><br />
<code>::</code><br />
<code>  Use by the others in this category to determine liveness and</code><br />
<code>  to select in what registers and stack locations arguments and return</code><br />
<code>  values get stored.</code></p>
<h3 id="misc-utilities">Misc utilities</h3>
<p><code>::</code><br />
<code>  Utility functions for making bitmaps (e.g. </code><code> with type </code><code>)</code><br />
<code>::</code><br />
<code>  Stores info about closures and bindings.</code><br />
<code>  Includes information about memory layout, how to call a binding (</code><code>)</code><br />
<code>  and information used to build the info table (</code><code>).</code><br />
<code>::</code><br />
<code>  Storage manager representation of closures.</code><br />
<code>  Part of !ClosureInfo but kept separate to &quot;keep nhc happy.&quot;</code><br />
<code>:: TODO</code><br />
<code>:: TODO</code></p>
<h3 id="special-runtime-support">Special runtime support</h3>
<p><code>:: Ticky-ticky profiling</code><br />
<code>:: Cost-centre profiling</code><br />
<code>:: Support for the Haskell Program Coverage (hpc) toolkit, inside GHC.</code><br />
<code>::</code><br />
<code>  Code generation for !GranSim (GRAN) and parallel (PAR).</code><br />
<code>  All the functions are dead stubs except </code><code> and </code><code>.</code></p>
<h1 id="ordering-the-core-to-core-optimisation-passes">Ordering the Core-to-Core optimisation passes</h1>
<p>This page has notes about the ordering of optimisation phases. An overview of the whole Core-to-Core optimisation pipeline can be found [wiki:Commentary/Compiler/Core2CorePipeline here].</p>
<p><strong>NOTE:</strong> This is old documentation and may not be very relevant any more!</p>
<h2 id="this-ordering-obeys-all-the-constraints-except-5">This ordering obeys all the constraints except (5)</h2>
<p><code>* full laziness</code><br />
<code>* simplify with foldr/build</code><br />
<code>* float-in</code><br />
<code>* simplify</code><br />
<code>* strictness</code><br />
<code>* float-in</code></p>
<p>[check FFT2 still gets benefits with this ordering]</p>
<h2 id="constraints-1">Constraints</h2>
<h3 id="float-in-before-strictness">1. float-in before strictness</h3>
<p>Reason: floating inwards moves definitions inwards to a site at which the binding might well be strict.  The strictness analyser will do a better job of the latter than the former.</p>
<h3 id="dont-simplify-between-float-in-and-strictness">2. Don't simplify between float-in and strictness</h3>
<p>...unless you disable float-let-out-of-let, otherwise the simiplifier's local floating might undo some useful floating-in.  This is a bad move, because now y isn't strict. In the pre-float case, the binding for y is strict. Mind you, this isn't a very common case, and it's easy to disable float-let-from-let.</p>
<h3 id="want-full-laziness-before-foldrbuild">3. Want full-laziness before foldr/build</h3>
<p>Reason: Give priority to sharing rather than deforestation.  In the post-full-laziness case, xs is shared between all applications of the function. If we did foldr/build first, we'd have got  and now we can't share xs.</p>
<h3 id="want-strictness-after-foldrbuild">4. Want strictness after foldr/build</h3>
<p>Reason: foldr/build makes new function definitions which can benefit from strictness analysis.  Here we clearly want to get strictness analysis on g.</p>
<h3 id="want-full-laziness-after-strictness">5. Want full laziness after strictness</h3>
<p>Reason: absence may allow something to be floated out which would not otherwise be.  TOO BAD. This doesn't look a common case to me.</p>
<h3 id="want-float-in-after-foldrbuild">6. Want float-in after foldr/build</h3>
<p>Reason: Desugaring list comprehensions + foldr/build gives rise to new float-in opportunities.  Now v could usefully be floated into the second branch.</p>
<h3 id="want-simplify-after-float-inwards">7. Want simplify after float-inwards</h3>
<p>(Occurred in the prelude, compiling `ITup2.hs`, function `dfun.Ord.(*,*)`) This is due to the following (that happens with dictionaries):  floating inwards will push the definition of a1 into m1 (supposing it is only used there):  if we do strictness analysis now we will not get a worker-wrapper for m1, because of the &quot;let a1 ...&quot; (notice that a1 is not strict in its body).</p>
<p>Not having this worker wrapper might be very bad, because it might mean that we will have to rebox arguments to m1 if they are already unboxed, generating extra allocations, as occurs with m2 (cc) above.</p>
<p>To solve this problem we have decided to run the simplifier after float-inwards, so that lets whose body is a HNF are floated out, undoing the float-inwards transformation in these cases. We are then back to the original code, which would have a worker-wrapper for m1 after strictness analysis and would avoid the extra let in m2.</p>
<p>What we lose in this case are the opportunities for case-floating that could be presented if, for example, a1 would indeed be demanded (strict) after the floating inwards.</p>
<p>The only way of having the best of both is if we have the worker/wrapper pass explicitly called, and then we could do with</p>
<p><code>* float-in</code><br />
<code>* strictness analysis</code><br />
<code>* simplify</code><br />
<code>* strictness analysis</code><br />
<code>* worker-wrapper generation</code></p>
<p>as we would</p>
<p><code>* be able to detect the strictness of m1 after the first call to the strictness analyser, and exploit it with the simplifier (in case it was strict).</code><br />
<code>* after the call to the simplifier (if m1 was not demanded) it would be floated out just like we currently do, before stricness analysis II and worker/wrapperisation.</code></p>
<p>The reason to not do worker/wrapperisation twice is to avoid generating wrappers for wrappers which could happen.</p>
<h3 id="if-full-laziness-is-ever-done-after-strictness">8. If full laziness is ever done after strictness</h3>
<p>...remember to switch off demandedness flags on floated bindings! This isn't done at the moment.</p>
<h3 id="ignore-inline-pragmas-flag-for-final-simplification">9. Ignore-inline-pragmas flag for final simplification</h3>
<p>[Occurred in the prelude, compiling ITup2.hs, function dfun.Ord.(*,*)] Sometimes (e.g. in dictionary methods) we generate worker/wrappers for functions but the wrappers are never inlined. In dictionaries we often have  and if we create worker/wrappers for f1,...,fn the wrappers will not be inlined anywhere, and we will have ended up with extra closures (one for the worker and one for the wrapper) and extra function calls, as when we access the dictionary we will be acessing the wrapper, which will call the worker. The simplifier never inlines workers into wrappers, as the wrappers themselves have INLINE pragmas attached to them (so that they are always inlined, and we do not know in advance how many times they will be inlined).</p>
<p>To solve this problem, in the last call to the simplifier we will ignore these inline pragmas and handle the workers and the wrappers as normal definitions. This will allow a worker to be inlined into the wrapper if it satisfies all the criteria for inlining (e.g. it is the only occurrence of the worker etc.).</p>
<h3 id="run-float-inwards-once-more-after-strictness-simplify">10. Run Float Inwards once more after strictness-simplify</h3>
<p>[Occurred in the prelude, compiling `IInt.hs`, function `const.Int.index.wrk`] When workers are generated after strictness analysis (worker/wrapper), we generate them with &quot;reboxing&quot; lets, that simply reboxes the unboxed arguments, as it may be the case that the worker will need the original boxed value:  in this case the simplifier will remove the binding for y as it is not used (we expected this to happen very often, but we do not know how many &quot;reboxers&quot; are eventually removed and how many are kept), and will keep the binding for x. But notice that x is only used in *one* of the branches in the case, but is always being allocated! The floating inwards pass would push its definition into the True branch. A similar benefit occurs if it is only used inside a let definition. These are basically the advantages of floating inwards, but they are only exposed after the S.A./worker-wrapperisation of the code! As we also have reasons to float inwards before S.A. we have to run it twice.</p>
<h1 id="overall-organisation-of-ghc">Overall organisation of GHC</h1>
<p>Start at the <a href="http://haskell.org/ghc">GHC home page</a>. The most important links are in the left-hand column:</p>
<p><code>* </code><a href="http://haskell.org/haskellwiki/GHC"><code>Documentation</code></a><code>.  This is the </code><em><code>user</code></em><code> documentation, aimed at people who use GHC, but don't care how it works.  It's on the Haskell Wiki (powered by MediaWiki), and we strongly encourage people to edit and improve it.</code></p>
<p><code>* </code><a href="http://hackage.haskell.org/trac/ghc"><code>Developers</code></a><code>.  This link takes you to the home page for </code><em><code>developers</code></em><code>; that is, people interested in hacking on GHC itself (i.e. you).  It's a Wiki too, but powered by Trac, and includes bug-tracking etc.  There is a big section called Developer Documentation: </code><strong><code>please</code> <code>help</code> <code>us</code> <code>to</code> <code>improve</code> <code>it</code></strong><code>.</code></p>
<p><code>* </code><a href="http://www.haskell.org/ghc/download.html"><code>Download</code></a><code>.  At any moment, GHC has a </code><strong><code>STABLE</code> <code>branch</code></strong><code> and the </code><strong><code>HEAD</code></strong><code>, both of which you can download from this page.</code><a href="BR" title="wikilink"><code>BR</code></a><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  * The STABLE branch is the current released version.  It has an even version number (e.g. 6.4, 6.6), with an extra suffix for patch-level release (e.g. 6.4.2).  Patch-level releses fix bugs; they do not change any APIs.</code><a href="BR" title="wikilink"><code>BR</code></a><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  * The HEAD is simply the latest, greatest version that we are working on; it may be broken on any given day, although you are encouraged not to break it gratuitiously.  The HEAD has an odd version numbers (e.g 6.5, 6.7).  Every night we build the HEAD, and dump the result on the download site under &quot;Development snapshots&quot;, with a version number that encodes the date (e..g 6.5.20060831).</code></p>
<p><code>  A very useful link on the download page is the </code><a href="http://www.haskell.org/ghc/dist/current/docs/"><code>documentation</code> <code>for</code> <code>the</code> <code>HEAD</code></a><code> (under Development snapshots).  Useful because typesetting the documentation uses DocBook, which easy to install on every platform.</code></p>
<h1 id="ghc-source-code">GHC source code</h1>
<p>GHC's source code is several Darcs repositories. The important ones are:</p>
<p><a href="http://darcs.haskell.org/ghc" class="uri">http://darcs.haskell.org/ghc</a>:: All of GHC: compiler, run-time system, support utilities.</p>
<p><a href="http://darcs.hasekll.org/packages/pkg" class="uri">http://darcs.hasekll.org/packages/pkg</a>:: A library package <em>pkg</em>. A certain number of packages are essential to build GHC. They are listed in  and currently comprise: , , , , , , , , , , , .</p>
<p><a href="http://darcs.haskell.org/testsuite" class="uri">http://darcs.haskell.org/testsuite</a>:: GHC's test suite.</p>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<p><code> </code><meta http-equiv="Content-Type" content="text/html; charset=utf-8" /><br />
<code> </code><meta http-equiv="Content-Style-Type" content="text/css" /><br />
<code> </code><meta name="generator" content="pandoc" /><br />
<code> </code></p>
<title>
</title>
<style type="text/css">
<p>code{white-space: pre;}</p>
</style>
</head>
<body>
<div id="TOC">
<ul>
<li><a href="#the-ghc-commentary-checking-types">The GHC Commentary: Checking Types</a>
<ul>
<li><a href="#the-overall-flow-of-things">The Overall Flow of Things</a>
<ul>
<li><a href="#entry-points-into-the-type-checker">Entry Points Into the Type Checker</a></li>
<li><a href="#renaming-and-type-checking-a-module">Renaming and Type Checking a Module</a></li>
</ul></li>
<li><a href="#type-checking-a-declaration-group">Type Checking a Declaration Group</a></li>
<li><a href="#type-checking-type-and-class-declarations">Type checking Type and Class Declarations</a></li>
<li><a href="#more-details">More Details</a>
<ul>
<li><a href="#types-variables-and-zonking">Types Variables and Zonking</a></li>
<li><a href="#type-representation">Type Representation</a></li>
<li><a href="#type-checking-environment">Type Checking Environment</a></li>
<li><a href="#expressions">Expressions</a></li>
<li><a href="#handling-of-dictionaries-and-method-instances">Handling of Dictionaries and Method Instances</a></li>
</ul></li>
<li><a href="#connection-with-ghcs-constraint-solver">Connection with GHC's Constraint Solver</a></li>
<li><a href="#generating-evidence">Generating Evidence</a></li>
<li><a href="#the-solver">The Solver</a>
<ul>
<li><a href="#given-constraints">Given Constraints</a></li>
<li><a href="#derived-constraints">Derived Constraints</a></li>
<li><a href="#wanted-constraints">Wanted Constraints</a></li>
</ul></li>
</ul></li>
</ul>
</div>
<h1 id="the-ghc-commentary-checking-types">
<p>The GHC Commentary: Checking Types</p>
</h1>
<p>Probably the most important phase in the frontend is the type checker, which is located at <a href="GhcFile(compiler/typecheck/)" class="uri" title="wikilink">GhcFile(compiler/typecheck/)</a>. GHC type checks programs in their original Haskell form before the desugarer converts them into Core code. This complicates the type checker as it has to handle the much more verbose Haskell AST, but it improves error messages, as those message are based on the same structure that the user sees.</p>
<p>GHC defines the abstract syntax of Haskell programs in <a href="GhcModule(compiler/hsSyn/HsSyn.lhs)" class="uri" title="wikilink">GhcModule(compiler/hsSyn/HsSyn.lhs)</a> using a structure that abstracts over the concrete representation of bound occurences of identifiers and patterns. The module <a href="GhcModule(compiler/typecheck/TcHsSyn.lhs)" class="uri" title="wikilink">GhcModule(compiler/typecheck/TcHsSyn.lhs)</a> defines a number of helper function required by the type checker. Note that the type <a href="GhcModule(compiler/typecheck/TcRnTypes.lhs)" class="uri" title="wikilink">GhcModule(compiler/typecheck/TcRnTypes.lhs)</a>.`TcId` used to represent identifiers in some signatures during type checking is, in fact, nothing but a synonym for a [wiki:Commentary/Compiler/EntityTypes#Typevariablesandtermvariables plain Id].</p>
<p>It is also noteworthy, that the representations of types changes during type checking from `HsType` to `TypeRep.Type`. The latter is a [wiki:Commentary/Compiler/TypeType hybrid type] representation that is used to type Core, but still contains sufficient information to recover source types. In particular, the type checker maintains and compares types in their `Type` form.</p>
<h2 id="the-overall-flow-of-things">
<p>The Overall Flow of Things</p>
</h2>
<p><code>*</p>
<h1 id="updates">Updates</h1>
<p>Source files: <br /><span class="math display">$$\[GhcFile(rts/Updates.h)$$</span><br />\], <br /><span class="math display">$$\[GhcFile(rts/Updates.cmm)$$</span><br />\]</p>
<p>----CategoryStub</p>
<p>.. contents::</p>
<p><code>  :depth: 3</code></p>
<p>..</p>
<h2 id="unique">Unique</h2>
<p>``Unique``\ s provide a fast comparison mechanism for more complex things. Every ``RdrName``, ``Name``, ``Var``, ``TyCon``, ``TyVar``, etc. has a ``Unique``. When these more complex structures are collected (in ``UniqFM``\ s or other types of collection), their ``Unique`` typically provides the key by which the collection is indexed.</p>
<p>+----------------------------+ | == Current design == | +----------------------------+ | A ``Unique`` consists of | | the <em>domain</em> of the | | thing it identifies and a | | unique integer value | | 'within' that domain. The | | two are packed into a | | single ``Int#``, with the | | <em>domain</em> being the top 8 | | bits. | +----------------------------+ | The domain is never | | inspected (SLPJ believes). | | The sole reason for its | | existence is to provide a | | number of different ranges | | of ``Unique`` values that | | are guaranteed not to | | conflict. | +----------------------------+ | === Lifetime | +----------------------------+ | The lifetime of a | | ``Unique`` is a single | | invocation of GHC, i.e. | | they must not 'leak' to | | compiler output, the | | reason being that | | ``Unique``\ s may be | | generated/assigned | | non-deterministically. | | When compiler output is | | non-deterministic, it | | becomes significantly | | harder to, for example, | | [wiki:Commentary/Compiler/ | | RecompilationAvoidance | | avoid recompilation]. | | Uniques do not get | | serialised into .hi files, | | for example. | +----------------------------+ | Note, that &quot;one compiler | | invocation&quot; is not the | | same as the compilation of | | a single ``Module``. | | Invocations such as | | ``ghc --make`` or | | ``ghc --interactive`` give | | rise to longer invocation | | life-times. | +----------------------------+ | This is also the reasons | | why ``OccName``\ s are | | <em>not</em> ordered based on | | the ``Unique``\ s of their | | underlying | | ``FastString``\ s, but | | rather | | <em>lexicographically</em> (see | | <a href="GhcFile(compiler/basicTy" title="wikilink"> | pes/OccName.lhs)</a> | | for details). &gt; &gt; | | <strong>SLPJ:</strong> I am far from | | sure that the Ord instance | | for ``OccName`` is ever | | used, so this remark is | | probably misleading. Try | | deleting it and see where | | it is used (if at all). &gt; | | <strong>PKFH:</strong> At least | | ``Name`` and ``RdrName`` | | (partially) define their | | own ``Ord`` instances in | | terms of the instance of | | ``OccName``. Maybe these | | ``Ord`` instances are also | | redundant, but for now it | | seems wise to keep them | | in. When everything has | | ``Data`` instances (after | | this and many other | | redesigns), I'm sure it | | will be easier to find | | such dependency relations. | +----------------------------+ | === Known-key things === | +----------------------------+ | A hundred or two library | | entities (types, classes, | | functions) are so-called | | &quot;known-key things&quot;. See | | [wiki:Commentary/Compiler/ | | WiredIn | | this page]. A known-key | | thing has a fixed | | ``Unique`` that is fixed | | when the compiler is | | built, and thus lives | | across all invocations of | | that compiler. These | | known-key ``Unique``\ s | | <em>are</em> written into .hi | | files. But that's ok | | because they are fully | | deterministic and never | | change. | +----------------------------+ | &gt; <strong>PKFH</strong> That's fine | | then; we also know for | | sure these things fit in | | the 30 bits used in the | | ``hi``-files. I'll comment | | appropriately. | +----------------------------+ | === Interface files === | +----------------------------+ | Entities in a interface | | file (.hi file) are, for | | the most part, stored in a | | symbol table, and referred | | to (from elsewhere in the | | same interface file) by an | | index into that table. | | Here are the details from | | <a href="GhcFile(compiler/iface/B" title="wikilink"> | inIface.lhs)</a>: | | </p>
<hr />
<h2 id="redesign-2014">Redesign (2014)</h2>
<p>=== TL;DR The redesign is to accomplish the following: \* Allow derivation of type class instances for ``Unique`` \* Restore invariants from the original design; hide representation details \* Eliminate violations of invariants and design-violations in other places of the compiler (e.g. ``Unique``\ s shouldn't be written to ``hi``-files, but are). &gt; &gt; <strong>SLPJ</strong> I don't think this is a design violation; see above. Do you have any other examples in mind? &gt; <strong>PKFH</strong> Not really of design-violations (and no other compiler-output stuff) other than the invariants mentioned above it, just yet. The key point, though, is that there are a lot of comments in ``Unique`` about not exporting things so that we know X, Y and Z, but then those things <em>are</em> exported, so we don't know them to be true. Case in point is the export of ``mkUnique``, but also ``mkUniqueGrimily``. The latter has a comment 'only for ``UniqSupply``' but is also used in other places (like Template Haskell). One redesign is to put this restriction in the name, so there still is the facility offered by ``mkUniqueGrimily``, but now it's called ``mkUniqueOnlyForUniqSupply`` (and ``mkUniqueOnlyForTemplateHaskell``), the ugliness of which should help, over time, to get rid of them.</p>
<p>=== Longer</p>
<p>In an attempt to give more of GHC's innards well-behaved instances of ``Typeable``, ``Data``, ``Foldable``, ``Traversable``, etc. the implementation of ``Unique``\ s was a bit of a sore spot. They were implemented (20+ years earlier) using custom boxing, viz.  making automatic derivation of such type class instances hard. There was already a comment asking why it wasn't simply a ``newtype`` around a normal (boxed) ``Int``. Independently, there was some discussion on the mailinglists about the use of (signed) ``Int``\ s in places where ``Word``\ s would be more appropriate. Further inspection of the ``Unique`` implementation made clear that a lot of invariants mentioned in comments had been violated by incremental edits. This is discussed in more detail below, but these things together (the desire for automatic derivation and the restoration of some important invariants) motivated a moderate redesign.</p>
<p>=== Status Quo (pre redesign)</p>
<p>A ``Unique`` has a domain (``TyCon``, ``DataCon``, ``PrelName``, ``Builtin``, etc.) that was codified by a character. The remainder of the ``Unique`` was an integer that should be unique for said domain. This <strong>was</strong> once guaranteed through the export list of <a href="GhcFile(compiler/basicTypes/Unique.lhs)" class="uri" title="wikilink">GhcFile(compiler/basicTypes/Unique.lhs)</a>, where direct access to the domain-character was hidden, i.e.  were not exported. This should have guaranteed that every domain was assigned its own unique character, because only in <a href="GhcFile(compiler/basicTypes/Unique.lhs)" class="uri" title="wikilink">GhcFile(compiler/basicTypes/Unique.lhs)</a> could those ``Char``\ s be assigned. However, through  this separation of concerns leaked out to <a href="GhcFile(compiler/basicTypes/UniqSupply.lhs)" class="uri" title="wikilink">GhcFile(compiler/basicTypes/UniqSupply.lhs)</a>, because its ``Int`` argument is the <em>entire</em> ``Unique`` and not just the integer part 'under' the domain character. &gt; &gt; <strong>SLPJ</strong> OK, but to eliminate ``mkUniqueGrimily`` you need to examine the calls, decide how to do it better, and document the new design. &gt; <strong>PKFH</strong> See above; the solution for now is ``mkUniqueOnlyForUniqSupply``. A separate patch will deal with trying to refactor/redesign ``UniqSupply`` if this is necessary.</p>
<p>The function ``mkSplitUniqSupply`` made the domain-character accessible to all the other modules, by having a wholly separate implementation of the functionality of ``mkUnique``.</p>
<p>Where the intention was still to have a clean interface, the (would-be) hidden ``mkUnique`` is only called by functions defined in the ``Unique`` module with the corresponding character, e.g. </p>
<p>=== New plan</p>
<p>In the new design, the domains are explicitly encoded in a sum-type ``UniqueDomain``. At the very least, this should help make the code a little more self-documenting <em>and</em> prevent accidental overlap in the choice of bits to identify the domain. Since the purpose of ``Unique``\ s is to provide <em>fast</em> comparison for different types of things, the redesign should remain performance concious. With this in mind, keeping the ``UniqueDomain`` and the integer-part explicitly in the type  seems unwise, but by choosing  we win the ability to automatically derive things and should also be able to test how far optimisation has come in the past 20+ years; does default boxing with ``newtype``-style wrapping have (nearly) the same performance as manual unboxing? This should follow from the tests.</p>
<p>The encoding is kept the same, i.e. the ``Word`` is still built up with the domain encoded in the most significant bits and the integer-part in the remaining bits. However, instead encoding the domain as a ``Char`` in the (internal <em>and</em> external interface), we now create an ADT (sum-type) that encodes the domain. This has two advantages. First, it prevents people from picking domain-tags ad hoc an possibly overlapping. Second, encoding in the ``Word`` does not rely on the assumption that the domain requires and/or fits in 8 bits. Since Haskell ``Char``\ s are unicode, the 8-bit assumption is wrong for the old design. In other words, the above examples are changed to:</p>
<p></p>
<p>Ideal world scenario, the entire external interface would be:  and the instances for ``Eq``, ``Ord``, ``Data``, etc. For now, though, it will also have </p>
<p><code>       </code><strong><code>SLPJ</code></strong><code> I agree that a ``newtype`` around a ``Word`` is</code><br />
<code>       better than a ``data`` type around ``Int#``. That is a small,</code><br />
<code>       simple change. But I think you plan to do more than this, and</code><br />
<code>       that &quot;more&quot; is not documented here. E.g. what is the new API to</code><br />
<code>       ``Unique``? </code><strong><code>PKFH</code></strong><code> Added. See above.</code></p>
<p><a href="PageOutline" class="uri" title="wikilink">PageOutline</a></p>
<h1 id="the-data-type-and-its-friends">The data type  and its friends</h1>
<p>GHC compiles a typed programming language, and GHC's intermediate language is explicitly typed. So the data type that GHC uses to represent types is of central importance.</p>
<p>The single data type  is used to represent \* Types (possibly of higher kind); e.g. ``[Int]``, ``Maybe`` \* Kinds (which classify types and coercions); e.g. ``(* -&gt; *)``, ``T :=: [Int]``. See [wiki:Commentary/Compiler/Kinds] \* Sorts (which classify types); e.g. ``TY``, ``CO``</p>
<p>GHC's use of [wiki:Commentary/Compiler/FC coercions and equality constraints] is important enough to deserve its own page.</p>
<p>The module  exposes the representation because a few other modules (, , , etc) work directly on its representation. However, you should not lightly pattern-match on ; it is meant to be an abstract type. Instead, try to use functions defined by ,  etc.</p>
<h2 id="views-of-types">Views of types</h2>
<p>Even when considering only types (not kinds, sorts, coercions) you need to know that GHC uses a <em>single</em> data type for types. You can look at the same type in different ways:</p>
<p>- The &quot;typechecker view&quot; regards the type as a Haskell type, complete</p>
<p><code>  with implicit parameters, class constraints, and the like. For</code><br />
<code>  example: </code><code> Functions in</code><br />
<code>  ``TcType`` take this view of types; e.g. ``tcSplitSigmaTy`` splits up</code><br />
<code>  a type into its forall'd type variables, its constraints, and the</code><br />
<code>  rest.</code></p>
<p>- The &quot;core view&quot; regards the type as a Core-language type, where class</p>
<p><code>  and implicit parameter constraints are treated as function arguments:</code><br />
<code>  </code><code> Functions in ``Type`` take</code><br />
<code>  this view.</code></p>
<p>The data type ``Type`` represents type synonym applications in un-expanded form. E.g.  Here ``f``'s type doesn't look like a function type, but it really is. The function ``Type.coreView :: Type -&gt; Maybe Type`` takes a type and, if it's a type synonym application, it expands the synonym and returns ``Just <expanded-type>``. Otherwise it returns ``Nothing``.</p>
<p>Now, other functions use ``coreView`` to expand where necessary, thus:  Notice the first line, which uses the view, and recurses when the view 'fires'. Since ``coreView`` is non-recursive, GHC will inline it, and the optimiser will ultimately produce something like: </p>
<h2 id="the-representation-of">The representation of </h2>
<p>Here, then is the representation of types (see <a href="GhcFile(compiler/types/TypeRep.hs)" class="uri" title="wikilink">GhcFile(compiler/types/TypeRep.hs)</a> for more details): </p>
<p>Invariant: if the head of a type application is a , GHC <em>always</em> uses the  constructor, not . This invariant is maintained internally by 'smart constructors'. A similar invariant applies to ;  is never used with an arrow type.</p>
<p>Type variables are represented by the ``TyVar`` constructor of the [wiki:Commentary/Compiler/EntityTypes data type Var].</p>
<h2 id="overloaded-types">Overloaded types</h2>
<p>In Haskell we write  but in Core the ``=&gt;`` is represented by an ordinary ``FunTy``. So f's type looks like this:  Nevertheless, we can tell when a function argument is actually a predicate (and hence should be displayed with ``=&gt;``, etc), using  The various forms of predicate can be extracted thus:  These functions are defined in module ``Type``.</p>
<h2 id="classifying-types">Classifying types</h2>
<p>GHC uses the following nomenclature for types:</p>
<p><strong>Unboxed</strong>:: A type is unboxed iff its representation is other than a pointer. Unboxed types are also unlifted.</p>
<p><strong>Lifted</strong>:: A type is lifted iff it has bottom as an element. Closures always have lifted types: i.e. any let-bound identifier in Core must have a lifted type. Operationally, a lifted object is one that can be entered. Only lifted types may be unified with a type variable.</p>
<p><strong>Data</strong>:: A type declared with <strong></strong>. Also boxed tuples.</p>
<p><strong>Algebraic</strong>:: An algebraic data type is a data type with one or more constructors, whether declared with  or . An algebraic type is one that can be deconstructed with a case expression. &quot;Algebraic&quot; is <strong>NOT</strong> the same as &quot;lifted&quot;, because unboxed (and thus unlifted) tuples count as &quot;algebraic&quot;.</p>
<p><strong>Primitive</strong>:: a type is primitive iff it is a built-in type that can't be expressed in Haskell.</p>
<p>Currently, all primitive types are unlifted, but that's not necessarily the case. (E.g. Int could be primitive.)</p>
<p>Some primitive types are unboxed, such as Int#, whereas some are boxed but unlifted (such as ``ByteArray#``). The only primitive types that we classify as algebraic are the unboxed tuples.</p>
<p>Examples of type classifications:</p>
<p>\|\| \|\| <strong>Primitive</strong> \|\| <strong>Boxed</strong> \|\| <strong>Lifted</strong> \|\| <strong>Algebraic</strong> \|\| \|\| ``Int#`` \|\| Yes \|\| No \|\| No \|\| No \|\| \|\| ``ByteArray#`` \|\| Yes \|\| Yes \|\| No \|\| No \|\| \|\| ``(# a, b #)`` \|\| Yes \|\| No \|\| No \|\| Yes \|\| \|\| ``( a, b )`` \|\| No \|\| Yes \|\| Yes \|\| Yes \|\| \|\| ``[a]`` \|\| No \|\| Yes \|\| Yes \|\| Yes \|\|</p>
<h1 id="package-compatibility">Package Compatibility</h1>
<p>In GHC 6.8.1 we reorganised some of the contents of the packages we ship with GHC, see #710. The idea was to lessen the problem caused by the base package being essentially static between GHC major releases. By separating modules out of base and putting them into separate packages, it is possible to updgrade these modules independently of GHC.</p>
<p>The reorganisations unfortunately exposed some problems with our package infrastructure, in particular most packages that compiled with 6.6 do not compile with 6.8.1 because they don't depend on the new packages. Some instructions for upgrading packages are here: <a href="http://haskell.org/haskellwiki/Upgrading_packages">Upgrading packages</a>.</p>
<p>We anticipated the problem to some extent, adding &quot;configurations&quot; to Cabal to make it possible to write conditional package specifications that work with multiple sets of dependencies. We are still left with the problem that the `.cabal` files for all packages need to be updated for GHC 6.8.1. This seems like the wrong way around: the change we made to a few packages has to be propagated everywhere, when there should be a way to confine it locally, at least for the purposes of continued compatibility with existing source code. In many cases, the underlying APIs are still available, just from a different place. (in general this may not be true - modifications to packages may make changes to APIs which require real changes to dependent packages).</p>
<p>Some of the problems that contributed to this situation can be addressed. We wrote the <a href="http://haskell.org/haskellwiki/Package_versioning_policy">Package Versioning Policy</a> so that packages can start using versions that reflect API changes, and so that dependencies can start being precise about which dependencies they work with. If we follow these guidelines, then</p>
<p><code>* failures will be more predictable</code><br />
<code>* failures will be more informative</code></p>
<p>because dependencies and API changes are better documented. However, we have no fewer failures than before, in fact we have more because packages cannot now &quot;accidentally work&quot; by specifying loose dependency ranges.</p>
<p>So the big question is, what changes do we need to make in the future to either prevent this happening, or to reduce the pain when it does happen? Below are collected various proposals. If the proposals get too long we can separate them out into new pages.</p>
<h2 id="dont-reorganise-packages">1. Don't reorganise packages</h2>
<p>We could do this, but that just hides the problem and we're still left with a monolithic base package. We still have to contend with API changes causing breakage.</p>
<h2 id="provide-older-versions-of-base-with-a-new-ghc-release">2. Provide older version(s) of base with a new GHC release</h2>
<p>We could fork the base package for each new release, and keep compiling the old one(s). Unfortunately we would then have to compile every other package two (or more) times, once against each version of base. And if we were to give the same treatment to any other library, we end up with exponential blowup in the number of copies.</p>
<p>The GHC build gets slower, and the testing surface increases for each release.</p>
<p>Furthermore, the package database cannot currently understand multiple packages compiled against different versions of dependencies. One workaround is to have multiple package databases, but that's not too convenient.</p>
<h2 id="allow-packages-to-re-export-modules">4. Allow packages to re-export modules</h2>
<p>Packages currently cannot re-export modules from other packages. Well, that's not strictly true, it is possible to do this but it currently requires an extra package and two stub modules per module to be re-exported (see <a href="http://www.haskell.org/pipermail/haskell-cafe/2007-October/033141.html">7</a>).</p>
<p>This could be made easier. Suppose you could write this:</p>
<p></p>
<p>to construct a module called `Data.Maybe` that re-exports the module `Data.Maybe` from package `base-2.0`. This extension to the import syntax was proposed in PackageImports.</p>
<p>Using this extension, we can construct packages that re-export modules using only one stub module per re-exported module, and Cabal could generate the stubs for us given a suitable addition to the `.cabal` file syntax.</p>
<p>Package re-exports are useful for</p>
<p><code>* Constructing packages that are backwards-compatible with old packages by re-exporting parts of the new API.</code><br />
<code>* Providing a single wrapper for choosing one of several underlying providers</code></p>
<h2 id="provide-backwards-compatible-versions-of-base">4.1 Provide backwards-compatible versions of base</h2>
<p>So using re-exports we can construct a backwards-compatible version of base (`base-2.0` that re-exports `base-3.0` and the other packages that were split from it). We can do this for other packages that have changed, too. This is good because:</p>
<p><code>* Code is shared between the two versions of the package</code><br />
<code>* Multiple versions of each package can coexist in the same program easily (unlike in proposal 2)</code></p>
<p>However, this approach runs into problems when types or classes, rather than just functions, change. Suppose in `base-3.0` we changed a type somewhere; for example, we remove a constructor from the `Exception` type. Now `base-2.0` has to provide the old `Exception` type. It can do this, but the `Exception` type in `base-2.0` is now incompatible with the `Exception` type in `base-3.0`, so every function that refers to `Exception` must be copied into `base-2.0`. At this point we start to need to recompile other packages against `base-2.0` too, and before long we're back in the state of proposal (2) above.</p>
<p>This approach therefore doesn't scale to API changes that include types and classes, but it can cope with changes to functions only.</p>
<h2 id="rename-base-and-provide-a-compatibility-wrapper">4.2 Rename base, and provide a compatibility wrapper</h2>
<p>This requires the re-exporting functionality described above. When splitting base, we would rename the base package, creating several new packages. e.g. `base-3.0` would be replaced by `newbase-1.0`, `concurrent-1.0`, `generics-1.0`, etc. Additionally, we would provide a wrapper called `base-4.0` that re-exports all of the new packages.</p>
<p>Advantages:</p>
<p><code>* Updates to existing packages are much easier (no configurations required)</code><br />
<code>* Doesn't fall into the trap of trying to maintain a completely backwards-compatible version of the old API, as in 4.1</code></p>
<p>Disadvantages:</p>
<p><code>* All packages still break when the base API changes (if they are using precise dependencies on base, which they should be)</code><br />
<code>* Backwards compatibility cruft in the form of the `base` wrapper will be hard to get rid of; there's no</code><br />
<code>  incentive for packages to stop using it.  Perhaps we need a deprecation marker on packages.</code><br />
<code>* Each time we split base we have to invent a new name for it, and we accumulate a new compatibility wrapper</code><br />
<code>  for the old one.</code></p>
<h2 id="dont-rename-base">4.3 Don't rename base</h2>
<p>This is a slight variation on 4.2, in which instead of renaming `base` to `newbase`, we simply provide two versions of `base` after the split. Take the example of splitting `base-3.0` into `base + concurrent + generics` again:</p>
<p><code> * `base-4.0` is the remaining contents of `base-3.0` after the split</code><br />
<code> * `base-3.1` is a compatibility wrapper, re-exporting `base-4.0 + concurrent-1.0 + generics-1.0`.</code></p>
<p>The idea is that all existing packages that worked with `base-3.0` will have  or similar. To make these work after the split, all that is needed is to modify the upper bound:  which is better than requiring a conditional dependency, as was the case with the `base-3.0` split. In due course, these packages can be updated to use the new `base-4.0`.</p>
<p>Advantages: the same as 4.2, plus there's no need to rename `base` for each split. Disadvantages: multiple versions of `base` could get confusing. The upgrade path is still not completely smooth (existing packages all need to be modified manually).</p>
<h2 id="do-some-kind-of-providesrequires-interface-in-cabal">5. Do some kind of provides/requires interface in Cabal</h2>
<p>Currently, Cabal's idea of API is asymmetric and very coarse: the client depends on a package by name and version only, the provider implements a single package name and version by exposing a list of modules. That has several disadvantages:</p>
<p><code>* Cabal cannot ensure build safety: most errors will not show up before build-time (contrast that with Haskell's usual model of static type safety).</code><br />
<code>* Cabal has no idea what a dependency consists of unless it is installed. even if it is installed, it only knows the modules exposed. The actual API might be defined in Haddock comments, but is not formally specified or verified.</code></p>
<h3 id="make-api-specifications-more-symmetric">5.1 Make API specifications more symmetric</h3>
<p>Just as a provider lists the modules it exposes, clients should list the modules they import (this field should be inferred by a 'ghc -M'-style dependency analysis). Advantages:</p>
<p><code>* Cabal would have an idea which parts of a package a client depends on instead of defaulting to &quot;every client needs everything&quot; (example: clients using only parts of the old base not split off should be happy with the new base)</code><br />
<code>* Cabal would have an idea what a missing dependency was meant to provide (example: clients using parts of the old base that have been split off could be offered the split-off packages as alternative providers of the modules imported)</code></p>
<h3 id="make-api-specifications-explicit">5.2 Make API specifications explicit</h3>
<p>Currently, the name and version of a package are synonymous with its API. That is like modules depending on concrete data type representations instead of abstract types. It should not really matter that the functionality needed by package P was only available in package Q-2.3.42 at the time P was written. What should matter is which parts of Q are needed for P, and which packages are able to provide those parts when P is built.</p>
<p>Section 5.1 above suggests to make this specification at least at the level of modules, in both providers and clients. But even if one wanted to stay at the coarser level of API names and versions, one should distinguish between an API and one of its implementing packages. Each client should list the APIs it depends on, each provider should list the APIs it can be called upon to provide.</p>
<p>One can achieve some of this in current Cabal by introducing intermediate packages that represent named APIs to clients while re-exporting implementations of those APIs by providers. Apart from needing re-export functionality, this is more complicated than it should be.</p>
<h3 id="make-api-specifications-more-specific">5.3 Make API specifications more specific</h3>
<p>If one compares Cabal's ideas of packages and APIs with Standard ML's module language, with its structures, functors, and interfaces forming part of a statically typed functional program composition language, one can see a lot of room for development.</p>
<h2 id="distributions-at-the-hackage-level">6. Distributions at the Hackage level</h2>
<p>The idea here is to group packages into &quot;distributions&quot; in Hackage, with the property that all packages within a distribution are mutually compatible. Todo... expand.</p>
<h2 id="allow-package-overlaps">7. Allow package overlaps</h2>
<p>This is not a solution to the problem of splitting a package but helps in the case that we want to use a new package that provides an updated version of some modules in an existing package. An example of this is the bytestring and base package. The base-2.0 package included Data.ByteString but it was split off into a bytestring package and not included in base-3.0. At the moment ghc allows local .hs files to provide modules that can shadow modules from a package but does not packages to shadow each other.</p>
<p>So an extension that would help this case would be to let packages shadow each other. The user would need to specify an ordering on packages so ghc knows which way round the shadowing should go. This could be specified by the order of the -package flags on the command line, which is equivalent to the order in which they are listed in the build-depends field in a .cabal file. This would be a relatively easy extension to implement.</p>
<p>Note that it only solves the problem of backporting packages to be used on top of older versions of the package they were split from. It also provides a way for people to experiment with packages that provide alternative implementations of standard modules.</p>
<p>There is potential for confusion if this is used too heavily however. For example two packages built against standard and replacement modules may not be able to be used together because they will re-export different types.</p>
<h2 id="the-problem-of-lax-version-dependencies">The problem of lax version dependencies</h2>
<p>Supposing that we used solution 2 above and had a base-2.x and a base-3.x. If we take an old package and build it against base-2.x then it will work and if we build it against base-3.x then it'll fail because it uses modules from the split out packages like directory, bytestring etc. So obviously Cabal should select base-2.x, but how is this decision actually supposed to be made automatically? From a quick survey of the packages on hackage we find that 85% specify unversioned dependencies on the base package and none of them specify upper bounds for the version of base. So presented with a package that says:</p>
<p></p>
<p>how are we to know if we should use base-2.x or base-3.x. It may be that this package has been updated to work with base-3.x or that it only ever used the parts of base-2.x that were not split off. This dependency does not provide us with enough information to know which to choose. So we are still left with the situation that every package must be updated to specify an api version of base.</p>
<p>One possible remedy would be to call version 3 something other than base. Any dependency on 'base' would then refer to the set of modules that comprise base-2.x (this is (4.2) above, incedentally).</p>
<h1 id="note-about-this-page">Note about this page</h1>
<p><code>   </code><em><code>Apparently,</code> <code>this</code> <code>page</code> <code>is</code> <code>out</code> <code>of</code> <code>date</code> <code>and</code> <code>the</code> <code>issue</code> <code>has</code> <code>been</code> <code>settled</code> <code>in</code> <code>favour</code> <code>of</code> <code>the</code> <code>syntax:</code></em><br />
<code>   </code><br />
<code>   See also:</code><br />
<code>   </code><a href="http://haskell.org/ghc/docs/latest/html/users_guide/syntax-extns.html#package-imports"><code>8</code></a></p>
<h1 id="explicit-package-imports">Explicit package imports</h1>
<p>This proposal is one possibility for addressing the question of identifying which package is meant in an import declaration. For the context, read the [wiki:Commentary/Packages/GhcPackagesProposal GHC packages summary page] first.</p>
<p>The main idea of this proposal is to allow the programmer to specify the source package in the import line, something like this:  That would presumably get the most recent installed incarnation of the  package. If you want a particular version of the package, we could allow  The exact syntax is unimportant. The important thing is that the programmer can specify the package in the source text. Note that this fundamentally conflicts with the second assumption we started with. We were trying to avoid specifying &quot;provenance&quot; at the same time as &quot;purpose&quot;, on the grounds that we wanted to avoid editing lots of source text when the provenance changed. (And so it begs the question, if we need to edit the source anyway, why separate the syntax of packages from modules at all?)</p>
<p>If we adopt the idea that an import statement can specify the source package, several design choices arise:</p>
<h2 id="is-the-from-compulsory">Is the 'from <package>' compulsory?</h2>
<p>If you want to import A.B.C, a module exported by package &quot;foo&quot;, can you say just , or must you say ?</p>
<p>We think of this as rather like the question &quot;If you import f from module M, can you refer to it as plain &quot;f&quot;, or must you refer to it as &quot;M.f&quot;? The answer in Haskell 98 is that you can refer to it as plain &quot;f&quot; so long as plain &quot;f&quot; is umambiguous; otherwise you can use a qualified reference &quot;M.f&quot; to disambiguate.</p>
<p>We propose to adopt the same principle for imports. That is, an import with no package specified, such as &quot;&quot;, means:</p>
<p><code>  Find all modules A.B.C exported by all exposed packages, or the package or program being compiled. If there is exactly one such module, that's the one to import. Otherwise report &quot;ambiguous import&quot;.</code></p>
<p>If the reference to A.B.C is ambiguous, you can qualify the import by adding &quot;&quot;.</p>
<h2 id="package-versions">Package versions</h2>
<p>We probably want some special treatment for multiple versions of the same package. What if you have both &quot;foo-3.9&quot; and &quot;foo-4.0&quot; installed, both exporting A.B.C? This is jolly useful when you want to install new packages, but keep old ones around so you can try your program with the older one. So we propose that this is not regarded as ambiguous: importing A.B.C gets the latest version, unless some compiler flag (-hide-package) takes it of the running.</p>
<p>In short, an installed package can be of two kinds:</p>
<p><code> * </code><strong><code>Exposed</code></strong><code>: the package's modules populate the global module namespace, and can be imported without mentioning the pacckage name explicitly (</code><code>).  Explicit &quot;from&quot; imports may be used to resolve ambiguity.</code><br />
<code> * </code><strong><code>Available</code></strong><code>, but not exposed: the package can be used only by an explicit &quot;from&quot; import.  This is rather like &quot;</code><code>, except at the package level.  </code></p>
<p>Typically, if multiple versions of the same package are installed, then all will be available, but only one will be exposed.</p>
<p>GHC's command-line flags (, ) can be used to manipulate which packages are exposed, but typically an entire package or program will be compiled with a single set of such flags. GHC does not curretly support in-module control, thus , and we do not propose to change that.</p>
<p>Simon suggested that an installed package might be hidden (so that it cannot be used at all) but I'm not sure why we need that.</p>
<h2 id="importing-from-the-home-package">Importing from the home package</h2>
<p>If A.B.C is in the package being compiled (which we call &quot;the home package&quot;), and in an exposed package, and you say , do you get an &quot;ambiguous import&quot; error , or does the current package override. And if the former, how can you say &quot;import A.B.C from the current package&quot;?</p>
<p>One possibility is to reuqire the code to know its own package name, and mention that in the import. For exmaple, in a module that is being compiled as part package &quot;foo&quot;, you'd say . What about modules that are part of the main program (not a package at all). Perhaps you could then say .</p>
<p>Another way is to have a special package name meaning &quot;the home package&quot;. The special name could be</p>
<p><code>* &quot;&quot;</code><br />
<code>* &quot;home&quot;</code><br />
<code>* &quot;this&quot;</code><br />
<code>* this (with no quotes)</code></p>
<h2 id="the-as-p-alias">The 'as P' alias</h2>
<p>We propose to maintain the local, within-module &quot;as P&quot; alias mechanism unchanged. Thus:  Here, the qualified name &quot;M.T&quot; refers to the T imported from A.B.C in package &quot;foo&quot;.</p>
<h2 id="qualified-names">Qualified names</h2>
<p>We propose that the default qualified name of an entity within a module is just the module name plus the entity name. Thus  If you want to import multiple A.B.C's (from different packages) then perhaps they define different entities, in which case there is no problem:  But if they both export entities with the same name, there is no alternative to using the 'as M' mechanism: </p>
<h2 id="exporting-modules-from-other-packages">Exporting modules from other packages</h2>
<p>It is perfectly OK to export entities, or whole modules, imported from other packages: </p>
<h2 id="syntax">Syntax</h2>
<p>Should package names be in quotes? Probably yes, because they have a different lexcal syntax to the rest of Haskell. (&quot;foo-2.3&quot; would parse as three tokens, &quot;foo&quot;, &quot;-&quot;, and &quot;2.3&quot;.</p>
<p>It's been suggested that one might want to import several modules from one package in one go:  What we don't like about that is that it needs a new keyword &quot;&quot;. Perhaps all imports can start with the keyword , and then we are free to use extra (context-specific) keywords. (Haskell already has several of these, such as . Something like this:</p>
<p> Here the layout is explicit, but the braces and semicolons could be avoided by making use of the layout rule as usual.</p>
<p>Indeed, we could allow this multiple form even for ordinary imports: </p>
<p>It is clear from the above examples that the keyword  is redundant - the presence of a string literal (or special keyword to denote the home package) after the keyword  is sufficient to distinguish per-package imports from the ordinary shared-namespace imports, so the above could instead be written as </p>
<h3 id="syntax-formalised-and-summarised">Syntax formalised and summarised</h3>
<p>A possible syntax which covers everything in this proposal is therefore:</p>
<p><code>  </code><strong><code>import</code></strong><code> [</code><em><code>package-name</code></em><code>] </code><strong><code>{</code></strong><code> </code><em><code>import-specifier</code></em><code> [</code><strong><code>;</code></strong><code> </code><em><code>import-specifier</code></em><code>] </code><strong><code>}</code></strong></p>
<p>where <em>package-name</em> is a string literal or the keyword , the <em>import-specifier</em> corresponds to everything that is currently allowed after the keyword , and the braces and semicolons would be added by the layout rule. </p>
<h3 id="proposal-for-package-mounting">Proposal for Package Mounting</h3>
<p>It may help to refer to [wiki:Commentary/Packages/GhcPackagesProposal] for an introduction to some of the issues mentioned here.</p>
<p>A message by Frederik Eaton to the Haskell mailing list describing the present proposal is archived: <a href="http://www.haskell.org/pipermail/libraries/2005-June/004009.html">9</a>. (Also, see note at the end of this document regarding an earlier proposal by Simon Marlow)</p>
<p>This document will go over Frederik's proposal again in brief. The proposal doesn't involve any changes to syntax, only an extra command line option to , etc., and a small change to Cabal syntax.</p>
<p>In this proposal, during compilation of a module, every package would have a &quot;mount point&quot; with respect to which its particular module namespace would be resolved. Each package should have a default &quot;mount point&quot;, but this default would be overridable with an option to , etc.</p>
<p>For example, the  library currently has module namespace:</p>
<p></p>
<p>In this proposal, it might instead have default mount point  and (internal) module namespace:</p>
<p></p>
<p>To most users of the X11 package, there would be no change - because of the mounting, modules in that package would still appear with the same names in places where the X11 package is imported: , etc. However, if someone wanted to specify a different the mount point, he could use a special compiler option, for instance :</p>
<p></p>
<p>(so the imported namespace would appear as , , etc.) Note that the intention is for each  option to refer to the package specified in the preceding  option, so to give package  a mount point of  we use the syntax</p>
<p></p>
<p>Ideally one would also be able to link to two different versions of the same package, at different mount points:</p>
<p></p>
<p>(yielding , , ...; , , ...)</p>
<p>However, usually the default mount point would be sufficient, so most users wouldn't have to learn about .</p>
<p>Additionally, Cabal syntax should be extended to support mounting. I would suggest that the optional mount point should appear after a package in the Build-Depends clause of a Cabal file:</p>
<p></p>
<p>And in the package Cabal file, a new clause to specify the default mount point:</p>
<p></p>
<h3 id="evaluation">Evaluation</h3>
<p>This proposal has several advantages over the [wiki:Commentary/Packages/PackageImportsProposal] proposal.</p>
<p><code>* </code><em><code>No</code> <code>package</code> <code>names</code> <code>in</code> <code>code</code></em><code>. In this proposal, package names would be decoupled from code. This is very important. It should be possible to rename a package (or create a new version of a package with a new name), and use it in a project, without editing every single module of the project and/or package. Even if the edits could be done automatically, they would still cause revision control headaches. Any proposal which puts package names in Haskell source code should be considered unacceptable.</code></p>
<p><code>* </code><em><code>No</code> <code>syntax</code> <code>changes</code></em><code>. The [wiki:Commentary/Packages/PackageImportsProposal] proposal requires new syntax, but this proposal does not. Of course, in this proposal it would be slightly more difficult for the programmer to find out which package a module is coming from. He would have to look at the command line that compiles the code he's reading. However, I think that that is appropriate. Provenance should not be specified in code, since it changes all the time. (And there could be a simple debugging option to GHC which outputs a description of the namespace used when compiling each file)</code></p>
<p><code>* </code><em><code>Simpler</code> <code>module</code> <code>names</code></em><code>. This proposal would allow library authors to use simpler module names in their packages, which would in turn make library code more readable, and more portable between projects. For instance, imagine that I wanted to import some of the code from the </code><code> library into my own project. Currently, I would have to delete every occurrence of </code><code> in those modules. Merging future changes after such an extensive modification would become difficult. This is a real problem, which I have encountered while using John Meacham's curses library. There are several different versions of that library being used by different people in different projects, and it is difficult to consolidate them because they all have different module names. The reason they have different module names is that package mounting hasn't been implemented yet. The [wiki:Commentary/Packages/PackageImportsProposal] proposal would not fix the problem.</code></p>
<p><code>* </code><em><code>Development</code> <code>decoupled</code> <code>from</code> <code>naming</code></em><code>. (there is a bit of overlap with previous points here) In the present proposal, programmers would be able to start writing a library before deciding on a name for the library. For instance, every module in the </code><code> library contains the prefix </code><code>. This means that either the author of the library had to choose the name </code><code> at the very beginning, or he had to make several changes to the text of each module after deciding on the name. Under the present proposal, he would simply call his modules </code><code>, </code><code>, </code><code>, etc.; the </code><code> prefix would be specified in the build system, for instance in the Cabal file.</code></p>
<p>Frederik's mailing list message discusses some other minor advantages, but the above points are the important ones. In summary, it is argued that the above proposal should be preferred to [wiki:Commentary/Packages/PackageImportsProposal] because it is both easier to implement (using command line options rather than syntax), and more advantageous for the programmer.</p>
<h3 id="note-on-package-grafting">Note on Package Grafting</h3>
<p>A proposal by Simon Marlow for &quot;package grafting&quot; predates this one: <a href="http://www.haskell.org/pipermail/libraries/2003-August/001310.html">10</a>. However, the &quot;package grafting&quot; proposal is different in that it suggests selecting a &quot;mount point&quot; at library installation time, where in the present proposal, the &quot;mount point&quot; is selected each time a module using the library in question is compiled. The difference is important, as one doesn't really want to have to install a new copy of a library just to use it with a different name. Also, Simon Marlow's proposal puts package versions in the module namespace and therefore source code, where we argue for decoupling source code from anything to do with provenance - be it package names or version numbers.</p>
<p><a href="PageOutline" class="uri" title="wikilink">PageOutline</a></p>
<h2 id="alternative-proposal-for-packages-with-explicit-namespaces">Alternative Proposal for Packages (with explicit namespaces)</h2>
<p>This proposal is an alternative to [wiki:Commentary/Packages/GhcPackagesProposal]. Large parts overlap with that proposal. To motivate this new proposal, let's consider another proposed and desirable feature of the import/export language, which may interact in interesting ways with packages.</p>
<h2 id="a-different-but-related-problem">A different, but related, problem</h2>
<p>A problem that has been mentioned several times on mailing lists, is grafting part of a directory hierarchy into an arbitrary location elsewhere in the hierarchy. (See <a href="http://www.haskell.org/pipermail/libraries/2005-June/004009.html">11</a>)</p>
<p>Another way of expressing a similar wish is the ability to re-export imports with a different qualified name, as in the scenario suggested by the developers of the package gtk2hs: <a href="http://www.haskell.org/pipermail/libraries/2004-December/002800.html">12</a></p>
<p>There are several desires in play here:</p>
<p><code>* a desire to minimise typing of long qualified names</code><br />
<code>* a desire to refer to &quot;leaf&quot; nodes of the hierarchy in a way that makes it easy to relocate those modules in the hierarchy, without needing to edit every import declaration that uses them</code><br />
<code>* a desire to partially-qualify names for disambiguation</code></p>
<h2 id="proposal">Proposal</h2>
<p>We introduce the new concept of <em>namespace</em> as something that can be declared in source code. A namespace can contain only module names. (The specification of what module names are contained in a namespace is rather like our current concept of a package, i.e. not declared in the source code, but rather by some external mechanism e.g. grouping of files in a filesystem hierarchy.)</p>
<p>There are now two separate kinds of .</p>
<p><code>* </code><br />
<code>* </code></p>
<p>The new semi-reserved word  is introduced, having special meaning only directly after the  keyword. There is a <em>level</em> difference in what this new form of import means. The declaration  brings into availability the subset of the hierarchy of <em>module</em> names rooted in the package , at the position . That is, if the package  version  contains the modules</p>
<p><code>* Data.Foo.Bar</code><br />
<code>* Data.Foo.Baz</code><br />
<code>* Data.Bar</code></p>
<p>then the namespace import brings into the &quot;importable&quot; namespace only the modules</p>
<p><code>* Data.Foo.Bar</code><br />
<code>* Data.Foo.Baz</code></p>
<p>However, for the program to use those modules, it is still necessary to go ahead and actually  them in the normal way, although the names used to import them will now be <em>relative</em> to the available namespaces, rather than absolute. So the declaration  brings into scope all the entities defined in . Like any normal import, these can be qualified or hidden.</p>
<p>Thus,</p>
<p><code>* </code><code> brings into scope a bunch of names for modules</code><br />
<code>  from the given provenance.</code><br />
<code>* </code><code> brings into scope a bunch of entities from the given</code><br />
<code>  module.</code></p>
<h3 id="naming-a-namespace">Naming a namespace</h3>
<p>Are namespaces first class? Can we give them a name? Indeed, why not?</p>
<p><code>* </code><br />
<code>* </code></p>
<p>Here, we have declared that we want to be able to refer to the namespace as , and so, a subsequent  specifically asks for the  from the package , just in case there might be a  module also available from another namespace.</p>
<h3 id="what-namespaces-are-available-by-default">What namespaces are available by default?</h3>
<p>If no namespaces are explicitly brought into scope, what modules are implicitly available?</p>
<p><code>* Anything in the </code><em><code>current</code></em><code> package, i.e. the executable or library</code><br />
<code>  whose modules are all physically rooted at the same location in the</code><br />
<code>  filesystem as this module.</code></p>
<p><code>* Is there an implicit </code><code>, just as there is an</code><br />
<code>  implicit </code><code>?</code></p>
<h3 id="namespace-resolution">Namespace resolution</h3>
<p>In essence, namespaces take over the role formerly played by commandline arguments like  and . The search path used by the compiler for finding modules is now partially declared in the source code itself. (Note however that that the search path is declared symbolically, involving package names, not directories. This is a very important separation of the thing itself from where it is stored.)</p>
<p>Resolution of which module is referred to by an import statement (taking into account the namespaces) is just like the current process of resolving which entity is referred to by program text (taking into account the imported modules). The source text may import multiple namespaces. If any module import is ambiguous (i.e. the module exists in more than one namespace), it is a static error. Resolution is lazy, in the sense that there is no error if namespaces contain the same module name, only if the program tries to import that module name.</p>
<p>So when you say &quot;import A.B.C&quot;, from what package does A.B.C come?</p>
<p>There must be a single namespace in scope containing a module called . (Sidenote: or in fact a namespace called , containing a module named )</p>
<h3 id="syntax-1">Syntax</h3>
<p>The precise syntax can be debated. New keywords like  or  could be substituted for . The key important features however are the inclusion of:</p>
<p><code>* the package name (mandatory)</code><br />
<code>* an optional package version, if several are available</code><br />
<code>* an optional path to use as the root of the available namespace</code><br />
<code>* an optional renaming</code></p>
<h3 id="exports-1">Exports</h3>
<p>One might wonder whether it is now either necessary or desirable to permit <em>namespaces</em> to be re-exported in the same way that <em>modules</em> can be? For instance:</p>
<p></p>
<p>The idea is that any module saying  would thereby implicitly open the namespace of package  at the root , in addition to having access to entities defined in  itself.</p>
<p>Note that, just as with a current module re-export it is no longer possible for the importing location to use the original module name as a qualifier; so with a namespace re-export, there is no way to refer to the namespace in the importing location either. It is purely a signal to the compiler telling it where to look for modules when resolving imports.</p>
<p>I argue that namespace export <em>is</em> desirable, because it allows (but does not require) all package (namespace) dependencies to be gathered together in a single module for an entire project. With such an organising principle, when dependencies change, there is only one source file to update. But without namespace re-exports, it would be impossible to localise those dependencies to a single file.</p>
<p>Note how this feature addresses several of the initial stated desires, of reducing the verbosity of imports, and of referring to leaf modules conveniently. For instance:</p>
<p></p>
<h3 id="implicit-imports">Implicit imports</h3>
<p>One could go further. If I write a qualified name  in the source text, must I also write  at the top? The qualified entity is unambiguous, whether or not there is an explicit import for it, because the module qualification  must be unambiguous within the current namespaces. In the Gtk example above, this would eliminate the need for , and who knows how many other imports, leaving a single  to bring all of the qualified entities into scope.</p>
<h3 id="exposed-vs-hidden-packages">Exposed vs Hidden packages</h3>
<p>GHC's scheme of exposed vs hidden packages can now be replaced with full source-code control of namespace visibility. To setup a default set of exposed packages, you just write a module to export their namespaces:</p>
<p></p>
<p>and import it in every module of your project. Or if importing it everywhere sounds too painful, one can even imagine that a compiler might provide a command-line option (or use a configuration file) to specify one distinguished module to be implicitly imported everywhere:</p>
<p></p>
<h3 id="what-if-you-wanted-to-import-a.b.c-from-p1-and-a.b.c-from-p2-into-the-same-module">What if you wanted to import A.B.C from P1 and A.B.C from P2 into the <em>same</em> module?</h3>
<p></p>
<p><a href="PageOutline" class="uri" title="wikilink">PageOutline</a></p>
<h1 id="package-reorg">Package Reorg</h1>
<p>In this page we collect proposals and design discussion for reorganising the packages that come with compilers, and the contents of those packages.</p>
<p>None of the ideas herein are claimed to belong to any particular person, many of the ideas have been extracted from mailing list discussions, eg.</p>
<p><a href="http://www.haskell.org/pipermail/libraries/2006-November/006396.html"><code>13</code></a></p>
<p>Some of the points are GHC-specific. Please feel free to insert points specific to other compilers.</p>
<h2 id="goals-1">Goals</h2>
<p><code>* It would be good to have set of 'core' packages that is installed with</code><br />
<code>  every Haskell implementation.  More on this at PackageReorg/Rationale page</code><br />
<code>* Forwards compatibility.  Users would like their programs written against the 'core' packages to continue to work, without</code><br />
<code>  modification to source text or build system, after upgrading the</code><br />
<code>  compiler, or its packages, or switching to a different compiler.</code><br />
<code>* Backwards compatibility.  Users would like to be able to take a</code><br />
<code>  program written against some version of the 'core' packages, and</code><br />
<code>  build it with an older compiler, accepting that they may have to</code><br />
<code>  install newer versions of the 'core' packages in order to do so.</code></p>
<p>It may not be possible to fully achieve these goals (in particular, backwards compatibility), but that does not mean we should not aim for them.</p>
<h2 id="proposal-1">Proposal</h2>
<p>Here's a straw-man proposal</p>
<p><code>* There is a set of packages that come with every conforming Haskell</code><br />
<code>  implementation.  Let's call these the </code><strong><code>Core</code> <code>Packages</code></strong><code> to</code><br />
<code>  avoid confusion (Bulat called these the &quot;base packages&quot;, but that's an </code><br />
<code>  over-used term given that there is a package called `base`).</code><br />
<code>  The good thing about the Core Packages is that</code><br />
<code>  users know that they will be there, and they are consistent with</code><br />
<code>  each other.</code></p>
<p><code>* Any particular implementation may install more packages by default;</code><br />
<code>  for example GHC will install the `template-haskell` and `stm`</code><br />
<code>  packages.  Let's call these the </code><strong><code>GHC</code> <code>Install</code> <code>Packages</code></strong><code>, '''Hugs</code><br />
<code>  Install Packages''' etc; the Install Packages are a superset of the</code><br />
<code>  Core Packages.</code></p>
<h3 id="what-is-in-the-core-packages">What is in the Core Packages?</h3>
<p>The Core Packages are installed with every conforming Haskell implementation. What should be in the Core? There is a tension:</p>
<p><code> 1. </code><strong><code>As</code> <code>much</code> <code>as</code> <code>possible</code></strong><code>; which means in practice widely-used and reasonably stable packages.  It is convenient for programmers to have as much as possible in a consistent, bundle that is (a) known to work together bundle, and (b) known to work on all implementations.  </code><a href="BR" title="wikilink"><code>BR</code></a><a href="BR" title="wikilink"><code>BR</code></a><br />
<code> 2. </code><strong><code>As</code> <code>little</code> <code>as</code> <code>possible</code></strong><code>; which in practice means enough to run Cabal so that you can run the Setup files that come when downloading new packages.  As Ian puts it: the less we force the implementations to come with, the quicker compilation will be when developing, the smaller Debian packages (for example) can be, the lower the disk space requirements to build GHC, the lower the time wasted when a Debian package (for example) build fails and the fewer packages we are tangling up with compiler release schedules.</code></p>
<p>There's a real choice here: Bulat wants (1) and Ian wants (2).</p>
<p>Initial stab at (1):</p>
<p><code> * `base`</code><br />
<code> * `Cabal`</code><br />
<code> * `haskell98`</code><br />
<code> * Some `regex` packages (precisely which?)</code><br />
<code> * `unix` or `Win32`. Questionable, partly because it means the Core interface becomes platform-dependent; and partly because `Win32` would double the size of the Hugs distribution.</code><br />
<code> * `parsec`</code><br />
<code> * `mtl`</code><br />
<code> * `time`</code><br />
<code> * `network`</code><br />
<code> * `QuickCheck` (questionable)</code><br />
<code> * `HUnit` (questionable)</code></p>
<p>Initial stab at (2):</p>
<p><code> * `base`</code><br />
<code> * `haskell98`</code><br />
<code> * `Cabal`</code><br />
<code> * `filepath` (?)</code></p>
<p>Bulat: i think that all regex packages should be included and of course libs that helps testing. overall, it should be any general-purpose lib that porters accept (enlarging this set makes users live easier, and porters live harder)</p>
<p>about unix/win32 - these libs provide access to OS internals, not some everywhere-portable API. moreover, other world-interfacing libs (i/o, networking) should use APIs provided by these libs with a conditional compilation (CPPery) tricks in order to provide portable APIs! current situation where such libs use FFI isn't ideal. WinHugs size problem is rather technical - it includes a lot of DLLs which contains almost the same code</p>
<p>i agree to start with minimal stub, and then proceed with discussing inclusion of each library. what we need now is requirements to include library in this set and lifetime support procedure. so:</p>
<h3 id="requirements-to-libraries-to-be-included-in-core-set">Requirements to libraries to be included in core set</h3>
<p><code>* BSD-licensed, and even belongs to Haskell community?</code><br />
<code>* portable (is sense of compiler and OS), may be just Haskell' compatible?</code><br />
<code>* already widely used</code><br />
<code>* shouldn't duplicate existing core libs functionality (?)</code></p>
<p>Exact inclusion, support and exclusion processes?</p>
<h3 id="the-base-package">The base package</h3>
<p>The base package is a bit special</p>
<p><code>* Package `base` is rather big at the moment.  </code></p>
<p><code>* From a user's point of view it would be nicer to give it a</code><br />
<code>  compiler-independent API.  (A module like `GHC.Exts` would move to</code><br />
<code>  a new package `ghc-base`.)</code></p>
<p>Thinking of GHC alone for a moment, we could have a package `ghc-base` (which is pretty much the current `base`) and a thin wrapper package `base` that re-exposes some, but not all, of what `ghc-base` exposes. To support this re-exposing, we need a small fix to both GHC and Cabal, but one that is independently desirable.</p>
<p>Similarly, Hugs could build `hugs-base` from the same souce code, by using CPP-ery, exactly as now. The thin `base` wrapper package would not change.</p>
<p>To make `base` smaller, we could remove stuff, and put it into separate packages. But be careful: packages cannot be cyclic, so anything that is moved out can't be used in `base`. Some chunks that would currently be easy to split off are:</p>
<p><code>* Data.!ByteString.* (plus future packed Char strings)</code><br />
<code>* Control.Applicative (?), Data.Foldable, Data.Monoid (?), Data.Traversable, Data.Graph, Data.!IntMap, Data.!IntSet, Data.Map, Data.Sequence, Data.Set, Data.Tree</code><br />
<code>* System.Console.!GetOpt</code><br />
<code>* Text.!PrettyPrint.*</code><br />
<code>* Text.Printf</code></p>
<p>Some other things, such as arrays and concurrency, have nothing else depending on them, but are so closely coupled with GHC's internals that extracting them would require exposing these internals in the interface of `base`.</p>
<p>Bulat: my ArrayRef library contains portable implementation of arrays. there is only thin ghc/hugs-specific layer which should be provided by ghcbase/hugsbase libs. except for MPTC problem (IArray/MArray classes has multiple parameters), this library should be easily portable to any other haskell compiler</p>
<p>See also BaseSplit.</p>
<h3 id="other-packages">Other packages</h3>
<p>Other non-core packages would probably have their own existence. That is, they don't come with an implementation; instead you use `cabal-get`, or some other mechanism, such as your OS's package manager. Some of these currently come with GHC, and would no longer do so</p>
<p><code> * `GLUT`</code><br />
<code> * `ALUT`</code><br />
<code> * `OpenAL`</code><br />
<code> * `OpenGL`</code><br />
<code> * `HGL`</code><br />
<code> * `HUnit`</code><br />
<code> * `ObjectIO`</code><br />
<code> * `X11`</code><br />
<code> * `arrows`</code><br />
<code> * `cgi`</code><br />
<code> * `fgl`</code><br />
<code> * `html`</code><br />
<code> * `xhtml`</code></p>
<p>Bulat: i propose to unbundle only graphics/sound libs because these solves particular problems and tends to be large, non-portable (?) and some are just legacy ones - like ObjectIO. we should keep everything small &amp; general purpose, including HUnit, arrows, fgl, html and xhtml, and include even more: ByteString, regex-*, Edison, Filepath, MissingH, NewBinary, QuickCheck, monads</p>
<h2 id="testing-1">Testing</h2>
<p>We should separate out package-specifc tests, which should be part of the repository for each package. Currently they are all squashed together into the testsuite repository.</p>
<h2 id="implementation-specific-notes">Implementation-specific notes</h2>
<h3 id="notes-about-ghc">Notes about GHC</h3>
<p>Currently GHC installs a set of packages by default, the so-called <strong>GHC Boot Packages</strong>. They are graphed here, with arrows representing dependencies between them: <a href="Image(packagegraph.png,_800)" title="wikilink">Image(packagegraph.png, 800)</a></p>
<p>These are exactly the libraries required to build GHC. That shouldn't be the criterion for the core packages.</p>
<p>One reason we do this is because it means that every GHC installation can build GHC. Less configure-script hacking. (NB: even today if you upgrade any of these packages, and then build GHC, the build might fail because the CPP-ery in GHC's sources uses only the version number of GHC, not the version number of the package.)</p>
<p>Still, for convenience we'd probably arrange that the GHC Install Packages included all the GHC Boot Packages.</p>
<p>Every GHC installation must include packages: `base`, `ghc-prim`, `integer` and `template-haskell`, else GHC itself will not work. (In fact `haskell98` is also required, but only because it is linked by default.)</p>
<p>So GHC's Install Packages would be the Core Packages plus</p>
<p><code>* `template-haskell`</code><br />
<code>* `editline`</code><br />
<code>* `integer`</code><br />
<code>* `ghc-prim`</code></p>
<p>You can upgrade any package, including `base` after installing GHC. However, you need to take care. You must not change a number of things that GHC &quot;knows about&quot;. In particular, these things must not change</p>
<p><code>* Name</code><br />
<code>* Defining module</code></p>
<p>GHC knows even more about some things, where you must not change</p>
<p><code>* Type signature</code><br />
<code>* For data types, the names, types, and order of the constructors</code></p>
<p>The latter group are confined to packages base and template-haskell.</p>
<p>(Note: a few other packages are used by tests in GHC's test suite, currently: `mtl`, `QuickCheck`. We should probably eliminate the mtl dependency; but `QuickCheck` is used as part of the test infrastructure itself, so we'll make it a GHC Boot Package.)</p>
<h3 id="notes-about-hugs">Notes about Hugs</h3>
<p>Recent distributions of Hugs come in two sizes, jumbo and minimal. Minimal distributions include only the packages `base`, `haskell98` and `Cabal`. (Hugs includes another package `hugsbase` containing interfaces to Hugs primitives.) The requirements for this set are to</p>
<p><code>* run Haskell 98 programs</code><br />
<code>* allow packages to be added and upgraded using Cabal</code></p>
<p>(Currently `cpphs` is a Haskell 98 program, so the latter implies the former.)</p>
<p>It should be possible to upgrade even the core packages using Cabal.</p>
<h1 id="commentary-the-package-system">Commentary: The Package System</h1>
<p>See also: [wiki:Commentary/Compiler/Packages Packages], where we describe how this is implemented in GHC.</p>
<h2 id="architecture">Architecture</h2>
<p>GHC maintains a package database, that is basically a list of `InstalledPackageInfo`. The `InstalledPackageInfo` type is defined in `Distribution.InstalledPackageInfo` in Cabal, and both `ghc-pkg` and GHC itself import it directly from there.</p>
<p>There are four main components of the package system:</p>
<p><code>Cabal::</code><br />
<code>  Cabal is a Haskell library, which provides basic data types for the package system, and support for building,</code><br />
<code>  configuring, and installing packages.</code></p>
<p><code>GHC itself::</code><br />
<code>  GHC reads the package database(s), understands the flags `-package`, `-hide-package`, etc., and uses the package database</code><br />
<code>  to find `.hi` files and library files for packages.  GHC imports modules from Cabal.</code></p>
<p><code>`ghc-pkg`::</code><br />
<code>  The `ghc-pkg` tool manages the package database, including registering/unregistering packages, queries, and</code><br />
<code>  checking consistency.  `ghc-pkg ` also imports modules from Cabal.</code></p>
<p><code>`cabal-install`::</code><br />
<code>  A tool built on top of Cabal, which adds support for downloading packages from Hackage, and building and installing</code><br />
<code>  multiple packages with a single command.</code></p>
<p>For the purposes of this commentary, we are mostly concerned with GHC and `ghc-pkg`.</p>
<h2 id="identifying-packages">Identifying Packages</h2>
<p><code>`Cabal.PackageName` (&quot;base&quot;)::</code><br />
<code>   A string.  Defined in `Distribution.Package`.  Does not uniquely identify a package: the package</code><br />
<code>   database can contain several packages with the same name.</code></p>
<p><code>`Cabal.PackageId` (&quot;base-4.1.0.0&quot;)::</code><br />
<code>   A `PackageName` plus a `Version`.  A `PackageId` names an API.  If two `PackageId`s are</code><br />
<code>   the same, they are assumed to have the same API.</code><br />
<code>   </code><a href="BR" title="wikilink"><code>BR</code></a><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>   `InstalledPackageInfo` contains the field `sourcePackageId :: PackageId`.</code><br />
<code>   </code><a href="BR" title="wikilink"><code>BR</code></a><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>   In GHC 6.11, the `PackageId` also uniquely identifies a package instance in the package database, but</code><br />
<code>   only by convention (we may lift this restriction in the future, and allow the database to contain</code><br />
<code>   multiple package instances with the same `PackageId` (and different `InstalledPackageId`s).</code></p>
<p><code>`Cabal.InstalledPackageId` (&quot;base-4.1.0.0-1mpgjN&quot;):: </code><br />
<code>   (introduced in GHC 6.12 / Cabal 1.7.2) A string that uniquely identifies a package instance in the database.</code><br />
<code>   An `InstalledPackageId` identifies an ABI: if two `InstalledPackageIds` are the same, they have the</code><br />
<code>   same ABI.</code><br />
<code>   </code><a href="BR" title="wikilink"><code>BR</code></a><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>   `InstalledPackageInfo` contains the field `installedPackageId :: InstalledPackageId`.</code><br />
<code>   </code><a href="BR" title="wikilink"><code>BR</code></a><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>   Dependencies between installed packages are identified by the `InstalledPackageId`.  An `InstalledPackageId` is</code><br />
<code>   chosen when a package is registered. It is chosen by calling `ghc --abi-hash` on the compiled modules and appending</code><br />
<code>   the hash as a suffix to the string representing the `PackageIdentifier`.</code></p>
<p><code>`GHC.PackageId` (these currently look like &quot;base-4.1.0.0&quot; in GHC 6.12)::</code><br />
<code>   Inside GHC, we use the type `PackageId`, which is a `FastString`.  The (Z-encoding of) `PackageId` prefixes each</code><br />
<code>   external symbol in the generated code, so that the modules of one package do not clash with those of another package,</code><br />
<code>   even when the module names overlap.</code></p>
<h2 id="design-constraints">Design constraints</h2>
<p><code>1. We want [wiki:Commentary/Compiler/RecompilationAvoidance recompilation avoidance] to work.  This means that symbol names should not contain any information that varies too often, such as the ABI hash of the module or package.  The ABI of an entity should depend only on its definition, and the definitions of the things it depends on.</code></p>
<p><code>2. We want to be able to detect ABI incompatibility.  If a package is recompiled and installed over the top of the old one, and the new version is ABI-incompatible with the old one, then packages that depended on the old version should be detectably broken using the tools.</code></p>
<p><code>3. ABI compatibility:</code><br />
<code>   * We want repeatable compilations.  Compiling a package with the same inputs should yield the same outputs.</code><br />
<code>   * Furthermore, we want to be able to make compiled packages that expose an ABI that is compatible (e.g. a superset)</code><br />
<code>     of an existing compiled package.</code><br />
<code>   * Modular upgrades: we want to be able to upgrade an existing package without recompiling everything that depends</code><br />
<code>     on it, by ensuring that the replacement is ABI-compatible.</code><br />
<code>   * Shared library upgrades.  We want to be able to substitute a new ABI-compatible shared library for an old one, and all the existing binaries linked against the old version continue to work.</code><br />
<code>   * ABI compatibility is dependent on GHC too; changes to the compiler and RTS can introduce ABI incompatibilities.  We</code><br />
<code>     guarantee to only make ABI incompatible changes in a major release of GHC.  Between major releases, ABI compatibility</code><br />
<code>     is ensured; so for example it should be possible to use GHC 6.12.2 with the packages that came with GHC 6.12.1.</code></p>
<p>Right now, we do not have repeatable compilations, so while we cannot do (3), we keep it in mind.</p>
<h2 id="the-plan-1">The Plan</h2>
<p>We need to talk about some more package Ids:</p>
<p><code> * `PackageSymbolId`: the symbol prefix used in compiled code.</code><br />
<code> * `PackageLibId`: the package Id in the name of a compiled library file (static and shared).</code></p>
<h3 id="detecting-abi-incompatibility">Detecting ABI incompatibility</h3>
<p><code> * in the package database, dependencies specify the `InstalledPackageId`.</code></p>
<p><code> * The package database will contain at most one instance of a given package/version combination.  The tools</code><br />
<code>   are not currently able to cope with multiple instances (e.g. GHC's -package flag selects by name/version).</code></p>
<p><code> * If, say, package P-1.0 is recompiled and re-installed, the new instance of the package will almost</code><br />
<code>   certainly have an incompatible ABI from the previous version.  We give the new package a distinct</code><br />
<code>   `InstalledPackageId`, so that packages that depend on the old P-1.0 will now be detectably broken.</code></p>
<p><code> * `PackageSymbolId`: We do not use the `InstalledPackageId` as the symbol prefix in the compiled code, because </code><br />
<code>   that interacts badly with [wiki:Commentary/Compiler/RecompilationAvoidance recompilation avoidance].  Every time we pick a</code><br />
<code>   new unique `InstalledPackageId` (e.g. when reconfiguring the package), we would have to recompile</code><br />
<code>   the entire package.  Hence, the `PackageSymbolId` is picked deterministically for the package, e.g.</code><br />
<code>   it can be the `PackageIdentifier`.</code></p>
<p><code> * `PackageLibId`: we do want to put the `InstalledPackageId` in the name of a library file, however.  This allows</code><br />
<code>   ABI incompatibility to be detected by the linker.  This is important for shared libraries too: we</code><br />
<code>   want an ABI-incompatible shared library upgrade to be detected by the dynamic linker.  Hence,</code><br />
<code>   `PackageLibId` == `InstalledPackageId`.</code></p>
<h3 id="allowing-abi-compatibilty">Allowing ABI compatibilty</h3>
<p><code>* The simplest scheme is to have an identifier for each distinct ABI, e.g. a pair of the package name and an integer</code><br />
<code>  that is incremented each time an ABI change of any kind is made to the package.  The ABI identifier</code><br />
<code>  is declared by the package, and is used as the `PackageSymbolId`.  Since packages with the same ABI identifier</code><br />
<code>  are ABI-compatible, the `PackageLibId` can be the same as the `PackageSymbolId`.</code></p>
<p><code>* The previous scheme does not allow ABI-compatible changes (e.g. ABI extension) to be made.  Hence, we could</code><br />
<code>  generalise it to a major/minor versioning scheme.</code><br />
<code>  * the ABI major version is as before, the package name + an integer.  This is also the `PackageSymbolId`.</code><br />
<code>  * the ABI minor version is an integer that is incremented each time the ABI is extended in a compatible way.</code><br />
<code>  * package dependencies in the database specify the major+minor ABI version they require, in addition to the</code><br />
<code>    `InstalledPackageId`.  They may be satisfied by a greater minor version; when upgrading a package with an </code><br />
<code>    ABI-compatible replacement, ghc-pkg updates dependencies to point to the new `InstalledPackageId`.</code><br />
<code>  * `PackageLibId` is the major version.  In the case of shared libraries, we may name the library using the</code><br />
<code>    major + minor versions, with a symbolic link from the major version to major+minor.</code><br />
<code>  * the shared library `SONAME` is the major version.</code></p>
<p><code>* The previous scheme only allows ABI-compatible changes to be made in a linear sequence.  If we want a tree-shaped</code><br />
<code>  compatibility structure, then something more complex is needed (ToDo).</code></p>
<p><code>* The previous schemes only allow compatible ABI changes to be made.  If we want to allow incompatible changes to be</code><br />
<code>  made, then we need something like ELF's symbol versioning.  This is probably overkill, since we will be making</code><br />
<code>  incompatible ABI changes in the compiler and RTS at regular intervals anyway, so long-term ABI compatibility is</code><br />
<code>  impractical at this stage.</code></p>
<p><a href="PageOutline" class="uri" title="wikilink">PageOutline</a></p>
<h1 id="the-parser">The Parser</h1>
<p>[Very incomplete. Please extend as you learn more.]</p>
<p>The parser is written using</p>
<p><code>* </code><a href="http://www.haskell.org/alex/"><code>Alex</code></a><code>, for lexical analysis.  Source file </code><a href="GhcFile(compiler/parser/Lexer.x)" title="wikilink"><code>GhcFile(compiler/parser/Lexer.x)</code></a><br />
<code>* </code><a href="http://www.haskell.org/happy/"><code>Happy</code></a><code>, for the parser itself.  Source file </code><a href="GhcFile(compiler/parser/Parser.y)" title="wikilink"><code>GhcFile(compiler/parser/Parser.y)</code></a><code>.</code><br />
<code>* `RdrHsSyn`, for Haskell support functions.  Source file </code><a href="GhcFile(compiler/parser/RdrHsSyn.lhs)" title="wikilink"><code>GhcFile(compiler/parser/RdrHsSyn.lhs)</code></a></p>
<h2 id="principles">Principles</h2>
<p>Making a parser parse <em>precisely</em> the right language is hard. So GHC's parser follows the following principle:</p>
<p><code>* </code><strong><code>We</code> <code>often</code> <code>parse</code> <code>&quot;over-generously&quot;,</code> <code>and</code> <code>filter</code> <code>out</code> <code>the</code> <code>bad</code> <code>cases</code> <code>later.</code></strong></p>
<p>Here are some examples:</p>
<p><code>* Patterns are parsed as expressions, and transformed from `HsExpr.HsExp` into `HsPat.HsPat` in `RdrHsSyn.checkPattern`.  An expression like `[x | x&lt;-xs]` that doesn't look like a pattern is rejected by `checkPattern`.</code></p>
<p><code>* The context of a type is parsed as a type, and then converted into a context by `RdrHsSyn.checkContext`.  For example, when parsing</code></p>
<p></p>
<p><code>  the parser can only discover that `(Read a, Num a)` is a context, rather than a type, when it meets the `=&gt;`.  That requires infinite lookahead.  So instead we parse `(Read a, Num a)` as a tuple type, and then convert it to a context when we see the `=&gt;`.</code></p>
<p>Sometimes the over-generous parsing is only dealt with by the renamer. For example:</p>
<p><code>* Infix operators are parsed as if they were all left-associative. The renamer uses the fixity declarations to re-associate the syntax tree.</code></p>
<p>There are plenty more examples. A good feature of this approach is that the error messages later in compilation tend to produce much more helpful error messages. Errors generated by the parser itself tend to say &quot;Parse error on line X&quot; and not much more.</p>
<p>The main point is this. If you are changing the parser, feel free to make it accept more programs than it does at the moment, provided you also add a later test that rejects the bad programs. Typically you need this flexibility if some new thing you want to add makes the pars ambiguous, and you need more context to disambiguate. Delicate hacking of the LR grammar is to be discouraged. It's very hard to maintain and debug.</p>
<h2 id="avoiding-right-recursion">Avoiding right-recursion</h2>
<p>Be sure to read <a href="https://www.haskell.org/happy/doc/html/sec-sequences.html">this section</a> of the Happy manual for tips on avoiding right recursion. In GHC, the preferred method is using a left-recursive `OrdList`, as below:</p>
<p></p>
<p>`OrdList` operationally works the same way as building a list in reverse (as in the Happy manual), but it makes it less likely you'll forget to call `reverse` when you need to get the `final` list out.</p>
<p>One interesting, non-obvious fact, is that if you *do* use a right-recursive parser, the &quot;extra semi-colons&quot; production should NOT be pluralized:</p>
<p></p>
<h2 id="indentation">Indentation</h2>
<p>Probably the most complicated interaction between the lexer and parser is with regards to //whitespace-sensitive layout.// The most important thing to know is that the lexer understands layout, and will output virtual open/close curlies (productions `vocurly` and `vccurly`) as well as semicolons, which can then be used as part of productions in `Parser.y`. So for example, if you are writing a rule that will make use of indentation, you should accept both virtual and literal curlies:</p>
<p></p>
<p>Notice the use of `close` rather than `vccurly`: `close` is a production that accepts both `vccurly` and a Happy `error`; that is, if we encounter an error in parsing, we try exiting an indentation context and trying again. This ensures, for example, that the top-level context can be closed even if no virtual curly was output.</p>
<p>The top-level of a Haskell file does not automatically have a layout context; when there is no `module` keyword, a context is implicitly pushed using `missing_module_keyword`.</p>
<p>When writing grammars that accept semicolon-separated sequences, be sure to include a rule allowing for trailing semicolons (see the previous section), otherwise, you will reject layout.</p>
<h2 id="syntax-extensions">Syntax extensions</h2>
<p>Many syntactic features must be enabled with a `LANGUAGE` flag, since they could cause existing Haskell programs to stop compiling, as turn some identifiers into keywords. We primarily affect this change of behavior in the lexer, by turning on/off certain tokens. This is done using predicates, which let Alex turn token rules on and off depending on what extensions are enabled:</p>
<p></p>
<p>To add a new syntax extension, add a constructor to `ExtBits` and set the bit appropriately in `mkPState`.</p>
<h1 id="pinned-objects">Pinned Objects</h1>
<p>The GC does not support pinning arbitrary objects. Only objects that have no pointer fields can be pinned. Nevertheless, this is a useful case, because we often want to allocate garbage-collectable memory that can be passed to foreign functions via the FFI, and we want to be able to run the GC while the foreign function is still executing (for a `safe` foreign call). Hence, the memory we allocated must not move.</p>
<p>Bytestrings are currently allocated as pinned memory, so that the bytestring contents can be passed to FFI calls if necessary.</p>
<p>The RTS provides an API for allocating pinned memory, in <a href="GhcFile(includes/rts/storage/GC.h)" class="uri" title="wikilink">GhcFile(includes/rts/storage/GC.h)</a>:</p>
<p></p>
<p>This allocates memory from the given Capability's nursery.</p>
<p>Pinned objects work in the GC as follows:</p>
<p><code>* Pinned objects are allocated into a block of their own, not mixed up with unpinned objects.</code><br />
<code>* The block containing pinned objects is marked as a </code><em><code>large</code> <code>block</code></em><code>, i.e. the `BF_LARGE` bit is set in `bd-&gt;flags`.</code><br />
<code>* When encountering a live object in a `BF_LARGE` block, the GC never copies the object, instead it just re-links the whole block onto the `large_objects` list of the destination generation.</code><br />
<code>* The GC doesn't have to scavenge the pinned object, since it does not contain any pointers.  This is just as well, because we cannot scan blocks for live pinned objects, due to [wiki:Commentary/Rts/Storage/Slop slop].  Hence the restriction that pinned objects do not contain pointers.</code></p>
<p>This means that using pinned objects may lead to memory fragmentation, since a single pinned object keeps alive the whole block in which it resides. If we were to implement a non-moving collector such as [wiki:Commentary/Rts/Storage/GC/Sweeping mark-region], then we would be able to reduce the impact of fragmentation due to pinned objects.</p>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>
<p>Commentary/Rts/Storage/GC/Pinned</p>
<h1 id="overview-5">Overview</h1>
<p>GHC is structured into two parts:</p>
<p><code>* The `ghc` package (in subdirectory `compiler`), which implements almost all GHC's functionality. It is an ordinary Haskell library, and can be imported into a Haskell program by saying `import GHC`.</code><br />
<code>* The `ghc` binary (in subdirectory `ghc`) which imports the `ghc` package, and implements the I/O for the `ghci` interactive loop.</code></p>
<p>Here's an overview of the module structure of the top levels of GHC library. (Note: more precisly, this is the plan. Currently the module `Make` below is glommed into the giant module `GHC`.) </p>
<h1 id="the-driver-pipeline">The driver pipeline</h1>
<p>The driver pipeline consist of a couple of phases that call other programs and generate a series of intermediate files. Code responsible for managing the order of phases is in <a href="GhcFile(compiler/main/DriverPhases.hs)" class="uri" title="wikilink">GhcFile(compiler/main/DriverPhases.hs)</a>, while managing the driver pipeline as a whole is coded in <a href="GhcFile(compiler/main/DriverPipeline.hs)" class="uri" title="wikilink">GhcFile(compiler/main/DriverPipeline.hs)</a>. Note that driver pipeline is not the same thing as compilation pipeline: the latter is part of the former.</p>
<p>Let's take a look at the overall structure of the driver pipeline. When we compile  or  (&quot;lhs&quot; extension means that Literate Haskell is being used) the following phases are being called (some of them depending on additional conditions like file extensions or enabled flags):</p>
<p><code>* Run the </code><strong><code>unlit</code> <code>pre-processor</code></strong><code>, </code><code>, to remove the literate markup, generating </code><code>.  The </code><code> processor is a C program kept in </code><a href="GhcFile(utils/unlit)" title="wikilink"><code>GhcFile(utils/unlit)</code></a><code>.</code></p>
<p><code>* Run the </code><strong><code>C</code> <code>preprocessor</code></strong><code>, `cpp`, (if </code><code> is specified), generating </code><code>.</code></p>
<p><code>* Run </code><strong><code>the</code> <code>compiler</code> <code>itself</code></strong><code>. This does not start a separate process; it's just a call to a Haskell function.  This step always generates an [wiki:Commentary/Compiler/IfaceFiles </code><strong><code>interface</code> <code>file</code></strong><code>] </code><code>, and depending on what flags you give, it also generates a compiled file. As GHC supports three backend code generators currently (a native code generator, a C code generator and an llvm code generator) the possible range of outputs depends on the backend used. All three support assembly output:</code><br />
<code>  * Object code: no flags required, file </code><code> (supported by all three backends)</code><br />
<code>  * Assembly code: flag </code><code>, file </code><code> (supported by all three backends)</code><br />
<code>  * C code: flags </code><code>, file </code><code> (only supported by C backend)</code></p>
<p><code> * In the </code><code> case:</code><br />
<code>   * Run the </code><strong><code>C</code> <code>compiler</code></strong><code> on `Foo.hc`, to generate `Foo.s`.</code></p>
<p><code> * If `-split-objs` is in force, run the </code><strong><code>splitter</code></strong><code> on `Foo.s`.  This splits `Foo.s` into lots of small files.  The idea is that the static linker will thereby avoid linking dead code.</code></p>
<p><code> * Run the assembler on `Foo.s`, or if `-split-objs` is in force, on each individual assembly file.</code></p>
<h1 id="the-compiler-pipeline">The compiler pipeline</h1>
<p>The <strong>compiler itself</strong>, independent of the external tools, is also structured as a pipeline. For details (and a diagram), see [wiki:Commentary/Compiler/HscMain]</p>
<h1 id="video">Video</h1>
<p>Video of compilation pipeline explanation from 2006: <a href="http://www.youtube.com/watch?v=dzSc8ACz_mw&amp;list=PLBkRCigjPwyeCSD_DFxpd246YIF7_RDDI">Compilation Pipeline</a> and interface files (17'30&quot;)</p>
<h1 id="platforms">Platforms</h1>
<p>Please read [wiki:CrossCompilation this wiki page] on cross compilation for a better understanding of the situation here. There are three platforms of interest to GHC when compiling and running:</p>
<p><code>* The </code><strong><code>Build</code></strong><code> platform. This is the platform on which we are building GHC.</code><br />
<code>* The </code><strong><code>Host</code></strong><code> platform. This is the platform on which we are going to run this GHC binary, and associated tools.</code><br />
<code>* The </code><strong><code>Target</code></strong><code> platform. This is the platform for which this GHC binary will generate code.</code></p>
<h2 id="limitations">Limitations</h2>
<p>At the moment, there is limited support for having different values for build, host, and target. Please refer to the [wiki:CrossCompilation cross compilation] page for more details. In particular:</p>
<p>The build platform is currently always the same as the host platform. The build process needs to use some of the tools in the source tree, for example ghc-pkg and hsc2hs.</p>
<p>If the target platform differs from the host platform, then this is generally for the purpose of building .hc files from Haskell source for porting GHC to the target platform. Full cross-compilation isn't supported (yet).</p>
<h2 id="macros">Macros</h2>
<p>In the compiler's source code, you may make use of the following CPP symbols:</p>
<p><code>* </code><em><code>xxx</code></em><code>`_TARGET_ARCH`</code><br />
<code>* </code><em><code>xxx</code></em><code>`_TARGET_VENDOR`</code><br />
<code>* </code><em><code>xxx</code></em><code>`_TARGET_OS`</code><br />
<code>* </code><em><code>xxx</code></em><code>`_HOST_ARCH`</code><br />
<code>* </code><em><code>xxx</code></em><code>`_HOST_VENDOR`</code><br />
<code>* </code><em><code>xxx</code></em><code>`_HOST_OS`</code></p>
<p>where <em>xxx</em> is the appropriate value: eg. `i386_TARGET_ARCH`. However <strong>GHC is moving away from using CPP for this purpose</strong> in many cases due to the problems it creates with supporting cross compilation.</p>
<p>So instead of it the new plan is to always build GHC as a cross compiler and select the appropriate values and backend code generator to run and runtime. For this purpose there is the Platform module (<a href="GhcFile(compiler/utils/Platform.hs)" class="uri" title="wikilink">GhcFile(compiler/utils/Platform.hs)</a>). That contains various methods for querying the !DynFlags (<a href="GhcFile(compiler/main/DynFlags.hs)" class="uri" title="wikilink">GhcFile(compiler/main/DynFlags.hs)</a>) value for what platform GHC is currently compiling for. You should use these when appropriate over the CPP methods.</p>
<h1 id="pointer-tagging-1">Pointer Tagging</h1>
<p>Paper: <a href="http://research.microsoft.com/pubs/67969/ptr-tagging.pdf">Faster laziness using dynamic pointer tagging</a></p>
<p>In GHC we &quot;tag&quot; pointers to heap objects with information about the object they point to. The tag goes in the low 2 bits (3 bits on a 64-bit platform) of the pointer, which would normally be zero since heap objects are always [wiki:Commentary/Rts/Word word]-aligned.</p>
<h2 id="meaning-of-the-tag-bits">Meaning of the tag bits</h2>
<p>The way the tag bits are used depends on the type of object pointed to:</p>
<p><code>* If the object is a </code><strong><code>constructor</code></strong><code>, the tag bits contain the </code><em><code>constructor</code> <code>tag</code></em><code>, if the number of</code><br />
<code>  constructors in the datatype is less than 4 (less than 8 on a 64-bit platform).  If the number of</code><br />
<code>  constructors in the datatype is equal to or more than 4 (resp 8), then the tag bits have the value 1, and the constructor tag</code><br />
<code>  is extracted from the constructor's info table instead.</code></p>
<p><code>* If the object is a </code><strong><code>function</code></strong><code>, the tag bits contain the </code><em><code>arity</code></em><code> of the function, if the arity fits</code><br />
<code>  in the tag bits.</code></p>
<p><code>* For a pointer to any other object, the tag bits are always zero.</code></p>
<h2 id="optimisations-enabled-by-tag-bits">Optimisations enabled by tag bits</h2>
<p>The presence of tag bits enables certain optimisations:</p>
<p><code>* In a case-expression, if the variable being scrutinised has non-zero tag bits, then we know</code><br />
<code>  that it points directly to a constructor and we can avoid </code><em><code>entering</code></em><code> it to evaluate it.</code><br />
<code>  Furthermore, for datatypes with only a few constructors, the tag bits will tell us </code><em><code>which</code></em><br />
<code>  constructor it is, eliminating a further memory load to extract the constructor tag from the</code><br />
<code>  info table.</code></p>
<p><code>* In a [wiki:Commentary/Rts/HaskellExecution/FunctionCalls#Genericapply generic apply], if the function being applied has a tag value that indicates it has exactly the</code><br />
<code>  right arity for the number of arguments being applied, we can jump directly to the function, instead of</code><br />
<code>  inspecting its info table first.</code></p>
<p>Pointer-tagging is a fairly significant optimisation: we measured 10-14% depending on platform. A large proportion of this comes from eliminating the indirect jumps in a case expression, which are hard to predict by branch-prediction. The paper has full results and analysis.</p>
<h2 id="garbage-collection-with-tagged-pointers">Garbage collection with tagged pointers</h2>
<p>The [wiki:Commentary/Rts/Storage/GC garbage collector] maintains tag bits on the pointers it traverses. This is easier, it turns out, than <em>reconstructing</em> tag bits. Reconstructing tag bits would require that the GC knows not only the tag of the constructor (which is in the info table), but also the family size (which is currently not in the info table), since a constructor from a large family should always have tag 1. To make this practical we would probably need different closure types for &quot;small family&quot; and &quot;large family&quot; constructors, and we already subdivide the constructor closures types by their layout.</p>
<p>Additionally, when the GC eliminates an indirection it takes the tag bits from the pointer inside the indirection. Pointers to indirections always have zero tag bits.</p>
<h2 id="invariants">Invariants</h2>
<p>Pointer tagging is <em>not</em> optional, contrary to what the paper says. We originally planned that it would be: if the GC threw away all the tags, then everything would continue to work albeit more slowly. However, it turned out that in fact we really want to assume tag bits in some places:</p>
<p><code> * In the continuation of an algebraic case, R1 is assumed tagged</code><br />
<code> * On entry to a non-top-level function, R1 is assumed tagged</code></p>
<p>If we don't assume the value of the tag bits in these places, then extra code is needed to untag the pointer. If we can assume the value of the tag bits, then we just take this into account when indexing off R1.</p>
<p>This means that everywhere that enters either a case continuation or a non-top-level function must ensure that R1 is correctly tagged. For a case continuation, the possibilities are:</p>
<p><code> * the scrutinee of the case jumps directly to the alternative if R1 is already tagged.</code><br />
<code> * the constructor entry code returns to an alternative.  This code adds the correct tag.</code><br />
<code> * if the case alternative fails a heap or stack check, then the RTS will re-enter the alternative after</code><br />
<code>   GC.  In this case, our re-entry arranges to enter the constructor, so we get the correct tag by</code><br />
<code>   virtue of going through the constructor entry code.</code></p>
<p>For a non-top-level function, the cases are:</p>
<p><code> * unknown function application goes via `stg_ap_XXX` (see [wiki:Commentary/Rts/HaskellExecution/FunctionCalls#Genericapply Generic Apply]).  </code><br />
<code>   The generic apply functions must therefore arrange to correctly tag R1 before entering the function.</code><br />
<code> * A known function can be entered directly, if the call is made with exactly the right number of arguments.</code><br />
<code> * If a function fails its heap check and returns to the runtime to garbage collect, on re-entry the closure</code><br />
<code>   pointer must be still tagged.</code><br />
<code> * the PAP entry code jumps to the function's entry code, so it must have a tagged pointer to the function</code><br />
<code>   closure in R1.  We therefore assume that a PAP always contains a tagged pointer to the function closure.</code></p>
<p>In the second case, calling a known non-top-level function must pass the function closure in R1, and this pointer <em>must</em> be correctly tagged. The code generator does not arrange to tag the pointer before calling the function; it assumes the pointer is already tagged. Since we arrange to tag the pointer when the closure is created, this assumption is normally safe. However, if the pointer has to be saved on the stack, say across a call, then when the pointer is retrieved again we must either retag it, or be sure that it is still tagged. Currently we do the latter, but this imposes an invariant on the garbage collector: all tags must be retained on non-top-level function pointers.</p>
<p>Pointers to top-level functions are not necessarily tagged, because we don't always know the arity of a function that resides in another module. When optimisation is on, we do know the arities of external functions, and this information is indeed used to tag pointers to imported functions, but when optimisation is off we do not have this information. For constructors, the interface doesn't contain information about the constructor tag, except that there may be an unfolding, but the unfolding is not necessarily reliable (the unfolding may be a constructor application, but in reality the closure may be a CAF, e.g. if any of the fields are references outside the current shared library).</p>
<h2 id="compacting-gc">Compacting GC</h2>
<p>Compacting GC also uses tag bits, because it needs to distinguish between a heap pointer and an info pointer quickly. The compacting GC has a complicated scheme to ensure that pointer tags are retained, see the comments in <a href="GhcFile(rts/sm/Compact.c)" class="uri" title="wikilink">GhcFile(rts/sm/Compact.c)</a>.</p>
<h2 id="dealing-with-tags-in-the-code">Dealing with tags in the code</h2>
<p>Every time we dereference a pointer to a heap object, we must first zero the tag bits. In the RTS, this is done with the inline function (previously: macro) `UNTAG_CLOSURE()`; in `.cmm` code this is done with the `UNTAG()` macro. Surprisingly few places needed untagging to be added.</p>
<h1 id="position-independent-code-and-dynamic-linking">Position-Independent Code and Dynamic Linking</h1>
<p>We need to generate position-independent code on most platforms when we want our code to go into dynamic libraries (also referred to as shared libraries or DLLs). On some platforms (AIX, powerpc64-linux, x86_64-darwin), PIC is required for all code.</p>
<p>To access things defined in a dynamic library, we might need to do special things, such as look up the address of the imported thing in a table of pointers, depending on what platform we are on.</p>
<h2 id="how-to-access-symbols">How to access symbols</h2>
<p>A C compiler is in an unfortunate position when generating PIC code, as it does not have any hints, whether an accessed symbol ends up in the same dynamic library or if it is truely an external symbol (from the dynamic library point of view). It can only generate non-PIC access for symbols generated within the same object file. In Haskell, we can do better as we assume all package code to end up in a single dynamic library. Hence, all intra-package symbol accesses can be generated as code that does direct access. For all inter-package accesses (package haskell98 accessing symbols in package base, e.g.), we have to generate PIC code. For the following we establish the following:</p>
<p><code>* </code><em><code>object-local</code> <code>symbols</code></em><code>, symbols within the same object file. Always generate direct access. </code><br />
<code>* </code><em><code>package-local</code> <code>symbols</code></em><code>, symbols within the same Haskell package. The NCG can generate direct access code, C compilers can't.</code><br />
<code>* </code><em><code>local</code> <code>symbols</code></em><code>, either object-local or package-local.</code><br />
<code>* </code><em><code>global</code> <code>symbols</code></em><code>, symbol in different libraries/packages. Always generate PIC.</code></p>
<h2 id="clabel.labeldynamic">CLabel.labelDynamic</h2>
<p>On most platforms, we can access any global symbol as if it was imported from a dynamic library; this usually means a small performance hit (an extra pointer dereference), but it is otherwise harmless. On some platforms, we have to access all global symbols this way. On Windows, we must know exactly which symbols are DLL-imported and which aren't.</p>
<p>Module `CLabel` contains a function `labelDynamic :: CLabel -&gt; Bool` which is supposed to know whether a `CLabel` is imported from a dynamic library. On Windows, this function needs to be exact; everywhere else, we don't mind the occasional false positive.</p>
<h2 id="info-tables-1">Info Tables</h2>
<p>Info tables are in the text segment, which is supposed to be read-only and position-independent. Therefore, an info table <em>must not</em> contain any absolute address; instead, all addresses in info tables are instead encoded as relative offsets from the info label.</p>
<p>Note that this is done even when we are generating code that is otherwise position-dependent, in order to preserve binary compatibility between PIC and non-PIC.</p>
<p>It is not possible to generate those relative references from C code, so for the via-C compilation route, we pretty-print these relative references (`CmmLabelDiffOff` in cmm) as absolute references and have the mangler convert them to relative references again.</p>
<h2 id="imported-labels-in-srts-windows">Imported labels in SRTs (Windows)</h2>
<p>Windows doesn't support references to imported labels in the data segment; on other platforms, the dynamic linker will just relocate the pointers in the SRTs to point to the right symbols. There is a hack in the code that tries to work around it; it might be bitrotted, and it might have been made unnecessary by the GNU linker's new auto-import on Windows.</p>
<h2 id="pic-and-dynamic-linking-support-in-the-ncg">PIC and dynamic linking support in the NCG</h2>
<p>The module `PositionIndependentCode` lies at the heart of PIC and dynamic linking support in the native code generator.</p>
<p>The basic idea is to call a function `cmmMakeDynamicReference` for all labels accessed from the code during the cmm-to-cmm transformation phase. This function will decide on the appropriate way to access the given label for the current platform and the current combination of -fPIC and -dynamic flags.</p>
<p>We extend Cmm and the `CLabel` module by a few things to allow us to express all the different things that occur on different platforms:</p>
<p>The `Cmm.GlobalReg` datatype has a constructor `PicBaseReg`. This PIC base register is the register relative to which position-independent references are calculated. This can be a general-purpose register that is allocated on a per-!CmmProc basis, or it can be a dedicated register, like the instruction pointer `%rip` on x86_64.</p>
<h2 id="how-things-are-done-on-different-platforms">How things are done on different platforms</h2>
<p>This section is a survey of how PIC and dynamic linking works on different platforms. There are small snippets of assembly code for several platforms, platforms that are similar to other platforms are left out (e.g. powerpc-darwin is left out, because the logic is the same as for i386-darwin). I hope the reader will not be too confused by irrelevant differences between the platforms, such as the fact that Darwin and Windows prefix all symbols with an underscore, and Linux doesn't.</p>
<h3 id="position-dependent-code">Position dependent code</h3>
<p>In the absence of PIC and dynamic linking, things are simple; when we use a label in assembly code, the linker will make sure it points to the right place.</p>
<p></p>
<p>Now, to access a symbol `xfoo` that has been imported from a dynamic library, we do not want to mention the address of `xfoo` in the text section, because it would need to be modified at load-time.</p>
<p>One solution is to allocate a pointer to the imported symbol in a writable section and have the dynamic linker fill in this pointer table. The pointer table itself resides at a statically known address. The __imp__* symbols on Windows are automatically generated by the linker.</p>
<p></p>
<p>On Mac OS X, the same system is used for data imports, but this time we have to define the symbol pointers ourselves. For references to code, there is an additional mechanism available; we can jump to a small piece of stub code that will resolve the symbol the first time it is used, in order to reduce application load times. Unfortunately, everything on Mac OS X requires 16-byte stack alignment, even the dynamic linker, so we cannot use this for a tail call.</p>
<p></p>
<p>In theory, dynamic linking is transparent to position-dependent code on Linux, i.e. the code for accessing imported labels should look exactly the same as for non-imported labels. Unfortunately, things just don't work as they should for strange stuff like info tables.</p>
<p>When the ELF static linker finds a jump or call to an imported symbol, it automatically redirects the jump or call to a linker generated code stub (in the so-called procedure linkage table, or PLT). The linker then considers the label to be a code label and redirects all further references to the label to the code stub, even if they are data references. If this ever happens to an info label, our program will crash, as there is no info table in front of the code stub.</p>
<p>When the ELF static linker finds a data reference to an imported symbol (that it doesn't consider a code label), it allocates space for that symbol in the executable's data section and issues an `R_COPY` relocation, which instructs the dynamic linker to copy the (initial) contents of the symbol to its new place in the executable's image. All references to the symbol from the dynamic library are relocated to point to the symbol's new location, instead.</p>
<p>If `R_COPY` is ever used for an info label, our program will also crash, because the data we're interested in is *before* the info label and is not copied to the symbol's new home.</p>
<p>Fortunately, if the static linker finds a pointer to an imported symbol in a writable section, it just instructs the dynamic linker to update that pointer to the symbols address, without doing anything &quot;funny&quot;. We can therefore work around these problems.</p>
<p>The workaround is inspired by the position-independent code that GCC generates for powerpc-linux, a platform that is amazingly broken.</p>
<p></p>
<p>Things look pretty much the same on x86_64-linux, powerpc-linux and powerpc-darwin; PowerPC has the added handicap that it takes two instructions to load a 32 bit quantity into a register. On x86_64-darwin, powerpc64-linux and all versions of AIX, PIC is <em>required</em>.</p>
<h3 id="position-independent-code">Position independent code</h3>
<p>First, let it be said that there is no such thing as position-independent code on Windows. The dynamic linker will just patiently relocate all dynamic libraries that are not loaded at their preferred base address. On all other platforms, PIC is at least strongly recommended for dynamic libraries.</p>
<p>In an ideal world, there would be assembler instructions for referring to things via an offset from the current instruction pointer. Jump instructions are ip-relative on all platforms that GHC runs on, but for data accesses, only x86_64 is this ideal world.</p>
<p>On x86_64, on both Linux and Mac OS X, we can use `foo(%rip)` to encode an instruction pointer relative data reference to `foo`, and `foo@GOTPCREL(%rip)` to encode an instruction pointer relative referece to a linker-generated symbol pointer for symbol `foo`. A linker-generated code stub for imported code can be accessed by appending `@PLT` to the label on Linux, and is used implicitly when necessary on Mac OS X.</p>
<p>Again, we have to avoid the code stubs for tail-calls and use the symbol pointer instead, because there is a stack alignment requirement.</p>
<p></p>
<p>Other platforms are not nearly as nice; i386 and powerpc[64] do not have a way of accessing the current instruction pointer or referring to data relative to it. The *only* way to get at the current instruction pointer is to issue a call instruction. To generate PIC code, we have to do just that at the beginning of each function.</p>
<p>On Darwin, things are relatively straightforward: </p>
<p>There is one more small additional complication on Darwin. The assembler doesn't support label difference expressions involving labels not defined in the same source file, so we have to treat all symbols not defined in the same source file as dynamically imported.</p>
<p>On Linux, we need to first calculate the address of the Global Offset Table (GOT) and then use `bar@GOT` to refer to symbol pointers and `bar@GOTOFF` to refer to a local symbol relative to the GOT. Also, the linker-generated code-stubs (`xfoo@PLT`) require the address of the GOT to be in register `%ebx` when they are invoked. The NCG currently doesn't do this, so we avoid code stubs altogether on i386.</p>
<p></p>
<p><strong>To be done:</strong> powerpc-linux, AIX/powerpc64-linux</p>
<h2 id="linking-on-elf">Linking on ELF</h2>
<p>To generate a DSO on ELF platform, we use GNU ld. Except for `-Bsymbolic`, ld is invoked regularly with the `-shared` option, and `-o` pointing to the output DSO file followed objects that in its sum compose an entire package. In Haskell, we assume that there is a one-to-one mapping from packages to DSOs. So, all parts of the base package will end up in a libHSbase.so. As intra-package references are not generated as PIC code, we have to supply all objects that make up a package, so that ld is able to resolve these references before writing a (.text) relocation free DSO library file. To enable these cross-object relocations GNU ld needs `-Bsymbolic`.</p>
<h2 id="mangling-dynamic-library-names">Mangling dynamic library names</h2>
<p>As Haskell DSOs might end up in standard library paths, and as they might not be compatible among compilers and compiler version, we need to mangle their names to include the compiler and its version.</p>
<p>The scheme is libHS<em><package></em>-<em><package-version></em>-<em><compiler><compilerversion></em>.so. E.g. libHSbase-2.1-ghc6.6.so</p>
<h1 id="ghc-commentary-the-c-code-generator">GHC Commentary: The C code generator</h1>
<p>Source: <a href="GhcFile(compiler/cmm/PprC.hs)" class="uri" title="wikilink">GhcFile(compiler/cmm/PprC.hs)</a></p>
<p>This phase takes [wiki:Commentary/Compiler/CmmType Cmm] and generates plain C code. The C code generator is very simple these days, in fact it can almost be considered pretty-printing. It is only used for unregisterised compilers.</p>
<h2 id="header-files">Header files</h2>
<p>GHC was changed (from version 6.10) so that the C backend no longer uses header files specified by the user in any way. The `c-includes` field of a `.cabal` file is ignored, as is the `-#include` flag on the command-line. There were several reasons for making this change:</p>
<p>This has several advantages:</p>
<p><code>* Via C compilation is consistent with the other backend with respect to FFI declarations:</code><br />
<code>  all bind to the ABI, not the API.</code><br />
<code> </code><br />
<code>* foreign calls can now be inlined freely across module boundaries, since</code><br />
<code>  a header file is not required when compiling the call.</code><br />
<code> </code><br />
<code>* bootstrapping via C will be more reliable, because this difference</code><br />
<code>  in behavior between the two backends has been removed.</code><br />
<code> </code></p>
<p>There are some disadvantages:</p>
<p><code>* we get no checking by the C compiler that the FFI declaration</code><br />
<code>  is correct.</code></p>
<p><code>* we can't benefit from inline definitions in header files.</code><br />
<code> </code></p>
<h2 id="prototypes">Prototypes</h2>
<p>When a label is referenced by an expression, the compiler needs to know whether to declare the label first, and if so, at what type.</p>
<p>C only lets us declare an external label at one type in any given source file, even if the scopes of the declarations don't overlap. So we either have to scan the whole code to figure out what the type of each label should be, or we opt for declaring all labels at the same type and then casting later. Currently we do the latter.</p>
<p><code>* all labels referenced as a result of an FFI declaration</code><br />
<code>  are declared as `extern StgWord[]`, including function labels.</code><br />
<code>  If the label is called, it is first cast to the correct</code><br />
<code>  function type.  This is because the same label might be</code><br />
<code>  referred to both as a function and an untyped data label in</code><br />
<code>  the same module (e.g. Foreign.Marsal.Alloc refers to &quot;free&quot;</code><br />
<code>  this way).  </code></p>
<p><code>* An exception is made to the above for functions declared with</code><br />
<code>  the `stdcall` calling convention on Windows.  These functions must</code><br />
<code>  be declared with the `stdcall` attribute and a function type,</code><br />
<code>  otherwise the C compiler won't add the `@n` suffix to the symbol.</code><br />
<code>  We can't add the `@n` suffix ourselves, because it is illegal</code><br />
<code>  syntax in C.  However, we always declare these labels with the</code><br />
<code>  type `void (*)(void)`, to avoid conflicts if the same function</code><br />
<code>  is called at different types in one module (see `Graphics.Win32.GDI.HDC.SelectObject`).</code></p>
<p><code>* Another exception is made for functions that are marked `never returns` in C--.  We</code><br />
<code>  have to put an `__attribute__((noreturn))` on the declaration for these functions,</code><br />
<code>  and it only works if the function is declared with a proper function type and</code><br />
<code>  called without casting it to/from a pointer.  So only the correct prototype</code><br />
<code>  will do here.</code></p>
<p><code>* all RTS symbols already have declarations (mostly with the correct</code><br />
<code>  type) in </code><a href="GhcFile(includes/StgMiscClosures.h)" title="wikilink"><code>GhcFile(includes/StgMiscClosures.h)</code></a><code>, so no declarations are generated.</code></p>
<p><code>* certain labels are known to have been defined earlier in the same file,</code><br />
<code>  so a declaration can be omitted (e.g. SRT labels)</code></p>
<p><code>* certain math functions (`sin()`, `cos()` etc.) are already declared because</code><br />
<code>  we #include math.h, so we don't emit declarations for these.  We need</code><br />
<code>  to #include math.h because some of these functions have inline</code><br />
<code>  definitions, and we get terrible code otherwise.</code></p>
<p>When compiling the RTS cmm code, we have almost no information about labels referenced in the code. The only information we have is whether the label is defined in the RTS or in another package: a label that is declared with an import statement in the .cmm file is assumed to be defined in another package (this is for dynamic linking, where we need to emit special code to reference these labels).</p>
<p>For all other labels referenced by RTS .cmm code, we assume they are RTS labels, and hence already declared in <a href="GhcFile(includes/StgMiscClosures.h)" class="uri" title="wikilink">GhcFile(includes/StgMiscClosures.h)</a>. This is the only choice here: since we don't know the type of the label (info, entry etc.), we can't generate a correct declaration.</p>
<p><a href="PageOutline" class="uri" title="wikilink">PageOutline</a></p>
<h1 id="primitive-operations-primops">Primitive Operations (!PrimOps)</h1>
<p>!PrimOps are functions that cannot be implemented in Haskell, and are provided natively by GHC. For example, adding two  values is provided as the !PrimOp , and allocating a new mutable array is the !PrimOp .</p>
<p>!PrimOps are made available to Haskell code through the virtual module . This module has no implementation, and its interface never resides on disk: if  is imported, we use a built-in  value - see  in <a href="GhcFile(compiler/iface/LoadIface.hs)" class="uri" title="wikilink">GhcFile(compiler/iface/LoadIface.hs)</a>.</p>
<p>It would also be useful to look at the [wiki:Commentary/Compiler/WiredIn Wired-in and known-key things] wiki page to understand this topic.</p>
<h2 id="the-primops.txt.pp-file">The primops.txt.pp file</h2>
<p>The file <a href="GhcFile(compiler/prelude/primops.txt.pp)" class="uri" title="wikilink">GhcFile(compiler/prelude/primops.txt.pp)</a> includes all the information the compiler needs to know about a !PrimOp, bar its actual implementation. For each !PrimOp,  lists:</p>
<p><code>* Its name, as it appears in Haskell code (eg. int2Integer#)</code><br />
<code>* Its type</code><br />
<code>* The name of its constructor in GHC's </code><code> data type.</code><br />
<code>* Various properties, such as whether the operation is commutable, or has side effects.</code></p>
<p>For example, here's the integer multiplication !PrimOp:</p>
<p></p>
<p>The  file is processed first by CPP, and then by the  program (see <a href="GhcFile(utils/genprimopcode)" class="uri" title="wikilink">GhcFile(utils/genprimopcode)</a>).  generates the following bits from :</p>
<p><code>* Various files that are </code><code>d into </code><a href="GhcFile(compiler/prelude/PrimOp.hs)" title="wikilink"><code>GhcFile(compiler/prelude/PrimOp.hs)</code></a><code>,</code><br />
<code>  containing declarations of data types and functions describing the !PrimOps.  See</code><br />
<code>  </code><a href="GhcFile(compiler/Makefile)" title="wikilink"><code>GhcFile(compiler/Makefile)</code></a><code>.</code></p>
<p><code>* </code><code>, a file that contains (curried) wrapper</code><br />
<code>  functions for each of the !PrimOps, so that they are accessible from byte-code, and</code><br />
<code>  so that the [wiki:Commentary/Rts/Interpreter byte-code interpreter] doesn't need to implement any !PrimOps at all: it</code><br />
<code>  just invokes the compiled ones from </code><code>.</code></p>
<p><code>* </code><code>, a source file containing dummy declarations for</code><br />
<code>  all the !PrimOps, solely so that Haddock can include documentation for </code><br />
<code>  in its documentation for the </code><code> package.  The file </code><code> is never</code><br />
<code>  actually compiled, only processed by Haddock.</code></p>
<p>Note that if you want to create a polymorphic primop, you need to return , not .</p>
<h2 id="implementation-of-primops">Implementation of !PrimOps</h2>
<p>!PrimOps are divided into two categories for the purposes of implementation: inline and out-of-line.</p>
<h3 id="inline-primops">Inline !PrimOps</h3>
<p>Inline !PrimOps are operations that can be compiled into a short sequence of code that never needs to allocate, block, or return to the scheduler for any reason. An inline !PrimOp is compiled directly into [wiki:Commentary/Rts/Cmm Cmm] by the [wiki:Commentary/Compiler/CodeGen code generator]. The code for doing this is in <a href="GhcFile(compiler/codeGen/StgCmmPrim.hs)" class="uri" title="wikilink">GhcFile(compiler/codeGen/StgCmmPrim.hs)</a>.</p>
<h3 id="out-of-line-primops">Out-of-line !PrimOps</h3>
<p>All other !PrimOps are classified as out-of-line, and are implemented by hand-written C-- code in the file <a href="GhcFile(rts/PrimOps.cmm)" class="uri" title="wikilink">GhcFile(rts/PrimOps.cmm)</a>. An out-of-line !PrimOp is like a Haskell function, except that</p>
<p><code>* !PrimOps cannot be partially applied.  Calls to all !PrimOps are made at the correct arity; this is ensured by </code><br />
<code>  the [wiki:Commentary/Compiler/HscMain CorePrep] pass.</code></p>
<p><code>* Out-of-line !PrimOps have a special, fixed, [wiki:Commentary/Rts/HaskellExecution#CallingConvention calling convention]:</code><br />
<code>  all arguments</code><br />
<code>  are in the [wiki:Commentary/Rts/HaskellExecution#Registers registers] R1-R8.  This is to make it easy to write the</code><br />
<code>  C-- code for these !PrimOps: we don't have to write code for multiple calling conventions.</code></p>
<p>It's possible to provide inline versions of out-of-line !PimOps. This is useful when we have enough static information to generated a short, more efficient inline version. For example, a call to  can be implemented more efficiently as an inline !PrimOp as the heap check for the array allocation can be combined with the heap check for the surrounding code. See `shouldInlinePrimOp` in <a href="GhcFile(compiler/codeGen/StgCmmPrim.hs)" class="uri" title="wikilink">GhcFile(compiler/codeGen/StgCmmPrim.hs)</a>.</p>
<h3 id="foreign-out-of-line-primops-and-foreign-import-prim">Foreign out-of-line !PrimOps and `foreign import prim`</h3>
<p>A new and somewhat more flexible form of out-of-line !PrimOp is the foreign out-of-line !PrimOp. These are essentially the same but instead of their Cmm code being included in the RTS, they can be defined in Cmm code in any package and instead of knowledge of the !PrimOp being baked into the compiler, they can be imported using special FFI syntax:</p>
<p></p>
<p>The string (e.g. &quot;int2Integerzh&quot;) is the linker name of the Cmm function. Using this syntax requires the extensions `ForeignFunctionInterface`, `GHCForeignImportPrim`, `MagicHash`, `UnboxedTuples` and `UnliftedFFITypes`. The current type restriction is that all arguments and results must be unlifted types, with two additional possibilities: An argument may (since GHC 7.5) be of type `Any` (in which case the called function will receive a pointer to the heap), and the result type is allowed to be an unboxed tuple. The calling convention is exactly the same as for ordinary out-of-line primops. Currently it is not possible to specify any of the !PrimOp attributes.</p>
<p>The `integer-gmp` package now uses this method for all the primops that deal with GMP big integer values. The advantage of using this technique is that it is a bit more modular. The RTS does not need to include all the primops. For example in the integer case the RTS no longer needs to link against the GMP C library.</p>
<p>The future direction is to extend this syntax to allow !PrimOp attributes to be specified. The calling convention for primops and ordinary compiled Haskell functions may be unified in future and at that time it the restriction on using only unlifted types may be lifted.</p>
<p>It has been suggested that we extend this !PrimOp definition and import method to cover all !PrimOps, even inline ones. This would replace the current `primops.txt.pp` system of builtin !PrimOps. The inline !PrimOps would still be defined in the compiler but they would be imported in any module via `foreign import prim` rather than appearing magically to be exported from the `GHC.Prim` module. Hugs has used a similar system for years (with the syntax `primitive seq :: a -&gt; b -&gt; b`).</p>
<h2 id="adding-a-new-primop">Adding a new !PrimOp</h2>
<p>To add a new primop, you currently need to update the following files:</p>
<p><code>* </code><a href="GhcFile(compiler/prelude/primops.txt.pp)" title="wikilink"><code>GhcFile(compiler/prelude/primops.txt.pp)</code></a><code>, which includes the</code><br />
<code>  type of the primop, and various other properties.  Syntax and</code><br />
<code>  examples are in the file.</code></p>
<p><code>* if the primop is inline, then:</code><br />
<code>  </code><a href="GhcFile(compiler/codeGen/StgCmmPrim.hs)" title="wikilink"><code>GhcFile(compiler/codeGen/StgCmmPrim.hs)</code></a><code> defines the translation of</code><br />
<code>  the primop into </code><code>.</code><br />
<code>       </code><br />
<code>* for an out-of-line primop:</code><br />
<code>  * </code><a href="GhcFile(includes/stg/MiscClosures.h)" title="wikilink"><code>GhcFile(includes/stg/MiscClosures.h)</code></a><code> (just add the declaration),</code><br />
<code>  * </code><a href="GhcFile(rts/PrimOps.cmm)" title="wikilink"><code>GhcFile(rts/PrimOps.cmm)</code></a><code> (implement it here)</code><br />
<code>  * </code><a href="GhcFile(rts/Linker.c)" title="wikilink"><code>GhcFile(rts/Linker.c)</code></a><code> (declare the symbol for GHCi)</code></p>
<p><code>* for a foreign out-of-line primop You do not need to modify the rts or compiler at all.</code><br />
<code>  * `yourpackage/cbits/primops.cmm`: implement your primops here. You have to arrange for the .cmm file to be compiled and linked into the package. The GHC build system has support for this. Cabal does not yet.</code><br />
<code>  * `yourpackage/TheCode.hs`: use `foreign import prim` to import the primops.</code></p>
<p>In addition, if new primtypes are being added, the following files need to be updated:</p>
<p><code> * </code><a href="GhcFile(utils/genprimopcode/Main.hs)" title="wikilink"><code>GhcFile(utils/genprimopcode/Main.hs)</code></a><code> -- extend ppType :: Type -&gt; String function</code><br />
<code> </code><br />
<code> * </code><a href="GhcFile(compiler/prelude/PrelNames.hs)" title="wikilink"><code>GhcFile(compiler/prelude/PrelNames.hs)</code></a><code> -- add a new unique id using mkPreludeTyConUnique</code></p>
<p><code> * </code><a href="GhcFile(compiler/prelude/TysPrim.hs)" title="wikilink"><code>GhcFile(compiler/prelude/TysPrim.hs)</code></a><code> -- there are a raft of changes here; you need to create </code><code>, </code><code> and  </code><code> variables. The most important thing to make sure you get right is when you make a PrimTyCon, you pick the correct </code><code> for your type.  For example, if you</code></p>
<h1 id="profiling">Profiling</h1>
<p>GHC includes two types of profiling: cost-centre profiling and ticky-ticky profiling. Additionally, HPC code coverage is not &quot;technically&quot; profiling, but it uses a lot of the same mechanisms as cost-centre profiling (you can read more about it at [wiki:Commentary/Hpc]).</p>
<p>Cost-centre profiling operates at something close to the source level, and ticky-ticky profiling operates at something much closer to the machine level. This means that the two types of profiling are useful for different tasks. Ticky-ticky profiling is mainly meant for compiler implementors, and cost-centre profiling for mortals. However, because cost-centre profiling operates at a high level, it can be difficult (if not impossible) to use it to profile optimized code. Personally, I (Kirsten) have had a lot of success using cost-centre profiling to find problems that were due to my own bad algorithms, but less success once I was fairly sure that I wasn't doing anything obviously stupid and was trying to figure out why my code didn't get optimized as well as it could have been.</p>
<p>You can't use cost-centre profiling and ticky-ticky profiling at the same time; in the past, this was because ticky-ticky profiling relied on a different closure layout, but now that's no longer the case. You probably can't use both at the same time as it is unless you wanted to modify the build system to allow using way=p and way=t at the same time to build the RTS. I haven't thought about whether it would make sense to use both at the same time.</p>
<h2 id="cost-centre-profiling">Cost-centre profiling</h2>
<p>Cost-center profiling in GHC, e.g. of SCCs, consists of the following components:</p>
<p><code>* Data-structures for representing cost-centres in </code><a href="GhcFile(compiler/profiling/CostCentre.lhs)" title="wikilink"><code>GhcFile(compiler/profiling/CostCentre.lhs)</code></a><code>.</code><br />
<code>* Front-end support in </code><a href="GhcFile(compiler/deSugar/DsExpr.lhs)" title="wikilink"><code>GhcFile(compiler/deSugar/DsExpr.lhs)</code></a><code>, for converting </code><code> pragma into the </code><code> constructor in Core.</code><br />
<code>* Modifications to optimization behavior in </code><a href="GhcFile(compiler/coreSyn/CoreUtils.lhs)" title="wikilink"><code>GhcFile(compiler/coreSyn/CoreUtils.lhs)</code></a><code> and </code><a href="GhcFile(compiler/coreSyn/CorePrep.lhs)" title="wikilink"><code>GhcFile(compiler/coreSyn/CorePrep.lhs)</code></a><code> to prevent optimizations which would result in misleading profile information. Most of this is to handle the fact that SCCs also count entries (tickishCounts, also applies to [wiki:Commentary/Hpc]); otherwise the only relevant optimization is avoiding floating expressions out of SCCs. Note that the simplifier also has &quot;ticks&quot; (so it can decide when to stop optimizing); these are not the same thing at all.</code><br />
<code>* The </code><code> constructor in STG, and code generation for it </code><a href="GhcFile(compiler/codeGen/StgCmmProf.hs)" title="wikilink"><code>GhcFile(compiler/codeGen/StgCmmProf.hs)</code></a><br />
<code>* A pass over STG in </code><a href="GhcFile(compiler/profiling/SCCfinal.lhs)" title="wikilink"><code>GhcFile(compiler/profiling/SCCfinal.lhs)</code></a><code> to collect cost centres so that they can be statically declared by </code><a href="GhcFile(compiler/profiling/ProfInit.hs)" title="wikilink"><code>GhcFile(compiler/profiling/ProfInit.hs)</code></a><code>, and add extra SCCs in the case of </code><code>; see also </code><a href="GhcFile(compiler/profiling/NOTES)" title="wikilink"><code>GhcFile(compiler/profiling/NOTES)</code></a><br />
<code>* Code-generation for setting labels found in </code><a href="GhcFile(compiler/codeGen/StgCmmProf.hs)" title="wikilink"><code>GhcFile(compiler/codeGen/StgCmmProf.hs)</code></a><code>, in particular saving and restoring CC labels and well as counting ticks; note that cost-centres even get their own constructor in C-- as CC_Labels (cost-centre labels).</code><br />
<code>* Runtime support for initializing and manipulating the actual runtime </code><code> structs which store information, in </code><a href="GhcFile(rts/Profiling.c)" title="wikilink"><code>GhcFile(rts/Profiling.c)</code></a><code>; headers are located in </code><a href="GhcFile(includes/rts/prof/CCS.h)" title="wikilink"><code>GhcFile(includes/rts/prof/CCS.h)</code></a></p>
<h2 id="ticky-ticky-profiling">Ticky-ticky profiling</h2>
<p>Ticky-ticky profiling is very simple (conceptually): instrument the C code generated by GHC with a lot of extra code that updates counters when various (supposedly) interesting things happen, and generate a report giving the values of the counters when your program terminates. GHC does this instrumentation for you when you compile your program with a special flag. Then, you use another flag to tell the RTS to generate the profiling report.</p>
<p>You might want to use ticky-ticky profiling for one of the following two reasons:</p>
<ul>
<li>You are an implementor trying to understand the effect of an optimization in GHC more precisely.</li>
</ul>
<ul>
<li>You are a user trying to observe the behavior of your programs with optimization turned on. GHC doesn't do certain transformations in the presence of cost centres, so cost-centre profiling can be less than accurate if you're trying to understand what really happens when you're compiling with .</li>
</ul>
<p>I won't necessarily try to argue that ticky-ticky is useful at all for the second group of people, but it's better than nothing, and perhaps the ticky-ticky data could be used to build a better profiler.</p>
<p>For more info, including HOWTO details, see [wiki:Debugging/TickyTicky]. like &quot;computer is a net&quot;, nowadays language is a library. there is nothing exceptional in C++ and Java languages except for their huge library codebase that makes them so widely appreciated</p>
<p>while it's impossible for Haskell to have the same level of libraries maturity, we can try to do our best. Libraries was considered so important, that in H98 report libs required more pages than language itself. But, really, all libraries described there together is appropriate only for learning and small programs - to do real work, we need even much, much more</p>
<p>fortunately, now we have large enough set of libs. moreover, this set grows each year. but these libs don't have official/recommended status. now we have two languages - H98 as reported with its bare libs, which is appropriate only for teaching, and real Haskell language with many extensions and rich set of libs, used to develop real programs</p>
<p>with a language itself, now we go to standardize current practice and include into language definition all popular extensions. this will close the gap between standard and practice. Haskell' committee also plan to define new version of standard Haskell library. but what a library can be defined in this way? slightly extended version of standard Haskell98 lib? or, if it will be significantly extended - how much time this work will require and isn't that a duplication of work done at libraries list?</p>
<p>i propose not to try to define reality, but accept existing one and join committee's work on new library definition with a current discussion of core libraries, which should define a set of libs available on any Haskell compiler on any platform - aren't goals the same?</p>
<p>instead of providing rather small and meaningless standard Haskell library, now we can just include in Report docs existing and widely used libs, such as Network, mtl and so on. This will mean that language, defined in Haskell standard, can be used to write real programs, which will be guaranteed to run in any Haskell environment.</p>
<p>of course, this mind game can't change anything in one moment. but it will change *accents*</p>
<p>first, Haskell with its libraries will become language for a real work. such extended language isn't small nor easy to master in full, but it is normal for any mature programming environment. people learning Haskell should select in which area they need to specialize - be it gaming or web service development, and study appropriate subset of libs. people teaching Haskell now can show how *standard* Haskell may be used to solve real world problems, and this should change treatment of Haskell as academic language. also, we may expect that books teaching Haskell will start to teach on using standard libs, while their authors now don't consider teaching for non-standard libs</p>
<p>second, by declaring these libs as standard ones we create sort of lingua franca, common language spoken by all Haskell users. for example, now there are about 10 serialization libs. by declaring one of them as standard, we will make choice simpler for most of users (who don't need very specific features) and allow them to speak in common language. in other words, number of Haskell libs is so large now that we should define some core subset in order to escape syndrome of Babel tower. defining core libraries set is just sharing knowledge that some libraries are more portable, easier to use, faster and so on, so they become more popular than alternatives in this area</p>
<p>third. now we have Cabal that automates installation of any lib. next year we will got Hackage that automates downloading and checking dependencies. but these tools still can't replace a rich set of standard libs shipped with compiler. there are still many places and social situations where Internet downloading isn't available. Compiler can be sold on CD, transferred on USB stick. and separate Haskell libs probably will be not included here. Standard libraries bundled with compiler will ensure that at least this set of libs will be available for any haskell installation. Internet access shouldn't be a precondition for Haskell usage! :)</p>
<p>fourth. now there is tendency to write ghc-specific libs. by defining requirements to the standard libs we may facilitate development of more portable, well documented and quick-checked ones. or may be some good enough libraries will be passed to society which will &quot;polish&quot; them in order to include in the set. anyway, i hope that *extensible* set of standard libraries with a published requirements to such libs would facilitate &quot;polishing&quot; of all Haskell libs just because ;)</p>
<p>and this leads us to other question - whether this set and API of each library should be fixed in language standard or it can evolve during the time?...</p>
<p><a href="PageOutline" class="uri" title="wikilink">PageOutline</a></p>
<h1 id="and">, , and </h1>
<p>When the parser parses an identifier, it generates a . A  is pretty much just a string, or a pair of strings, for a qualified name, such as . Here's the data type declaration, from <a href="GhcFile(compiler/basicTypes/RdrName.hs)" class="uri" title="wikilink">GhcFile(compiler/basicTypes/RdrName.hs)</a>: </p>
<p>User-written code never gets translated into the last two alternatives. They are used only internally by the compiler. For example, code generated by  might use an  to refer to , ignoring whatever  might happen to be in scope (dammit).</p>
<h2 id="the-module-and-modulename-types">The `Module` and `ModuleName` types</h2>
<p>In GHC, a <em>module</em> is uniquely defined by a pair of the module name and the package where the module is defined. The details are in <a href="GhcFile(compiler/basicTypes/Module.hs)" class="uri" title="wikilink">GhcFile(compiler/basicTypes/Module.hs)</a> and <a href="GhcFile(compiler/main/PackageConfig.hs)" class="uri" title="wikilink">GhcFile(compiler/main/PackageConfig.hs)</a>, but here are the key definitions:  You'll notice that a `Qual` `RdrName` contains a `ModuleName`; which module is referred to depends on the import declarations in that module. In contrast, a `Orig` `RdrName` refers to a unique `Module`.</p>
<h2 id="the-type-2">The  type</h2>
<p>An  is more-or-less just a string, like &quot;foo&quot; or &quot;Tree&quot;, giving the (unqualified) name of an entity. Well, not quite just a string, because in Haskell a name like &quot;C&quot; could mean a type constructor or data constructor, depending on context. So GHC defines a type `OccName` that is a pair of a  and a  indicating which name space the name is drawn from. The data type is defined (abstractly) in <a href="GhcFile(compiler/basicTypes/OccName.hs)" class="uri" title="wikilink">GhcFile(compiler/basicTypes/OccName.hs)</a>:  The name spaces are: </p>
<p>Attaching the names to their name spaces makes it very convenient to build mappings from names to things; where such a mapping might contain two strings that are identical, they can be distinguished by the name space, so when mapping s, a single map suffices.</p>
<p><a href="PageOutline" class="uri" title="wikilink">PageOutline</a></p>
<h1 id="recompilation-avoidance">Recompilation Avoidance</h1>
<h2 id="what-is-recompilation-avoidance">What is recompilation avoidance?</h2>
<p>When GHC is compiling a module, it tries to determine early on whether</p>
<p><code>* The object file (or byte-code in the case of GHCi) and [wiki:Commentary/Compiler/IfaceFiles interface file] exist from a previous compilation</code><br />
<code>* Recompilation is sure to produce exactly the same results, so it</code><br />
<code>  is not necessary.</code></p>
<p>If both of these hold, GHC stops compilation early, because the existing object code and interface are still valid. In GHCi and `--make`, we must generate the `ModDetails` from the `ModIface`, but this is easily done by calling `MkIface.typecheckIface`.</p>
<h2 id="example-2">Example</h2>
<p>Let's use a running example to demonstrate the issues. We'll have four modules with dependencies like this:</p>
<p></p>
<p>`A.hs`:</p>
<p></p>
<p>`B.hs`:</p>
<p></p>
<p>`C.hs`:</p>
<p></p>
<p>`D.hs`: </p>
<h2 id="why-do-we-need-recompilation-avoidance">Why do we need recompilation avoidance?</h2>
<h3 id="ghci-and---make">GHCi and `--make`</h3>
<p>The simple fact is that when you make a small change to a large program, it is often not necessary to recompile every module that depends directly or indirectly on something that changed. In GHCi and `--make`, GHC considers every module in the program in dependency order, and decides whether it needs to be recompiled, or whether the existing object code and interface will do.</p>
<h3 id="make">`make`</h3>
<p>`make` works by checking the timestamps on dependencies and recompiling things when the dependencies are newer. Dependency lists for `make` look like this (generated by `ghc -M`):</p>
<p></p>
<p>Only the `.hi` files of the <em>direct imports</em> of a module are listed. For example, `A.o` depends on `C.hi` and `B.hi`, but not `D.hi`. Nevertheless, if D is modified, we might need to recompile A. How does this happen?</p>
<p><code> * first, make will recompile D because its source file has changed,</code><br />
<code>   generating a new `D.o` and `D.hi`.</code></p>
<p><code> * If after recompiling D, we notice that its interface is the same</code><br />
<code>   as before, there is no need to modify the `.hi` file.  If the `.hi`</code><br />
<code>   file is not modified by the compilation, then `make` will notice</code><br />
<code>   and not recompile `B` or `C`, or indeed `A`.  This is an important</code><br />
<code>   optimisation.</code></p>
<p><code> * Suppose the change to `D` did cause a change in the interface</code><br />
<code>   (e.g. the type of `f` changed).  Now, `make` will recompile both</code><br />
<code>   `B` and `C`.  Suppose that the interfaces to `B` and `C`</code><br />
<code>   remain the same: B's interface says only that it re-exports `D.f`,</code><br />
<code>   so the fact that `f` has a new type does not affect `B`'s</code><br />
<code>   interface.  </code></p>
<p><code> * Now, `A`'s dependencies are unchanged, so `A` will not be</code><br />
<code>   recompiled.  But this is wrong: `A` might depend on something from</code><br />
<code>   `D` that was re-exported via `B` or `C`, and therefore need</code><br />
<code>   recompiling.</code></p>
<p>To ensure that `A` is recompiled, we therefore have two options:</p>
<p><code> 1. arrange that make knows about the dependency of A on D.</code></p>
<p><code> 2. arrange to touch `B.hi` and `C.hi` even if they haven't changed.</code></p>
<p>GHC currently does (2), more about that in a minute.</p>
<p>Why not do (1)? Well, then <em>every</em> time `D.hi` changed, GHC would be invoked on `A` again. But `A` doesn't depend directly on `D`: it imports `B`, and it might be therefore be insensitive to changes in `D`. By telling make only about direct dependencies, we gain the ability to avoid recompiling modules further up the dependency graph, by not touching interface files when they don't change.</p>
<p>Back to (2). In addition to correctness (recompile when necessary), we also want to avoid unnecessary recompilation as far as possible. Make only knows about very coarse-grained dependencies. For example, it doesn't know that changing the type of `D.f` can have no effect on `C`, so `C` does not in fact need to be recompiled, because to do so would generate exactly the same `.o` and `.hi` files as last time. GHC does have enough information to figure this out, so when GHC is asked to recompile a module it invokes the <em>recompilation checker</em> to determine whether recompilation can be avoided in this case.</p>
<h2 id="how-does-it-work">How does it work?</h2>
<p>We use <a href="http://en.wikipedia.org/wiki/Fingerprint_%28computing%29">fingerprints</a> to uniquely identify the interface exposed by a module, and to detect when it changes. In particular, we currently use 128-bit hashes produced by the MD5 algorithm (see <a href="GhcFile(compiler/utils/Fingerprint.hsc)" class="uri" title="wikilink">GhcFile(compiler/utils/Fingerprint.hsc)</a>).</p>
<p>An [wiki:Commentary/Compiler/IfaceFiles interface file] contains:</p>
<p><code>* Various fingerprints:</code><br />
<code>  * The </code><em><code>interface</code> <code>hash</code></em><code>, which depends on the entire contents of the</code><br />
<code>    interface file.  This is used to detect whether we should</code><br />
<code>    update the interface on disk after recompiling the module.  If the</code><br />
<code>    interface didn't change at all, then we don't want to touch the</code><br />
<code>    on-disk version because that would cause `make` to perform more</code><br />
<code>    compilations.</code><br />
<code>  * The </code><em><code>ABI</code> <code>hash</code></em><code>, which depends on everything that the module</code><br />
<code>    exposes about its implementation: think of this as a hash of</code><br />
<code>    </code><em><code>export-list</code> <code>hash</code></em><code> and </code><em><code>decls</code></em><code>.</code><br />
<code>  * The </code><em><code>export-list</code> <code>hash</code></em><code>, which depends on </code><br />
<code>    * The export list itself.  The export-list hash only depends on the </code><em><code>names</code></em><code> of the exports for the modules. The </code><em><code>types</code></em><code> of these exports are ignored in calculating the hash. Only a change of name or removal or addition of an export will change the hash. Not a type change of definition change.</code><br />
<code>    * the </code><em><code>orphan</code> <code>hash</code></em><code>, which depends on all the orphan instances/rules in the, and the orphan hashes of all orphan modules below this module in the dependency tree (see [#Orphans Orphans]). </code><br />
<code>    * the package dependencies (see [#Packageversionchanges Package Version Changes]).</code><br />
<code>* </code><em><code>exports</code></em><code>: what the module exports</code><br />
<code>* </code><em><code>dependencies</code></em><code>: modules and packages that this module depends on</code><br />
<code>* </code><em><code>usages</code></em><code>: what specific entities the module depends on</code><br />
<code>* </code><em><code>decls</code></em><code>: what the module defines</code><br />
<code>* various other stuff, but the above are the important bits</code></p>
<p>To look at the contents of an interface, use `ghc --show-iface`. For example, here's the output of `ghc --show-iface D.hi` for the module `D` in our example:</p>
<p></p>
<p>Lines beginning `import` are the <em>usages</em>, and after the usages are the decls.</p>
<h3 id="deciding-whether-to-recompile">Deciding whether to recompile</h3>
<p>If we already have an object file and interface file for a module, we might not have to recompile it, if we can be sure the results will be the same as last time.</p>
<p><code>* If the source file has changed since the object file was created,</code><br />
<code>  we better recompile.</code></p>
<p><code>* If anything else has changed in a way that would affect the results</code><br />
<code>  of compiling this module, we must recompile.</code></p>
<p>In order to determine the second point, we look at the <em>dependencies</em> and <em>usages</em> fields of the old interface file. The dependencies contains:</p>
<p><code>* </code><em><code>dep_mods</code></em><code>: Transitive closure of home-package modules that are</code><br />
<code>  imported by this module.  That is, all modules below the current</code><br />
<code>  one in the dependency graph.</code></p>
<p><code>* </code><em><code>dep_pkgs</code></em><code>: Transitive closure of packages depended on by this</code><br />
<code>  module, or by any module in </code><em><code>dep_mods</code></em><code>.</code></p>
<p><code>* other less important stuff.</code></p>
<p>First, the direct imports of the current module are resolved to `Module`s using `Finder.findModule` (a `Module` contains a module name and a package identifier). If any of those `Module`s are not listed amongst the dependencies of the old interface file, then either:</p>
<p><code>* an exposed package has been upgraded</code><br />
<code>* we are compiling with different package flags</code><br />
<code>* a home module that was shadowing a package module has been removed</code><br />
<code>* a new home module has been added that shadows a package module</code></p>
<p>and we must recompile.</p>
<p>Second, the <em>usages</em> of the module are checked. The usages contains two types of information:</p>
<p><code>* for a module that was imported, the export-list fingerprint of the</code><br />
<code>  imported module is recorded.  If any of the modules we imported now</code><br />
<code>  has a different export list we must recompile, so we check the</code><br />
<code>  current export-list fingerprints against those recorded in the</code><br />
<code>  usages.</code></p>
<p><code>* for every external name mentioned in the source code, the</code><br />
<code>  fingerprint of that name is recorded in the usages.  This is so</code><br />
<code>  that if we mention for example an external function `M.f`, we'll</code><br />
<code>  recompile if `M.f`'s type has changed, or anything referred to</code><br />
<code>  by `M.f`'s type has changed, or `M.f`'s unfolding has changed</code><br />
<code>  (when -O is on), and so on.</code></p>
<p>The interface files for everything in the usages are read (they'll already be in memory if we're doing `--make`), and the current versions for each of these entities checked against the usages from the old interface file. If any of these versions has changed, the module must be recompiled.</p>
<h3 id="example-3">Example</h3>
<p>There are some tricky cases to consider.</p>
<p>Suppose we change the definition of `D.f` in the example, and make it  Now, ultimately we need to recompile `A`, because it might be using an inlined copy of the old `D.f`, which it got via `B`.</p>
<p>It works like this:</p>
<p><code>* `D` is recompiled; the fingerprint of `D.f` changes</code><br />
<code>* `B` is considered; it recorded a usage on the old `D.f`, so</code><br />
<code>  gets recompiled, and now its interface records a usage on the new `D.f`</code><br />
<code>* `C` is considered; it doesn't need to be recompiled.</code><br />
<code>* `A` is considered (if we're using make, this is because `B.hi`</code><br />
<code>  changed); it recorded a usage on the old `D.f`, and so gets</code><br />
<code>  recompiled.</code></p>
<p>Now a slightly more tricky case: suppose we add an INLINE pragma to `D.f` (this is a trick to prevent GHC from inlining `D.h`, so that we can demonstrate dependencies between unfoldings). The code for D.hs is now</p>
<p></p>
<p>Looking at the interface file we can see what happened (snipped slightly):</p>
<p></p>
<p>Note that the unfolding of `D.f` mentions `D.h`.</p>
<p>Now, let's modify `D.h`, and look at the interface file again:</p>
<p></p>
<p>The fingerprint for `D.h` has changed, because we changed its definition. The fingerprint for `D.f` has also changed, because it depends on `D.h`. And consequently, the ABI hash has changed, and so has the interface hash (although the export hash and orphan hash are still the same). Note that it is significant that we used '-O' here. If we hadn't used '-O' then a change of a definition doesn't change any of the hashes because of the lack of inlining.</p>
<p>Why did the fingerprint for `D.f` have to change? This is vital, because anything that referred to `D.f` must be recompiled, because it may now see the new unfolding for `D.h`.</p>
<p>So the fingerprint of an entity represents not just the definition of the entity itself, but also the definitions of all the entities reachable from it - its transitive closure. The consequence of this is that when recording usages we only have to record the fingerprints of entities that were referred to directly in the source code, because the transitive nature of the fingerprint means that we'll recompile if anything reachable from these entities changes.</p>
<h3 id="how-does-fingerprinting-work">How does fingerprinting work?</h3>
<p>We calculate fingerprints by serialising the data to be fingerprinted using the `Binary` module, and then running the md5 algorithm over the serlialised data. When the data contains external `Name`s, the serialiser emits the fingerprint of the `Name`; this is the way that the fingerprint of a declaration can be made to depend on the fingerprints of the things it mentions.</p>
<h3 id="mutually-recursive-groups-of-entities">Mutually recursive groups of entities</h3>
<p>When fingerprinting a recursive group of entities, we fingerprint the group as a whole. If any of the definitions changes, the fingerprint of every entity in the group changes.</p>
<h3 id="fixities">Fixities</h3>
<p>We include the fixity of an entity when computing its fingerprint.</p>
<h3 id="instances">Instances</h3>
<p>Instances are tricky in Haskell, because they aren't imported or exported explicitly. Haskell requires that any instance defined in a module directly or indirectly imported by the current module is visible. So how do we track instances for recompilation, such that if a relevant instance is changed, added, or removed anywhere beneath the current module we will trigger a recompilation?</p>
<p>Here's how it works. For each instance we pick a distinguished entity to attach the instance to - possibly the class itself, or a type constructor mentioned in the instance. The entity we pick must be defined in the current module; if there are none to pick, then the instance is an orphan (more about those in the section on Orphans, below).</p>
<p>Having picked the distinguished entity, when fingerprinting that entity we include the instances. For example, consider an instance for class C at type T. Any module that could use this instance must depend (directly or indirectly) on both C and T, so it doesn't matter whether we attach the instance to C or T - either way it will be included in the fingerprint of something that the module depends on. In this way we can be sure that if someone adds a new instance, or removes an existing instance, if the instance is relevant to a module then it will affect the fingerprint of something that the module depends on, and hence will trigger recompilation.</p>
<p>In fact, we don't need to include the instance itself when fingerprinting C or T, it is enough to include the DFun (dictionary function) Id, since the type of this Id includes the form of the instance. Furthermore, we <em>must</em> include the DFun anway, because we must have a dependency on the dictionary and its methods, just in case they are inlined in a client module. A DFun looks something like this:</p>
<p></p>
<p>Making a type or class depend on its instances can cause a lot of recompilation when an instance changes. For example:</p>
<p></p>
<p>now the DFun for the instance `C T` will be attached to `T`, and so `T`'s fingerprint will change when anything about the instance changes, including `C` itself. So there is now have a dependency of `T` on `C`, which can cause a lot of recompilation whenever `C` changes. Modules using `T` who do not care about `C` will still be recompiled.</p>
<p>This seems like it would cause a lot of unnecessary recompilation. Indeed, in GHC 7.0.1 and earlier we tried to optimise this case, by breaking the dependency of `T` on `C` and tracking usages of DFuns directly - whenever a DFun was used, the typechecker would record the fact, and a usage on the DFun would be recorded in the interface file. Unfortunately, there's a bug in this plan (see #4469). When we're using `make`, we only recompile a module when any of the interfaces that it directly imports have changed; but a DFun dependency can refer to any module, not just the directly imported ones. Instead, we have to ensure that if an instance related to a particular type or class has changed, then the fingerprint on either the type or class changes, which is what the current plan does. It would be nice to optimise this in a safe way, and maybe in the future we will be able to do that.</p>
<h3 id="orphans">Orphans</h3>
<p>What if we have no declaration to attach the instance to? Instances with no obvious parent are called <em>orphans</em>, and GHC must read the interface for any module that contains orphan instances below the current module, just in case those instances are relevant when compiling the current module.</p>
<p>Orphans require special treatment in the recompilation checker.</p>
<p><code>* Every module has an </code><em><code>orphan</code> <code>hash</code></em><code>, which is a fingerprint of all</code><br />
<code>  the orphan instances (and rules) in the current module.</code></p>
<p><code>* The </code><em><code>export</code> <code>hash</code></em><code> depends on the </code><em><code>orphan</code> <code>hash</code></em><code> of the current</code><br />
<code>  module, and all modules below the current module in the dependency</code><br />
<code>  tree.  This models the fact that all instances defined in modules</code><br />
<code>  below the current module are available to importers of this module.</code></p>
<p>So if we add, delete, or modify an orphan instance, the orphan hash of the current module will change, and so will the export hash of the current module. This will trigger recompilation of modules that import the current module, which will cause their export hashes to change, and so on up the dependency tree.</p>
<p>This means a lot of recompilation, but it is at least safe. The trick is to avoid orphan instances as far as possible, which is why GHC has the warning flag `-fwarn-orphans`.</p>
<h3 id="rules">Rules</h3>
<p>RULEs are treated very much like instances: they are attached to one particular parent declaration, and if a suitable parent cannot be found, they become orphans and are handled in the same way as orphan instances.</p>
<h3 id="on-ordering">On ordering</h3>
<p>When fingerprinting a collection of things, for example the export list, we must be careful to use a canonical ordering for the collection. Otherwise, if we recompile the module without making any changes, we might get a different fingerprint due to accidental reordering of the elements.</p>
<p>Why would we get accidental reordering? GHC relies heavily on &quot;uniques&quot; internally (see <a href="GhcFile(compiler/basicTypes/Unique.lhs)" class="uri" title="wikilink">GhcFile(compiler/basicTypes/Unique.lhs)</a>): every entity has a unique, and uniques are assigned semi-randomly. Asking for the contents of a `UniqSet` or `UniqFM` will return the elements in order of their uniques, which may vary from run to run of the compiler.</p>
<p>The solution is to sort the elements using a stable ordering, such as lexicographic ordering.</p>
<h3 id="packages">Packages</h3>
<p>We need to record usage information about package modules too, so that we can correctly trigger recompilation if we depend on a package that has changed. But packages change rarely, so it would be wasteful to record detailed usage information for every entity that we use from an external package (imagine recording the fingerprints for `Bool`, `Int`, etc.). Instead, we simply record the ABI fingerprint for every package module that was imported by the current module. That way, if anything about the ABI of that package module has changed, then we can trigger a recompilation.</p>
<p>(Correctly triggering recompilation when packages change was one of the things we fixed when implementing fingerprints, see #1372).</p>
<h3 id="package-version-changes">Package version changes</h3>
<p>If the version of a package is bumped, what forces recompilation of the things that depend on it?</p>
<p><code>1. If a module from the package is imported directly, then we will notice that the imported module is not amongst the dependencies of the module when it was compiled last, and force a recompilation (see [#Decidingwhethertorecompile Deciding whether to recompile]).</code></p>
<p><code>2. If a module from the old package is imported indirectly, then the old package will be amongst the package dependencies (`dep_pkgs . mi_deps`), so we must recompile otherwise these dependencies will be inconsistent.  The way we handle this case is by including the package dependencies in the </code><em><code>export</code> <code>hash</code></em><code> of a module, so that other modules which import this module will automatically be recompiled when one of the package dependencies changes.  The recompiled module will have new package dependencies, which will force recompilation of its importers, and so on.  Therefore if a package version changes, the change will be propagated throughout the module dependency graph.</code></p>
<h2 id="interface-stability">Interface stability</h2>
<p>For recompilation avoidance to be really effective, we need to ensure that fingerprints do not change unnecessarily. That is, if a module is modified, it should be the case that the only fingerprints that change are related to the parts of the module that were modified. This may seem obvious, but it's surprisingly easy to get wrong. Here are some of the ways we got it wrong in the past, and some ways we still get it wrong.</p>
<p><code>* Prior to GHC 6.12, dictionary functions were named something like `M.$f23`, where `M` is the module defining the instance, and the number `23` was generated by simply assigning numbers to the dictionary functions defined by `M` sequentially.  This is a problem for recompilation avoidance, because now removing or adding an instance in `M` will change the numbering, and force recompilation of anything that depends on any instance in `M`.  Worse, the numbers are assigned non-deterministically, so simply recompiling `M` without changing its code could change the fingerprints.  In GHC 6.12 we changed it so that dictionary functions are named after the class and type(s) of the instance, e.g. `M.$fOrdInteger`.</code></p>
<p><code>* compiler-generated bindings used to be numbered in the same way, non-deterministically.  The non-determinism arises because Uniques are assigned by the compiler non-deterministically.  Well, they are deterministic but not in a way that you can sensibly control, because it depends on the order in which interface bindings are read, etc.  Internal mappings use Uniques as the key, so asking for the elements of a mapping gives a non-deterministic ordering.  The list of bindings emitted by the simplifier, although in dependency order, can vary non-deterministically within the constraints of the dependencies.  So if we number the compiler-generated bindings sequentially, the result will be a non-deterministic ABI.</code><br />
<code>  </code><a href="BR" title="wikilink"><code>BR</code></a><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  In GHC 6.12 we changed this so that compiler-generated bindings are given names of the form `f_x`, where `f` is the name of the exported Id that refers to the binding.  If there are multiple `f_x`s, then they are disambiguated with an integer suffix, but the numbers are assigned deterministically, by traversing the definition of `f` in depth-first left-to-right order to find references.  See `TidyPgm.chooseExternalIds`.</code></p>
<p><code>* There are still some cases where an interface can change without changing the source code.  The ones we know about are listed in #4012</code></p>
<h1 id="the-register-allocator-1">The Register Allocator</h1>
<h2 id="overview-6">Overview</h2>
<p>The register allocator is responsible for assigning real/hardware regs (hregs) to each of the virtual regs (vregs) present in the code emitted by the native code generator. It also inserts spill/reload instructions to save vregs to the stack in situations where not enough hregs are available.</p>
<p>GHC currently provides three register allocation algorithms, one which does simple linear scan and two version of graph coloring. Support for linear scan is likely to be removed in a subequent version.</p>
<p><code>* </code><strong><code>Linear</code> <code>scan</code></strong><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  The linear allocator is turned on by default. This is what you get when you compile with </code><code>. The linear allocator does a single pass through the code, allocating registers on a first-come-first-served basis. It is quick, and does a reasonable job for code with little register pressure. </code></p>
<p><code> This algorithm has no look-ahead. If say, a particular hreg will be clobbered by a function call, it does not know to avoid allocating to it in the code before the call, and subsequently inserts more spill/reload instructions than strictly needed.</code></p>
<p><code>* </code><strong><code>Graph</code> <code>coloring</code></strong><code> (enabled with </code><code>)</code><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  The graph coloring algorithm operates on the code for a whole function at a time. From each function it extracts a register conflict graph which has a node for every vreg and an edge between two vregs if they are in use at the same time and thus cannot share the same hreg. The algorithm tries to assign hregs (imagined as colors) to the nodes so that no two adjacent nodes share the same color, if it can't then it inserts spill code, rebuilds the graph and tries again. </code></p>
<p><code> Graph coloring tends to do better than the linear allocator because the conflict graph helps it avoid the look-ahead problem. The coloring allocator also tries harder to allocate the source and destination of reg-to-reg move instructions to the same hreg. This is done by coalescing (merging) move-related nodes. If this succeeds then the associated moves can be erased.</code></p>
<p><code>* </code><strong><code>Graph</code> <code>coloring</code> <code>with</code> <code>iterative</code> <code>coalescing</code></strong><code> (enabled with </code><code>)</code><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  Iterative coalescing is an improvement over regular graph coloring whereby coalescing passes are interleaved with coloring passes. Iterative coalescing does a better job than regular graph coloring, but is slower because it must alternate between the coloring and coalescing of nodes.</code></p>
<h2 id="code-map">Code map</h2>
<p>For an outline of the code see [wiki:Commentary/Compiler/Backends/NCG/RegisterAllocator/Code]</p>
<h2 id="references-2">References</h2>
<p>If you decide to do some hacking on the register allocator then take a look at (at least) these papers first:</p>
<p><strong>Iterated Register Coalescing</strong><a href="BR" class="uri" title="wikilink">BR</a> <em>George, Appel, 1996</em><a href="BR" class="uri" title="wikilink">BR</a> Decribes the core graph coloring algorithm used.</p>
<p><strong>A Generalised Algorithm for Graph-Coloring Register Allocation</strong><a href="BR" class="uri" title="wikilink">BR</a> <em>Smith, Ramsey, Holloway, 2004</em><a href="BR" class="uri" title="wikilink">BR</a> For a decription of how to deal with overlapping register sets, which aren't fully implemented. Explains what the ,  and  functions are for.</p>
<p><strong>Design and Implementation of a Graph Coloring Register Allocator for GCC</strong><a href="BR" class="uri" title="wikilink">BR</a> <em>Matz, 2003</em><a href="BR" class="uri" title="wikilink">BR</a> For an overview of techniques for inserting spill code.</p>
<h2 id="register-pressure-in-haskell-code">Register pressure in Haskell code</h2>
<p>Present GHC compiled code places very little pressure on the register set. Even on x86 with only 3 allocable registers, most modules do not need spill/reloads. This is a mixed blessing - on one hand the conflict graphs are small so we can avoid performance problems related to how the graph is represented, on the other hand it can be hard to find code to test against. Register pressure is expected to increase as the Stg-&gt;Cmm transform improves.</p>
<p>In the meantime, here are some good sources for test code:</p>
<p><code>* </code><strong><code>Nofib</code></strong><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  Only a few nofib benchmarks create spills with </code><code>, two are </code><code> and </code><code>.</code></p>
<p><code>* </code><strong><code>Turn</code> <code>on</code> <code>profiling</code></strong><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  Register pressure increases significantly when the module is compiled with profiling. </code><a href="attachment:checkSpills.report"><code>14</code></a><code> gives tuples of </code><code> present in output code generated by the three algorithms when compiled with </code><code>. Left to right are the stats for the linear, graph coloring and iterative coalescing algorithms. Note that most modules compile with no spill/reloads inserted, but a few (notably </code><code>) need several hundred.</code></p>
<p><code> I've found it useful to maintain three darcs repos when working on the allocator. </code><code> compiled with </code><code> for fast compilation during hacking, </code><code> for testing with profiling turned on, and </code><code> for running the validate script. Patches are created in </code><code>, pushed into </code><code> where </code><code> is used to compile the nofib benchmarks with the most register pressure. Once we're happy that the performance is ok, the patch is then pushed into </code><code> for validation before pushing to the main repo on </code></p>
<p><code>* </code><strong><code>SHA</code> <code>from</code> <code>darcs</code></strong><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  The </code><code> module from the darcs source, compiled with </code><code> creates the most register pressure out of any Haskell code that I'm aware of. When compiling SHA1, GHC inlines several worker functions and the native code block that computes the hash ends up being around 1700 instructions long. vregs that live in the middle of the block have in the order of 30 conflict neighbors. (evidently, the conflict graph is too large for most of the graphviz layout algorithms to cope with)</code></p>
<p><code> For these reasons, </code><code> can be treated as a good worst-case input to the allocator. In fact, the current linear allocator cannot compile it with </code><code> on x86 as it runs out of stack slots, which are allocated from a static pool. Make sure to test any changes to the allocator against this module.</code></p>
<h2 id="hackingdebugging">Hacking/Debugging</h2>
<p><code>* </code><strong><code>Turn</code> <code>on</code> </strong><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  Breaking the allocator can result in compiled programs crashing randomly (if you're lucky) or producing the wrong output. Make sure to always turn on </code><code>. Doing this makes the allocator call </code><code> after every spill/color stage. </code><code> checks that all the edges point to valid nodes, that no conflicting nodes have the same color, and if the graph is supposed to be colored then all nodes are really colored.</code></p>
<p><code>* </code><strong><code>Some</code> <code>useful</code> <code>dump</code> <code>flags</code></strong></p>
<p><code> </code><a href="BR" title="wikilink"><code>BR</code></a><br />
<code> Shows the code and conflict graph after ever spill/color stage. Also shows spill costs, and what registers were coalesced.</code></p>
<p><code> </code><a href="BR" title="wikilink"><code>BR</code></a><br />
<code> Gives statistics about how many spills/reloads/reg-reg-moves are in the output program.</code></p>
<p><code> </code><a href="BR" title="wikilink"><code>BR</code></a><br />
<code> Gives the final output code. </code></p>
<p><code> </code><a href="BR" title="wikilink"><code>BR</code></a><br />
<code> Diverts dump output to files. This can be used to get dumps from each module in a nofib benchmark.</code><br />
<br />
<code> </code></p>
<p><code>* </code><strong><code>Visualisation</code> <code>of</code> <code>conflict</code> <code>graphs</code></strong><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  Graphviz, available from </code><a href="http://www.graphviz.org"><code>15</code></a><code> can be used to make nice visualisations of the register conflict graphs. Use </code><code>, and copy one of the graph descriptions into a new file </code></p>
<p><code> </code><br />
<code> Here's two from </code><code> compiled with </code><code>:</code></p>
<p><code>  </code><a href="attachment:graph.dot"><code>16</code></a><code> -&gt; </code><a href="attachment:graph.png"><code>17</code></a></p>
<p><code>  </code><a href="attachment:graph-colored.dot"><code>18</code></a><code> -&gt; </code><a href="attachment:graph-colored.png"><code>19</code></a></p>
<p><code>* </code><strong><code>checkSpills</code></strong><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  </code><a href="attachment:checkSpills.hs"><code>20</code></a><code> is a nasty, throw away script which can be used to automate the comparison of allocation algorithms. Copy it and a list of test like </code><a href="attachment:checkSpills.tests"><code>21</code></a><code> to the top level nofib directory, compile and run. It will build the nofib benchmarks in the list 6 times each, once each with each of the allocators to extract spill counts, and then once again to get compile timings which are unperterbed by the space leaks introduced by compiling with debugging turned on. It's only needed if you're hacking on the allocator, parses the nofib make output directly, and is likely to rot - which is why it isn't included in the main source tree.</code></p>
<h2 id="runtime-performance">Runtime performance</h2>
<p>Runtime performance of the graph coloring allocator is proportional to the size of the conflict graph and the number of build/spill cycles needed to obtain a coloring. Most functions have graphs &lt; 100 nodes and generate no spills, so register allocation is a small fraction of overall compile time.</p>
<h2 id="possible-improvements">Possible Improvements</h2>
<p>These are some ideas for improving the current allocator, most potentially useful first.</p>
<p><code>* </code><strong><code>Work</code> <code>lists</code> <code>for</code> <code>iterative</code> <code>coalescing.</code></strong><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  The iterative coalescing alternates between scanning the graph for trivially colorable (triv) nodes and perforing coalescing. When two nodes are coalesced, other nodes that are not adjacent to the coalesced nodes do not change and do not need to be rescanned straight away. Runtime performance of the iterative coalescer could probably be improved by keeping a work-list of &quot;nodes that might have become trivially colorable&quot;, to help find nodes that won't have changed.</code></p>
<p><code>* </code><strong><code>Improve</code> <code>spill</code> <code>code</code> <code>generator/cleaner.</code></strong><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  When spilling a particular vreg, the current spill code generator simply inserts a spill after each def and a reload before each use. This quickly reduces the density of conflicts in the graph, but produces inefficient code because more spill/reloads are inserted than strictly nessesary. Good code is recovered by the spill cleaner which runs after allocation and removes spill/reload instructions that aren't nessesary. Some things to try:</code><br />
<code>  * </code><strong><code>Spill</code> <code>coalescing</code></strong><a href="BR" title="wikilink"><code>BR</code></a><code> </code><br />
<code>    No attempt is currently made to share spill slots between different vregs. Each named vreg is spilled to its own static spill slot on the C stack. The amount of stack space needed could be reduced by sharing spill slots between vregs so long as their live ranges do not overlap.</code><br />
<code>  * </code><strong><code>Try</code> <code>to</code> <code>split</code> <code>live</code> <code>ranges</code> <code>before</code> <code>spilling</code></strong><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>    If a live range has several use/defs then we could insert fresh reg-reg moves to break it up into several smaller live ranges. We then might get away with spilling just one section instead of the whole range. Not sure if this would be a win over the current situation. We would need spill-coalescing to be implemented before this so that we don't require an extra slot for each new live range.</code><br />
<code>  * </code><strong><code>Rematerialization</code></strong><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>    As the spill cleaner walks through the code it builds a mapping of which slots and registers hold the same value. On each reload instruction, if the slot and reg are known to already have the same value then the reload can be erased. This mapping could be extended with constants, so that if a vreg holding a constant value cannot be allocated a hreg, the constant value can be rematerialized instead of being spilled/reloaded to a stack slot.</code></p>
<p><code>* </code><strong><code>Revisit</code> <code>choosing</code> <code>of</code> <code>spill</code> <code>candidates</code></strong><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  If the graph cannot be colored then a node/vreg must be chosen to be potentially spilled. Chaitin's forumula says to calculate the spill cost by adding up the number of uses and defs of that vreg and divide by the degree of the node. In the code that I've tested against, it's been better to just choose the live range that lives the longest. Perhaps this is because the 'real' spill cost would depend on the spills/reloads actually inserted, not a simple count of use/defs. Perhaps choosing the longest live range is just better for the particular kind of code that GHC generates.</code></p>
<p><code>* </code><strong><code>Revisit</code> <code>trivColorable</code> <code>/</code> <code>aliasing</code> <code>of</code> <code>register</code> <code>sets</code></strong><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>  For the architectures currently supported, x86, x86_64 and ppc, the native code generator currently emits code using only two register classes </code><code> and </code><code>. As these classes are disjoint (ie, none of the regs from one class alias with with regs from another), checking whether a node of a certain class is trivially colorable reduces to counting up the number of neighbours of that class.</code></p>
<p><code> If the NCG starts to use aliasing register classes eg: both 32bit </code><code>s and 64bit </code><code>s on sparc; combinations of 8, 16, and 32 bit integers on x86 / x86_x6 or usage of sse / altivec regs in different modes, then this can be supported via the method described in [Smith et al]. The allocator was designed with this in mind - ie, by passing a function to test if a node is trivially colorable as a parameter to the coloring function - and there is already a description of the register set for x86 in </code><a href="GhcFile(compiler/nativeGen/RegArchX86.hs)" title="wikilink"><code>GhcFile(compiler/nativeGen/RegArchX86.hs)</code></a><code>, but the native code generator doesn't currently emit code to test it against.</code></p>
<h1 id="haskell-excecution-registers">Haskell Excecution: Registers</h1>
<p>Source files: <a href="GhcFile(includes/stg/Regs.h)" class="uri" title="wikilink">GhcFile(includes/stg/Regs.h)</a>, <a href="GhcFile(includes/stg/MachRegs.h)" class="uri" title="wikilink">GhcFile(includes/stg/MachRegs.h)</a></p>
<p>During execution of Haskell code the following (virtual) registers are always valid:</p>
<p><code>* `Hp` points to the byte before the first free byte in the (contiguous) allocation space.</code></p>
<p><code>* `HpLim` points to the last available byte in the current chunk of allocation space.</code></p>
<p><code>* `Sp` points to the youngest allocated byte of stack.  The stack grows downwards.  Why?  Because that means a return address is at a lower address than the stack frame it &quot;knows about&quot;, and that in turn means that we can treat a stack frame very like a heap object, with an info pointer (return address) as its first word.</code></p>
<p><code>* `SpLim` points to the last (youngest) available byte in the current stack.</code></p>
<p>There are bunch of other virtual registers, used for temporary argument passing, for words, floats and doubles: `R1` .. `R10`, `F1` .. `F4`, `D1` .. `D4`, `L1` .. `L2`.</p>
<p>In a register-rich machine, many of these virtual registers will be mapped to real registers. In a register-poor machine, they are instead allocated in a static memory record, pointed to by a real register, `BaseReg`.</p>
<p>The code generator knows how many real registers there are, and tries to avoid using virtual registers that are not mapped to real registers. So, for example, it does not use `R5` if the latter is memory-mapped; instead, it passes arguments on the stack.</p>
<h1 id="relevant-ghc-parts-for-demand-analysis-results">Relevant GHC parts for Demand Analysis results</h1>
<p><code> * `compiler/basicTypes/Demand.lhs` -- contains all information about demands and operations on them, as well as about serialization/deserialization of demand signatures. This module is supposed to be changed whenever the demand nature should be enhanced;</code></p>
<p><code> * `compiler/stranal/DmdAnal.lhs` -- the demand analysis itself. Check multiple comments to figure out main principles of the algorithm.</code></p>
<p><code> * `compiler/stranal/WorkWrap.lhs` -- a worker-wrapper transform, main client of the demand analysis. The function split is performed in `worthSplittingFun` basing on demand annotations of a function's parameters. </code></p>
<p><code> *  `compiler/stranal/WwLib.lhs` -- a helper module for the worker-wrapper machinery. The &quot;deep&quot; splitting of a product type argument makes use of the strictness info and is implemented by the function `mkWWstr_one`. The function `mkWWcpr` makes use of the CPR info.</code></p>
<p><code> * `compiler/basicTypes/Id.lhs` -- implementation of identifiers contains a number of utility functions to check/set demand annotations of binders. All of them are just delegating to appropriate functions/fields of the `IdInfo` record;</code></p>
<p><code> * `compiler/basicTypes/IdInfo.lhs` -- `IdInfo` record contains all information about demand and strictness annotations of an identifier. `strictnessInfo` contains a representation of an abstract two-point demand transformer of a binder, considered as a reference to a value. `demandInfo` indicates, which demand is put to the identifier, which is a function parameter, if the function is called in a strict/used context. `seq*`-functions are invoked to avoid memory leaks caused by transforming new ASTs by each of the compiler passes (i.e., no thunks pointing to the parts of the processed trees are left). </code></p>
<p><code> * `compiler/basicTypes/MkId.lhs` -- A machinery, responsible for generation of worker-wrappers makes use of demands. For instance, when a signature for a worker is generated, the following strictness signature is created:</code></p>
<p></p>
<p><code> In words, a non-bottoming demand type with `N` lazy/used arguments (`top`) is created for a worker, where `N` is just a worker's pre-computed arity. Also, particular demands are used when creating signatures for dictionary selectors (see `mkDictSelId`). </code></p>
<p><code> * `compiler/prelude/primops.txt.pp` -- this file defines demand signatures for primitive operations, which are inserted by `cpp` pass on the module `compiler/basicTypes/MkId.lhs`;</code></p>
<p><code> * `compiler/coreSyn/CoreArity.lhs` -- demand signatures are used in order to compute the unfolding info of a function: bottoming functions should no be unfolded. See `exprBotStrictness_maybe` and `arityType`.</code></p>
<p><code> * `compiler/coreSyn/CoreLint.lhs` -- the checks are performed (in `lintSingleBinding`): </code><br />
<code>   * whether arity and demand type are consistent (only if demand analysis already happened);</code><br />
<code>   * if the binder is top-level or recursive, it's not demanded (i.e., its demand is not strict).</code></p>
<p><code> * `compiler/coreSyn/CorePrep.lhs` -- strictness signatures are examining before converting expression to A-normal form.</code></p>
<p><code> * `compiler/coreSyn/MkCore.lhs` -- a bottoming strictness signature created for `error`-like functions (see `pc_bottoming_Id`).</code></p>
<p><code> * `compiler/coreSyn/PprCore.lhs` -- standard pretty-printing machinery, should be modified to change PP of demands.</code></p>
<p><code> * `compiler/iface/IfaceSyn.lhs`  -- serialization, grep for `HsStrictness` constructors.</code></p>
<p><code> * `compiler/iface/MkIface.lhs`  -- a client of `IfaceSyn`, see usages of `HsStrictness`.</code></p>
<p><code> * `compiler/iface/TcIface.lhs` -- the function `tcUnfolding` checks if an identifier binds a bottoming function in order to decide if it should be unfolded or not</code></p>
<p><code> * `compiler/main/TidyPgm.lhs` -- Multiple checks of an identifier to bind a bottoming expression, running a cheap-an-cheerful bottom analyser. See `addExternal` and occurrences of `exprBotStrictness_maybe`.</code></p>
<p><code> * `compiler/simplCore/SetLevels.lhs` -- It is important to zap demand information, when an identifier is moved to a top-level (due to let-floating), hence look for occurrences of `zapDemandIdInfo`.</code></p>
<p><code> * `compiler/simplCore/SimplCore.lhs` -- this module is responsible for running the demand analyser and the subsequent worker-wrapper split passes. </code></p>
<p><code> * `compiler/simplCore/SimplUtils.lhs`  -- is a new arity is less than the arity of the demand type, a warning is emitted; check `tryEtaExpand`.</code></p>
<p><code> * `compiler/specialise/SpecConstr.lhs` -- strictness info is used when creating a specialized copy of a function, see `spec_one` and `calcSpecStrictness`.</code></p>
<p><a href="PageOutline" class="uri" title="wikilink">PageOutline</a></p>
<h1 id="remembered-sets">Remembered Sets</h1>
<p>Since in generational GC we may need to find all the live objects in a young generation without traversing the older generation(s), we need a record of the pointers from those old generations into the young generations. This is termed the &quot;remembered set&quot;.</p>
<p>In GHC each `generation` structure contains a field `mut_list`, which points to a chain of blocks. Each block in the chain contains a list of pointers to objects in that generation which contain pointers to objects in younger generations. There are alternative schemes, e.g.</p>
<p><code>* Keeping track of each </code><em><code>pointer</code></em><code>, rather than </code><em><code>object</code></em><code> that points to a younger generation.  The remembered set would</code><br />
<code>  be larger (possibly very much larger, in the case of arrays), but it would be more accurate, and traversing the</code><br />
<code>  remembered set at GC time would be faster.</code></p>
<p><code>* Some GCs use &quot;card-marking&quot; schemes whereby the heap is divided into &quot;cards&quot; of a fixed size, and each card has a bit to</code><br />
<code>  indicate whether that card contains pointers to a younger generation.  This is much less accurate than a remembered set,</code><br />
<code>  but it is faster at runtime if a lot of mutation is taking place, and it takes less space than a remembered set.  In GHC</code><br />
<code>  we typically do not have much mutation to worry about, so card marking would be a poor compromise in our case.</code></p>
<p>The remembered set may contain duplicates, or it may contain pointers to objects that don't really point to young generations.</p>
<h2 id="remembered-set-maintenance-during-mutation">Remembered set maintenance during mutation</h2>
<p>While the mutator is running, we have to add any old-to-new generation pointers that are created. Old-to-new pointers are created by mutating (writing to) an object in the old generation, and catching these writes is called a &quot;write barrier&quot;.</p>
<p>A pointer can be added to a remembered set using</p>
<p></p>
<p>This adds the pointer `p` to the remembered set for generation `gen`, using Capability `cap`. Each Capability has its own remembered set for each generation, so that when running in parallel we can update remembered sets without taking a lock, and also so that we can take advantage of locality in the GC, by traversing a remembered set on the same CPU that created it.</p>
<p>Here are the cases where we need a write barrier in GHC:</p>
<h3 id="thunk-updates">Thunk Updates</h3>
<p>Updating a thunk in an old generation. This is taken care of by the update code, see <a href="GhcFile(rts/Updates.h)" class="uri" title="wikilink">GhcFile(rts/Updates.h)</a>.</p>
<h3 id="mutable-objects-mut_var-mvar">Mutable objects: MUT_VAR, MVAR</h3>
<p>For `MUT_VAR`, the writer must call `dirty_MUT_VAR`:</p>
<p></p>
<p>(in <a href="GhcFile(rts/sm/Storage.c)" class="uri" title="wikilink">GhcFile(rts/sm/Storage.c)</a>). The code generator inserts calls to `dirty_MUT_VAR` when it compiles a call to the primitive `writeMutVar#`.</p>
<p>`dirty_MUT_VAR` does the following: if the object's header is `MUT_VAR_CLEAN`, then the header is set to `MUT_VAR_DIRTY`, and the object is added to the remembered set if it resides in an old generation. If the header was already `MUT_VAR_DIRTY`, no action is taken.</p>
<p>`MVAR` is handled in the same way, with </p>
<h3 id="arrays-mut_arr_ptrs">Arrays: MUT_ARR_PTRS</h3>
<p>Unlike mutable variables and MVARs, mutable arrays are kept in the remembered set permanently. This reflects the fact that mutable arrays are likely to be written to more often, and there are likely to be fewer of them. However, we still mark arrays according to whether the array is dirty or not, using `MUT_ARR_PTRS_DIRTY` and `MUT_ARR_PTRS_CLEAN`.</p>
<p>There are also `MUT_ARR_PTRS_FROZEN` and `MUT_ARR_PTRS_FROZEN0`, which are used to indicate arrays that have been frozen using `unsafeFreezeArray#`. A frozen array is different from a mutable array in the sense that while it may have old-to-new pointers, it is not going to be mutated any further, and so we probably want to use [wiki:Commentary/Rts/Storage/GC/EagerPromotion eager promotion] on it.</p>
<h3 id="threads-tso">Threads: TSO</h3>
<p>Threads (TSOs) have stacks, which are by definition mutable. Running a thread is therefore an act of mutation, and if the thread resides in an old generation, it must be placed in the remembered set. Threads have two dirty bits: `tso-&gt;dirty` is set to non-zero if the thread's stack or any part of the TSO structure may be dirty, and also there is a bit `TSO_LINK_DIRTY` in `tso-&gt;flags` which is set if the TSO's link field may be dirty. If the thread is executed, then `dirty_TSO()` must be called in order to set the `tso-&gt;dirty` bit and add the TSO to the appropriate remembered set.</p>
<p></p>
<p>To set the TSO's link field, use `setTSOLink()` (from <a href="GhcFile(rts/sm/Storage.c)" class="uri" title="wikilink">GhcFile(rts/sm/Storage.c)</a>) which arranges to add the TSO to the remembered set if necessary.</p>
<p></p>
<p>there are a few exceptions where `setTSOLink()` does not need to be called; see <a href="GhcFile(rts/sm/Storage.c)" class="uri" title="wikilink">GhcFile(rts/sm/Storage.c)</a> for details.</p>
<h2 id="remembered-set-maintenance-during-gc">Remembered set maintenance during GC</h2>
<p>During GC, the principle of write barriers is quite similar: whenever we create an old-to-new pointer, we have to record it in the remembered set. The GC achieves this as follows:</p>
<p><code>* The GC thread structure has a field `gct-&gt;evac_gen` which specifies the desired destination generation.</code><br />
<code>* there is a flag `gct-&gt;failed_to_evac`, which is set to true by `evacuate` if it did not manage to evacuate</code><br />
<code>  the object into the desired generation.</code><br />
<code>* after scavenging an object, `scavenge_block` checks the `failed_to_evac` flag, and if it is set, adds the object to the remembered set, using `recordMutableGen_GC()` (the equivalent of `recordMutableCap` for calling within the GC).</code></p>
<h1 id="the-renamer">The renamer</h1>
<p>The renamer's Number One task is to replace [wiki:Commentary/Compiler/RdrNameType RdrNames] with [wiki:Commentary/Compiler/NameType Names]. For example, consider  (where all the variables are s). The result of renaming module M is:  where all these names are now s.</p>
<p><code> * The top-level unqualifed </code><code> &quot;</code><code>&quot; has become the </code><code> </code><code> </code><code>.  </code><br />
<code> * The occurrences &quot;</code><code>&quot; and &quot;</code><code>&quot; are both bound to this </code><code>.  </code><br />
<code> * The qualified </code><code> &quot;</code><code>&quot; becomes the </code><code> </code><code>, because the function is defined in module K.  </code><br />
<code> * The lambda-bound &quot;</code><code>&quot; becomes an </code><code> name, here written </code><code>.  (All the </code><code> names have uniques too, but we often do not print them.)</code></p>
<p>In addition, the renamer does the following things:</p>
<p><code>* Sort out fixities. The parser parses all infix applications as </code><strong><code>left-associative</code></strong><code>, regardless of fixity.  For example &quot;</code><code>&quot; is parsed as &quot;</code><code>&quot;.  The renamer re-associates such nested operator applications, using the fixities declared in the module.</code></p>
<p><code>* Dependency analysis for mutually-recursive groups of declarations.  This divides the declarations into strongly-connected components.</code></p>
<p><code>* Lots of lexical error checking: variables out of scope, unused bindings, unused imports, patterns that use the same binder many times, etc.</code></p>
<p>The renamer sits between the parser and the typechecker. However, its operation is quite tightly interwoven with the typechecker. This is mainly due to support for Template Haskell, where spliced code has to be renamed and type checked. In particular, top-level splices lead to multiple rounds of renaming and type checking. It uses the [wiki:Commentary/Compiler/TcRnMonad same monad as the typechecker].</p>
<h2 id="the-global-renamer-environment">The global renamer environment, </h2>
<p>A big part of the renamer's task is to build the <strong>global rdr-env</strong> for the module, of type . This environment allows us to take a qualified or un-qualified  and figure out which  it means. The global rdr-env is built by looking at all the imports, and the top-level declarations of the module.</p>
<p>You might think that the global rdr-env would be a mapping from  to , but it isn't. Here is what it looks like, after at least three iterations (all in <a href="GhcFile(compiler/basicTypes/RdrName.hs)" class="uri" title="wikilink">GhcFile(compiler/basicTypes/RdrName.hs)</a>):  Here is how to understand these types:</p>
<p><code>* The environment (`GlobalRdrEnv`) maps an </code><code> to a list of all entities with that occurrence name that are in scope (in any way).  </code></p>
<p><code>* Each of these is represented by a </code><code>, which gives the entity's </code><code> plus a specification of how it is in scope, its </code><code>.  </code></p>
<p><code>* The </code><code> has one of two forms.  Either it is in scope because it is defined in this module (</code><code>), or because it is imported.  In the latter case, the </code><code> describes all the import statements that bring it into scope. </code></p>
<p><code>* An </code><code> has two components: </code><br />
<code>  * An </code><code> that describes the entire import declaration. This is shared between all entities brought into scope by a particular import declaration.</code><br />
<code>  * An </code><code> that describes the import item that brought the entity into scope.</code><br />
<code>For example, given</code></p>
<p></p>
<p><code>the </code><code> would describe the </code><code> and </code><code> part, while the </code><code> describes the </code><code> part.  You can look in </code><code> to see what an </code><code> and </code><code> are like!</code><br />
<code>* The </code><code> of an entity is the </code><code> under which it is grouped when the forms `T(..)` or `T(C,D)` are used in an export or import list.  In the `T(..)` form, all the things whose </code><code> is `T` are chosen.  In the `T(C,D)` form, it is required that `C` and `D` have `T` as parents.  </code><br />
<code>  For example, </code><br />
<code>  * The `Parent` of a data constructor is its data type</code><br />
<code>  * The `Parent` of a record field selector is its data type</code><br />
<code>  * The `Parent` of a class operation is its class</code></p>
<p>With all that information, we can give good error messages, especially in the case where an occurrence &quot;f&quot; is ambiguous (i.e. different entities, both called &quot;f&quot;, were imported by different import statements).</p>
<p>The global rdr-env is created by <a href="GhcFile(compiler/rename/RnNames.hs)" class="uri" title="wikilink">GhcFile(compiler/rename/RnNames.hs)</a>.</p>
<p>It is important to note that the global rdr-env is created <em>before</em> the renamer actually descends into the top-level bindings of a module. In other words, before  performs the renaming of a module by way of , it uses  to set up the global rdr-env environment, which contains  for all imported and all locally defined toplevel binders. Hence, when the helpers of  come across the defining occurences of a toplevel , they don't rename it by generating a new name, but they simply look up its name in the global rdr-env.</p>
<h2 id="unused-imports">Unused imports</h2>
<p>See [wiki:Commentary/Compiler/UnusedImports how the renamer reports unused imports]</p>
<h2 id="name-space-management">Name Space Management</h2>
<p>(too much detail?)</p>
<p>As anticipated by the variants  and  of , some names should not change during renaming, whereas others need to be turned into unique names. In this context, the two functions  and  are important:  The two functions introduces new toplevel and new local names, respectively, where the first two arguments to newTopSrcBinder determine the currently compiled module and the parent construct of the newly defined name. Both functions create new names only for [wiki:Commentary/Compiler/RdrNameType RdrNames] that are neither exact nor original.</p>
<h2 id="rebindable-syntax">Rebindable syntax</h2>
<p>(!ToDo: Not fully proof-read.)</p>
<p>In Haskell when one writes &quot;3&quot; one gets &quot;fromInteger 3&quot;, where &quot;fromInteger&quot; comes from the Prelude (regardless of whether the Prelude is in scope). If you want to completely redefine numbers, that becomes inconvenient. So GHC lets you say &quot;-fno-implicit-prelude&quot;; in that case, the &quot;fromInteger&quot; comes from whatever is in scope. (This is documented in the User Guide.)</p>
<p>This feature is implemented as follows (I always forget).</p>
<p><code>* Names that are implicitly bound by the Prelude, are marked by the type </code><code>. Moreover, the association list </code><code> is set up by the renamer to map rebindable names to the value they are bound to. </code><br />
<code>* Currently, five constructs related to numerals (</code><code>, </code><code>, </code><code>, </code><code>, and </code><code>) and two constructs related to do-expressions (</code><code> and </code><code>) have rebindable syntax. </code><br />
<code>* When the parser builds these constructs, it puts in the built-in Prelude Name (e.g. </code><code>). </code><br />
<code>* When the renamer encounters these constructs, it calls </code><code>. This checks for </code><code>; if not, it just returns the same Name; otherwise it takes the occurrence name of the Name, turns it into an unqualified </code><code>, and looks it up in the environment. The returned name is plugged back into the construct. </code><br />
<code>* The typechecker uses the </code><code> to generate the appropriate typing constraints. </code></p>
<h1 id="replacing-the-native-code-generator">Replacing the Native Code Generator</h1>
<p>The existence of LLVM is definitely an argument not to put any more effort into backend optimisation in GHC, at least for those optimisations that LLVM can already do. There's also the question of whether it's worth extending the NCG to support SIMD primops. At the moment only the LLVM backend supports these, but current processor architectures will rely more and more on wide vector SIMD instructions for performance. Given that the LLVM project is now stable and widely used, it may be better to drop the NCG entirely (and delete the code).</p>
<p>However, there are a few ways that the LLVM backend needs to be improved before it can be considered to be a complete replacement for the existing NCG:</p>
<p>1. Compilation speed. LLVM approximately doubles compilation time. Avoiding going via the textual intermediate syntax would probably help here.</p>
<p>2. Shared library support (#4210, #5786). It works (or worked?) on a couple of platforms. But even on those platforms it generated worse code than the NCG due to using dynamic references for *all* symbols, whereas the NCG knows which symbols live in a separate package and need to use dynamic references.</p>
<p>3. Some low-level optimisation problems (#4308, #5567). The LLVM backend generates bad code for certain critical bits of the runtime, perhaps due to lack of good aliasing information. This hasn't been revisited in the light of the new codegen, so perhaps it's better now.</p>
<p>Someone should benchmark the LLVM backend against the NCG with new codegen in GHC 7.8. It's possible that the new codegen is getting a slight boost because it doesn't have to split up proc points, so it can do better code generation for let-no-escapes. It's also possible that LLVM is being penalised a bit for the same reason.</p>
<p>Other considerations:</p>
<p>1. The GHC distribution would need to start shipping with its own copy of LLVM. The LLVM code that GHC produces typically lags the current version of LLVM, so we'd need to ensure there was a usable version.</p>
<p>2. If we did ship our own version of LLVM, we could add custom plugins to improve the GHC generated code. At one stage Max Bolingbroke wrote an LLVM alias analysis plugin, but making it work against an arbitrary existing LLVM version would be infeasible.</p>
<p>note (carter): If we're very thoughtful about the changes / extensions to llvm needed for GHC, I'm somewhat confident that we could get any such patches upstreamed to llvm proper. The down side of this is that any such features would be subject to the llvm release cycle, plus we'd want to make sure that we're not just completely changing what we'd like upstreamed every ghc release cycle. The upside is that we'd get a lot more scrutiny / feedback / checking by llvm devs than we'd get with our own patched variant</p>
<h1 id="resource-limits">Resource Limits</h1>
<p>This page describes a proposed resource limits capabilities for GHC. The idea is to give users the ability to create and utilize resource containers inside programs, and then provide in-program access to heap census and other information. The semantics of resource containers are quite similar to cost centers used in profiling, except that they do not have &quot;stack&quot; semantics (more on this later). The end result is the ability to impose resource limits on space usage.</p>
<h2 id="code-generation-changes">Code generation changes</h2>
<p>Resource limits is a new way (similar to profiled and dynamic). Here are the relevant changes:</p>
<h3 id="dynamic-closure-allocation">Dynamic closure allocation</h3>
<p><a href="GhcFile(compiler/codeGen/StgCmmHeap.hs)" class="uri" title="wikilink">GhcFile(compiler/codeGen/StgCmmHeap.hs)</a>:allocDynClosureCmm (via StgCmmCon, also handles StgCmmBind:mkRhsClosure/cgRhsStdThunk. link_caf needs special treatment.)</p>
<p></p>
<p>Changes to:</p>
<p></p>
<p>I.e. no change from un-profiled.</p>
<h3 id="caf-allocation">CAF Allocation</h3>
<p><a href="GhcFile(compiler/codeGen/StgCmmBind.hs)" class="uri" title="wikilink">GhcFile(compiler/codeGen/StgCmmBind.hs)</a>:thunkCode</p>
<p>Here is an interesting bugger:</p>
<p></p>
<p>Notice the heap check serves for the later branch too. On the other hand, the CCCS coincides with the later change. This seems to be the general pattern. So we might be able to handle this CAF by special-casing CAFs.</p>
<p></p>
<p>We also hit the slow function application path.</p>
<h3 id="thunk-code">Thunk code</h3>
<p><a href="GhcFile(compiler/codeGen/StgCmmBind.hs)" class="uri" title="wikilink">GhcFile(compiler/codeGen/StgCmmBind.hs)</a>:thunkCode</p>
<p></p>
<p>Changes to:</p>
<p></p>
<h3 id="foreign-calls">Foreign calls</h3>
<p></p>
<p>Changes to:</p>
<p></p>
<p>No change from unprofiled</p>
<h2 id="case-split">Case split</h2>
<p>Do a nursery swap.</p>
<ul>
<li><ul>
<li>Warning:** The rest of this document describes an old iteration of the system, which directly used</li>
</ul></li>
</ul>
<h2 id="front-end-changes">Front-end changes</h2>
<p>The basic idea behind this patch is that data collected during **profiling** can also be used at runtime to enforce limits. So most of the API involves (1) dynamically setting cost-centres, which GHC uses to do profiling, and (2) querying and receiving callbacks when certain events happen during profiling. Costs can be collected anywhere you could have placed an  annotation statically.</p>
<p></p>
<p>The general usage of this API goes like:</p>
<p></p>
<p>Another use-case is more fine-grained SCCs based on runtime properties, not source-level features.</p>
<p>I am planning on providing semantics, based on GHC</p>
<h1 id="garbage-collection-roots">Garbage Collection Roots</h1>
<p>The &quot;roots&quot; are the set of pointers that the GC starts traversing from, i.e. the roots of the live object graph.</p>
<p>Most roots belong to a particular Capability. Traversing the roots of a capbility is done by `markSomeCapabilities()` in <a href="GhcFile(rts/Capability.c)" class="uri" title="wikilink">GhcFile(rts/Capability.c)</a>. The roots of a Capability are:</p>
<p><code>* The run queue (head and tail)</code><br />
<code>* The wakeup queue (head and tail)</code><br />
<code>* For each Task on the `suspended_ccalling_tasks` list, the TSO for that Task</code><br />
<code>* The Spark Pool</code><br />
<code>* Only for the non-threaded RTS: The blocked queue (head and tail), and the sleeping queue</code></p>
<p>In addition, each Capability has a [wiki:Commentary/Rts/Storage/GC/RememberedSets remembered set] for each generation. A remembered set is a source of roots if that generation is <em>not</em> being collected during this cycle; otherwise the remembered set is discarded. During GC, all remembered sets are discarded and new ones will be constructed for each generation and Capability; see `scavenge_capability_mut_lists()` in <a href="GhcFile(rts/sm/Scav.c)" class="uri" title="wikilink">GhcFile(rts/sm/Scav.c)</a>.</p>
<p>There are also roots from other parts of the system:</p>
<p><code>* Signal handlers (only in the non-threaded RTS; in the threaded RTS signal handlers are maintained by the IO manager in `GHC.Conc` rather than the RTS).</code><br />
<code>* [wiki:Commentary/Rts/Storage/GC/Weak Weak pointers]</code><br />
<code>* [wiki:Commentary/Rts/Stable Stable pointers]</code></p>
<p><a href="PageOutline" class="uri" title="wikilink">PageOutline</a></p>
<h1 id="ghc-source-tree-roadmap-rts">GHC Source Tree Roadmap: rts/</h1>
<p>This directory contains the source code for the runtime system.</p>
<p>There are three types of files:</p>
<p><code>::</code><br />
<code>  Header files that are </code><em><code>private</code> <code>to</code> <code>the</code> <code>RTS</code></em><code>.  That is, header files in this directory are</code><br />
<code>  not shipped with GHC, and APIs they define are therefore intended to be private and not</code><br />
<code>  usable by client code (in practice, we do not and probably cannot enforce this).  Header</code><br />
<code>  files that we </code><em><code>do</code></em><code> ship with GHC are in the [wiki:Commentary/SourceTree/Includes includes]</code><br />
<code>  directory.</code></p>
<p><code>::</code><br />
<code>  C source code for the runtime system.  Conventions used in this code are described in</code><br />
<code>  [wiki:Commentary/Rts/Conventions].</code></p>
<p><code>::</code><br />
<code>  C-- code for parts of the runtime that are part of the Haskell execution environment: for</code><br />
<code>  example, the implementation of primitives, exceptions, and so on.  A </code><code> file is</code><br />
<code>  pseudo C--: more or less C-- syntax with some omissions and some additional macro-like</code><br />
<code>  extensions implemented by GHC.  The </code><code> files are compiled using GHC itself: see</code><br />
<code>  [wiki:Commentary/Rts/Cmm].</code></p>
<h3 id="subdirectories-of-rts">Subdirectories of rts/</h3>
<p><code>::</code><br />
<code>::</code><br />
<code>  POSIX and Win32-specific parts of the runtime respectively.  We try to put platform-specific stuff in these directories,</code><br />
<code>  however not all of the RTS follows this convention right now.</code></p>
<p><code>::</code><br />
<code>  Hooks for changing the RTS behaviour from client code, eg. changing the default heap size.</code><br />
<code>  (see </code><a href="https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/runtime-control.html#rts-hooks"><code>User's</code> <code>Guide</code> <code>for</code> <code>more</code> <code>about</code> <code>hooks</code></a><code>).</code></p>
<p><code>::</code><br />
<code>  The [wiki:Commentary/Rts/Storage Storage Manager].</code></p>
<h3 id="haskell-execution">Haskell Execution</h3>
<p>All this code runs on the Haskell side of the Haskell/C divide;  is the interface between the two layers.</p>
<p><a href="http://darcs.haskell.org/ghc/rts/Apply.cmm"><code>Apply.cmm</code></a><code>, </code><a href="http://darcs.haskell.org/ghc/rts/AutoApply.h"><code>AutoApply.h</code></a><code>, </code><code>, </code><a href="http://darcs.haskell.org/ghc/rts/Apply.h"><code>Apply.h</code></a><code>::</code><br />
<code> The eval/apply machinery.  Note: </code><code> is the family</code><br />
<code> of functions for performing generic application of unknown</code><br />
<code> functions, this code depends on the number of registers available</code><br />
<code> for argument passing, so it is generated automatically by the program</code><br />
<code> </code><code> in </code><code>.</code></p>
<p><a href="http://darcs.haskell.org/ghc/rts/Exception.cmm"><code>Exception.cmm</code></a><code>::</code><br />
<code> Support for execptions.</code></p>
<p><a href="http://darcs.haskell.org/ghc/rts/HeapStackCheck.cmm"><code>HeapStackCheck.cmm</code></a><code>::</code><br />
<code> Code for preparing the stack when the current Haskell thread needs</code><br />
<code> to return to the RTS, because we either ran out of heap or stack, or</code><br />
<code> need to block (eg. </code><code>), or yield.</code></p>
<p><a href="http://darcs.haskell.org/ghc/rts/PrimOps.cmm"><code>PrimOps.cmm</code></a><code>::</code><br />
<code> Implementation of out-of-line primitives (see [wiki:Commentary/PrimOps]).</code></p>
<p><a href="http://darcs.haskell.org/ghc/rts/StgMiscClosures.cmm"><code>StgMiscClosures.cmm</code></a><code>::</code><br />
<code> Some built-in closures, such as the family of small </code><code>s and</code><br />
<code> </code><code>, and some built-in info tables such as </code><br />
<code> and </code><code>.</code></p>
<p><a href="http://darcs.haskell.org/ghc/rts/StgStartup.cmm"><code>StgStartup.cmm</code></a><code>::</code><br />
<code> Code that executes when a Haskell thread begins and ends.</code></p>
<p><a href="http://darcs.haskell.org/ghc/rts/StgStdThunks.cmm"><code>StgStdThunks.cmm</code></a><code>::</code><br />
<code> Some built-in thunks: [wiki:Commentary/Rts/Storage/HeapObjects#Selectorthunks selector thunks] and &quot;apply&quot; thunks.</code></p>
<p><a href="http://darcs.haskell.org/ghc/rts/Updates.cmm"><code>Updates.cmm</code></a><code>, </code><a href="http://darcs.haskell.org/ghc/rts/Updates.h"><code>Updates.h</code></a><code>::</code><br />
<code> [wiki:Commentary Updates].</code></p>
<p><a href="http://darcs.haskell.org/ghc/rts/HCIncludes.h"><code>HCIncludes.h</code></a><code>::</code><br />
<code> Header file included when compiling </code><code> files via C.</code></p>
<p><a href="http://darcs.haskell.org/ghc/rts/StgCRun.c"><code>StgCRun.c</code></a><code>, </code><a href="http://darcs.haskell.org/ghc/rts/StgRun.h"><code>StgRun.h</code></a><code>::</code><br />
<code> The interface between the C execution layer and the Haskell</code><br />
<code> execution layer.</code></p>
<p><a href="http://darcs.haskell.org/ghc/rts/StgPrimFloat.c"><code>StgPrimFloat.c</code></a><code>::</code><br />
<code> Floating-point stuff.</code></p>
<p><a href="http://darcs.haskell.org/ghc/rts/STM.c"><code>STM.c</code></a><code>::</code><br />
<code> Implementation of Software Transactional Memory.</code></p>
<h3 id="the-wikicommentaryrtsstorage-storage-manager">The [wiki:Commentary/Rts/Storage Storage Manager]</h3>
<p><a href="http://darcs.haskell.org/ghc/rts/sm/Storage.c"><code>sm/Storage.c</code></a><code>::</code><br />
<code> Top-level of the storage manager.</code></p>
<p><a href="http://darcs.haskell.org/ghc/rts/sm/MBlock.c"><code>sm/MBlock.c</code></a><code>, </code><a href="http://darcs.haskell.org/ghc/rts/sm/MBlock.h"><code>sm/MBlock.h</code></a><code>, </code><a href="http://darcs.haskell.org/ghc/rts/sm/OSMem.h"><code>sm/OSMem.h</code></a><code>::</code><br />
<code> The &quot;megablock&quot; allocator; this is the thin layer between the RTS and</code><br />
<code> the operating system for allocating memory.</code></p>
<p><a href="http://darcs.haskell.org/ghc/rts/sm/BlockAlloc.c"><code>sm/BlockAlloc.c</code></a><code>, </code><a href="http://darcs.haskell.org/ghc/rts/sm/BlockAlloc.h"><code>sm/BlockAlloc.h</code></a><code>::</code><br />
<code> The low-level block allocator, requires only </code><code>.</code></p>
<p><a href="http://darcs.haskell.org/ghc/rts/sm/GC.c"><code>sm/GC.c</code></a><code>, </code><a href="http://darcs.haskell.org/ghc/rts/sm/Scav.c"><code>sm/Scav.c</code></a><code>, </code><a href="http://darcs.haskell.org/ghc/rts/sm/Evac.c"><code>sm/Evac.c</code></a><code>, </code><a href="http://darcs.haskell.org/ghc/rts/sm/GCUtils.c"><code>sm/GCUtils.c</code></a><code>, </code><a href="http://darcs.haskell.org/ghc/rts/sm/MarkWeak.c"><code>sm/MarkWeak.c</code></a><code>::</code><br />
<code> The generational copying garbage collector.</code></p>
<p><a href="http://darcs.haskell.org/ghc/rts/sm/Compact.c"><code>sm/Compact.c</code></a><code>, </code><a href="http://darcs.haskell.org/ghc/rts/sm/Compact.h"><code>sm/Compact.h</code></a><code>::</code><br />
<code> The compacting garbage collector.</code></p>
<p><a href="http://darcs.haskell.org/ghc/rts/ClosureFlags.c"><code>ClosureFlags.c</code></a><code>::</code><br />
<code> Determining properties of various types of closures.</code></p>
<p><a href="http://darcs.haskell.org/ghc/rts/Sanity.c"><code>Sanity.c</code></a><code>, </code><a href="http://darcs.haskell.org/ghc/rts/Sanity.h"><code>Sanity.h</code></a><code>::</code><br />
<code> A sanity-checker for the heap and related data structures.</code></p>
<p><a href="http://darcs.haskell.org/ghc/rts/Stats.c"><code>Stats.c</code></a><code>, </code><a href="http://darcs.haskell.org/ghc/rts/Stats.h"><code>Stats.h</code></a><code>::</code><br />
<code> Statistics for the garbage collector and storage manager.</code></p>
<p><a href="http://darcs.haskell.org/ghc/rts/Stable.c"><code>Stable.c</code></a><code>::</code><br />
<code> Stable names and stable pointers.</code></p>
<p><a href="http://darcs.haskell.org/ghc/rts/Weak.c"><code>Weak.c</code></a><code>, </code><a href="http://darcs.haskell.org/ghc/rts/Weak.h"><code>Weak.h</code></a><code>::</code><br />
<code> Weak pointers.</code></p>
<h3 id="data-structures">Data Structures</h3>
<p>Data structure abstractions for use in the RTS:</p>
<p><a href="http://darcs.haskell.org/ghc/rts/Arena.c"><code>Arena.c</code></a><code>, </code><a href="http://darcs.haskell.org/ghc/rts/Arena.h"><code>Arena.h</code></a><code>::</code><br />
<code> An arena allocator</code></p>
<p><a href="http://darcs.haskell.org/ghc/rts/Hash.c"><code>Hash.c</code></a><code>, </code><a href="http://darcs.haskell.org/ghc/rts/Hash.h"><code>Hash.h</code></a><code>::</code><br />
<code> A generic hash table implementation.</code></p>
<h3 id="the-wikicommentaryrtsscheduler-scheduler">The [wiki:Commentary/Rts/Scheduler Scheduler]</h3>
<p><a href="http://darcs.haskell.org/ghc/rts/Capability.c"><code>Capability.c</code></a><code>, </code><a href="http://darcs.haskell.org/ghc/rts/Capability.h"><code>Capability.h</code></a><code>::</code><br />
<code> Capabilities: virtual CPUs for executing Haskell code.</code></p>
<p><a href="http://darcs.haskell.org/ghc/rts/RaiseAsync.c"><code>RaiseAsync.c</code></a><code>, </code><a href="http://darcs.haskell.org/ghc/rts/RaiseAsync.h"><code>RaiseAsync.h</code></a><code>::</code><br />
<code> Asynchronous exceptions.</code></p>
<p><a href="http://darcs.haskell.org/ghc/rts/Schedule.c"><code>Schedule.c</code></a><code>, </code><a href="http://darcs.haskell.org/ghc/rts/Schedule.h"><code>Schedule.h</code></a><code>::</code><br />
<code> The scheduler itself.</code></p>
<p><a href="http://darcs.haskell.org/ghc/rts/Sparks.c"><code>Sparks.c</code></a><code>, </code><a href="http://darcs.haskell.org/ghc/rts/Sparks.h"><code>Sparks.h</code></a><code>::</code><br />
<code> Sparks: the implementation of </code><code>.</code></p>
<p><a href="http://darcs.haskell.org/ghc/rts/ThreadLabels.c"><code>ThreadLabels.c</code></a><code>, </code><a href="http://darcs.haskell.org/ghc/rts/ThreadLabels.h"><code>ThreadLabels.h</code></a><code>::</code><br />
<code> Labelling threads.</code></p>
<p><a href="http://darcs.haskell.org/ghc/rts/Threads.c"><code>Threads.c</code></a><code>, </code><a href="http://darcs.haskell.org/ghc/rts/Threads.h"><code>Threads.h</code></a><code>::</code><br />
<code> Various thread-related functionality.</code></p>
<p><a href="http://darcs.haskell.org/ghc/rts/ThreadPaused.c"><code>ThreadPaused.c</code></a><code>::</code><br />
<code> Suspending a thread before it returns to the RTS.</code></p>
<p><a href="http://darcs.haskell.org/ghc/rts/Task.c"><code>Task.c</code></a><code>, </code><a href="http://darcs.haskell.org/ghc/rts/Task.h"><code>Task.h</code></a><code>::</code><br />
<code> Task: an OS-thread abstraction.</code></p>
<p><a href="http://darcs.haskell.org/ghc/rts/AwaitEvent.h"><code>AwaitEvent.h</code></a><code>::</code><br />
<code> Waiting for events (non-threaded RTS only).</code></p>
<p><a href="http://darcs.haskell.org/ghc/rts/Timer.c"><code>Timer.c</code></a><code>, </code><a href="http://darcs.haskell.org/ghc/rts/Timer.h"><code>Timer.h</code></a><code>,  </code><a href="http://darcs.haskell.org/ghc/rts/Ticker.h"><code>Ticker.h</code></a><code>::</code><br />
<code> The runtime's interval timer, used for context switching and profiling.</code></p>
<h3 id="c-files-the-wikicommentaryrtsffi-ffi">C files: the [wiki:Commentary/Rts/FFI FFI]</h3>
<p><a href="http://darcs.haskell.org/ghc/rts/Adjustor.c"><code>Adjustor.c</code></a><code>::</code><br />
<code> Very hairy support for </code><code>.</code></p>
<p><a href="http://darcs.haskell.org/ghc/rts/HsFFI.c"><code>HsFFI.c</code></a><code>, </code><a href="http://darcs.haskell.org/ghc/rts/RtsAPI.c"><code>RtsAPI.c</code></a><code>::</code><br />
<code> Implementation of the Haskell FFI C interface: </code><code>,</code><br />
<code> </code><code>, etc.</code><br />
<code> </code></p>
<h3 id="the-wikicommentaryrtsinterpreter-byte-code-interpreter">The [wiki:Commentary/Rts/Interpreter Byte-code Interpreter]</h3>
<p><a href="http://darcs.haskell.org/ghc/rts/Disassembler.c"><code>Disassembler.c</code></a><code>, </code><a href="http://darcs.haskell.org/ghc/rts/Disassembler.h"><code>Disassembler.h</code></a><code>::</code><br />
<a href="http://darcs.haskell.org/ghc/rts/Interpreter.c"><code>Interpreter.c</code></a><code>, </code><a href="http://darcs.haskell.org/ghc/rts/Interpreter.h"><code>Interpreter.h</code></a><code>::</code><br />
<code> The [wiki:Commentary/Rts/Interpreter byte-code interpreter] and disassembler.</code></p>
<p><a href="http://darcs.haskell.org/ghc/rts/Linker.c"><code>Linker.c</code></a><code>::</code><br />
<a href="http://darcs.haskell.org/ghc/rts/LinkerInternals.h"><code>LinkerInternals.h</code></a><br />
<code> The [wiki:Commentary/Rts/Linker dynamic object-code linker].</code></p>
<h3 id="wikicommentaryprofiling-profiling">[wiki:Commentary/Profiling Profiling]</h3>
<p><a href="http://darcs.haskell.org/ghc/rts/LdvProfile.c"><code>LdvProfile.c</code></a><code>, </code><a href="http://darcs.haskell.org/ghc/rts/LdvProfile.h"><code>LdvProfile.h</code></a><code>::</code><br />
<code> Lag-drag-void profiling (also known as Biographical Profiling).</code></p>
<p><a href="http://darcs.haskell.org/ghc/rts/ProfHeap.c"><code>ProfHeap.c</code></a><code>, </code><a href="http://darcs.haskell.org/ghc/rts/ProfHeap.h"><code>ProfHeap.h</code></a><code>::</code><br />
<code> Generic heap-profilng support.</code></p>
<p><a href="http://darcs.haskell.org/ghc/rts/Profiling.c"><code>Profiling.c</code></a><code>, </code><a href="http://darcs.haskell.org/ghc/rts/Profiling.h"><code>Profiling.h</code></a><code>::</code><br />
<code> Generic profilng support.</code></p>
<p><a href="http://darcs.haskell.org/ghc/rts/Proftimer.c"><code>Proftimer.c</code></a><code>, </code><a href="http://darcs.haskell.org/ghc/rts/Proftimer.h"><code>Proftimer.h</code></a><code>::</code><br />
<code> The profiling timer.</code></p>
<p><a href="http://darcs.haskell.org/ghc/rts/RetainerProfile.c"><code>RetainerProfile.c</code></a><code>, </code><a href="http://darcs.haskell.org/ghc/rts/RetainerProfile.h"><code>RetainerProfile.h</code></a><code>::</code><br />
<a href="http://darcs.haskell.org/ghc/rts/RetainerSet.c"><code>RetainerSet.c</code></a><code>, </code><a href="http://darcs.haskell.org/ghc/rts/RetainerSet.h"><code>RetainerSet.h</code></a><code>::</code><br />
<code> Retainer profiling.</code></p>
<p><a href="http://darcs.haskell.org/ghc/rts/Ticky.c"><code>Ticky.c</code></a><code>, </code><a href="http://darcs.haskell.org/ghc/rts/Ticky.h"><code>Ticky.h</code></a><code>::</code><br />
<code> Ticky-ticky profiling (currently defunct; needs reviving).</code></p>
<h3 id="rts-debugging">RTS Debugging</h3>
<p><a href="http://darcs.haskell.org/ghc/rts/Printer.c"><code>Printer.c</code></a><code>, </code><a href="http://darcs.haskell.org/ghc/rts/Printer.h"><code>Printer.h</code></a><code>::</code><br />
<code> Generic printing for heap objects and stacks (not used much).</code></p>
<p><a href="http://darcs.haskell.org/ghc/rts/Trace.c"><code>Trace.c</code></a><code>, </code><a href="http://darcs.haskell.org/ghc/rts/Trace.h"><code>Trace.h</code></a><code>::</code><br />
<code> Generic support for various kinds of trace and debugging messages.  </code></p>
<h3 id="the-front-panel">The Front Panel</h3>
<p>The front panel is currently defunct. It offers a graphical view of the running Haskell program in real time, and was pretty cool when it worked.</p>
<p><a href="http://darcs.haskell.org/ghc/rts/FrontPanel.c"><code>FrontPanel.c</code></a><code>, </code><a href="http://darcs.haskell.org/ghc/rts/FrontPanel.h"><code>FrontPanel.h</code></a><code>::</code><br />
<a href="http://darcs.haskell.org/ghc/rts/VisCallbacks.c"><code>VisCallbacks.c</code></a><code>, </code><a href="http://darcs.haskell.org/ghc/rts/VisCallbacks.h"><code>VisCallbacks.h</code></a><code>::</code><br />
<a href="http://darcs.haskell.org/ghc/rts/VisSupport.c"><code>VisSupport.c</code></a><code>, </code><a href="http://darcs.haskell.org/ghc/rts/VisSupport.h"><code>VisSupport.h</code></a><code>::</code><br />
<a href="http://darcs.haskell.org/ghc/rts/VisWindow.c"><code>VisWindow.c</code></a><code>, </code><a href="http://darcs.haskell.org/ghc/rts/VisWindow.h"><code>VisWindow.h</code></a><code>::</code></p>
<h3 id="other">Other</h3>
<p><a href="http://darcs.haskell.org/ghc/rts/Main.c"><code>Main.c</code></a><code>::</code><br />
<code> The C </code><code> function for a standalone Haskell program;</code><br />
<code> basically this is just a client of </code><code>.</code></p>
<p><a href="http://darcs.haskell.org/ghc/rts/RtsFlags.c"><code>RtsFlags.c</code></a><code>::</code><br />
<code> Understands the </code><code> flags.</code></p>
<p><a href="http://darcs.haskell.org/ghc/rts/RtsMessages.c"><code>RtsMessages.c</code></a><code>::</code><br />
<code> Support for emitting messages from the runtime.</code></p>
<p><a href="http://darcs.haskell.org/ghc/rts/RtsSignals.c"><code>RtsSignals.c</code></a><code>, </code><a href="http://darcs.haskell.org/ghc/rts/RtsSignals.h"><code>RtsSignals.h</code></a><code>::</code><br />
<code> Signal-related stuff.</code></p>
<p>Miscellaneous stuff:</p>
<p><a href="http://darcs.haskell.org/ghc/rts/RtsUtils.c"><code>RtsUtils.c</code></a><code>, </code><a href="http://darcs.haskell.org/ghc/rts/RtsUtils.h"><code>RtsUtils.h</code></a><code>::</code><br />
<a href="http://darcs.haskell.org/ghc/rts/GetTime.h"><code>GetTime.h</code></a><code>::</code><br />
<a href="http://darcs.haskell.org/ghc/rts/PosixSource.h"><code>PosixSource.h</code></a><code>::</code><br />
<a href="http://darcs.haskell.org/ghc/rts/Prelude.h"><code>Prelude.h</code></a><code>::</code><br />
<a href="http://darcs.haskell.org/ghc/rts/Typeable.c"><code>Typeable.c</code></a><code>::</code><br />
<a href="http://darcs.haskell.org/ghc/rts/RtsDllMain.c"><code>RtsDllMain.c</code></a><code>::</code></p>
<h3 id="old-stuff">OLD stuff</h3>
<p><code>::</code><br />
<code> Code for GUM: parallel GHC.  This is heavily bitrotted and currently doesn't work (as of GHC 6.6; it last worked around</code><br />
<code> 5.02 I believe).</code></p>
<p><code>::</code><br />
<code> Bitrotted code for GHC.NET.</code></p>
<h1 id="sanity-checking">Sanity Checking</h1>
<p>Source code: <a href="GhcFile(rts/Sanity.c)" class="uri" title="wikilink">GhcFile(rts/Sanity.c)</a>, <a href="GhcFile(rts/Sanity.h)" class="uri" title="wikilink">GhcFile(rts/Sanity.h)</a>.</p>
<p>The purpose of sanity checking is to catch bugs in the RTS as early as possible; if the program is going to crash, we want it to crash as soon as possible after the error occurred. The problem with debugging the RTS is that heap corruption can go unnoticed through several GC cycles, making it particularly difficult to trace back to the erroneous code.</p>
<p>Sanity checking is turned on by the `+RTS -DS` option. We treat it like an expensive assertion: normal assertions are allowed to take a few extra percent of run time, so we don't mind having them on all the time in a `DEBUG` RTS, but sanity checking may double the run time of the program or worse. So the rule of thumb is that expensive assertions go into sanity checking, cheap assertions are on in `DEBUG`, or possibly even on all the time.</p>
<p>Sanity checking does a complete traversal of the heap after each GC to look for dangling pointers (see `checkHeap` in <a href="GhcFile(rts/Sanity.c)" class="uri" title="wikilink">GhcFile(rts/Sanity.c)</a>). For this it needs to ensure that there is no [wiki:Commentary/Rts/Storage/Slop slop], which is why we can only do this in a `DEBUG` runtime: the slop-avoiding machinery is only on with `DEBUG`.</p>
<p>Sanity checking also turns on some other expensive checks: for example in the [wiki:Commentary/Rts/HaskellExecution#Genericapply generic apply] code we check that the arguments point to valid closures.</p>
<p><a href="PageOutline" class="uri" title="wikilink">PageOutline</a></p>
<h1 id="the-scheduler">The Scheduler</h1>
<p>The scheduler is the heart of the runtime: it is the single part of the system through which all entry to the Haskell world goes, and it handles requests from outside to invoke Haskell functions (foreign export).</p>
<p>In this part of the commentary we'll discuss the <em>threaded</em> version of the runtime (see [wiki:Commentary/Rts/Config]), that is, the version of the runtime that uses multiple OS threads, because it is by far the most complex beast.</p>
<p>See also <a href="http://blog.ezyang.com/2013/01/the-ghc-scheduler/">Edward Yang's blog post</a> (2013); some of the material there has been incorporated here.</p>
<p>We begin by discussing the basic abstractions used in the scheduler.</p>
<h2 id="os-threads">OS Threads</h2>
<p>Source files: <a href="GhcFile(includes/rts/OSThreads.h)" class="uri" title="wikilink">GhcFile(includes/rts/OSThreads.h)</a>, <a href="GhcFile(rts/win32/OSThreads.c)" class="uri" title="wikilink">GhcFile(rts/win32/OSThreads.c)</a>, <a href="GhcFile(rts/posix/OSThreads.c)" class="uri" title="wikilink">GhcFile(rts/posix/OSThreads.c)</a></p>
<p>We assume that the OS provides some kind of native threads, and for SMP parallelism we assume that the OS will schedule multiple OS threads across the available CPUs.</p>
<p>OS threads are only used by the runtime for two reasons:</p>
<p><code>* To support non-blocking foreign calls: a foreign call</code><br />
<code>  should not block the other Haskell threads in the system from</code><br />
<code>  running, and using OS threads is the only way to ensure that.</code></p>
<p><code>* To support SMP parallelism.</code></p>
<p>Haskell threads are much lighter-weight (at least 100x) than OS threads.</p>
<p>When running on an SMP, we begin by creating the number of OS threads specified by the `+RTS -N` option, although during the course of running the program more OS threads might be created in order to continue running Haskell code while foreign calls execute. Spare OS threads are kept in a pool attached to each `Capability` (see [#Capabilities]).</p>
<p>The RTS provides a platform-independent abstraction layer for OS threads in <a href="GhcFile(includes/rts/OSThreads.h)" class="uri" title="wikilink">GhcFile(includes/rts/OSThreads.h)</a>.</p>
<h2 id="haskell-threads">Haskell threads</h2>
<p>A Haskell thread is represented by a Thread State Object ([wiki:Commentary/Rts/Storage/HeapObjects#ThreadStateObjects TSO]). These objects are <em>garbage-collected</em>, like other closures in Haskell. The TSO, along with the stack allocated with it (STACK), constitute the primary memory overhead of a thread. Default stack size, in particular, is controlled by the GC flag , and is 1k by default (Actually, your usable stack will be a little smaller than that because this size also includes the size of the  struct, so that a lot of allocated threads will fit nicely into a single block.) There are two kinds of Haskell thread:</p>
<p><code>* A </code><em><code>bound</code></em><code> thread is created as the result of a </code><em><code>call-in</code></em><code> from</code><br />
<code>  outside Haskell; that is, a call to </code><code> or</code><br />
<code>  </code><code>.  A bound thread is tied to the</code><br />
<code>  OS thread that made the call; all further foreign calls made by</code><br />
<code>  this Haskell thread are made in the same OS thread.  (this is part</code><br />
<code>  of the design of the FFI, described in the paper </code><br />
<code>  </code><a href="http://www.haskell.org/~simonmar/papers/conc-ffi.pdf"><code>Extending</code> <code>the</code> <code>Haskell</code> <code>Foreign</code> <code>Function</code> <code>Inteface</code> <code>with</code> <code>Concurrency</code></a><code>).</code></p>
<p><code>* An </code><em><code>unbound</code></em><code> thread is created by</code><br />
<code>  </code><code>.  Foreign calls made by an unbound</code><br />
<code>  thread are made by an arbitrary OS thread.</code></p>
<p>Initialization of TSOs is handled in  in <a href="GhcFile(rts/Threads.c)" class="uri" title="wikilink">GhcFile(rts/Threads.c)</a>; this function is in turn invoked by ,  and  in <a href="GhcFile(rts/RtsAPI.c)" class="uri" title="wikilink">GhcFile(rts/RtsAPI.c)</a>. These functions setup the initial stack state, which controls what the thread executes when it actually gets run. These functions are the ones invoked by the  and other primops (recall entry-points for primops are located in <a href="GhcFile(rts/PrimOps.cmm)" class="uri" title="wikilink">GhcFile(rts/PrimOps.cmm)</a>).</p>
<p>Being garbage collected has two major implications for TSOs. First, TSOs are not GC roots, so they will get GC'd if there is nothing holding on to them (e.g. <a href="http://blog.ezyang.com/2011/07/blockedindefinitelyonmvar">in the case of deadlock</a>), and their space is not automatically reclaimed when they finish executing (so  can cause memory leaks}}}. Usually, a TSO will be retained by a Capability</p>
<h1 id="seq-magic">Seq magic</h1>
<p>The innocent-looking `seq` operator causes all manner of mayhem in GHC. This page summarises the issues. See also discussion in Trac #5129, #5262</p>
<h2 id="the-baseline-position">The baseline position</h2>
<p>Our initial story was that `(seq e1 e2)` meant precisely  Indeed this was `seq`'s inlining. This translation validates some important rules </p>
<p>But this approach has problems; see `Note [Deguaring seq]` in `DsUtils`.</p>
<h3 id="problem-1-trac-1031">Problem 1 (Trac #1031)</h3>
<p>Consider  The `[CoreSyn let/app invariant]` (see `CoreSyn`) means that, other things being equal, because the argument to the outer `seq` has an unlifted type, we'll use call-by-value thus:  But that is bad for two reasons:</p>
<p><code> * we now evaluate `y` before `x`, and </code><br />
<code> * we can't bind `v` to an unboxed pair</code></p>
<p>Seq is very, very special! Treating it as a two-argument function, strict in both arguments, doesn't work. We &quot;fixed&quot; this by treating `seq` as a language construct, desugared by the desugarer, rather than as a function that may (or may not) be inlined by the simplifier. So the above term is desugared to: </p>
<h3 id="problem-2-trac-2273">Problem 2 (Trac #2273)</h3>
<p>Consider  Here the `seq` is designed to plug the space leak of retaining `(snd x)` for too long.</p>
<p>If we rely on the ordinary inlining of `seq`, we'll get  But since `chp` is cheap, and the case is an alluring contet, we'll inline `chp` into the case scrutinee. Now there is only one use of `chp`, so we'll inline a second copy. Alas, we've now ruined the purpose of the seq, by re-introducing the space leak:  We can try to avoid doing this by ensuring that the binder-swap in the case happens, so we get his at an early stage:  But this is fragile. The real culprit is the source program. Perhaps we should have said explicitly  But that's painful. So the desugarer does a little hack to make `seq` more robust: a saturated application of `seq` is turned <strong>directly</strong> into the case expression, thus:  So we desugar our example to:  And now all is well.</p>
<p>Be careful not to desugar  which stupidly tries to bind the datacon 'True'. This is easily avoided.</p>
<p>The whole thing is a hack though; if you define `mySeq=seq`, the hack won't work on `mySeq`.</p>
<h3 id="problem-3-trac-5262">Problem 3 (Trac #5262)</h3>
<p>Consider  With the above desugaring we get  and now ete expansion gives  Now suppose that we have  Plainly `(length xs)` should be evaluated... but it isn't because `f` has arity 2. (Without -O this doesn't happen.)</p>
<h3 id="problem-4-seq-in-the-io-monad">Problem 4: seq in the IO monad</h3>
<p>See the extensive discussion in Trac #5129.</p>
<h3 id="problem-5-the-need-for-special-rules">Problem 5: the need for special rules</h3>
<p>Roman found situations where he had  where he knew that `f` (which was strict in `n`) would terminate if n did. Notice that the result of `(f n)` is discarded. So it makes sense to transform to  Rather than attempt some general analysis to support this, I've added enough support that you can do this using a rewrite rule:  You write that rule. When GHC sees a case expression that discards its result, it mentally transforms it to a call to `seq` and looks for a RULE. (This is done in `Simplify.rebuildCase`.) As usual, the correctness of the rule is up to you.</p>
<p>To make this work, we need to be careful that `seq` is <strong>not</strong> desguared into a case expression on the LHS of a rule.</p>
<p>To increase applicability of these user-defined rules, we also have the following built-in rule for `seq`  This eliminates unnecessary casts and also allows other seq rules to match more often. Notably,  and now a user-defined rule for `seq` may fire.</p>
<h1 id="a-better-way">A better way</h1>
<p>Here's our new plan.</p>
<p><code>* Introduce a new primop `seq# :: a -&gt; State# s -&gt; (# a, State# s #)` (see be5441799b7d94646dcd4bfea15407883537eaaa)</code><br />
<code>* Implement `seq#` by turning it into the obvious eval in the backend.  In fact, since the return convention for `(# State# s, a #)` is exactly the same as for `a`, we can implement `seq# s a` by `a` (even when it appears as a case scrutinee).</code><br />
<code>* Define `evaluate` thus</code></p>
<p></p>
<p>That fixes problem 4.</p>
<p>We could go on and desugar `seq` thus: </p>
<p>and if we consider `seq#` to be expensive, then we won't eta-expand around it, and that would fix problem 3.</p>
<p>However, there is a concern that this might lead to performance regressions in examples like this:</p>
<p></p>
<p>so `f` turns into</p>
<p></p>
<p>and we won't get to eta-expand the `\s` as we would normally do (this is pretty important for getting good performance from IO and ST monad code).</p>
<p>Arguably `f` should be rewritten with a bang pattern, and we should treat bang patterns as the eta-expandable seq and translate them directly into `case`, not `seq#`. But this would be a subtle difference between `seq` and bang patterns.</p>
<p>Furthermore, we already have `pseq`, which is supposed to be a &quot;strictly ordered seq&quot;, that is it preserves evaluation order. So perhaps `pseq` should be the one that more accurately implements the programmer's intentions, leaving `seq` as it currently is.</p>
<p>We are currently pondering what to do here.</p>
<h1 id="the-ghc-commentary-signals">The GHC Commentary: Signals</h1>
<p>This section describes how the RTS interacts with the OS signal facilities. Throughout we use the term &quot;signal&quot; to refer to both POSIX-style signals and Windows <em>ConsoleEvents</em>.</p>
<p>Signal handling differs between the <em>threaded</em> version of the runtime and the non-threaded version (see [wiki:Commentary/Rts/Config]). Here we discuss only the threaded version, since we expect that to become the standard version in due course.</p>
<p>Source files:</p>
<p><code>* POSIX signal handling:</code><br />
<code>  * </code><a href="GhcFile(rts/posix/Signals.h)" title="wikilink"><code>GhcFile(rts/posix/Signals.h)</code></a><code>, </code><a href="GhcFile(rts/posix/Signals.c)" title="wikilink"><code>GhcFile(rts/posix/Signals.c)</code></a><br />
<code>* Windows console events:</code><br />
<code>  * </code><a href="GhcFile(rts/win32/ConsoleHandler.h)" title="wikilink"><code>GhcFile(rts/win32/ConsoleHandler.h)</code></a><code>, </code><a href="GhcFile(rts/win32/ConsoleHandler.c)" title="wikilink"><code>GhcFile(rts/win32/ConsoleHandler.c)</code></a></p>
<h2 id="signal-handling-in-the-rts">Signal handling in the RTS</h2>
<p>The RTS is interested in two signals: a timer signal, and an interrupt signal.</p>
<h3 id="the-timer-signal">The timer signal</h3>
<p>The timer signal is used for several things:</p>
<p><code>* To cause the [wiki:Commentary/Rts/Scheduler scheduler] to context switch</code><br />
<code>* Sampling for [wiki:Commentary/Profiling time profiling]</code><br />
<code>* To detect deadlock (see [wiki:Commentary/Rts/Scheduler])</code></p>
<p>Source files:</p>
<p><code>* The timer interrupt handler, and starting/stopping the timer:</code><br />
<code>  * </code><a href="GhcFile(rts/Timer.h)" title="wikilink"><code>GhcFile(rts/Timer.h)</code></a><code>, </code><a href="GhcFile(rts/Timer.c)" title="wikilink"><code>GhcFile(rts/Timer.c)</code></a><br />
<code>* Platform-independent ticker interface, used by the timer:</code><br />
<code>  * </code><a href="GhcFile(rts/Ticker.h)" title="wikilink"><code>GhcFile(rts/Ticker.h)</code></a><br />
<code>* Posix implementation of ticker:</code><br />
<code>  * </code><a href="GhcFile(rts/posix/Itimer.h)" title="wikilink"><code>GhcFile(rts/posix/Itimer.h)</code></a><code>, </code><a href="GhcFile(rts/posix/Itimer.h)" title="wikilink"><code>GhcFile(rts/posix/Itimer.h)</code></a><br />
<code>* Windows implementation of ticker:</code><br />
<code>  * </code><a href="GhcFile(rts/win32/Ticker.c)" title="wikilink"><code>GhcFile(rts/win32/Ticker.c)</code></a></p>
<p>On Posix, the timer signal is implemented by calling `timer_create()` to generate regular `SIGVTALRM` signals (this was changed from SIGALRM in #850).</p>
<p>On Windows, we spawn a new thread that repeatedly sleeps for the timer interval and then executes the timer interrupt handler.</p>
<h2 id="the-interrupt-signal">The interrupt signal</h2>
<p>The interrupt signal is `SIGINT` on POSIX systems or `CTRL_C_EVENT/CTRL_BREAK_EVENT`on Windows, and is normally sent to the process when the user hits Control-C. By default, interrupts are handled by the runtime. They can be caught and handled by Haskell code instead, using `System.Posix.Signals` on POSIX systems or `GHC.ConsoleHandler` on Windows systems. For example, [wiki:Commentary/Compiler/Backends/GHCi GHCi] hooks the interrupt signal so that it can abort the current interpreted computation and return to the prompt, rather than terminating the whole GHCi process.</p>
<p>When the interrupt signal is received, the default behaviour of the runtime is to attempt to shut down the Haskell program gracefully. It does this by calling `interruptStgRts()` in <a href="GhcFile(rts/Schedule.c)" class="uri" title="wikilink">GhcFile(rts/Schedule.c)</a> (see [wiki:Commentary/Rts/Scheduler#ShuttingDown]). If a second interrupt signal is received, then we terminate the process immediately; this is just in case the normal shutdown procedure failed or hung for some reason, the user is always able to stop the process with two control-C keystrokes.</p>
<h2 id="signal-handling-in-haskell-code">Signal handling in Haskell code</h2>
<p>Source files:</p>
<p><code> * POSIX: </code><a href="GhcFile(rts/posix/Signals.h)" title="wikilink"><code>GhcFile(rts/posix/Signals.h)</code></a><code>, </code><a href="GhcFile(rts/posix/Signals.c)" title="wikilink"><code>GhcFile(rts/posix/Signals.c)</code></a><br />
<code> * Windows: </code><a href="GhcFile(rts/win32/ConsoleHandler.h)" title="wikilink"><code>GhcFile(rts/win32/ConsoleHandler.h)</code></a><code>, </code><a href="GhcFile(rts/win32/ConsoleHandler.c)" title="wikilink"><code>GhcFile(rts/win32/ConsoleHandler.c)</code></a></p>
<p>A Haskell program can ask to install signal handlers, via the `System.Posix.Signals` API, or `GHC.ConsoleHandler` on Windows. When a signal arrives that has a Haskell handler, it is the job of the runtime to create a new Haskell thread to run the signal handler and place the new thread on the run queue of a suitable [wiki:Commentary/Rts/Scheduler#Capabilities Capability].</p>
<p>When the runtime is idle, the OS threads will all be waiting inside `yieldCapability()`, waiting for some work to arrive. We want a signal to be able to create a new Haskell thread and wake up one of these OS threads to run it, but unfortunately the range of operations that can be performed inside a POSIX signal handler is extremely limited, and doesn't include any inter-thread synchronisation (because the signal handler might be running on the same stack as the OS thread it is communicating with).</p>
<p>The solution we use, on both Windows and POSIX systems, is to pass all signals that arrive to the [wiki:Commentary/Rts/IOManager IO Manager] thread. On POSIX this works by sending the signal number down a pipe, on Windows it works by storing the signal number in a buffer and signaling the IO Manager's `Event` object to wake it up. The IO Manager thread then wakes up and creates a new thread for the signal handler, before going back to sleep again.</p>
<h2 id="rts-alarm-signals-and-foreign-libraries">RTS Alarm Signals and Foreign Libraries</h2>
<p>When using foreign libraries through the Haskell FFI, it is important to ensure that the foreign code is capable of dealing with system call interrupts due to alarm signals GHC is generating.</p>
<p>For example, in this `strace` output a `select` call is interrupted, but the foreign C code interprets the interrupt as an application error and closes a critical file descriptor:</p>
<p></p>
<p>Once the C code was modified to deal with the interrupt properly, it proceeded correctly (note that foreign call is restarted 3 times before it succeeds).</p>
<p></p>
<h1 id="slop">Slop</h1>
<p>Slop is unused memory between objects in the heap.</p>
<p>|| Object1 || ... Slop ... || Object2 ||</p>
<h2 id="why-do-we-want-to-avoid-slop">Why do we want to avoid slop?</h2>
<p>Slop makes it difficult to traverse an area of memory linearly, visiting all the objects, because we can't tell where `Object2` starts in the above diagram. We need to do linear traversals for two reasons, currently:</p>
<p><code>* [wiki:Commentary/Profiling/Heap Heap profiling] needs to perform a census on the whole heap.</code><br />
<code>* [wiki:Commentary/Rts/Sanity Sanity checking] needs to ensure that all the pointers in the heap</code><br />
<code>  point to valid objects.</code></p>
<p>Additionally, linear traversals are useful for the mark phase of the [wiki:Commentary/Rts/Storage compacting garbage collector], and would be useful if we were to allow objects to be pinned arbitrarily (currently pinned objects cannot contain pointers, which means they don't need to be scavenged by the GC).</p>
<h2 id="how-does-slop-arise">How does slop arise?</h2>
<p>Slop can arise for two reasons:</p>
<p><code>* The compiled code allocates too much memory, and only fills part of it with objects.  For example,</code><br />
<code>  when compiling code for a function like this:</code></p>
<p></p>
<p><code>  the code generator takes the maximum of the heap requirements of e1 and e2 and aggregates it into</code><br />
<code>  the heap check at the beginning of the function `f` (to avoid doing too many heap checks).  </code><br />
<code>  Unfortunately that means either `e1` or `e2` has too much heap allocated to it, leaving some slop.</code><br />
<code>  We solve this problem by moving the heap pointer </code><em><code>backwards</code></em><code> before making a tail-call if</code><br />
<code>  there is any heap slop.</code></p>
<p><code>* When an object is overwritten with a smaller object.  This happens in two ways:</code><br />
<code>  [wiki:Commentary/Rts/HaskellExecution/Updates Updates] and [wiki:Commentary/Rts/Storage/HeapObjects#Blackholes Black Holes].</code></p>
<h2 id="what-do-we-do-about-it">What do we do about it?</h2>
<p>We avoid the problem for [wiki:Commentary/Profiling/Heap heap profiling] by arranging that we only ever do a census on a newly garbage-collected heap, which has no slop in it (the garbage collector never leaves slop between objects in the heap).</p>
<p>Slop does arise due to updates and black holes during normal execution, and GHC does not attempt to avoid it (because avoiding or filling slop during an update is costly). However, if we're doing [wiki:Commentary/Rts/Sanity sanity checking], then we need to arrange that slop is clearly marked: so in a `DEBUG` version of the RTS (see [wiki:Commentary/Rts/Config RTS configurations]) the update code and the blackhole code both arrange to fill slop with zeros: see the `FILL_SLOP` macro in <a href="GhcFile(rts/Updates.h)" class="uri" title="wikilink">GhcFile(rts/Updates.h)</a>. Hence sanity checking only works with a `DEBUG` version of the RTS.</p>
<p><a href="PageOutline" class="uri" title="wikilink">PageOutline</a></p>
<h1 id="layout-of-important-files-and-directories">Layout of important files and directories</h1>
<p>This page summarises the overall file and directory structure of GHC. We include both source files and generated files; the latter are always identified &quot;build-tree only&quot;.</p>
<p>Everything starts with the main GHC repository (see [wiki:Building/GettingTheSources]). The build system calls that directory `$(TOP)`. All the paths below are relative to `$(TOP)`.</p>
<h2 id="files-in-top">Files in `$(TOP)`</h2>
<p><strong><code>`packages`</code></strong><code>::</code><br />
<code> Despite the name &quot;package&quot;, this file contains the master list of the *repositories* that make up GHC. It is parsed by `./boot`.</code></p>
<p><strong><code>`tarballs`</code></strong><code>::</code><br />
<code> Lists the various tarballs (binary packages) that ghc relies on and where to unpack them during a build.</code></p>
<p><strong><code>`validate`</code></strong><code>:: Run `validate` (a shell script) before committing (see [wiki:TestingPatches]). The script is documented in the file itself.</code></p>
<p><strong><code>Documentation</code> <code>files</code></strong><code>:: `README`, `ANNOUNCE`, `HACKING`, `LICENSE`, `new_tc_notes`</code></p>
<p><strong><code>GNU</code> <code>autoconf</code> <code>machinery</code></strong><code>:: `aclocal.m4`, `config.guess`, `config.sub`, `configure.ac`, `install-sh`, `config.mk.in`, `settings.in`</code></p>
<p><strong><code>`Makefile`</code></strong><code>:: The top-level </code><code>: see [wiki:Building/Architecture GHC Build System Architecture]. GHC requires</code><br />
<code> </code><a href="http://www.gnu.org/software/make/"><code>GNU</code> <code>make</code></a><code>.</code></p>
<p><strong><code>Make</code> <code>system</code> <code>files</code></strong><code>:: `ghc.mk`, `MAKEHELP`, `SUBMAKEHELP`</code></p>
<h2 id="libraries">`libraries/`</h2>
<p>The `libraries/` directory contains all the packages that GHC needs to build. It has one sub-directory for each package repository (e.g. `base`, `haskell98`, `random`). Usually each such repository builds just one package, but there is more than one in `dph`.</p>
<p>GHC's libraries are described in more detail on the [wiki:Commentary/Libraries libraries page].</p>
<h2 id="compiler-docs-ghc">`compiler/`, `docs/`, `ghc/`</h2>
<p>These directories contain the main GHC compiler and documentation. The `compiler/` directory contains the ghc package, which is linked into an executable in the `ghc/` directory.</p>
<p>There is [wiki:ModuleDependencies documentation of the intended module dependency structure] of the `compiler/` directory.</p>
<p><code>* </code><strong><code>`compiler/ghc.cabal.in`</code></strong><code>: the Cabal file for GHC is generated from this. If you add a module to GHC's source code, you must add it in the `ghc.cabal.in` file too, else you'll get link errors.</code></p>
<p>The following directories appear only in the build tree:</p>
<p><code>* </code><strong><code>`compiler/stage1`</code></strong><code>: generated files for the stage1 build of GHC. There are a handful of files (`ghc_boot_platform.h` etc), and a directory `compiler/stage1/build/` that contains all the `.o` and `.hi` files for the compiler.</code><br />
<code>* </code><strong><code>`compiler/stage2`</code></strong><code>: similarly stage2.</code></p>
<p>You can't run a binary from here: look in the `inplace/` directory below for that.</p>
<h2 id="rts">`rts/`</h2>
<p>Sources for the runtime system; see [wiki:Commentary/SourceTree/Rts].</p>
<h2 id="includes">`includes/`</h2>
<p>Header files for the runtime system; see [wiki:Commentary/SourceTree/Includes].</p>
<h2 id="utils-libffi">`utils/`, `libffi/`</h2>
<p>The `utils` directory contains support utilities that GHC uses.</p>
<p>These utils may be built with the bootstrapping compiler, for use during the build, or with the stage1 or stage2 compiler, for installing. Some of them are built with both; we can't install the utils built with the bootstrapping compiler as they may use different versions of C libraries. The reason we use sometimes stage2 rather than stage1 is that some utils, e.g. haddock, need the GHC API package.</p>
<p><code>* </code><strong><code>`utils/ghc-cabal`</code></strong><code> is a little program we use for building the libraries. It's similar to cabal-install, but without the dependencies on `http` etc.</code><br />
<code>* </code><strong><code>`utils/count_lines`</code></strong><code> is a program that counts the number of source-code lines in GHC's code-base. It distinguishes comments from non-comments.</code></p>
<h2 id="driver">`driver/`</h2>
<p>This contains some simple wrapper programs and scripts, for example the `ghci` wrapper that invokes the `ghc` binary with the `--interactive` flag. These wrappers tend to be executable programs on Windows and scripts on Unix systems.</p>
<h2 id="ghc-tarballs-windows-only">`ghc-tarballs/` (Windows only)</h2>
<p>This contains some tarball files (binary packages) that GHC relies upon. Used for easier development / deployment on windows.</p>
<h2 id="testsuite-nofib">`testsuite/`, `nofib/`</h2>
<p>The `testsuite/` and `nofib/` directories contain apparatus for testing GHC.</p>
<p><code>* [wiki:Building/RunningTests]</code><br />
<code>* [wiki:Building/RunningNoFib]</code></p>
<h2 id="mk-rules">`mk/`, `rules/`</h2>
<p>The `mk/` and `rules.mk` directories contains all the build system Makefile boilerplate; see [wiki:Building/Architecture GHC Build System Architecture]. Some particular files are interesting:</p>
<p><code> * </code><strong><code>`mk/build.mk`</code></strong><code>: contains Makefile settings that control your build. Details [wiki:Building/Using here].  The file `mk/build.mk.sample` contains a starting point that you can copy to `mk/build.mk` if you want.</code><br />
<code> * </code><strong><code>`mk/are-validating.mk`</code></strong><code>: this file records the fact that you are doing [wiki:TestingPatches validation], by containing the single line `Validating=YES`.  That in turn means the the build system gets its settings from `mk/validate-settings.mk` instead of from `mk/build.mk`.  Remove the file to stop validating.</code><br />
<code> * </code><strong><code>`mk/validate.mk`</code></strong><code>: just like `build.mk`, but applies when validating.  Use this file to override the default settings for validation, which are in `mk/validate-settings.mk`.</code></p>
<h2 id="distrib">`distrib/`</h2>
<p>Miscellaneous files for building distributions.</p>
<h2 id="stuff-that-appears-only-in-a-build-tree">Stuff that appears only in a build tree</h2>
<h3 id="inplace">`inplace/`</h3>
<p>The `inplace/` directory is where we &quot;install&quot; stage1 and stage2 compilers, and other utility programs, when they are built, to be used when building other things in the build tree. The layout is exactly the same as that of an installed GHC on the host platform.</p>
<p><code>  * </code><strong><code>`inplace/bin/`</code></strong><code>: executables, including </code><br />
<code>    * `ghc-stage1`</code><br />
<code>    * `ghc-stage2`</code><br />
<code>    * `ghc-pkg`</code><br />
<code>    * `hasktags`</code><br />
<code>    * `hsc2hs`</code><br />
<code>    * `haddock`</code><br />
<code>    * `count_lines`</code><br />
<code>    * `compareSizes`</code></p>
<p><code>  * </code><strong><code>`inplace/lib/`</code></strong><code>: suppporting libraries for the executables.</code></p>
<h3 id="dist">`.../dist*/`</h3>
<p>In many directories, `dist*` subdirectories appear. These are where Cabal, and the build system makefiles, put all of the files generated while building. Some particularly interesting files are:</p>
<p><code> * </code><strong><code>`docs/users_guide/users_guide/index.html`</code></strong><code>: the HTML for the user manual</code><br />
<code> * </code><strong><code>`libraries/`</code><em><code>lib</code></em><code>`/dist-install/doc/html/`</code><em><code>lib</code></em></strong><code>: contains the Haddock'd documentation for library </code><em><code>lib</code></em></p>
<p><a href="PageOutline" class="uri" title="wikilink">PageOutline</a></p>
<hr />
<h2 id="stack-layout-1">Stack Layout</h2>
<p>The stack-layout phase decides where to spill variables. The important goals are to avoid memory traffic and to minimize the size of the stack frame. Both of these goals are accomplished by reusing stack slots.</p>
<h3 id="representing-stack-slots">Representing Stack Slots</h3>
<p>For each stack slot, we introduce a new name, then treat the name as the addressing expression for the slot. At the end of the pipeline, we choose a stack layout, then replace each stack slot with its offset from the stack pointer. The benefit is that we break the phase-ordering problem: any phase of the compiler can name a stack slot.</p>
<p>For example, for a variable `x`, the expression `SS(x)` is the address of the stack slot where we can spill `x`. (I don't think we output any C-- that uses SS anymore, but the new code generator marks its stack slots prior to layout with `young<k> + 4`, etc. -- Edward) The stack is assumed to grow down, and we assume that the address `SS(x)` points to the old end of the slot. Therefore, to address the low address of a 4-byte slot, we would use the expression `SS(x + 4)`. And we would spill `x` using the following instruction: </p>
<p>where  refers to an address  in memory.</p>
<p>But what about parameter passing? We use a similar technique, but this time we describe the slot for each location as an offset within the area where the parameters are passed. For example, we lower a function call</p>
<p></p>
<p>into approximately the following C--:</p>
<p></p>
<p>We use the following types to represent stack slots and parameter-passing areas:</p>
<p></p>
<p>An `Area` represents space on the stack; it may use either the `RegSlot` constructor to represent a single stack slot for a register or the `CallArea` constructor to represent parameters passed to/from a function call/return. In a young `CallArea`, the `BlockId` is the label of the function call's continuation, and it passes parameters to the call.</p>
<p><strong>Area layout and addressing</strong></p>
<p><code>* Each `Area` grows down, towards lower machine addresses. </code><br />
<code>* </code><em><code>Offsets</code></em><code> are always-positive byte displacements within an `Area`.</code><br />
<code>* The low-offset end is also called the &quot;old end&quot; of the area, the high-offset end is also called the &quot;young end&quot;.</code><br />
<code>* Notice that the low-offset (old) end has higher machine addresses.</code><br />
<code>* Offset 0 (if we allowed it) would address the byte one </code><em><code>beyond</code></em><code> the high-address end of the `Area`. </code><br />
<code>* Larger offsets (from the beginning of the `Area`) correspond to lower machine addresses.</code><br />
<code>* Hence, to address a 4-byte object at the old end of `Area` a, we use the offset +4, thus `(CmmStackSlot a 4)`.</code></p>
<p>The `Old` call area is the initial state of the stack on entry to the function (the overflow parameters and the return address) as well as any arguments that will be passed to a tail call. (SLPJ believes that:) On entry to the function, register `Sp` contains the address of the youngest (lowest-address, highest offset) byte in the `Old` area.</p>
<p>Note that `RegSlot` areas are very small (since they only need to store a single register), while `CallArea` are contiguous chunks of arguments.</p>
<p>To name a specific location on the stack, we represent its address with a new kind of `CmmExpr`: the `CmmStackSlot`. A `CmmStackSlot` is just an integer offset into an `Area`. <a href="BR" class="uri" title="wikilink">BR</a></p>
<p>Notice that a `CmmStackSlot` is an <em>address</em>, so we can say  to make `Sp` point to a particular stack slot. Use a `CmmLoad` to load from the stack slot.</p>
<p>The following figure shows the layout of a `CallArea` for both the outgoing parameters (function call) and incoming results (continuation after returning from the function call). Note that the incoming and outgoing parameters may be different, and they may overlap.</p>
<p><a href="Image(CallArea.png)" class="uri" title="wikilink">Image(CallArea.png)</a></p>
<p>A `RegSlot` is laid out in the same fashion, with the offset 0 pointing off the high byte of the stack slot. To address an 8-byte double-word, we would use the offset 8. To address only the high word of the same stack slot, we would use the offset 4.</p>
<p>Currently, the intermediate code does not explicitly use a virtual frame pointer, but when we talk about offsets into the stack, we implicitly assume that there is a virtual frame pointer that points just off the oldest byte of the return address on entry to the procedures. Therefore, on entry to the procedure, the offset of the (4-byte) return address is 4.</p>
<h3 id="laying-out-the-stack">Laying out the stack</h3>
<p>The business of the stack-layout pass is to construct a mapping (fixed across a single procedure)  which assigns a virtual stack slot (i.e. offset in bytes, relative to the virtual frame pointer) to each `Area`.</p>
<p>A naive approach to laying out the stack would be to give each variable its own stack slot for spilling, and allocate only the ends of the stack frame for parameter-passing areas. But this approach misses two opportunities for optimization:</p>
<p><code>* Stack slots can be reused by variables that are never on the stack at the same time</code><br />
<code>* If a function returns a variable on the stack, we might be able to use the return location as the variable's stack slot.</code></p>
<p>As it turns out, it is quite common in GHC that the first definition of a variable comes when its value is returned from a function call. If the value is returned on the stack, then an important optimization is to avoid copying that value to some other location on the stack. How is that achieved? By making sure the location where the value is returned is also its spill slot.</p>
<h3 id="a-greedy-algorithm">A greedy algorithm</h3>
<p>We rewrite the stack slots in two passes:</p>
<p><code>1. Walk over the graph and choose an offset for each `Area`.</code><br />
<code>1. Walk over the graph, keeping track of the stack pointer, and rewrite each address of a stack slot with an offset from the stack pointer. Also, insert adjustments to the stack pointer before and after proc points.</code></p>
<p>The details are in cmm/CmmProcPointZ.hs (they have not yet been committed, but will be soon - Aug 4, 2008).</p>
<h1 id="layout-of-the-stack">Layout of the stack</h1>
<p>Every [wiki:Commentary/Rts/HeapObjects#ThreadStateObjects TSO object] contains a stack. The stack of a TSO grows downwards, with the topmost (most recently pushed) word pointed to by , and the bottom of the stack given by .</p>
<p>The stack consists of a sequence of <em>stack frames</em> (also sometimes called <em>activation records</em>) where each frame has the same layout as a heap object:</p>
<p>|| Header || Payload... ||</p>
<p>There are several kinds of [wiki:Commentary/Rts/Storage/Stack#KindsofStackFrame stack frames], but the most common types are those pushed when evaluating a  expression:  The code for evaluating a  pushes a new stack frame representing the alternatives of the case, and continues by evaluating . When  completes, it returns to the stack frame pushed earlier, which inspects the value and selects the appropriate branch of the case. The stack frame for a  includes the values of all the free variables in the case alternatives.</p>
<h2 id="info-tables-for-stack-frames">Info tables for stack frames</h2>
<p>The info table for a stack frame has a couple of extra fields in addition to the [wiki:Commentary/Rts/HeapObjects#InfoTables basic info table layout]. A stack-frame info table is defined by  in <a href="GhcFile(includes/rts/storage/InfoTables.h)" class="uri" title="wikilink">GhcFile(includes/rts/storage/InfoTables.h)</a>.</p>
<p><a href="Image(ret-itbl-no-rv.png)" class="uri" title="wikilink">Image(ret-itbl-no-rv.png)</a></p>
<p>The <em>SRT</em> field points to the static reference table (SRT) for this stack frame (see [wiki:Commentary/Rts/Storage/GC/CAFs] for details of SRTs).</p>
<h2 id="layout-of-the-payload">Layout of the payload</h2>
<p>Unlike heap objects which mainly have &quot;pointers first&quot; layout, in a stack frame the pointers and non-pointers are intermingled. This is so that we can support &quot;stack stubbing&quot; whereby a live variable stored on the stack can be later marked as dead simply by pushing a new stack frame that identifies that slot as containing a non-pointer, so the GC will not follow it.</p>
<p>Stack frames therefore have [wiki:Commentary/Rts/HeapObjects#Bitmaplayout bitmap layout].</p>
<h2 id="kinds-of-stack-frame">Kinds of Stack Frame</h2>
<p>The constants for the different types of stack frame are defined in <a href="GhcFile(includes/rts/storage/ClosureTypes.h)" class="uri" title="wikilink">GhcFile(includes/rts/storage/ClosureTypes.h)</a>. More details about the layouts are available in <a href="GhcFile(includes/rts/storage/Closures.h)" class="uri" title="wikilink">GhcFile(includes/rts/storage/Closures.h)</a></p>
<p><code>* </code><br />
<code>* </code><br />
<code>* </code><br />
<code>* </code><code> - (Explained a bit here: </code><a href="https://ghc.haskell.org/trac/ghc/wiki/Commentary/Compiler/CPS#Notes"><code>https://ghc.haskell.org/trac/ghc/wiki/Commentary/Compiler/CPS#Notes</code></a><code>)</code><br />
<code>* </code><br />
<code>* </code><br />
<code>* </code><code> - The stack is chunked now. Connected as a linked list. (Since Dec 2010: f30d527344db528618f64a25250a3be557d9f287,  </code><a href="https://ghc.haskell.org/trac/ghc/blog/stack-chunks"><code>Blogpost</code></a><code>)</code><br />
<code>* </code><br />
<code>* </code><br />
<code>* </code><br />
<code>* </code></p>
<p>Video: <a href="http://www.youtube.com/watch?v=v0J1iZ7F7W8&amp;list=PLBkRCigjPwyeCSD_DFxpd246YIF7_RDDI">STG language</a> (17'21&quot;)</p>
<h1 id="the-stg-syntax-data-types">The STG syntax data types</h1>
<p>Before code generation, GHC converts the Core-language program into . The basic ideas are still pretty much exactly as described in the paper <a href="http://research.microsoft.com/en-us/um/people/simonpj/papers/spineless-tagless-gmachine.ps.gz">Implementing lazy functional languages on stock hardware: the Spineless Tagless G-machine</a>.</p>
<p>The best way to think of STG is as special form of [wiki:Commentary/Compiler/CoreSynType Core]. Specifically, the differences are these (see <a href="GhcFile(compiler/stgSyn/StgSyn.hs)" class="uri" title="wikilink">GhcFile(compiler/stgSyn/StgSyn.hs)</a>):</p>
<p><code>* Function arguments are atoms (literals or variables), of type </code><code>.</code><br />
<code>* The right hand side of a let-binding, </code><code>, is either</code><br />
<code>   * `StgRhsCon`: a constructor application, or </code><br />
<code>   * `StgRhsClosure`: </code><strong><code>lambda-form</code></strong><code> (possibly with zero arguments, in which case it's a thunk).</code><br />
<code>* Constructor applications are saturated.</code><br />
<code>* Applications of primitive operators are saturated.</code><br />
<code>* Lambdas can only appear the right-hand side of a let-binding.  (There is an expression form </code><code>, but it is only used during the Core-to-STG transformation, not in a valid STG program.)</code><br />
<code>* Types have largely been discarded, retaining only enough type information as is needed to guide code generation. There is an </code><code> checker, which makes some consistency checks, but the !CoreLint guarantee that &quot;if the program passes Lint it cannot crash&quot; has been lost.</code></p>
<p>In addition, the STG program is decorated with the results of some analyses:</p>
<p><code> * Every lambda-form (`StgRhsClosure`) lists its free variables.  These are the variables that are in the thunk of function closure that is allocated by the let.</code></p>
<p><code> * Every lambda-form gives its [wiki:Commentary/Rts/CAFs </code><strong><code>Static</code> <code>Reference</code> <code>Table</code></strong><code>] or </code><strong><code>SRT</code></strong><code>.  You should think of the SRT as the </code><em><code>top-level</code></em><code> free variables of the body.  They do not need to be dynamically allocated in the heap object, but they do need to be accessible from the object's info-table, so that the garbage collector can find the CAFs kept alive by the object.</code></p>
<p><code> * A </code><code> expression is decorated with its </code><strong><code>live</code> <code>variables</code></strong><code>; that is, variables reachable from the continuation of the case.  More precisely, two sets of live variables, plus the SRT for the continuation.  Todo: say more.</code></p>
<p><code> * The STG program has a new construct called </code><strong><code>let-no-escape</code></strong><code>, that encodes so-called </code><strong><code>join</code> <code>points</code></strong><code>. Variables bound by a let-no-escape are guaranteed to be tail-calls, not embedded inside a data structure, in which case we don</code></p>
<h1 id="ghc-commentary-software-transactional-memory-stm">GHC Commentary: Software Transactional Memory (STM)</h1>
<p>This document gives an overview of the runtime system (RTS) support for GHC's STM implementation. We will focus on the case where fine grain locking is used ().</p>
<p>Some details about the implementation can be found in the papers <a href="http://research.microsoft.com/en-us/um/people/simonpj/papers/stm/stm.pdf">&quot;Composable Memory Transactions&quot;</a> and <a href="http://research.microsoft.com/en-us/um/people/simonpj/papers/stm/stm-invariants.pdf">&quot;Transactional memory with data invariants&quot;</a>. Additional details can be found in the Harris et al book <a href="http://www.morganclaypool.com/doi/abs/10.2200/s00272ed1v01y201006cac011">&quot;Transactional memory&quot;</a>. Some analysis on performance can be found in the paper <a href="https://www.bscmsrc.eu/sites/default/files/cf-final.pdf">&quot;The Limits of Software Transactional Memory&quot;</a> though this work only looks at the coarse grain lock version. Many of the other details here are gleaned from the comments in the source code.</p>
<h1 id="background">Background</h1>
<p>This document assumes the reader is familiar with some general details of GHC's execution and memory layout. A good starting point for this information is can be found here: [wiki:Commentary/Compiler/GeneratedCode Generated Code].</p>
<h2 id="definitions">Definitions</h2>
<h3 id="useful-rts-terms">Useful RTS terms</h3>
<p></p>
<p><code>Corresponds to a CPU. The number of capabilities should match the number of CPUs. See [wiki:Commentary/Rts/Scheduler#Capabilities Capabilities].</code></p>
<p>TSO</p>
<p><code> Thread State Object. The state of a Haskell thread. See [wiki:Commentary/Rts/Storage/HeapObjects#ThreadStateObjects Thread State Objects].</code></p>
<p>Heap object</p>
<p><code> Objects on the heap all take the form of an </code><code> structure with a header pointing and a payload of data. The header points to code and an info table. See [wiki:Commentary/Rts/Storage/HeapObjects Heap Objects].</code></p>
<h3 id="transactional-memory-terms">Transactional Memory terms</h3>
<p>Read set</p>
<p><code> The set of </code><code>s that are read, but not written to during a transaction.</code></p>
<p>Write set</p>
<p><code> The set of </code><code>s that are written to during a transaction. In the code each written </code><code> is called an &quot;update entry&quot; in the transactional record.</code></p>
<p>Access set</p>
<p><code> All </code><code>s accessed during the transaction.</code></p>
<p>While GHC's STM does not have a separate read set and write set these terms are useful for discussion.</p>
<p>Retry</p>
<p><code> Here we will use the term retry exclusively for the blocking primitive in GHC's STM. This should not be confused with the steps taken when a transaction detects that it has seen an inconsistent view of memory and must start again from the beginning.</code></p>
<p>Failure</p>
<p><code> A failed transaction is one that has seen inconsistent state. This should not be confused with a successful transaction that executes the </code><code> primitive.</code></p>
<hr />
<h1 id="overview-of-features">Overview of Features</h1>
<p>At the high level, transactions are computations that read and write to s with changes only being committed atomically after seeing a consistent view of memory. Transactions can also be composed together, building new transactions out of existing transactions. In the RTS each transaction keeps a record of its interaction with the s it touches in a . A pointer to this record is stored in the TSO that is running the transaction.</p>
<h2 id="reading-and-writing">Reading and Writing</h2>
<p>The semantics of a transaction require that when a  is read in a transaction, its value will stay the same for the duration of execution. Similarly a write to a  will keep the same value for the duration of the transaction. The transaction itself, however, from the perspective of other threads can apply all of its effects in one moment. That is, other threads cannot see intermediate states of the transaction, so it is as if all the effects happen in a single moment.</p>
<p>As a simple example we can consider a transaction that transfers value between two accounts:</p>
<p></p>
<p>No other thread can observe the value  in  without also observing  in .</p>
<h2 id="blocking">Blocking</h2>
<p>Transactions can choose to block until changes are made to s that allow it to try again. This is enabled with an explicit . Note that when changes are made the transaction is restarted from the beginning.</p>
<p>Continuing the example, we can choose to block when there are insufficient funds:</p>
<p></p>
<h2 id="choice">Choice</h2>
<p>Any blocking transaction can be composed with  to choose an alternative transaction to run instead of blocking. The  primitive operation creates a nested transaction and if this first transaction executes , the effects of the nested transaction are rolled back and the alternative transaction is executed. This choice is biased towards the first parameter. A validation failure in the first branch aborts the entire transaction, not just the nested part. An explicit  is the only mechanism that gives partial rollback.</p>
<p>We now can choose the account that has enough funds for the transfer:</p>
<p></p>
<h2 id="data-invariants">Data Invariants</h2>
<p>Invariants support checking global data invariants beyond the atomicity transactions demand. For instance, a transactional linked list (written correctly) will never have an inconsistent structure due to the atomicity of updates. It is no harder to maintain this property in a concurrent setting then in a sequential one with STM. It may be desired, however, to make statements about the consistency of the <em>data</em> in a particular a sorted linked list is sorted, not because of the structure (where the s point to) but instead because of the data in the structure (the relation between the data in adjacent nodes). Global data invariant checks can be introduced with the  operation which demands that the transaction it is given results in  and that it continues to hold for every transaction that is committed globally.</p>
<p>We can use data invariants to guard against negative balances:</p>
<p></p>
<h2 id="exceptions">Exceptions</h2>
<p>Exceptions inside transactions should only propagate outside if the transaction has seen a consistent view of memory. Note that the semantics of exceptions allow the exception itself to capture the view of memory from inside the transaction, but this transaction is not committed.</p>
<hr />
<h1 id="overview-of-the-implementation">Overview of the Implementation</h1>
<p>We will start this section by considering building GHC's STM with only the features of reading and writing. Then we will add  then  and finally data invariants. Each of the subsequent features adds more complexity to the implementation. Taken all at once it can be difficult to understand the subtlety of some of the design choices.</p>
<hr />
<h2 id="transactions-that-read-and-write.">Transactions that Read and Write.</h2>
<p>With this simplified view we only support , , and  as well as all the STM type class instances except .</p>
<h3 id="transactional-record">Transactional Record</h3>
<p>The overall scheme of GHC's STM is to perform all the effects of a transaction locally in the transactional record or . Once the transaction has finished its work locally, a value based consistency check determines if the values read for the entire access set are consistent. This only needs to consider the  and the main memory view of the access set as it is assumed that main memory is always consistent. This check also obtains locks for the write set and with those locks we can update main memory and unlock. Rolling back the effects of a transaction is just forgetting the current  and starting again.</p>
<p>The transactional record itself will have an entry for each transactional variable that is accessed. Each entry has a pointer to the  heap object and a record of the value that the  held when it was first accessed.</p>
<h3 id="starting">Starting</h3>
<p>A transaction starts by initializing a new  () assigning the TSO's  pointer to the new  then executing the transaction's code.</p>
<p>(See <a href="GhcFile(rts/PrimOps.cmm)" class="uri" title="wikilink">GhcFile(rts/PrimOps.cmm)</a>  and <a href="GhcFile(rts/STM.c)" class="uri" title="wikilink">GhcFile(rts/STM.c)</a> ).</p>
<h3 id="reading">Reading</h3>
<p>When a read is attempted we first search the  for an existing entry. If it is found, we use that local view of the variable. On the first read of the variable, a new entry is allocated and the value of the variable is read and stored locally. The original  does not need to be accessed again for its value until a validation check is needed.</p>
<p>In the coarse grain version, the read is done without synchronization. With the fine grain lock, the lock variable is the  of the  structure. While reading an inconsistent value is an issue that can be resolved later, reading a value that indicates a lock and handing that value to code that expects a different type of heap object will almost certainly lead to a runtime failure. To avoid this the fine grain lock version of the code will spin if the value read is a lock, waiting to observe the lock released with an appropriate pointer to a heap object.</p>
<p>(See <a href="GhcFile(rts/STM.c)" class="uri" title="wikilink">GhcFile(rts/STM.c)</a> )</p>
<h3 id="writing">Writing</h3>
<p>Writing to a  requires that the variable first be in the . If it is not currently in the , a read of the 's value is stored in a new entry (this value will be used to validate and ensure that no updates were made concurrently to this variable).</p>
<p>In both the fine grain and coarse grain lock versions of the code no synchronization is needed to perform the write as the value is stored locally in the  until commit time.</p>
<p>(See <a href="GhcFile(rts/STM.c)" class="uri" title="wikilink">GhcFile(rts/STM.c)</a> )</p>
<h3 id="validation">Validation</h3>
<p>Before a transaction can make its effects visible to other threads it must check that it has seen a consistent view of memory while it was executing. Most of the work is done in  by checking that s hold their expected values.</p>
<p>For the coarse grain lock version the lock is held before entering  through the writing of values to s. With the fine grain lock, validation acquires locks for the write set and reads a version number consistent with the expected value for each  in the read set. After all the locks for writes have been acquired, The read set is checked again to see if each value is still the expected value and the version number still matches ().</p>
<p>(See <a href="GhcFile(rts/STM.c)" class="uri" title="wikilink">GhcFile(rts/STM.c)</a>  and )</p>
<h3 id="committing">Committing</h3>
<p>Before committing, each invariant associated with each accessed  needs to be checked by running the invariant transaction with its own . The read set for each invariant is merged into the transaction as those reads must be included in the consistency check. The  is then validated. If validation fails, the transaction must start over from the beginning after releasing all locks. In the case of the coarse grain lock validation and commit are in a critical section protected by the global STM lock. Updates to s proceeds while holding the global lock.</p>
<p>With the fine grain lock version when validation, including any read-only phase, succeeds, two properties will hold simultaneously that give the desired atomicity:</p>
<ul>
<li>Validation has witnessed all s with their expected value.</li>
<li>Locks are held for all of the s in the write set.</li>
</ul>
<p>Commit can proceed to increment each locked 's  field and unlock by writing the new value to the  field. While these updates happen one-by-one, any attempt to read from this set will spin while the lock is held. Any reads made before the lock was acquired will fail to validate as the number of updates will change.</p>
<p>(See <a href="GhcFile(rts/PrimOps.cmm)" class="uri" title="wikilink">GhcFile(rts/PrimOps.cmm)</a>  and <a href="GhcFile(rts/STM.c)" class="uri" title="wikilink">GhcFile(rts/STM.c)</a> )</p>
<h3 id="aborting">Aborting</h3>
<p>Aborting is simply throwing away changes that are stored in the .</p>
<p>(See <a href="GhcFile(rts/STM.c)" class="uri" title="wikilink">GhcFile(rts/STM.c)</a> )</p>
<h3 id="exceptions-1">Exceptions</h3>
<p>An exception in a transaction will only propagate outside of the transaction if the transaction can be validated. If validation fails, the whole transaction will abort and start again from the beginning. Nothing special needs to be done to support the semantics allowing the view <em>inside</em> the aborted transaction.</p>
<p>(See <a href="GhcFile(rts/Exception.cmm)" class="uri" title="wikilink">GhcFile(rts/Exception.cmm)</a> which calls  from <a href="GhcFile(rts/STM.c)" class="uri" title="wikilink">GhcFile(rts/STM.c)</a>).</p>
<hr />
<h2 id="blocking-with">Blocking with </h2>
<p>We will now introduce the blocking feature. To support this we will add a watch queue to each  where we can place a pointer to a blocked TSO. When a transaction commits we will now wake up the TSOs on watch queues for s that are written.</p>
<p>The mechanism for  is similar to exception handling. In the simple case of only supporting blocking and not supporting choice, an encountered retry should validate, and if valid, add the TSO to the watch queue of every accessed  (see <a href="GhcFile(rts/STM.c)" class="uri" title="wikilink">GhcFile(rts/STM.c)</a>  and ). Locks are acquired for all s when validating to control access to the watch queues and prevent missing an update to a  before the thread is sleeping. In particular if validation is successful the locks are held after the return of , through the return to the scheduler, after the thread is safely paused (see <a href="GhcFile(rts/HeapStackCheck.cmm)" class="uri" title="wikilink">GhcFile(rts/HeapStackCheck.cmm)</a> ), and until  is called. This ensures that no updates to the s are made until the TSO is ready to be woken. If validation fails, the  is discarded and the transaction is started from the beginning. (See <a href="GhcFile(rts/PrimOps.cmm)" class="uri" title="wikilink">GhcFile(rts/PrimOps.cmm)</a> )</p>
<p>When a transaction is committed, each write that it makes to a  is preceded by waking up each TSO in the watch queue. Eventually these TSOs will be run, but before restarting the transaction its  is validated again if valid then nothing has changed that will allow the transaction to proceed with a different result. If invalid, some other transaction has committed and progress may be possible (note there is the additional case that some other transaction is merely holding a lock temporarily, causing validation to fail). The TSO is not removed from the watch queues it is on until the transaction is aborted (at this point we no longer need the ) and the abort happens after the failure to validate on wakeup. (See <a href="GhcFile(rts/STM.c)" class="uri" title="wikilink">GhcFile(rts/STM.c)</a>  and )</p>
<hr />
<h2 id="choice-with">Choice with </h2>
<p>When  executes it searches the stack for either a  or the outer  (the boundary between normal execution and the transaction). The former is placed on the stack by an  (see <a href="GhcFile(rts/PrimOps.cmm)" class="uri" title="wikilink">GhcFile(rts/PrimOps.cmm)</a> ) and if executing the first branch we can partially abort and switch to the second branch, otherwise we propagate the  further. In the latter case this  represents a transaction that should block and the behavior is as above with only .</p>
<p>How do we support a &quot;partial abort&quot;? This introduces the need for a nested transaction. Our  will now have a pointer to an outer  (the  field). This allows us to isolate effects from the branch of the  that we might need to abort. Let's revisit the features that need to take this into account.</p>
<p><code>* </code><strong><code>Reading</code></strong><code> -- Reads now search the chain of nested transactions in addition to the local </code><code>. When an entry is found in a parent it is copied into the local </code><code>. Note that there is still only a single access to the actual </code><code> through the life of the transaction (until validation).</code><br />
<code>* </code><strong><code>Writing</code></strong><code> -- Writes, like reads, now search the parent </code><code>s and the write is stored in the local copy.</code><br />
<code>* </code><strong><code>Retry</code></strong><code> -- As described above, we now need to search the stack for a </code><code> and if found, aborting the nested transaction and attempting the alternative or propagating the retry instead of immediately working on blocking.</code><br />
<code>* </code><strong><code>Validation</code></strong><code> -- If we are validating in the middle of a running transaction we will need to validate the whole nest of transactions.</code><a href="BR" title="wikilink"><code>BR</code></a><code>(See </code><a href="GhcFile(rts/STM.c)" title="wikilink"><code>GhcFile(rts/STM.c)</code></a><code> </code><code> and its uses in </code><a href="GhcFile(rts/Exception.cmm)" title="wikilink"><code>GhcFile(rts/Exception.cmm)</code></a><code> and </code><a href="GhcFile(rts/Schedule.c)" title="wikilink"><code>GhcFile(rts/Schedule.c)</code></a><code>)</code><br />
<code>* </code><strong><code>Committing</code></strong><code> -- Just as we now have a partial abort, we need a partial commit when we finish a branch of an </code><code>. This commit is done with </code><code> which validates just the inner </code><code> and merges updates back into its parent. Note that an update is distinguished from a read only entry by value. This means that if a nested transaction performs a write that reverts a value this is a change and must still propagate to the parent (see ticket #7493).</code><br />
<code>* </code><strong><code>Aborting</code></strong><code> -- There is another subtle issue with how choice and blocking interact. When we block we need to wake up if there is a change to </code><em><code>any</code></em><code> accessed </code><code>. Consider a transaction:</code><a href="BR" title="wikilink"><code>BR</code></a><a href="BR" title="wikilink"><code>BRIf</code></a><code> both </code><code> and </code><code> execute </code><code> then even though the effects of </code><code> are thrown away, it could be that a change to a </code><code> that is only in the access set of </code><code> will allow the whole transaction to succeed when it is woken.</code><a href="BR" title="wikilink"><code>BRTo</code></a><code> solve this problem, when a branch on a nested transaction is aborted the access set of the nested transaction is merged as a read set into the parent </code><code>. Specifically if the </code><code> is in </code><em><code>any</code></em><code> </code><code> up the chain of nested transactions it must be ignored, otherwise it is entered as a new entry (retaining just the read) in the parent </code><code>.</code><a href="BR" title="wikilink"><code>BR</code></a><code>(See again ticket #7493 and </code><a href="GhcFile(rts/STM.c)" title="wikilink"><code>GhcFile(rts/STM.c)</code></a><code> </code><code>)</code><br />
<code>* </code><strong><code>Exceptions</code></strong><code> -- The only change needed here each </code><code> on the stack represents a nested transaction. As the stack is searched for a handler, at each encountered </code><code> the nested transaction is aborted. When the </code><code> is encountered we then know that there is no nested transaction.</code><a href="BR" title="wikilink"><code>BR</code></a><code>(See </code><a href="GhcFile(rts/Exception.cmm)" title="wikilink"><code>GhcFile(rts/Exception.cmm)</code></a><code> </code><code>)</code></p>
<p>(See <a href="GhcFile(rts/PrimOps.cmm)" class="uri" title="wikilink">GhcFile(rts/PrimOps.cmm)</a>  and )</p>
<hr />
<h2 id="invariants-1">Invariants</h2>
<p>We will start this section with an overview of some of the details then review with notes on the changes from the choice case.</p>
<h3 id="details">Details</h3>
<p>As a transaction is executing it can collect dynamically checked data invariants. These invariants are transactions that are never committed, but if they raise an exception when executed successfully that exception will propagate out of the atomic frame.</p>
<p></p>
<p><code> Primitive operation that adds an invariant (transaction to run) to the queue of the current </code><code> by calling </code><code>.</code></p>
<p></p>
<p><code> A wrapper for </code><code> (to give it the </code><code> type).</code></p>
<p></p>
<p><code> This is the </code><code> from the &quot;Transactional memory with data invariants&quot; paper. The action immediately runs, wrapped in a nested transaction so that it will never commit but will have an opportunity to raise an exception. If successful, the originally passed action is added to the invariant queue.</code></p>
<p></p>
<p><code> Takes an </code><code> action that results in a </code><code> and adds an invariant that throws an exception when the result of the transaction is </code><code>.</code></p>
<p>The bookkeeping for invariants is in each s  queue and the s  field. Each invariant is in a  structure that includes the  action, the  where it was last executed, and a lock. This is added to the current s queue when  is executed.</p>
<p>When a transaction completes, execution will reach the  and the s  will be  (a nested transaction would have a  before the  to handle cases of non-empty ). The frame will then check the invariants by collecting the invariants it needs to check with , dequeuing each, executing, and when (or if) we get back to the frame, aborting the invariant action. If the invariant failed to hold, we would not get here due to an exception and if it succeeds we do not want its effects. Once all the invariants have been checked, the frame will to commit.</p>
<p>Which invariants need to be checked for a given transaction? Clearly invariants introduced in the transaction will be checked these are added to the s  queue directly when  is executed. In addition, once the transaction has finished executing, we can look at each entry in the write set and search its watch queue for any invariants.</p>
<p>Note that there is a  in the  package in  which matches the  from the <a href="http://research.microsoft.com/pubs/74063/beautiful.pdf">beauty</a> chapter of &quot;Beautiful code&quot;:</p>
<p></p>
<p>It requires no additional runtime support. If it is a transaction that produces the  argument it will be committed (when ) and it is only a one time check, not an invariant that will be checked at commits.</p>
<h3 id="changes-from-choice">Changes from Choice</h3>
<p>With the addition of data invariants we have the following changes to the implementation:</p>
<p><code>* </code><strong><code>Retrying</code></strong><code> -- A retry in an invariant indicates that the invariant could not proceed and the whole transaction should block. This special case is detected when an </code><code> is encountered with a nest of transactions (i.e. when the </code><code> field is not </code><code>). The invariant is simply aborted and execution proceeds to </code><code> (see </code><a href="GhcFile(rts/PrimOps.cmm)" title="wikilink"><code>GhcFile(rts/PrimOps.cmm)</code></a><code> </code><code>).</code><br />
<code>* </code><strong><code>Commiting</code></strong><code> -- Commit now needs a phase where it runs invariants after the code of the transaction has completed but before commit. The implementation recycles the structure already in place for this phase so special cases are needed in the </code><code> that collects invariants and works through them one at a time then moves on to committing (see </code><a href="GhcFile(rts/PrimOps.cmm)" title="wikilink"><code>GhcFile(rts/PrimOps.cmm)</code></a><code> </code><code>).</code><a href="BR" title="wikilink"><code>BRTo</code></a><code> efficiently handle invariants they need to only be checked when a relevant data dependency changes. This means we can associate them with the </code><code> of the last commit that needed to check the invariant at the cost of serializing invariant handling commits. This is enforced by the lock on each invariant. If it cannot be acquired the whole transaction must start over.</code><a href="BR" title="wikilink"><code>BRAt</code></a><code> commit time, each invariant is locked and the read set for the last commited transaction of each invariant is merged into the </code><code>.</code><a href="BR" title="wikilink"><code>BRValidation</code></a><code> acuqires lock for all entries in the </code><code> (not just the writes). After validation, each invariant is removed from the watch queue of each </code><code> it previously depended on, then the </code><code> that was used when executing the invariant code is updated to reflect the values from the final execution of the main transaction and each </code><code>, being a data depenency of the invariant, has the invariant added to its watch queue.</code><a href="BR" title="wikilink"><code>BR</code></a><code>(See </code><a href="GhcFile(rts/STM.c)" title="wikilink"><code>GhcFile(rts/STM.c)</code></a><code> </code><code>, </code><code> and </code><code>)</code><br />
<code>* </code><strong><code>Exceptions</code></strong><code> -- When an exception propagates to the </code><code> there are now two states that it could encounter. If there is no enclosing </code><code> we are not dealing with an exception from an invariant and it proceeds as above. Seeing a nest of transactions indicates that the transaction was checking an invariant when it encountered the exception. The effect of a failed invariant </code><em><code>is</code></em><code> this exception so nothing special needs to be done except to validate and abort both the outer transaction and the nested transaction (see </code><a href="GhcFile(rts/Exception.cmm)" title="wikilink"><code>GhcFile(rts/Exception.cmm)</code></a><code> </code><code>).</code></p>
<hr />
<h2 id="other-details">Other Details</h2>
<p>This section describes some details that can be discussed largely in isolation from the rest of the system.</p>
<h3 id="detecting-long-running-transactions">Detecting Long Running Transactions</h3>
<p>While the type system enforces STM actions to be constrained to STM side effects, pure computations in Haskell can be non-terminating. It could be that a transaction sees inconsistent data that leads to non-termination that would never happen in a program that only saw consistent data. To detect this problem, every time a thread yields it is validated. A validation failure causes the transaction to be condemned.</p>
<h3 id="transaction-state">Transaction State</h3>
<p>Each  has a  field that holds the status of the transaction. It can be one of the following:</p>
<p></p>
<p><code> The transaction is actively running.</code></p>
<p></p>
<p><code> The transaction has seen an inconsistency.</code></p>
<p></p>
<p><code> The transaction has committed and is in the process of updating </code><code> values.</code></p>
<p></p>
<p><code> The transaction has aborted and is working to release locks.</code></p>
<p></p>
<p><code> The transaction has hit a </code><code> and is waiting to be woken.</code></p>
<p>If a  state is  (some inconsistency was seen) validate does nothing. When a top-level transaction is aborted in , if the state is  it will remove the watch queue entries for the . Similarly if a waiting  is condemned via an asynchronous exception when a validation failure is observed after a thread yield, its watch queue entries are removed. Finally a  in the  state is not condemned by a validation. In this case the  is already waiting for a wake up from a  that changes and observing an inconsistency merely indicates that this will happen soon.</p>
<p>In the work of Keir Fraser a transaction state is used for cooperative efforts of transactions to give lock-free properties for STM systems. The design of GHC's STM is clearly influenced by this work and seems close to some of the algorithms in Fraser's work. It does not, however, implement what would be required to be lock-free or live-lock free (in the fine grain lock code). For instance, if two transactions  and  are committing at the same time and  has read  and written  while  has read  and written , both the transactions can fail to commit. For example, consider the interleaving:</p>
<p>||<strong>`T1`</strong> || <strong>`TVar`</strong> || <strong>`T2`</strong> ||<strong>Action</strong> || ||`A 0 0` || `A 0` || ||`T1` read A || || || `B 0` || `B 0 0` ||`T2` read B || ||`B 0 1` || || ||`T1` write B 1 || || || || `A 0 1` ||`T2` write A 1 || ||`A 0 0 0` || `A 0` || ||`T1` Validation Part 1 (read A) || || || `A T2` || ||`T2` Validation (Lock A) || || || `B 0` || `B 0 0 0` ||`T2` Validation (Read B) || || || `B T1` || ||`T1` Validation Part 2 (Lock B) ||</p>
<p>Note: the first and third columns are the local state of the s and the second column is the values of the  structures. Each  entry has the expected value followed by the new value and a number of updates field when it is read for validation.</p>
<p>At this point  and  both perform their  and both could (at least one will) discover that a  in their read set is now locked. This leads to both transactions aborting. The chances of this are narrow but not impossible (see ticket #7815). Fraser's work avoids this by using the transaction status and the fact that locks point back to the  holding the lock to detect other transactions in a read only check (read phase) and resolving conflicts so that at least one of the transactions can commit.</p>
<p>A simpler example can also cause both transactions to abort. Consider two transactions with the same write set, but the writes entered the s in a different order. Both transactions could encounter a lock from the other before they have a chance to release locks and get out of the way. Having an ordering on lock could avoid this problem but would add a little more complexity.</p>
<h3 id="gc-and-aba">GC and ABA</h3>
<p>GHC's STM does comparisons for validation by value. Since these are always pure computations these values are represented by heap objects and a simple pointer comparison is sufficient to know if the same value is in place. This presents an ABA problem however if the location of some value is recycled it could appear as though the value has not changed when, in fact, it is a different value. This is avoided by making the  fields of the  entries pointers into the heap followed by the garbage collector. As long as a  is still alive it will keep the original value it read for a  alive.</p>
<h3 id="management-of-s">Management of s</h3>
<p>The  structure is built as a list of chunks to give better locality and amortize the cost of searching and allocating entries. Additionally s are recycled to aid locality further when a transaction is aborted and started again. Both of these details add a little complexity to the implementation that is abated with some macros such as  and .</p>
<h3 id="tokens-and-version-numbers.">Tokens and Version Numbers.</h3>
<p>When validating a transaction each entry in the  is checked for consistency. Any entry that is an update (in the write set) is locked. This locking is a visible effect to the rest of the system and prevents other committing transactions from progress. Reads, however, are not going to be updated. Instead we check that a read to the value matches our expected value, then we read a version number (the  field) and check again that the expected value holds. This gives us a read of  that is consistent with the  holding the expected value. Once all the locks for the write set are acquired we know that only our transaction can have an effect on the write set. All that remains is to rule out some change to the read set while we were still acquiring locks for the writes. This is done in the read phase (with ) which checks first if the value matches the expectation then checks if the version numbers match. If this holds for each entry in the read set then there must have existed a moment, while we held the locks for all the write set, where the read set held all its values. Even if some other transaction committed a new value and yet another transaction committed the expected value back the version number will have been incremented.</p>
<p>All that remains is managing these version numbers. When a  is updated its version number is incremented before the value is updated with the lock release. There is the unlikely case that the finite version numbers wrap around to an expected value while the transaction is committing (even with a 32-bit version number this is <em>highly</em> unlikely to happen). This is, however, accounted for by allocating a batch of tokens to each capability from a global  variable. Each time a transaction is started it decrements it's batch of tokens. By sampling  at the beginning of commit and after the read phase the possibility of an overflow can be detected (when more then 32-bits worth of commits have been allocated out).</p>
<p>(See <a href="GhcFile(rts/STM.c)" class="uri" title="wikilink">GhcFile(rts/STM.c)</a> , , , , and )</p>
<h3 id="implementation-invariants">Implementation Invariants</h3>
<p>Some of the invariants of the implementation:</p>
<p><code>* Locks are only acquired in </code><a href="GhcFile(rts/STM.c)" title="wikilink"><code>GhcFile(rts/STM.c)</code></a><code> and are always released before the end of a function call (with the exception of </code><code> which must release locks after the thread is safe).</code><br />
<code>* When running a transaction each </code><code> is read exactly once and if it is a write, is updated exactly once.</code><br />
<code>* Main memory (</code><code>s) always holds consistent values or locks of a partially updated commit. That is a set of reads at any moment from </code><code>s will result in consistent data if none of the values are locks.</code><br />
<code>* A nest of </code><code>s has a matching nest of </code><code>s ending with an </code><code> on the stack. One exception to this is when checking data invariants the invariant's </code><code> is nested under the top level </code><code> without a </code><code>.</code></p>
<h3 id="fine-grain-locking">Fine Grain Locking</h3>
<p>The locks in fine grain locking () are at the  level and are implemented by placing the locking thread's  in the s current value using a compare and swap (). The value observed when locking is returned by . To test if a  is locked the value is inspected to see if it is a  (checking that the closure's info table pointer is to ). If a  is found  will spin reading the s current value until it is not a  and then attempt again to obtain the lock. Unlocking is simply a write of the current value of the . There is also a conditional lock  which will obtain the lock if the s current value is the given expected value. If the  is already locked this will not be the case (the value would be a ) and if the  has been updated to a new (different) value then locking will fail because the value does not match the expected value. A compare and swap is used for .</p>
<p>This arrangement is useful for allowing a transaction that encounters a locked  to know which particular transaction is locked (used in algorithms in from Fraser). GHC's STM does not, however, use this information.</p>
<h2 id="bibliography">Bibliography</h2>
<p>Fraser, Keir. <em>Practical lock-freedom</em>. Diss. PhD thesis, University of Cambridge Computer Laboratory, 2004.</p>
<p>Jones, Simon Peyton. &quot;Beautiful concurrency.&quot; <em>Beautiful Code: Leading Programmers Explain How They Think</em> (2007): 385-406.</p>
<p>Harris, Tim, et al. &quot;Composable memory transactions.&quot; <em>Proceedings of the tenth ACM SIGPLAN symposium on Principles and practice of parallel programming.</em> ACM, 2005.</p>
<p>Harris, Tim, James Larus, and Ravi Rajwar. &quot;Transactional memory.&quot; <em>Synthesis Lectures on Computer Architecture</em> 5.1 (2010): 1-263.</p>
<p>Harris, Tim, and Simon Peyton Jones. &quot;Transactional memory with data invariants.&quot; <em>First ACM SIGPLAN Workshop on Languages, Compilers, and Hardware Support for Transactional Computing (TRANSACT'06), Ottowa.</em> 2006.</p>
<h1 id="ghc-commentary-storage">GHC Commentary: Storage</h1>
<p>GHC's storage manager is designed to be quite flexible: there are a large number of tunable parameters in the garbage collector, and partly the reason for this was because we wanted to experiment with tweaking these settings in the context of Haskell.</p>
<p><a href="Image(sm-top.png)" class="uri" title="wikilink">Image(sm-top.png)</a></p>
<p><code>* [wiki:Commentary/Rts/Storage/HeapObjects Layout of Heap Objects]</code><br />
<code>* [wiki:Commentary/Rts/Storage/Stack Layout of the Stack]</code><br />
<code>* [wiki:Commentary/Rts/Storage/Slop Slop]</code><br />
<code>* [wiki:Commentary/Rts/Storage/BlockAlloc The Block Allocator]</code><br />
<code>* [wiki:Commentary/Rts/Storage/GC The Garbage Collector]</code><br />
<code>* [wiki:Commentary/Rts/Storage/HeapAlloced The HEAP_ALLOCED() macro]</code></p>
<p>See also:</p>
<p><code>* [wiki:Commentary/Rts/HaskellExecution/PointerTagging Pointer tagging]</code></p>
<h1 id="general-overview">General overview</h1>
<p>GHC's approach to strictness analysis is that of &quot;demand analysis&quot;, a backwards analysis in which strictness analysis and absence analysis are done in a single pass. In the future, analysis to perform unboxing, as well as other analyses, may be implemented within this framework as well.</p>
<h1 id="important-note">IMPORTANT NOTE</h1>
<p>The rest of this commentary describes code that is not checked in to the HEAD yet.</p>
<p>Update: as of 2014-02-12, newer documentation (apparently on the same topic and apparently more up-to-date) is available at <a href="Commentary/Compiler/Demand" class="uri" title="wikilink">Commentary/Compiler/Demand</a> (I am not an expert on the GHC internals though). Also, <a href="GhcFile(compiler/basicTypes/NewDemand.lhs)" class="uri" title="wikilink">GhcFile(compiler/basicTypes/NewDemand.lhs)</a> is not any more in the sources, replaced by (or renamed to?) <a href="GhcFile(compiler/basicTypes/Demand.lhs)" class="uri" title="wikilink">GhcFile(compiler/basicTypes/Demand.lhs)</a>.</p>
<h1 id="the-demand-analyzer">The demand analyzer</h1>
<p>Most of the demand analyzer lives in two files:</p>
<p><code>* </code><a href="GhcFile(compiler/basicTypes/NewDemand.lhs)" title="wikilink"><code>GhcFile(compiler/basicTypes/NewDemand.lhs)</code></a><code> (defines the datatypes used by the demand analyzer, and some functions on them)</code><br />
<code>* </code><a href="GhcFile(compiler/stranal/DmdAnal.lhs)" title="wikilink"><code>GhcFile(compiler/stranal/DmdAnal.lhs)</code></a><code> (the demand analyzer itself)</code></p>
<p>The demand analyzer does strictness analysis, absence analysis, and box-demand analysis in a single pass. (!ToDo: explain what these are.)</p>
<p>In <a href="GhcFile(compiler/stranal/DmdAnal.lhs)" class="uri" title="wikilink">GhcFile(compiler/stranal/DmdAnal.lhs)</a>,  is the function that performs demand analysis on an expression. It has the following type:  The first argument is an environment mapping variables onto demand signatures. (!ToDo: explain more.) The second argument is the demand that's being placed on the expression being analyzed, which was determined from the context already. The third argument is the expression being analyzed.  returns a pair of a new expression (possibly with demand information added to any [wiki:Commentary/Compiler/NameType Ids] in it), and a .</p>
<h2 id="important-datatypes">Important datatypes</h2>
<p> A demand consists of usage information, along with information about usage of the subcomponents of the expression it's associated with.</p>
<p> Usage information consists of a triple of three properties: strictness (or evaluation demand), usage demand, and box demand.</p>
<p> Something that is  may or may not be evaluated. Something that is  will definitely be evaluated at least to its outermost constructor. Something that is  will be fully evaluated (e.g., in ,  can be said to have strictness , because it doesn't matter how much we evaluate  -- this expression will diverge anyway.)</p>
<p> In the context of function arguments, an argument that is  is never used by its caller (e.g., syntactically, it doesn't appear in the body of the function at all). An argument that is  will be used zero or one times, but not more. Something that is  may be used zero, one, or many times -- we don't know.</p>
<p> Again in the context of function arguments, an argument that is  is a value constructed by a data constructor of a product type whose &quot;box&quot; is going to be needed. For example, we say that } &quot;uses the box&quot;, so in ,  has box-demand information . In },  doesn't &quot;use the box&quot; for its argument, so in ,  has box-demand information . When in doubt, we assume .</p>
<p> For a compound data value, the  type describes demands on its components.  means that we don't know anything about the expression's type.  says &quot;this expression has a product type, and the demands on its components consist of the demands in the following list&quot;. If the  is supplied, that means that this expression must be cast using the given coercion before it is evaluated. (!ToDo: explain this more.)</p>
<p>(!ToDo: explain why all the above information is important)</p>
<p>Though any expression can have a  associated with it, another datatype, , is associated with a function body.</p>
<p> A  consists of a  (which provides demands for all explicitly mentioned free variables in a functions body), a list of s on the function's arguments, and a , which indicates whether this function returns an explicitly constructed product:</p>
<p></p>
<p>The  function takes a strictness environment, an [wiki:Commentary/Compiler/NameType Id] corresponding to a function, and a  representing demand on the function -- in a particular context -- and returns a , representing the function's demand type in this context.  Demand analysis is implemented as a backwards analysis, so  takes the demand on a function's result (which was inferred based on how the function's result is used) and uses that to compute the demand type of this particular occurrence of the function itself.</p>
<p> has four cases, depending on whether the function being analyzed is a [wiki:Commentary/Compiler/EntityTypes data constructor] worker, an imported (global) function, a local -bound function, or &quot;anything else&quot; (e.g., a local lambda-bound function).</p>
<p>The data constructor case checks whether this particular constructor call is saturated. If not, it returns , indicating that we know nothing about the demand type. If so, it returns a  with an empty environment (since there are no free variables), a list of arg-demands based on the  that was passed in to  (that is, the demand on the result of the data constructor call), and a  taken from the constructor Id's strictness signature.</p>
<p>There are a couple of tricky things about the list of arg-demands:</p>
<p><code>* If the result demand (i.e., the passed-in demand) has its box demanded, then we want to make sure the box is demanded in each of the demands for the args. (!ToDo: this may not be true)</code><br />
<code>* If the result demand is not strict, we want to use </code><em><code>n</code></em><code> copies of </code><code> as the list of arg-demands, where </code><em><code>n</code></em><code> is this data constructor's arity.</code></p>
<p>(!ToDo: explain the other cases of )</p>
<p>[wiki:Commentary/Compiler/StrictnessAnalysis/KirstenNotes even more sketchy notes]</p>
<p>[wiki:Commentary/Compiler/StrictnessAnalysis/Examples]</p>
<h1 id="symbol-names">Symbol Names</h1>
<p>Since Haskell allows many symbols in constructor and variable names that C compilers or assembly might not allow (e.g. :, %, #) these have to be encoded using z-encoding. The encoding is as follows. See <a href="GhcFile(compiler/utils/Encoding.hs)" class="uri" title="wikilink">GhcFile(compiler/utils/Encoding.hs)</a>.</p>
<h2 id="tuples">Tuples</h2>
<p>|| Decoded || Encoded || Comment || || `()` || Z0T || Unit / 0-tuple || || || || There is no Z1T || || `(,)` || Z2T || 2-tuple || || `(,,)` || Z3T || 3-tuple || || ... || || And so on ||</p>
<h2 id="unboxed-tuples">Unboxed Tuples</h2>
<p>|| Decoded || Encoded || Comment || || || || There is no Z0H || || `(# #)` || Z1H || unboxed 1-tuple (note the space) || || `(#,#)` || Z2H || unboxed 2-tuple || || `(#,,#)` || Z3H || unboxed 3-tuple || || ... || || And so on ||</p>
<h2 id="alphanumeric-characters">Alphanumeric Characters</h2>
<p>|| Decoded || Encoded || Comment || || a-y, A-Y, 0-9 || a-y, A-Y, 0-9 || Regular letters don't need escape sequences || || z, Z || zz, ZZ || 'Z' and 'z' must be escaped ||</p>
<h2 id="constructor-characters">Constructor Characters</h2>
<p>|| Decoded || Encoded || Comment || || `(` || ZL || Left || || `)` || ZR || Right || || `[` || ZM || 'M' before 'N' in [] || || `]` || ZN || || || `:` || ZC || Colon ||</p>
<h2 id="variable-characters">Variable Characters</h2>
<p>|| Decoded || Encoded || Mnemonic || || `&amp;` || za || Ampersand || || `|` || zb || Bar || || `^` || zc || Caret || || `$` || zd || Dollar || || `=` || ze || Equals || || `&gt;` || zg || Greater than || || `#` || zh || Hash || || `.` || zi || The dot of the 'i' || || `&lt;` || zl || Less than || || `-` || zm || Minus || || `!` || zn || Not || || `+` || zp || Plus || || `'` || zq || Quote || || `\` || zr || Reverse slash || || `/` || zs || Slash || || `*` || zt || Times sign || || `_` || zu || Underscore || || `%` || zv || (TODO: I don't know what the mnemonic for this one is. Perhaps relatiVe or diVide?) ||</p>
<h2 id="other-1">Other</h2>
<p>Any other character is encoded as a 'z' followed by its hex code (lower case, variable length) followed by 'U'. If the hex code starts with 'a', 'b, 'c', 'd', 'e' or 'f', then an extra '0' is placed before the hex code to avoid conflicts with the other escape characters.</p>
<h2 id="examples">Examples</h2>
<p>|| Before || After || || `Trak` || `Trak` || || `foo_wib` || `foozuwib` || || `&gt;` || `zg` || || `&gt;1` || `zg1` || || `foo#` || `foozh` || || `foo##` || `foozhzh` || || `foo##1` || `foozhzh1` || || `fooZ` || `fooZZ` || || `:+` || `ZCzp` || || `()` || `Z0T` || || `(,,,,)` || `Z5T` || || `(# #)` || `Z1H` || || `(#,,,,#)` || `Z5H` ||</p>
<p>[ Up: [wiki:Commentary/Compiler/TypeChecker] ]</p>
<h1 id="the-monad-for-renaming-typechecking-desugaring">The monad for renaming, typechecking, desugaring</h1>
<p>The renamer, typechecker, interface-file typechecker, and desugarer all share a certain amount in common: they must report errors, handle environments, do I/O, etc. Furthermore, because of Template Haskell we have to interleave renaming and typechecking. So all four share a common monad, called . This infrastructure is defined by the following modules:</p>
<p><code> * </code><a href="GhcFile(compiler/utils/IOEnv.lhs)" title="wikilink"><code>GhcFile(compiler/utils/IOEnv.lhs)</code></a><code>: extends the IO monad with an environment (just a simple reader monad).</code><br />
<code> * </code><a href="GhcFile(compiler/typecheck/TcRnTypes)" title="wikilink"><code>GhcFile(compiler/typecheck/TcRnTypes)</code></a><code>: builds the </code><code> monad on top of </code><code>:</code><br />
<code> * </code><a href="GhcFile(compiler/typecheck/TcRnMonad)" title="wikilink"><code>GhcFile(compiler/typecheck/TcRnMonad)</code></a><code>: defines lots of access functions for the renamer, typechecker, and interface typechecker.</code><br />
<code> * </code><a href="GhcFile(compiler/typecheck/DsMonad)" title="wikilink"><code>GhcFile(compiler/typecheck/DsMonad)</code></a><code>: specialises the </code><code> monad for the desugarer.</code></p>
<p>The typechecker and renamer use <em>exactly</em> the same monad, ; the desugarer and interface-file checker use different instantiations of . To give you the idea, here is how the  monad looks:  The details of the global environment type  and local environment type  are also defined in <a href="GhcFile(compiler/typecheck/TcRnTypes.lhs)" class="uri" title="wikilink">GhcFile(compiler/typecheck/TcRnTypes.lhs)</a>. Side effecting operations, such as updating the unique supply, are done with TcRefs, which are simply a synonym for IORefs.</p>
<p>(NB out-of-date, but maybe historically useful; cf [wiki:Debugging/TickyTicky])</p>
<h1 id="kirstens-sketchy-notes-on-getting-ticky-to-work">Kirsten's sketchy notes on getting ticky to work</h1>
<p>Macros for bumping ticky counters are now defined in <a href="GhcFile(includes/Cmm.h)" class="uri" title="wikilink">GhcFile(includes/Cmm.h)</a>. Currently, code compiled with the  flag fails to link because the macros rely on counter variables (things with names like  being declared, but there are actually no declarations for them. I'll add those declarations to <a href="GhcFile(includes/RtsExternal.h)" class="uri" title="wikilink">GhcFile(includes/RtsExternal.h)</a> so I can get something working. Really, there should be something that automatically generates both the macros that are in <a href="GhcFile(includes/Cmm.h)" class="uri" title="wikilink">GhcFile(includes/Cmm.h)</a> and the declarations for the corresponding variables, so that they stay in sync.</p>
<p>Actually, maybe it would make more sense to add a new file,  or something, which contains only ticky counter declarations (the same declarations that still exist in <a href="GhcFile(includes/StgTicky.h)" class="uri" title="wikilink">GhcFile(includes/StgTicky.h)</a>, which isn't used anymore), and that include that from <a href="GhcFile(includes/RtsExternal.h)" class="uri" title="wikilink">GhcFile(includes/RtsExternal.h)</a>.</p>
<p>No -- put actual declarations for counter variables in another file,  or something, and include that only from <a href="GhcFile(rts/Ticky.c)" class="uri" title="wikilink">GhcFile(rts/Ticky.c)</a>; put <em>extern</em> declarations for those counters in , still included from <a href="GhcFile(includes/RtsExternal.h)" class="uri" title="wikilink">GhcFile(includes/RtsExternal.h)</a>. Then later we can automatically generate both  and . The reason for this is that the ticky <strong>macros</strong> are all over the place and they refer to the ticky counters, so the ticky counters have to be <strong>declared</strong> someplace that everyone includes, but of course the actual initializations only need to happen in one place. (Maybe there's a better way to do this...)</p>
<p>No, there don't need to be two files; I was confused. Just .</p>
<p>Huh - we define ticky macros now in  but we can only include that in CMM files and some C files, like , use ticky macros. This makes my brain hurt a little. ''' Index by Title ''' | ''' [RecentChanges Index by Date] '''</p>
<p><a href="TitleIndex(format=group,min=4)" class="uri" title="wikilink">TitleIndex(format=group,min=4)</a></p>
<h1 id="the-ghc-commentary-checking-types">The GHC Commentary: Checking Types</h1>
<p>Probably the most important phase in the frontend is the type checker, which is located at <a href="GhcFile(compiler/typecheck/)" class="uri" title="wikilink">GhcFile(compiler/typecheck/)</a>. GHC type checks programs in their original Haskell form before the desugarer converts them into Core code. This complicates the type checker as it has to handle the much more verbose Haskell AST, but it improves error messages, as those message are based on the same structure that the user sees.</p>
<p>GHC defines the abstract syntax of Haskell programs in <a href="GhcModule(compiler/hsSyn/HsSyn.lhs)" class="uri" title="wikilink">GhcModule(compiler/hsSyn/HsSyn.lhs)</a> using a structure that abstracts over the concrete representation of bound occurences of identifiers and patterns. The module <a href="GhcModule(compiler/typecheck/TcHsSyn.lhs)" class="uri" title="wikilink">GhcModule(compiler/typecheck/TcHsSyn.lhs)</a> defines a number of helper function required by the type checker. Note that the type <a href="GhcModule(compiler/typecheck/TcRnTypes.lhs)" class="uri" title="wikilink">GhcModule(compiler/typecheck/TcRnTypes.lhs)</a>.`TcId` used to represent identifiers in some signatures during type checking is, in fact, nothing but a synonym for a [wiki:Commentary/Compiler/EntityTypes#Typevariablesandtermvariables plain Id].</p>
<p>It is also noteworthy, that the representations of types changes during type checking from `HsType` to `TypeRep.Type`. The latter is a [wiki:Commentary/Compiler/TypeType hybrid type] representation that is used to type Core, but still contains sufficient information to recover source types. In particular, the type checker maintains and compares types in their `Type` form.</p>
<h2 id="the-overall-flow-of-things">The Overall Flow of Things</h2>
<p><code>* `TcRnDriver` is the top level.  It calls</code><br />
<code>  * `TcTyClsDecls`: type and class declaration</code><br />
<code>  * `TcInstDcls`: instance declarations</code><br />
<code>  * `TcBinds`: value bindings</code><br />
<code>    * `TcExpr`: expressions</code><br />
<code>    * `TcMatches`: lambda, case, list comprehensions</code><br />
<code>    * `TcPat`: patterns</code><br />
<code>  * `TcForeign`: FFI declarations</code><br />
<code>  * `TcRules`: rewrite rules</code><br />
<code>  * `TcHsTypes`: kind-checking type signatures</code><br />
<code>  * `TcValidity`: a second pass that walks over things like types or type constructors, checking a number of extra side conditions.</code></p>
<p><code>* The constraint solver consists of:</code><br />
<code>  * `TcSimplify`: top level of the constraint solver</code><br />
<code>  * `TcCanonical`: canonicalising constraints</code><br />
<code>  * `TcInteract`: solving constraints where they interact with each other</code><br />
<code>  * `TcTypeNats`: solving natural-number constraints</code><br />
<code>  * `TcSMonad`: the monad of the constraint solver (built on top of the main typechecker monad)</code><br />
<code>  * `TcEvidence`: the data types used for evidence (mostly pure)</code><br />
<code>  * `TcUnify`: solves unification constraints &quot;on the fly&quot;; if it can't, it generates a constraint for the constraint solver to deal with later</code><br />
<code>  * `TcErrors`: generates good error messages from the residual, unsolved constraints.</code><a href="BR" title="wikilink"><code>BR</code></a><br />
<code>The best place reading for the constraint solver is the paper </code><a href="http://www.haskell.org/haskellwiki/Simonpj/Talk:OutsideIn"><code>Modular</code> <code>type</code> <code>inference</code> <code>with</code> <code>local</code> <code>assumptions</code></a></p>
<p><code>* Underlying infrastructure:</code><br />
<code>  * `TcRnTypes`: a big collection of the types used during type checking</code><br />
<code>  * [wiki:Commentary/Compiler/TcRnMonad TcRnMonad]: the main typechecker monad</code><br />
<code>  * `TcType`: pure functions over types, used by the type checker</code><br />
<code>  </code></p>
<h3 id="entry-points-into-the-type-checker">Entry Points Into the Type Checker</h3>
<p>The interface of the type checker (and [wiki:Commentary/Compiler/Renamer renamer]) to the rest of the compiler is provided by <a href="GhcModule(compiler/typecheck/TcRnDriver.lhs)" class="uri" title="wikilink">GhcModule(compiler/typecheck/TcRnDriver.lhs)</a>. Entire modules are processed by calling `tcRnModule` and GHCi uses `tcRnStmt`, `tcRnExpr`, and `tcRnType` to typecheck statements and expressions, and to kind check types, respectively. Moreover, `tcTopSrcDecls` is used by Template Haskell - more specifically by `TcSplice.tc_bracket` - to type check the contents of declaration brackets.</p>
<h3 id="renaming-and-type-checking-a-module">Renaming and Type Checking a Module</h3>
<p>The functions `tcRnModule` and `tcRnModuleTcRnM` control the complete static analysis of a Haskell module. They set up the combined renamer and type checker monad, resolve all import statements, take care of hi-boot files, initiate the actual renaming and type checking process, and finally, wrap off by processing the export list.</p>
<p>The actual type checking and renaming process is initiated via `TcRnDriver.tcRnSrcDecls`, which uses a helper called `tc_rn_src_decls` to implement the iterative renaming and type checking process required by <a href="http://darcs.haskell.org/ghc/docs/comm/exts/th.html">Template Haskell</a> (TODO: Point at new commentary equivalent). After it invokes `tc_rn_src_decls`, it simplifies type constraints and zonking (see below regarding the later).</p>
<p>The function `tc_rn_src_decls` partitions static analysis of a whole module into multiple rounds, where the initial round is followed by an additional one for each toplevel splice. It collects all declarations up to the next splice into an `HsDecl.HsGroup`. To rename and type check that declaration group it calls `TcRnDriver.rnTopSrcDecls` and `TcRnDriver.tcTopSrcDecls`. Afterwards, it executes the splice (if there are any left) and proceeds to the next group, which includes the declarations produced by the splice.</p>
<p>The renamer, apart from renaming, computes the global type checking environment, of type `TcRnTypes.TcGblEnv`, which is stored in the [wiki:Commentary/Compiler/TcRnMonad type checking monad] before type checking commences.</p>
<h2 id="type-checking-a-declaration-group">Type Checking a Declaration Group</h2>
<p>The type checking of a declaration group, performed by `tcTopSrcDecls` and its helper function `tcTyClsInstDecls`, starts by processing of the type and class declarations of the current module, using the function `TcTyClsDecls.tcTyAndClassDecls`. This is followed by a first round over instance declarations using `TcInstDcls.tcInstDecls1`, which in particular generates all additional bindings due to the deriving process. Then come foreign import declarations (`TcForeign.tcForeignImports`) and default declarations (`TcDefaults.tcDefaults`).</p>
<p>Now, finally, toplevel value declarations (including derived ones) are type checked using `TcBinds.tcTopBinds`. Afterwards, `TcInstDcls.tcInstDecls2` traverses instances for the second time. Type checking concludes with processing foreign exports (`TcForeign.tcForeignExports`) and rewrite rules (`TcRules.tcRules`). Finally, the global environment is extended with the new bindings.</p>
<h2 id="type-checking-type-and-class-declarations">Type checking Type and Class Declarations</h2>
<p>Type and class declarations are type checked in a couple of phases that contain recursive dependencies - aka <em>knots</em>. The first knot encompasses almost the whole type checking of these declarations and forms the main piece of `TcTyClsDecls.tcTyAndClassDecls`.</p>
<p>Inside this big knot, the first main operation is kind checking, which again involves a knot. It is implemented by `kcTyClDecls`, which performs kind checking of potentially recursively-dependent type and class declarations using kind variables for initially unknown kinds. During processing the individual declarations some of these variables will be instantiated depending on the context; the rest gets by default kind * (during <em>zonking</em> of the kind signatures). Type synonyms are treated specially in this process, because they can have an unboxed type, but they cannot be recursive. Hence, their kinds are inferred in dependency order. Moreover, in contrast to class declarations and other type declarations, synonyms are not entered into the global environment as a global `TyThing`. (`TypeRep.TyThing` is a sum type that combines the various flavours of typish entities, such that they can be stuck into type environments and similar.)</p>
<h2 id="more-details">More Details</h2>
<h3 id="types-variables-and-zonking">Types Variables and Zonking</h3>
<p>During type checking type variables are represented by mutable variables - cf. the <a href="http://darcs.haskell.org/ghc/docs/comm/the-beast/vars.html#TyVar">variable story</a> (TODO: Point at new commentary equivalent). Consequently, unification can instantiate type variables by updating those mutable variables. This process of instantiation is (for reasons that elude me) called <a href="http://dictionary.reference.com/browse/zonk">zonking</a> in GHC's sources. The zonking routines for the various forms of Haskell constructs are responsible for most of the code in the module <a href="GhcModule(compiler/typecheck/TcHsSyn.lhs)" class="uri" title="wikilink">GhcModule(compiler/typecheck/TcHsSyn.lhs)</a>, whereas the routines that actually operate on mutable types are defined in <a href="GhcModule(compiler/typecheck/TcMType.lhs)" class="uri" title="wikilink">GhcModule(compiler/typecheck/TcMType.lhs)</a>; this includes the zonking of type variables and type terms, routines to create mutable structures and update them as well as routines that check constraints, such as that type variables in function signatures have not been instantiated during type checking. The actual type unification routine is `uTys` in the module <a href="GhcModule(compiler/typecheck/TcUnify.lhs)" class="uri" title="wikilink">GhcModule(compiler/typecheck/TcUnify.lhs)</a>.</p>
<p>All type variables that may be instantiated (those in signatures may not), but haven't been instantiated during type checking, are zonked to `()`, so that after type checking all mutable variables have been eliminated.</p>
<h3 id="type-representation">Type Representation</h3>
<p>The representation of types is fixed in the module <a href="GhcModule(compiler/types/TypeRep.lhs)" class="uri" title="wikilink">GhcModule(compiler/types/TypeRep.lhs)</a> and exported as the data type `Type`. Read the comments in the `TypeRep` module! A couple of points:</p>
<p><code>* Type synonym applications are represented as a `TyConApp` with a `TyCon` that contains the expansion.  The expansion is done on-demand by `Type.coreView`.  Unexpanded type synonyms are useful for generating comprehensible error messages.</code></p>
<p><code>* The `PredTy` constructor wraps a type constraint argument (dictionary, implicit parameter, or equality).  They are expanded on-demand by `coreView`.</code></p>
<p>As explained in <a href="GhcModule(compiler/typecheck/TcType.lhs)" class="uri" title="wikilink">GhcModule(compiler/typecheck/TcType.lhs)</a>, GHC supports rank-N types, but during type inference maintains the restriction that type variables cannot be instantiated to quantified types (i.e., the type system is predicative). However the type system of Core is fully impredicative.</p>
<h3 id="type-checking-environment">Type Checking Environment</h3>
<p>During type checking, GHC maintains a <em>type environment</em> whose type definitions are fixed in the module <a href="GhcModule(compiler/typecheck/TcRnTypes.lhs)" class="uri" title="wikilink">GhcModule(compiler/typecheck/TcRnTypes.lhs)</a> with the operations defined in <a href="GhcModule(compiler/typecheck/TcEnv.lhs)" class="uri" title="wikilink">GhcModule(compiler/typecheck/TcEnv.lhs)</a>. Among other things, the environment contains all imported and local instances as well as a list of <em>global</em> entities (imported and local types and classes together with imported identifiers) and <em>local</em> entities (locally defined identifiers). This environment is threaded through the [wiki:Commentary/Compiler/TcRnMonad type checking monad].</p>
<h3 id="expressions-1">Expressions</h3>
<p>Expressions are type checked by <a href="GhcModule(compiler/typecheck/TcExpr)" class="uri" title="wikilink">GhcModule(compiler/typecheck/TcExpr)</a>.</p>
<p>Usage occurences of identifiers are processed by the function tcId whose main purpose is to [#HandlingofDictionariesandMethodInstances instantiate overloaded identifiers]. It essentially calls `TcInst.instOverloadedFun` once for each universally quantified set of type constraints. It should be noted that overloaded identifiers are replaced by new names that are first defined in the LIE (Local Instance Environment?) and later promoted into top-level bindings.</p>
<h3 id="handling-of-dictionaries-and-method-instances">Handling of Dictionaries and Method Instances</h3>
<p>GHC implements overloading using so-called <em>dictionaries</em>. A dictionary is a tuple of functions -- one function for each method in the class of which the dictionary implements an instance. During type checking, GHC replaces each type constraint of a function with one additional argument. At runtime, the extended function gets passed a matching class dictionary by way of these additional arguments. Whenever the function needs to call a method of such a class, it simply extracts it from the dictionary.</p>
<p>This sounds simple enough; however, the actual implementation is a bit more tricky as it wants to keep track of all the instances at which overloaded functions are used in a module. This information is useful to optimise the code. The implementation is the module <a href="GhcModule(compiler/typecheck/Inst.lhs)" class="uri" title="wikilink">GhcModule(compiler/typecheck/Inst.lhs)</a>.</p>
<p>The function `instOverloadedFun` is invoked for each overloaded usage occurrence of an identifier, where overloaded means that the type of the identifier contains a non-trivial type constraint. It proceeds in two steps: (1) Allocation of a method instance (`newMethodWithGivenTy`) and (2) instantiation of functional dependencies. The former implies allocating a new unique identifier, which replaces the original (overloaded) identifier at the currently type-checked usage occurrence.</p>
<p>The new identifier (after being threaded through the LIE) eventually will be bound by a top-level binding whose rhs contains a partial application of the original overloaded identifier. This papp applies the overloaded function to the dictionaries needed for the current instance. In GHC lingo, this is called a <em>method</em>. Before becoming a top-level binding, the method is first represented as a value of type Inst.Inst, which makes it easy to fold multiple instances of the same identifier at the same types into one global definition. (And probably other things, too, which I haven't investigated yet.)</p>
<p><strong>Note:</strong> As of 13 January 2001 (wrt. to the code in the CVS HEAD), the above mechanism interferes badly with RULES pragmas defined over overloaded functions. During instantiation, a new name is created for an overloaded function partially applied to the dictionaries needed in a usage position of that function. As the rewrite rule, however, mentions the original overloaded name, it won't fire anymore -- unless later phases remove the intermediate definition again. The latest CVS version of GHC has an option '-fno-method-sharing', which avoids sharing instantiation stubs. This is usually/often/sometimes sufficient to make the rules fire again.</p>
<h2 id="connection-with-ghcs-constraint-solver">Connection with GHC's Constraint Solver</h2>
<p>The solver for the type nats is implemented as an extra stage in GHC's constrraint solver (see `TcInteract.thePipeline`).</p>
<p>The following modules contain most of the code relevant for the solver:</p>
<p><code> * `TcTypeNats`:      The main solver machinery      </code><br />
<code> * `TcTypeNatsRules`: The rules used by the solver</code><br />
<code> * `TcTYpeNatsEval`:  Functions for direct evaluation on constants</code></p>
<h2 id="generating-evidence">Generating Evidence</h2>
<p>The solver produces evidence (i.e., proofs) when computing new &quot;given&quot; constraints, or when solving existing &quot;wanted&quot; constraints. The evidence is constructed by applications of a set of pre-defined rules. The rules are values of type `TypeRep.CoAxiomRule`. Conceptually, rules have the form:  The rules have the usual logical meaning: the variables are universally quantified, and the assumptions imply the concluson. As a concrete example, consider the rule for left-cancellation of addtion: </p>
<p>The type `CoAxiomRule` also supports infinte literal-indexed families of simple axioms using constructor `CoAxiomTyLit`. These have the form:  In this case `conclusion` is an equation that contains no type variables but may depend on the literals in the name of the family. For example, the basic definitional axiom for addition, `TcTypeNatsRules.axAddDef`, uses this mechanism:  At present, the assumptions and conclusion of all rules are equations between types but this restriction is not important and could be lifted in the future.</p>
<p>The rules used by the solver are in module `TcTypeNatsRules`.</p>
<h2 id="the-solver">The Solver</h2>
<p>The entry point to the solver is `TcTypeNats.typeNatStage`.</p>
<p>We start by examining the constraint to see if it is obviously unsolvable (using function `impossible`), and if so we stash it in the constraint-solver's state and stop. Note that there is no assumption that `impossible` is complete, but it is important that it is sound, so if `impossible` returns `True`, then the constraint is definitely unsolvable, but if `impossible` returns `False`, then we don't know if the constraint is solvable or not.</p>
<p>The rest of the stage proceeds depending on the type of constraint, as follows.</p>
<h3 id="given-constraints">Given Constraints</h3>
<p>Given constraints correspond to adding new assumptions that may be used by the solver. We start by checking if the new constraint is trivial (using function `solve`). A constraint is considered to be trivial if it matches an already existing constraint or a rule that is known to the solver. Such given constraints are ignored because they do not contribute new information. If the new given is non-trivial, then it will be recorded to the inert set as a new fact, and we proceed to &quot;interact&quot; it with existing givens, in the hope of computing additional useful facts (function `computeNewGivenWork`).</p>
<p>IMPORTANT: We assume that &quot;given&quot; constraints are processed before &quot;wanted&quot; ones. A new given constraint may be used to solve any existing wanted, so every time we added a new given to the inert set we should move all potentially solvable &quot;wanted&quot; constraint from the inert set back to the work queue. We DON'T do this, because it is quite inefficient: there is no obvious way to compute which &quot;wanted&quot;s might be affected, so we have to restart all of them!</p>
<p>The heart of the interaction is the function `interactCt`, which performs one step of &quot;forward&quot; reasoning. The idea is to compute new constraints whose proofs are made by an application of a rule to the new given, and some existing givens. These new constraints are added as new work, to be processed further on the next iteration of GHC's constraint solver.</p>
<p>Aside: when we compute the new facts, we check to see if any are obvious contradictions. This is not strictly necessary because they would be detected on the next iteration of the solver. However, by doing the check early we get slightly better error messages because we can report the original constraint as being unsolvable (it leads to a contradiction), which tends to be easier to relate to the original program. Of course, this is not completely fool-proof---it is still possible that a contradiction is detected at a later iteration. An alternative idea---not yet implemented---would be to examine the proof of a contradiction and extract the original constraints that lead to it in the first place.</p>
<h3 id="derived-constraints">Derived Constraints</h3>
<p>``Derived`` constraints are facts that are implied by the constraints in the inert set. They do not have complete proofs because they may depend on proofs of as yet unsolved wanted constraints. GHC does not associate any proof terms with derived constraints (to keep things simple?). In the constraint solver, they are mostly used as &quot;hints&quot;. For example, consider the wanted constraint , where  is a free unification variable. These are the steps we'll take to solve the constraint:</p>
<p></p>
<p>The type-nat solver processes derived constraints in a similar fashion to given constraints (`computeNewDerivedWork`): it checks to see if they are trivially known and, if not, then it tries to generate some additional derived constraints. The main difference is that derived constraints can be interacted with all existing constraints to produce new facts, while given constraints only interact with other givens.</p>
<h3 id="wanted-constraints">Wanted Constraints</h3>
<p>The main purpose of the solver is to discharge ``wanted`` constraints (the purpose of processing given and derived constraints is to help solve existing wanted goals). When we encounter a new wanted goals we proceed as follows:</p>
<p><code>   1. Try to solve the goal, using a few different strategies:</code><br />
<code>       1. Try to see if it matches the conclusion of an iff rule (`solveIff`). Aassumptions of rule become new wanted work.</code><br />
<code>       2. Try to see if it matches an axiom exactly (`solve`)</code><br />
<code>       3. Try the ordering solver for `&lt;=` goals (`solveLeq`)</code><br />
<code>       4. Try to use a (possibly synthesized) assumption</code></p>
<p><code>   2. If that didn't work:</code><br />
<code>     1. Wanted is added to the inert set</code><br />
<code>     2. Check to see if any of the existing wanteds in the inert set can be solved in terms of the new goal (`reExamineWanteds`)</code><br />
<code>     3. Generate new derived facts.</code></p>
<h4 id="using-iff-rules">Using IFF Rules</h4>
<p>These rules are used to replace a wanted constraint with a collection of logically equivalent wanted constraints. If a wanted constraint matches the head of one of these rules, than it is solved using the rules, and the we generate new wanted constraints for the rule's assumptions.</p>
<p>The following are important properties of IFF rules:</p>
<p><code> * They need to be sound (of course!)</code><br />
<code> * The assumptions need to be logically equivalent to the conclusion (i.e., they should not result in a harder problem to solve than the original goal).</code><br />
<code> * The assumptions need to be </code><em><code>simpler</code></em><code> from the point of view of the constraint solver (i.e., we shouldn't end up with the original goal after some steps---this would lead to non-termination).</code></p>
<p>At present, IFF rules are used to define certain operators in terms of others. For example, this is the only rule for solving constraints about subtraction: </p>
<h4 id="using-axioms">Using Axioms</h4>
<p>Basic operators are defined with an infinite family of axiom schemes. As we can't have these written as a long list (searching might never stop!), we have some custom code that checks to see if a constraint might be solvable using one of the definitional axioms (see `solveWithAxiom`, `byAxiom`).</p>
<h4 id="using-the-order-model">Using the Order Model</h4>
<p>Constraints about the ordering of type-level numbers are kept in a datastructure (`LeqFacts`) which forms a ``model'' of the information represented by the constraints (in a similar fashion to how substitutions form a model for a set of equations).</p>
<p>The purpose of the model is to eliminate redundant constraints, and to make it easy to find proofs for queries of the form `x &lt;= y`. In practise, of particular interest are questions such as `1 &lt;= x` because these appear as assumptions on a number of rules (e.g., cancellation of multiplication). In the future, this model could also be used to implement an interval analysis, which would compute intervals approximating the values of variables.</p>
<p>TODO: At present, this model is reconstructed every time it needs to be used, which is a bit inefficient. Perhaps it'd be better to use this directly as the representation of `&lt;=` constraints in the inert set.</p>
<p>The model is a directed acyclic graph, as follows:</p>
<p><code> * vertices: constants or variables (of kind `Nat`)</code><br />
<code> * edges: the edge from `A` to `B` is a proof that `A &lt;= B`.</code></p>
<p>So, to find a proof of `A &lt;= B`, we insert `A` and `B` in the model, and then look for a path from `A` to `B`. The proofs on the path can be composed using the rule for transitivity of `&lt;=` to form the final proof.</p>
<p>When manipulating the model, we maintain the following &quot;minimality&quot; invariant: there should be no direct edge between two vertices `A` and `B`, if there is a path that can already get us from `A` to `B. Here are some examples (with edges pointing upwards) </p>
<p>The purpose of the invariant is to eliminate redundant information. Note, however, that it does not guarantee that there is a unique way to prove a goal.</p>
<h4 id="using-extended-assumptions">Using Extended Assumptions</h4>
<p>Another way to prove a goal is to look it up in the assumptions. If the goal matched an assumption exactly, then GHC would have already solved it in one of its previous stages of the constraint solver. However, due to the commutativity and associativity of some of the operators, it is possible to have goal that could be solved by assumption, only if the assumption was &quot;massaged&quot; a bit.</p>
<p>This &quot;massaging&quot; is implemented by the function `widenAsmps`, which extends the set of assumption by performing a bit of forward reasoning using a limited set of rules. Typically, these are commutativity an associativity rules, and the `widenAsmps` function tries to complete the set of assumptions with respect to these operations. For example: </p>
<p>Note that the extended assumptions are very similar to derived constraints, except that we keep their proofs.</p>
<h4 id="re-examining-wanteds">Re-examining Wanteds</h4>
<p>If none of the strategies for solving a wanted constraint worked, then the constraint is added to the inert set. Since we'd like to keep the inert set minimal, we have to see if any of the existing wanted constraints might be solvable in terms of the new wanted (`reExamineWanteds`).</p>
<p>It is good to keep the inert set minimal for the following reasons:</p>
<p><code> * Inferred types are nicer,</code><br />
<code> * It helps GHC to solve constraints by &quot;inlining&quot; (e.g., if we</code><br />
<code>   have only a single constraint `x + y ~ z`, then we can eliminate it</code><br />
<code>   by replacing all occurrences of `z` with `x + y`, however we can't</code><br />
<code>   do that if we ended up with two constraints `(x + y ~ z, y + x ~ z)).</code></p>
<p>We consider each (numeric) wanted constraint in the inert set and check if we can solve it in terms of the new wanted and all other wanteds. If so, then it is removed from the inert set, otherwise it stays there.</p>
<p>Note that we can't implement this by kicking out the existing wanted constraints and putting them back on the work queue, because this would lead to non-termination. Here is an example of how this might happen: </p>
<p>Perhaps there is a way around this but, for the moment, we just re-examine the numeric wanteds locally, without going through the constraint solver pipe-line.</p>
<p><a href="PageOutline" class="uri" title="wikilink">PageOutline</a></p>
<h1 id="the-data-type-and-its-friends-1">The data type  and its friends</h1>
<p>GHC compiles a typed programming language, and GHC's intermediate language is explicitly typed. So the data type that GHC uses to represent types is of central importance.</p>
<p>The single data type  is used to represent</p>
<p><code>* Types (possibly of higher kind); e.g. `[Int]`, `Maybe`</code><br />
<code>* Kinds (which classify types and coercions); e.g. `(* -&gt; *)`, `T :=: [Int]`.  See [wiki:Commentary/Compiler/Kinds]</code><br />
<code>* Sorts (which classify types); e.g. `TY`, `CO`</code></p>
<p>GHC's use of [wiki:Commentary/Compiler/FC coercions and equality constraints] is important enough to deserve its own page.</p>
<p>The module  exposes the representation because a few other modules (, , , etc) work directly on its representation. However, you should not lightly pattern-match on ; it is meant to be an abstract type. Instead, try to use functions defined by ,  etc.</p>
<h2 id="views-of-types-1">Views of types</h2>
<p>Even when considering only types (not kinds, sorts, coercions) you need to know that GHC uses a <em>single</em> data type for types. You can look at the same type in different ways:</p>
<p><code>* The &quot;typechecker view&quot; regards the type as a Haskell type, complete with implicit parameters, class constraints, and the like.  For example:</code></p>
<p></p>
<p><code>Functions in `TcType` take this view of types; e.g. `tcSplitSigmaTy` splits up a type into its forall'd type variables, its constraints, and the rest.</code></p>
<p><code>* The &quot;core view&quot; regards the type as a Core-language type, where class and implicit parameter constraints are treated as function arguments:</code></p>
<p></p>
<p><code>Functions in `Type` take this view.</code></p>
<p>The data type `Type` represents type synonym applications in un-expanded form. E.g.  Here `f`'s type doesn't look like a function type, but it really is. The function `Type.coreView :: Type -&gt; Maybe Type` takes a type and, if it's a type synonym application, it expands the synonym and returns `Just <expanded-type>`. Otherwise it returns `Nothing`.</p>
<p>Now, other functions use `coreView` to expand where necessary, thus:  Notice the first line, which uses the view, and recurses when the view 'fires'. Since `coreView` is non-recursive, GHC will inline it, and the optimiser will ultimately produce something like: </p>
<h2 id="the-representation-of-1">The representation of </h2>
<p>Here, then is the representation of types (see <a href="GhcFile(compiler/types/TypeRep.hs)" class="uri" title="wikilink">GhcFile(compiler/types/TypeRep.hs)</a> for more details): </p>
<p>Invariant: if the head of a type application is a , GHC <em>always</em> uses the  constructor, not . This invariant is maintained internally by 'smart constructors'. A similar invariant applies to ;  is never used with an arrow type.</p>
<p>Type variables are represented by the `TyVar` constructor of the [wiki:Commentary/Compiler/EntityTypes data type Var].</p>
<h2 id="overloaded-types-1">Overloaded types</h2>
<p>In Haskell we write  but in Core the `=&gt;` is represented by an ordinary `FunTy`. So f's type looks like this:  Nevertheless, we can tell when a function argument is actually a predicate (and hence should be displayed with `=&gt;`, etc), using  The various forms of predicate can be extracted thus:  These functions are defined in module `Type`.</p>
<h2 id="classifying-types-1">Classifying types</h2>
<p>GHC uses the following nomenclature for types:</p>
<p><strong><code>Unboxed</code></strong><code>:: A type is unboxed iff its representation is other than a pointer. Unboxed types are also unlifted.</code></p>
<p><strong><code>Lifted</code></strong><code>:: A type is lifted iff it has bottom as an element. Closures always have lifted types:  i.e. any let-bound identifier in Core must have a lifted type.  Operationally, a lifted object is one that can be entered. Only lifted types may be unified with a type variable.</code></p>
<p><strong><code>Data</code></strong><code>:: A type declared with </code><strong></strong><code>.  Also boxed tuples.</code></p>
<p><strong><code>Algebraic</code></strong><code>:: An algebraic data type is a data type with one or more constructors, whether declared with </code><code> or </code><code>.   An algebraic type is one that can be deconstructed with a case expression.  &quot;Algebraic&quot; is </code><strong><code>NOT</code></strong><code> the same as &quot;lifted&quot;,  because unboxed (and thus unlifted) tuples count as &quot;algebraic&quot;.</code></p>
<p><strong><code>Primitive</code></strong><code>:: a type is primitive iff it is a built-in type that can't be expressed    in Haskell.</code><br />
<code> </code><br />
<code> Currently, all primitive types are unlifted, but that's not necessarily the case.  (E.g. Int could be primitive.)</code></p>
<p><code> Some primitive types are unboxed, such as Int#, whereas some are boxed but unlifted (such as `ByteArray#`).  The only primitive types that we classify as algebraic are the unboxed tuples.</code></p>
<p>Examples of type classifications:</p>
<p>|| || <strong>Primitive</strong> || <strong>Boxed</strong> || <strong>Lifted</strong> || <strong>Algebraic</strong> || || `Int#` || Yes || No || No || No || || `ByteArray#` || Yes || Yes || No || No || || `(# a, b #)` || Yes || No || No || Yes || || `( a, b )` || No || Yes || Yes || Yes || || `[a]` || No || Yes || Yes || Yes ||</p>
<h2 id="unique-1">Unique</h2>
<p>`Unique`s provide a fast comparison mechanism for more complex things. Every `RdrName`, `Name`, `Var`, `TyCon`, `TyVar`, etc. has a `Unique`. When these more complex structures are collected (in `UniqFM`s or other types of collection), their `Unique` typically provides the key by which the collection is indexed.</p>
<hr />
<h2 id="current-design">Current design</h2>
<p>A `Unique` consists of the <em>domain</em> of the thing it identifies and a unique integer value 'within' that domain. The two are packed into a single `Int#`, with the <em>domain</em> being the top 8 bits.</p>
<p>The domain is never inspected (SLPJ believes). The sole reason for its existence is to provide a number of different ranges of `Unique` values that are guaranteed not to conflict.</p>
<p>=== Lifetime</p>
<p>The lifetime of a `Unique` is a single invocation of GHC, i.e. they must not 'leak' to compiler output, the reason being that `Unique`s may be generated/assigned non-deterministically. When compiler output is non-deterministic, it becomes significantly harder to, for example, [wiki:Commentary/Compiler/RecompilationAvoidance avoid recompilation]. Uniques do not get serialised into .hi files, for example.</p>
<p>Note, that &quot;one compiler invocation&quot; is not the same as the compilation of a single `Module`. Invocations such as `ghc --make` or `ghc --interactive` give rise to longer invocation life-times.</p>
<p>This is also the reasons why `OccName`s are <em>not</em> ordered based on the `Unique`s of their underlying `FastString`s, but rather <em>lexicographically</em> (see <a href="GhcFile(compiler/basicTypes/OccName.lhs)" class="uri" title="wikilink">GhcFile(compiler/basicTypes/OccName.lhs)</a> for details). &gt; &gt; <strong>SLPJ:</strong> I am far from sure that the Ord instance for `OccName` is ever used, so this remark is probably misleading. Try deleting it and see where it is used (if at all). &gt; <strong>PKFH:</strong> At least `Name` and `RdrName` (partially) define their own `Ord` instances in terms of the instance of `OccName`. Maybe these `Ord` instances are also redundant, but for now it seems wise to keep them in. When everything has `Data` instances (after this and many other redesigns), I'm sure it will be easier to find such dependency relations.</p>
<h3 id="known-key-things">Known-key things</h3>
<p>A hundred or two library entities (types, classes, functions) are so-called &quot;known-key things&quot;. See [wiki:Commentary/Compiler/WiredIn this page]. A known-key thing has a fixed `Unique` that is fixed when the compiler is built, and thus lives across all invocations of that compiler. These known-key `Unique`s <em>are</em> written into .hi files. But that's ok because they are fully deterministic and never change.</p>
<p>&gt; <strong>PKFH</strong> That's fine then; we also know for sure these things fit in the 30 bits used in the `hi`-files. I'll comment appropriately.</p>
<h3 id="interface-files-1">Interface files</h3>
<p>Entities in a interface file (.hi file) are, for the most part, stored in a symbol table, and referred to (from elsewhere in the same interface file) by an index into that table. Here are the details from <a href="GhcFile(compiler/iface/BinIface.lhs)" class="uri" title="wikilink">GhcFile(compiler/iface/BinIface.lhs)</a>: </p>
<hr />
<h2 id="redesign-2014-1">Redesign (2014)</h2>
<p>=== TL;DR The redesign is to accomplish the following:</p>
<p><code>* Allow derivation of type class instances for `Unique`</code><br />
<code>* Restore invariants from the original design; hide representation details</code><br />
<code>* Eliminate violations of invariants and design-violations in other places of the compiler (e.g. `Unique`s shouldn't be written to `hi`-files, but are).</code></p>
<p>&gt; &gt; <strong>SLPJ</strong> I don't think this is a design violation; see above. Do you have any other examples in mind? &gt; <strong>PKFH</strong> Not really of design-violations (and no other compiler-output stuff) other than the invariants mentioned above it, just yet. The key point, though, is that there are a lot of comments in `Unique` about not exporting things so that we know X, Y and Z, but then those things <em>are</em> exported, so we don't know them to be true. Case in point is the export of `mkUnique`, but also `mkUniqueGrimily`. The latter has a comment 'only for `UniqSupply`' but is also used in other places (like Template Haskell). One redesign is to put this restriction in the name, so there still is the facility offered by `mkUniqueGrimily`, but now it's called `mkUniqueOnlyForUniqSupply` (and `mkUniqueOnlyForTemplateHaskell`), the ugliness of which should help, over time, to get rid of them.</p>
<p>=== Longer</p>
<p>In an attempt to give more of GHC's innards well-behaved instances of `Typeable`, `Data`, `Foldable`, `Traversable`, etc. the implementation of `Unique`s was a bit of a sore spot. They were implemented (20+ years earlier) using custom boxing, viz.  making automatic derivation of such type class instances hard. There was already a comment asking why it wasn't simply a `newtype` around a normal (boxed) `Int`. Independently, there was some discussion on the mailinglists about the use of (signed) `Int`s in places where `Word`s would be more appropriate. Further inspection of the `Unique` implementation made clear that a lot of invariants mentioned in comments had been violated by incremental edits. This is discussed in more detail below, but these things together (the desire for automatic derivation and the restoration of some important invariants) motivated a moderate redesign.</p>
<p>=== Status Quo (pre redesign)</p>
<p>A `Unique` has a domain (`TyCon`, `DataCon`, `PrelName`, `Builtin`, etc.) that was codified by a character. The remainder of the `Unique` was an integer that should be unique for said domain. This <strong>was</strong> once guaranteed through the export list of <a href="GhcFile(compiler/basicTypes/Unique.lhs)" class="uri" title="wikilink">GhcFile(compiler/basicTypes/Unique.lhs)</a>, where direct access to the domain-character was hidden, i.e.  were not exported. This should have guaranteed that every domain was assigned its own unique character, because only in <a href="GhcFile(compiler/basicTypes/Unique.lhs)" class="uri" title="wikilink">GhcFile(compiler/basicTypes/Unique.lhs)</a> could those `Char`s be assigned. However, through  this separation of concerns leaked out to <a href="GhcFile(compiler/basicTypes/UniqSupply.lhs)" class="uri" title="wikilink">GhcFile(compiler/basicTypes/UniqSupply.lhs)</a>, because its `Int` argument is the <em>entire</em> `Unique` and not just the integer part 'under' the domain character. &gt; &gt; <strong>SLPJ</strong> OK, but to eliminate `mkUniqueGrimily` you need to examine the calls, decide how to do it better, and document the new design. &gt; <strong>PKFH</strong> See above; the solution for now is `mkUniqueOnlyForUniqSupply`. A separate patch will deal with trying to refactor/redesign `UniqSupply` if this is necessary.</p>
<p>The function `mkSplitUniqSupply` made the domain-character accessible to all the other modules, by having a wholly separate implementation of the functionality of `mkUnique`.</p>
<p>Where the intention was still to have a clean interface, the (would-be) hidden `mkUnique` is only called by functions defined in the `Unique` module with the corresponding character, e.g. </p>
<p>=== New plan</p>
<p>In the new design, the domains are explicitly encoded in a sum-type `UniqueDomain`. At the very least, this should help make the code a little more self-documenting <em>and</em> prevent accidental overlap in the choice of bits to identify the domain. Since the purpose of `Unique`s is to provide <em>fast</em> comparison for different types of things, the redesign should remain performance concious. With this in mind, keeping the `UniqueDomain` and the integer-part explicitly in the type  seems unwise, but by choosing  we win the ability to automatically derive things and should also be able to test how far optimisation has come in the past 20+ years; does default boxing with `newtype`-style wrapping have (nearly) the same performance as manual unboxing? This should follow from the tests.</p>
<p>The encoding is kept the same, i.e. the `Word` is still built up with the domain encoded in the most significant bits and the integer-part in the remaining bits. However, instead encoding the domain as a `Char` in the (internal <em>and</em> external interface), we now create an ADT (sum-type) that encodes the domain. This has two advantages. First, it prevents people from picking domain-tags ad hoc an possibly overlapping. Second, encoding in the `Word` does not rely on the assumption that the domain requires and/or fits in 8 bits. Since Haskell `Char`s are unicode, the 8-bit assumption is wrong for the old design. In other words, the above examples are changed to:</p>
<p></p>
<p>Ideal world scenario, the entire external interface would be:  and the instances for `Eq`, `Ord`, `Data`, etc. For now, though, it will also have </p>
<p>&gt; &gt; <strong>SLPJ</strong> I agree that a `newtype` around a `Word` is better than a `data` type around `Int#`. That is a small, simple change. But I think you plan to do more than this, and that &quot;more&quot; is not documented here. E.g. what is the new API to `Unique`? &gt; <strong>PKFH</strong> Added. See above.</p>
<h1 id="unpacking-primitive-fields">Unpacking primitive fields</h1>
<p>This page describes a proposal to automatically unpack (strict) primitive fields. A primitive fields is a field that when unpacked has a pointer-sized representation. Examples include `Int`, `Word`, `Float`, and `newtype`s thereof.</p>
<h2 id="goals-and-non-goals">Goals and non-goals</h2>
<p>This proposal is about changing the default behavior of GHC, not changing expressiveness. Users can still use `UNPACK` and `NOUNPACK` to explicitly control the memory representation of fields.</p>
<p>There are two goals:</p>
<p><code>1. Reduce the amount of boilerplate experienced programmers have to write: As of Feb 18th 2012, the </code><a href="http://hackage.haskell.org/package/bytestring"><code>bytestring</code></a><code>, </code><a href="http://hackage.haskell.org/package/text"><code>text</code></a><code>, and </code><a href="http://hackage.haskell.org/package/containers"><code>containers</code></a><code> packages had 46 fields that matched the definition of primitive given above. 43 of these had an explicit `UNPACK` pragma (and the remaining 3 could have had one without changing the performance of the program.)</code></p>
<p><code>2. To provide better defaults for beginner and intermediate level Haskellers. Not unpacking e.g. `Int` fields can have a large, negative effect on performance and many beginner and intermediate level Haskellers are bitten by this.</code></p>
<h2 id="detailed-design">Detailed design</h2>
<h2 id="benchmarks">Benchmarks</h2>
<h1 id="unused-imports-1">Unused imports</h1>
<p>GHC has a series of bugs related to the &quot;report unused imports&quot; flags, including #1148, #2267, #1074, #2436, #10117.</p>
<p>This page describes a new design.</p>
<h2 id="the-current-story">The current story</h2>
<p>Currently (GHC 6.10) we report three different things:</p>
<p><code>* warnUnusedModules: import M, where nothing is used from M</code><br />
<code>* warnUnusedImports: import M(f), where f is unused, and M doesn't fall under warnUnusedModules</code><br />
<code>* warnDuplicateImports: import M + import M(f), even when f is used complain about duplicate import of f</code></p>
<h2 id="examples-1">Examples</h2>
<p>The hard bit is to specify what the warning should do. Consider these examples, where `Foo` exports `x` and `y`, and `FooPlus` re-exports all of `Foo`, plus `z`:  Which import is redudant, in each case?</p>
<p>Also: we might warn if you import the same module more than once, and the imports can be combined (ie they have the same 'qualified' and 'as' attributes)  Here both are used, but we might want to suggest combining them.</p>
<h2 id="specfication">Specfication</h2>
<p>We can at least agree on this:</p>
<p><code>* If the warning suggests that an import can be omitted, and you omit it,</code><br />
<code>  the program should still compile.</code><br />
<code>* It's not worth trying to be too subtle.  The 90% case is very simple.</code></p>
<p>Say that an <em>import-item</em> is either an entire import-all decl (eg `import Foo`), or a particular item in an import list (eg `import Foo( ..., x, ...)`). The general idea is that for each use of an imported name, we will attribute that use to one (or possibly more) import-items. Then, any import items with no uses attributed to them are unused, and are warned about. More precisely:</p>
<p><code>1.  For every `RdrName` in the program text, find all the import-items that brought it     into scope.  The lookup mechanism on `RdrNames` already takes account of whether the `RdrName` was qualified, and which imports have the right qualification etc, so this step is very easy.</code></p>
<p><code>2. Choose one of these, the &quot;chosen import-item&quot;, and mark it &quot;used&quot;.  </code></p>
<p><code>3.  Now bleat about any import-items that are unused.  For a decl</code><br />
<code>`import Foo(x,y)`, if both the `x` and `y` items are unused, it'd be better</code><br />
<code>to bleant about the entire decl rather than the individual items.</code></p>
<p>The import-item choosing step 2 implies that there is a total order on import-items. We say import-item A ``dominates`` import-item B if we chooose A over B. Here is one possible dominance relationship:</p>
<p><code>* `import Foo` dominates `import Foo(x)`.  (You could also argue that the </code><br />
<code>  reverse should hold.)</code><br />
<code>* Otherwise choose the textually first one.</code></p>
<p>Other notes:</p>
<p><code>* The algorithm chooses exactly one import-item in step 2.  It would</code><br />
<code>also be sound to choose more than one if there was a tie, but then completely-duplicate</code><br />
<code>imports might not be reported.</code></p>
<p><code>* Note that if we have an import item `import Foo (Bar(bar))`, then</code><br />
<code>it's marked as used if either `Bar` or `bar` are used.  We could have yet finer</code><br />
<code>resolution and report even unused sub-items.</code></p>
<p><code>* We should retain the special case of not warning about `import Foo ()`, which implies &quot;instance declarations only&quot;.</code></p>
<hr />
<h2 id="implementation-1">Implementation</h2>
<p>We want to collect the set of all `RdrNames` that are mentioned in the program. We must collect <strong>`RdrNames`</strong> not `Names`:  Here both imports are required, but you can only tell that by seeing the RdrNames, not by knowing that the name 'x' is used.</p>
<p>I think that all lookups go through either, `RnEnv.lookupGreRn_maybe` or `RnEnv.lookup_sub_bndr`. So in `RnEnv.lookupGreRn_maybe`, if `(gre_prov gre)` is `(Imported _)`, and in `RnEnv.lookup_sub_bndr`, put `rdr_name` in a new  in `TcGblEnv`. All the `tcg_used_rdrnames` are in scope; if not, we report an error and do not add it to `tcg_used_rdrnames`.</p>
<p>Other notes</p>
<p><code> * Any particular (in-scope) used `RdrName` is bought into scope by</code><br />
<code> one or more `RdrName.ImportSpec`'s.  You can find these `ImportSpecs`</code><br />
<code> in the GRE returned by the lookup.</code></p>
<p><code> * The unit of &quot;unused import&quot; reporting is one of these `ImportSpecs`.</code></p>
<p><code> * Suppose that 'rn' is a used, imported `RdrName`, and 'iss' is </code><br />
<code> the `[ImportSpecs]` that brought it into scope.  Then, to a first </code><br />
<code> approximation all the iss are counted 'used'.  </code></p>
<p><code> * We can compare `ImportSpecs` for equality by their `SrcSpans`</code></p>
<p><code> * In `TcRnDriver.tcRnImports`, save import_decls in a new</code><br />
<code> `tcg_rn_rdr_imports :: Maybe [LImportDecl RdrName]`</code><br />
<code> in `TcGblEnv`</code></p>
<hr />
<h2 id="algorithm">Algorithm</h2>
<p>The algorithm for deciding which imports have been used is based around this datatype: </p>
<p>We convert import declarations into trees of `ImportInfo`s, e.g.  becomes (only the `SDoc` and `[RdrName]` fields are given, as that's the interesting bit)  If a node in the tree is marked as used, then so are all nodes above it. For example, given the tree a use of `&quot;D&quot;` marks both the first and third lines as used.</p>
<p>When we come to giving warnings, if a node is unused then we warn about it, and do not descend into the rest of that subtree, as the node we warn about subsumes its children. If the node is marked as used then we descend, looking to see if any of its children are unused.</p>
<p>Here are how some example imports map to trees of `ImportInfo`, assuming `Foo` exports `a`, `b`, `D(c1, c2)`. </p>
<p>These trees are built by `RnNames.mkImportInfo`. In `RnNames.warnUnusedImportDecls` we make two lists of `ImportInfo`s; one list contains all the explicit imports, e.g.  and the other contains the implicit imports, e.g. </p>
<p>Then `RnNames.markUsages` is called for each `RdrName` that was used in the program. The current implementation marks all explicit import as used unless there are no such imports, in which case it marks all implicit imports as used. A small tweak to `markUsages` would allow it to mark only the first import it finds as used.</p>
<p>As well as the `RdrName`s used in the source, we also need to mark as used the names that are exported. We first call `RnNames.expandExports` to expand `D(..)` into `D(c1, c2)`, and then call `RnNames.markExportUsages`. Normally this just marks the `RdrName`s as used in the same way that uses in the module body are handled, but it is also possible for an entire module to be &quot;used&quot;, if `module Foo` is in the export list. In this case `RnNames.markModuleUsed` does the hard work, marking every module imported with that name as used.</p>
<h1 id="updates-1">Updates</h1>
<p>Source files: <a href="GhcFile(rts/Updates.h)" class="uri" title="wikilink">GhcFile(rts/Updates.h)</a>, <a href="GhcFile(rts/Updates.cmm)" class="uri" title="wikilink">GhcFile(rts/Updates.cmm)</a></p>
<hr />
<p>CategoryStub</p>
<h1 id="the-user-manual">The user manual</h1>
<p>GHC's user manual contains documentation intended for users of GHC. They are not interested in how GHC works; they just want to use it.</p>
<p>The user manual is held in <a href="GhcFile(docs/user_guide)" class="uri" title="wikilink">GhcFile(docs/user_guide)</a>, and is written in !ReStructuredText format (`.rst` files). This allows us to typeset it as HTML pages, or as LaTeX.</p>
<p>See also the [wiki:Building/Docs notes on building the documentation].</p>
<p>See the &quot;Care and feeding of your GHC User's Guide&quot; section for conventions and a basic introduction to ReST.</p>
<h1 id="ghc-boot-library-version-history">GHC Boot Library Version History</h1>
<p>This table lists the versions of GHC against those of its boot libraries, including most notably the `base` library. This may be useful if you ever want to find out which version of the `base` package was bundled with which version of GHC or vice versa.</p>
<p>See also: LanguagePragmaHistory, which lists the language extensions added and/or removed in each GHC version.</p>
<p></p>
<p>|| ||= <strong>HEAD</strong> =||= <strong>7.10.3</strong> =||= <strong>7.10.2</strong> =||= <strong>7.10.1</strong> =||= <strong>7.8.4</strong> =||= <strong>7.8.3</strong> =||= <strong>7.8.2</strong> =||= <strong>7.8.1</strong> =||= <strong>7.6.3</strong> =||= <strong>7.6.2</strong> =||= <strong>7.6.1</strong> =||= <strong>7.4.2</strong> =||= <strong>7.4.1</strong> =||= <strong>7.2.2</strong> =||= <strong>7.2.1</strong> =||= <strong>7.0.4</strong> =||= <strong>7.0.3</strong> =||= <strong>7.0.2</strong> =||= <strong>7.0.1</strong> =|| ||=`Cabal` =|| 1.23.0.0 || 1.22.5.0 || 1.22.4.0 || 1.22.2.0 || 1.18.1.5 |||||| 1.18.1.3 |||||| 1.16.0 |||| 1.14.0 |||| 1.12.0 || 1.10.2.0 |||| 1.10.1.0 || 1.10.0.0 || ||=`Win32` =|||||||| 2.3.1.0 |||||||| 2.3.0.2 |||||| 2.3.0.0 |||| 2.2.2.0 |||| 2.2.1.0 |||||||| 2.2.0.2 || ||=`array` =|||||||| 0.5.1.0 |||||||| 0.5.0.0 |||||| 0.4.0.1 |||| 0.4.0.0 |||| 0.3.0.3 |||||||| 0.3.0.2 || ||=`base` =|| 4.9.0.0 || 4.8.2.0 || 4.8.1.0 || 4.8.0.0 || 4.7.0.2 || 4.7.0.1 |||| 4.7.0.0 |||| 4.6.0.1 || 4.6.0.0 || 4.5.1.0 || 4.5.0.0 || 4.4.1.0 || 4.4.0.0 |||||| 4.3.1.0 || 4.3.0.0 || ||=`bin-package-db` =|| <em>none</em> |||||||||||||||||||||||||||||||||||| 0.0.0.0 || ||=`binary` =|| 0.8.0.0 |||| 0.7.5.0 || 0.7.3.0 |||||||| 0.7.1.0 |||||| 0.5.1.1 |||| 0.5.1.0 |||| 0.5.0.2* |||||||| <em>none</em> || ||=`bytestring` =|| 0.10.7.0 |||||| 0.10.6.0 |||||||| 0.10.4.0 |||| 0.10.0.2 || 0.10.0.0 |||| 0.9.2.1 |||| 0.9.2.0 |||||| 0.9.1.10 || 0.9.1.8 || ||=`containers` =|| 0.5.7.1 |||||| 0.5.6.2 |||||||| 0.5.5.1 |||||| 0.5.0.0 |||| 0.4.2.1 |||| 0.4.1.0 |||||||| 0.4.0.0 || ||=`deepseq` =|| 1.4.2.0 |||||| 1.4.1.1 |||||||| 1.3.0.2 |||||| 1.3.0.1 |||| 1.3.0.0 |||||||||||| <em>none</em> || ||=`directory` =|| 1.2.5.0 |||||| 1.2.2.0 |||||||| 1.2.1.0 |||| 1.2.0.1 || 1.2.0.0 |||| 1.1.0.2 |||| 1.1.0.1 |||||||| 1.1.0.0 || ||=`extensible-exceptions` =|||||||||||||||||||||| <em>none</em> |||| 0.1.1.4 |||| 0.1.1.3 |||||||| 0.1.1.2 || ||=`ffi` =|||||||||||||||||||||||||| <em>none</em> |||||||||||| 1.0 || ||=`filepath` =|| 1.4.1.0 |||||| 1.4.0.0 |||||||| 1.3.0.2 |||||| 1.3.0.1 |||| 1.3.0.0 |||| 1.2.0.1 |||||||| 1.2.0.0 || ||=`ghc` =|| 7.11.20151220* || 7.10.3* || 7.10.2* || 7.10.1* || 7.8.4* || 7.8.3* || 7.8.2* || 7.8.1* || 7.6.3* || 7.6.2* || 7.6.1* || 7.4.2* || 7.4.1* || 7.2.2* || 7.2.1* || 7.0.4* || 7.0.3* || 7.0.2* || 7.0.1* || ||=`ghc-binary` =|||||||||||||||||||||||||||||| <em>none</em> |||||||| 0.5.0.2* || ||=`ghc-boot` =|| 0.0.0.0 |||||||||||||||||||||||||||||||||||| <em>none</em> || ||=`ghc-prim` =|| 0.5.0.0 |||||| 0.4.0.0 |||||||| 0.3.1.0 |||||| 0.3.0.0 |||||||||||||||| 0.2.0.0 || ||=`ghci` =|| 0 |||||||||||||||||||||||||||||||||||| <em>none</em> || ||=`haskeline` =|| 0.7.2.2 |||||| 0.7.2.1 |||| 0.7.1.2 |||||||||||||||||||||||||| <em>none</em> || ||=`haskell2010` =|||||||| <em>none</em> |||||||| 1.1.2.0* |||||| 1.1.1.0* |||| 1.1.0.1* |||| 1.1.0.0* |||||||| 1.0.0.0* || ||=`haskell98` =|||||||| <em>none</em> |||||||| 2.0.0.3* |||||| 2.0.0.2* |||| 2.0.0.1* |||| 2.0.0.0* |||||| 1.1.0.1 || 1.1.0.0 || ||=`hoopl` =|| 3.10.2.0 |||||| 3.10.0.2 |||||||| 3.10.0.1 |||||| 3.9.0.0 |||| 3.8.7.3 |||| 3.8.7.1 |||||||| <em>none</em> || ||=`hpc` =|||||||| 0.6.0.2 |||||||| 0.6.0.1 |||||| 0.6.0.0 |||| 0.5.1.1 |||| 0.5.1.0 |||||||| 0.5.0.6 || ||=`integer-gmp` =|||||||| 1.0.0.0 |||||||| 0.5.1.0 |||||| 0.5.0.0 |||| 0.4.0.0 |||| 0.3.0.0 |||||| 0.2.0.3 || 0.2.0.2 || ||=`old-locale` =|||||||| <em>none</em> |||||||| 1.0.0.6 |||||| 1.0.0.5 |||| 1.0.0.4 |||| 1.0.0.3 |||||||| 1.0.0.2 || ||=`old-time` =|||||||| <em>none</em> |||||||| 1.1.0.2 |||||| 1.1.0.1 |||| 1.1.0.0 |||| 1.0.0.7 |||||||| 1.0.0.6 || ||=`pretty` =|| 1.1.3.2 |||||| 1.1.2.0 |||||||| 1.1.1.1 |||||||||| 1.1.1.0 |||| 1.1.0.0 |||||||| 1.0.1.2 || ||=`process` =|| 1.4.1.0 |||||| 1.2.3.0 |||||||| 1.2.0.0 |||||| 1.1.0.2 |||| 1.1.0.1 |||| 1.1.0.0 |||||| 1.0.1.5 || 1.0.1.4 || ||=`random` =|||||||||||||||||||||||||||||| <em>none</em> |||||||| 1.0.0.3 || ||=`rts` =|||||||||||||||||||||||||||||||||||||| 1.0 || ||=`template-haskell` =|| 2.11.0.0 |||||| 2.10.0.0 |||||||| 2.9.0.0 |||||| 2.8.0.0 |||| 2.7.0.0 |||| 2.6.0.0 |||||||| 2.5.0.0 || ||=`terminfo` =|| 0.4.0.2 |||||| 0.4.0.1 |||| 0.4.0.0 |||||||||||||||||||||||||| <em>none</em> || ||=`time` =|| 1.6 |||||| 1.5.0.1 |||||||| 1.4.2 |||||| 1.4.0.1 |||| 1.4 |||| 1.2.0.5 |||||||| 1.2.0.3 || ||=`transformers` =|| 0.5.0.0 |||||| 0.4.2.0 |||||||| 0.3.0.0 |||||||||||||||||||||| <em>none</em> || ||=`unix` =|| 2.7.1.1 |||||| 2.7.1.0 |||||||| 2.7.0.1 |||| 2.6.0.1 || 2.6.0.0 || 2.5.1.1 || 2.5.1.0 |||| 2.5.0.0 |||||| 2.4.2.0 || 2.4.1.0 || ||=`xhtml` =|||||||||||| 3000.2.1 |||||||||||||||||||||||||| <em>none</em> || || ||= <strong>HEAD</strong> =||= <strong>7.10.3</strong> =||= <strong>7.10.2</strong> =||= <strong>7.10.1</strong> =||= <strong>7.8.4</strong> =||= <strong>7.8.3</strong> =||= <strong>7.8.2</strong> =||= <strong>7.8.1</strong> =||= <strong>7.6.3</strong> =||= <strong>7.6.2</strong> =||= <strong>7.6.1</strong> =||= <strong>7.4.2</strong> =||= <strong>7.4.1</strong> =||= <strong>7.2.2</strong> =||= <strong>7.2.1</strong> =||= <strong>7.0.4</strong> =||= <strong>7.0.3</strong> =||= <strong>7.0.2</strong> =||= <strong>7.0.1</strong> =||</p>
<p>Note: A `*` after the version number denotes the package being hidden by default.</p>
<p>A table covering some GHC 6.* releases can be found at <a href="https://wiki.haskell.org/Libraries_released_with_GHC" class="uri">https://wiki.haskell.org/Libraries_released_with_GHC</a></p>
<p>= Warnings and Deprecations</p>
<p>For now, see the relevant <a href="http://downloads.haskell.org/~ghc/latest/docs/html/users_guide/pragmas.html#warning-deprecated-pragma">GHC User's Guide Section</a> describing the `DEPRECATE` and `WARNING` pragmas.</p>
<p>TODO</p>
<h1 id="ghc-commentary-weak-pointers-and-finalizers">GHC Commentary: Weak Pointers and Finalizers</h1>
<hr />
<p>CategoryStub <a href="PageOutline" class="uri" title="wikilink">PageOutline</a></p>
<h1 id="work-in-progress-on-the-llvm-backend">Work in Progress on the LLVM Backend</h1>
<p>This page is meant to collect together information about people working on (or interested in working on) LLVM in GHC, and the projects they are looking at. See also the [wiki:Commentary/Compiler/NewCodeGen state of play of the whole back end]. This is more a page of ideas for improvements to the LLVM backend and less so an indication of actual work going on.</p>
<h3 id="llvm-ir-representation">LLVM IR Representation</h3>
<p>The LLVM IR is modeled in GHC using an algebraic data type to represent the first order abstract syntax of the LLVM assembly code. The LLVM representation lives in the 'Llvm' subdirectory and also contains code for pretty printing. This is the same approach taken by EHC's LLVM Back-end, and we adapted the module developed by them for this purpose.</p>
<p>The current design is overly complicated and could be faster. It uses String + show operations for printing for example when it should be using !FastString + Outputable. Before simplifying this design though it would be good to investigate using the LLVM API instead of the assembly language for interacting with LLVM. This would be done most likely by using the pre-existing Haskell LLVM API bindings found <a href="http://hackage.haskell.org/package/llvm">here</a>. This should hopefully provide a speed up in compilation speeds which is greatly needed since the LLVM back-end is ~2x slower at the moment.</p>
<h3 id="tables_next_to_code-1">TABLES_NEXT_TO_CODE</h3>
<p>We now support [wiki:Commentary/Compiler/Backends/LLVM/Issues#TABLES_NEXT_TO_CODE TNTC] using an approach of gnu as subsections. This seems to work fine but we would like still to move to a pure LLVM solution. Ideally we would implement this in LLVM by allowing a global variable to be associated with a function, so that LLVM is aware that the two will be laid out next to each other and can better optimise (e.g using this approach LLVM should be able to perform constant propagation on info-tables).</p>
<p><strong>Update (30/06/2010):</strong> The current TNTC solution doesn't work on Mac OS X. So we need to implement an LLVM based solution. We currently support OS X by post processing the assembly. Pure LLVM is a nicer way forward.</p>
<h3 id="llvm-alias-analysis-pass">LLVM Alias Analysis Pass</h3>
<p><strong>Update: This has been implemented, needs more work though</strong></p>
<p>LLVM doesn't seem to do a very good job of figuring out what can alias what in the code generated by GHC. We should write our own alias analysis pass to fix this.</p>
<h3 id="optimise-llvm-for-the-type-of-code-ghc-produces">Optimise LLVM for the type of Code GHC produces</h3>
<p>At the moment only a some fairly basic benchmarking has been done of the LLVM back-end. Enough to give an indication of how it performs on the whole (well as far as you trust benchmarks anyway) and of what it can sometimes achieve. However this is by no means exauhstive or probably even close to it and doesn't give us enough information about the areas where LLVM performs badly. The LLVM optimisation pass also at the moment just uses the standard '-O[123]' levels, which like GCC entail a whole bunch of optimisation passes. These groups are designed for C programs mostly.</p>
<p>So:</p>
<p><code>* More benchmarking, particularly finding some bad spots for the LLVM back-end and generating a good picture of the characteristics of the back-end.</code><br />
<code>* Look into the LLVM optimiser, e.g perhaps some more work in the style of </code><a href="http://donsbot.wordpress.com/2010/03/01/evolving-faster-haskell-programs-now-with-llvm/"><code>Don's</code> <code>work</code></a><br />
<code>* Look at any new optimisation passes that could be written for LLVM which would help to improve the code it generates for GHC.</code><br />
<code>* Look at general fixes/improvement to LLVM to improve the code it generates for LLVM.</code><br />
<code>* Sometimes there is a benefit from running the LLVM optimiser twice of the code (e.g opt -O3 | opt -O3 ...). We should add a command line flag that allows you to specify the number of iterations you want the LLVM optimiser to do.</code></p>
<h3 id="update-the-back-end-to-use-the-new-cmm-data-types-new-code-generator">Update the Back-end to use the new Cmm data types / New Code Generator</h3>
<p>There is ongoing work to produce a new, nicer, more modular code generator for GHC (the slightly confusingly name code generator in GHC refers to the pipeline stage where the Core IR is compiled to the Cmm IR). The LLVM back-end could be updated to make sure it works with the new code generator and does so in an efficient manner.</p>
<h3 id="llvms-link-time-optimisations">LLVM's Link Time Optimisations</h3>
<p>One of LLVM's big marketing features is its support for link time optimisation. This does things such as in-lining across module boundaries, more aggressive dead code elimination... etc). The LLVM back-end could be updated to make use of this. Roman apparently tried to use the new 'gold' linker with GHC and it doesn't support all the needed features.</p>
<p><code>* </code><a href="http://llvm.org/releases/2.6/docs/LinkTimeOptimization.html"><code>22</code></a><br />
<code>* </code><a href="http://llvm.org/docs/GoldPlugin.html"><code>23</code></a></p>
<h3 id="llvm-cross-compiler-port">LLVM Cross Compiler / Port</h3>
<p>This is more of an experimental idea but the LLVM back-end looks like it would make a great choice for Porting LLVM. That is, instead of porting LLVM through the usual route of via-C and then fixing up the NCG, just try to do it all through the LLVM back-end. As LLVM is quite portable and supported on more platforms then GHC, it would be an interesting and valuable experiment to try to port GHC to a new platform by simply getting the LLVM back-end working on it. (The LLVM back-end works in both unregistered and registered mode, another advantage for porting compared to the C and NCG back-ends).</p>
<p>It would also be interesting to looking into improving GHC to support cross compiling and doing this through the LLVM back-end as it should be easier to fix up to support this feature than the C or NCG back-ends.</p>
<h3 id="get-rid-of-proc-point-splitting">Get rid of Proc Point Splitting</h3>
<p>When Cmm code is first generated a single Haskell function will be mostly compiled to one Cmm function. This Cmm function isn't passed to the backends though as the CPS style used in it requires that the backends be able to take the address of labels in a function since they're used as return points. The C backend can't support this. While there is a GNU C extension allowing the address of a label to be taken, the address can only be used locally (in the same function). So what proc point splitting does is cut a single Cmm function into multiple top level Cmm functions so that instead of needing to take the address of a label, we now take the address of a function.</p>
<p>It would be nice to get rid of proc point splitting. This is one of the goals for the new code generator. This will give us much bigger Cmm functions which should give more room for LLVM to optimise. There is an issue though that LLVM doesn't support taking the address of a local label either. So will need to add support to LLVM for taking label addresses or convert CPS style into something more direct if thats possible.</p>
<h3 id="dont-pass-around-dead-stg-registers">Don't Pass Around Dead STG Registers</h3>
<p><strong>Update: This has been implemented</strong></p>
<p>At the moment in the LLVM backend we always pass around the pinned STG registers as arguments for every Cmm function. A huge amount of the time though we aren't storing anything in the STG registers, they are dead really. If we can treat the correctly as dead then LLVM will have more free registers and the allocator should do a better job. We need to change the STG -&gt; Cmm code generator to attach register liveness information at function exit points (e.g calls, jumps, returns).</p>
<p>e.g This <a href="http://hackage.haskell.org/trac/ghc/ticket/4308">bug (#4308)</a> is as a result of this problem.</p>
<p><a href="PageOutline" class="uri" title="wikilink">PageOutline</a></p>
<h1 id="wired-in-and-known-key-things">Wired-in and known-key things</h1>
<p>There are three categories of entities that GHC &quot;knows about&quot;; that is, information about them is baked into GHC's source code.</p>
<p><code> * [wiki:Commentary/Compiler/WiredIn#Wiredinthings Wired-in things] --- GHC knows everything about these</code><br />
<code> * [wiki:Commentary/Compiler/WiredIn#Knownkeythings Known-key things] --- GHC knows the </code><em><code>name</code></em><code>, including the </code><code>, but not the definition</code><br />
<code> * [wiki:Commentary/Compiler/WiredIn#OrigRdrNamethings Orig RdrName  things] --- GHC knows which module it's defined in</code></p>
<h2 id="wired-in-things">Wired-in things</h2>
<p>A <strong>Wired-in thing</strong> is fully known to GHC. Most of these are `TyCon`s such as `Bool`. It is very convenient to simply be able to refer to `boolTyCon :: TyCon` without having to look it up in an environment.</p>
<p>All [wiki:Commentary/Compiler/TypeType#Classifyingtypes primitive types] are wired-in things, and have wired-in `Name`s. The primitive types (and their `Names`) are all defined in <a href="GhcFile(compiler/prelude/TysPrim.hs)" class="uri" title="wikilink">GhcFile(compiler/prelude/TysPrim.hs)</a>.</p>
<p>The non-primitive wired-in type constructors are defined in <a href="GhcFile(compiler/prelude/TysWiredIn.hs)" class="uri" title="wikilink">GhcFile(compiler/prelude/TysWiredIn.hs)</a>. There are a handful of wired-in `Id`s in <a href="GhcFile(compiler/basicTypes/MkId.hs)" class="uri" title="wikilink">GhcFile(compiler/basicTypes/MkId.hs)</a>. There are no wired-in classes (they are too complicated).</p>
<p>All the non-primitive wired-in things are <em>also</em> defined in GHC's libraries, because even though GHC knows about them we still need to generate code for them. For example, `Bool` is a wired-in type constructor, but it is still defined in `GHC.Base` because we need the info table etc for the data constructors. Arbitrarily bad things will happen if the wired-in definition in <a href="GhcFile(compiler/prelude/TysWiredIn.hs)" class="uri" title="wikilink">GhcFile(compiler/prelude/TysWiredIn.hs)</a> differs from that in the library module.</p>
<p>All wired-in things have a `WiredIn` `Name` (see [wiki:Commentary/Compiler/NameType Names]), which in turn contains the thing. See [wiki:Commentary/Compiler/CaseStudies/Bool a case study of Bool implementation] for more details.</p>
<h2 id="known-key-things-1">Known-key things</h2>
<p>A <strong>known-key thing</strong> has a fixed, pre-allocated `Unique` or <strong>key</strong>. They should really be called &quot;known-Name&quot; things, because the baked-in knowledge is:</p>
<p><code>* Its defining `Module`</code><br />
<code>* Its `OccName`</code><br />
<code>* Its `Unique`</code></p>
<p>Almost all known-key names are defined in <a href="GhcFile(compiler/prelude/PrelNames.hs)" class="uri" title="wikilink">GhcFile(compiler/prelude/PrelNames.hs)</a>; for example: .</p>
<p>The point about known-key things is that GHC knows its <em>name</em>, but not its <em>definition</em>. The definition must still be read from an interface file as usual. The known key just allows an efficient lookup in the environment.</p>
<h2 id="initialisation">Initialisation</h2>
<p>When reading an interface file, GHC might come across &quot;GHC.Base.Eq&quot;, which is the name of the `Eq` class. How does it match up this occurrence in the interface file with `eqClassName` defined in `PrelNames`? Because the global name cache maintained by the renamer is initialised with all the known-key names. This is done by the (hard-to-find) function `HscMain.newHscEnv`:  Notice that the initialisation embraces both the wired-in and (&quot;basic&quot;) known-key names.</p>
<h2 id="orig-rdrname-things">`Orig` `RdrName` things</h2>
<p>An <strong>Orig !RdrName thing</strong> has a top-level definition of a `RdrName`, using the `Orig` constructor. Here, the baked-in information is:</p>
<p><code> * Its defining `Module`</code><br />
<code> * Its `OccName`</code></p>
<p>Again, almost all of these are in <a href="GhcFile(compiler/prelude/PrelNames.hs)" class="uri" title="wikilink">GhcFile(compiler/prelude/PrelNames.hs)</a>. Example: .</p>
<h1 id="ghc-commentary-the-word">GHC Commentary: The Word</h1>
<p>The most important type in the runtime is , defined in <a href="GhcFile(includes/stg/Types.h)" class="uri" title="wikilink">GhcFile(includes/stg/Types.h)</a>. A word is defined to be the same size as a pointer on the current platform. All these types are interconvertible without losing information, and have the same size (as reported by ):</p>
<p><code>::</code><br />
<code> An unsiged integral type of word size</code></p>
<p><code>::</code><br />
<code> A signed integral type of word size</code></p>
<p><code>::</code><br />
<code> Pointer to </code></p>
<p>The word is the basic unit of allocation in GHC: the heap and stack are both allocated in units of a word. Throughout the runtime we often use sizes that are in units of words, so as to abstract away from the real word size of the underlying architecture.</p>
<p>The `StgWord` type is also useful for storing the <em>size</em> of a memory object, since an `StgWord` is guaranteed to at least span the range of addressable memory. It is rather like `size_t` in this respect, although we prefer to use `StgWord` in the RTS sources.</p>
<p>C-- only understands units of bytes, so we have various macros in <a href="GhcFile(includes/Cmm.h)" class="uri" title="wikilink">GhcFile(includes/Cmm.h)</a> to make manipulating things in units of words easier in  files.</p>
</body>
</html>
