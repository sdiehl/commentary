-   [GHC Source Code Abbreviations](#ghc-source-code-abbreviations)
-   [Aging in the generational GC](#aging-in-the-generational-gc)
-   [Improving LLVM Alias Analysis](#improving-llvm-alias-analysis)
    -   [LLVM Alias Analysis
        Infrastructure](#llvm-alias-analysis-infrastructure)
    -   [Max's Work](#maxs-work)
    -   [TBAA](#tbaa)
    -   [STG / Cmm Alias Properties](#stg-cmm-alias-properties)
    -   [How to Track TBAA information](#how-to-track-tbaa-information)
    -   [LLVM type system](#llvm-type-system)
    -   [Problems / Optmisations to
        Solve](#problems-optmisations-to-solve)
        -   [LLVM Optimisations](#llvm-optimisations)
        -   [Safe Loads
            (speculative load)](#safe-loads-speculative-load)
        -   [GHC Heap Check
            (case merging)](#ghc-heap-check-case-merging)
-   [GHC Commentary: The GHC API](#ghc-commentary-the-ghc-api)
    -   [Targets](#targets)
    -   [Dependency Analysis](#dependency-analysis)
    -   [The !ModSummary type](#the-modsummary-type)
    -   [Loading (compiling) the
        Modules](#loading-compiling-the-modules)
-   [GHC Commentary: Asynchronous
    Exceptions](#ghc-commentary-asynchronous-exceptions)
-   [GHC Commentary: Backends](#ghc-commentary-backends)
-   [Types in the back end (aka "The
    \`Rep\` swamp")](#types-in-the-back-end-aka-the-rep-swamp)
    -   [\`CmmType\`](#cmmtype)
    -   [The \`MachOp\` type](#the-machop-type)
    -   [Foreign calls and hints](#foreign-calls-and-hints)
    -   [Native code generation and the \`Size\`
        type](#native-code-generation-and-the-size-type)
-   [The Block Allocator](#the-block-allocator)
    -   [Structure of blocks](#structure-of-blocks)
-   [GHC Commentary: Garbage Collecting
    CAFs](#ghc-commentary-garbage-collecting-cafs)
    -   [Static Reference Tables](#static-reference-tables)
    -   [Evacuating Static Objects](#evacuating-static-objects)
-   [Calling Convention](#calling-convention)
-   [Return Convention](#return-convention)
    -   [Historical page](#historical-page)
-   [Cleanup after the new codegen is
    enabled](#cleanup-after-the-new-codegen-is-enabled)
    -   [Independent tasks](#independent-tasks)
    -   [Towards removing codeGen/Cg\*](#towards-removing-codegencg)
    -   [Towards removing \`OldCmm\`](#towards-removing-oldcmm)
    -   [Later](#later)
-   [Cmm: Implementing Exception
    Handling](#cmm-implementing-exception-handling)
    -   [An Integral Exception Example](#an-integral-exception-example)
    -   [A Floating Point Exception
        Example](#a-floating-point-exception-example)
    -   [Note To Reader](#note-to-reader)
-   [Table of Contents](#table-of-contents)
-   [The Cmm language](#the-cmm-language)
    -   [Additions in Cmm](#additions-in-cmm)
    -   [Compiling Cmm with GHC](#compiling-cmm-with-ghc)
    -   [Basic Cmm](#basic-cmm)
        -   [Code Blocks in Cmm](#code-blocks-in-cmm)
        -   [Variables, Registers and
            Types](#variables-registers-and-types)
        -   [Literals and Labels](#literals-and-labels)
        -   [Sections and Directives](#sections-and-directives)
        -   [Expressions](#expressions)
        -   [Statements and Calls](#statements-and-calls)
        -   [Operators and Primitive
            Operations](#operators-and-primitive-operations)
    -   [Cmm Design: Observations and Areas for Potential
        Improvement](#cmm-design-observations-and-areas-for-potential-improvement)
-   [GHC Commentary: What the hell is a \`.cmm\`
    file?](#ghc-commentary-what-the-hell-is-a-.cmm-file)
    -   [Reading references](#reading-references)
    -   [Other information](#other-information)
-   [Code Generator](#code-generator)
    -   [A brief history of code
        generator](#a-brief-history-of-code-generator)
    -   [Overview](#overview)
    -   [First stage: STG to Cmm
        conversion](#first-stage-stg-to-cmm-conversion)
    -   [Second stage: the Cmm pipeline](#second-stage-the-cmm-pipeline)
    -   [Dumping and debugging Cmm](#dumping-and-debugging-cmm)
    -   [Register Allocator Code](#register-allocator-code)
        -   [The register allocator](#the-register-allocator)
        -   [Graph coloring](#graph-coloring)
        -   [Miscellanea](#miscellanea)
-   [The GHC Commentary - Coding Style Guidelines for the
    compiler](#the-ghc-commentary---coding-style-guidelines-for-the-compiler)
    -   [General Style](#general-style)
    -   [Comments](#comments)
        -   [Comments on top-level
            entities](#comments-on-top-level-entities)
        -   [Comments in the source code](#comments-in-the-source-code)
        -   [Comments and examples](#comments-and-examples)
        -   [Longer comments or architectural
            commentary](#longer-comments-or-architectural-commentary)
        -   [Commit messages](#commit-messages)
    -   [Warnings](#warnings)
    -   [Exports and Imports](#exports-and-imports)
        -   [Exports](#exports)
        -   [Imports](#imports)
    -   [Compiler versions and language
        extensions](#compiler-versions-and-language-extensions)
        -   [](#section)
        -   [Literate Haskell](#literate-haskell)
        -   [The C Preprocessor (CPP)](#the-c-preprocessor-cpp)
        -   [Platform tests](#platform-tests)
    -   [Tabs vs Spaces](#tabs-vs-spaces)
-   [Coercions in GHC's core language](#coercions-in-ghcs-core-language)
    -   [Difficulties with the current
        approach](#difficulties-with-the-current-approach)
    -   [Main proposal](#main-proposal)
-   [Parsing of command line
    arguments](#parsing-of-command-line-arguments)
    -   [Static flags](#static-flags)
    -   [Dynamic flags](#dynamic-flags)
-   [The GHC Commentary](#the-ghc-commentary)
    -   [Editing the Commentary](#editing-the-commentary)
    -   [Contents](#contents)
    -   [Contributed Documentation](#contributed-documentation)
-   [Compiler and runtime system ways in
    GHC](#compiler-and-runtime-system-ways-in-ghc)
    -   [Available ways in a standard
        GHC](#available-ways-in-a-standard-ghc)
        -   [Ways for parallel execution on clusters and
            multicores](#ways-for-parallel-execution-on-clusters-and-multicores)
    -   [Combining ways](#combining-ways)
-   [Internals](#internals)
-   [GHC Commentary: The Compiler](#ghc-commentary-the-compiler)
    -   [Overall Structure](#overall-structure)
    -   [What problems do we need to
        solve?](#what-problems-do-we-need-to-solve)
    -   [Current mechanisms](#current-mechanisms)
    -   [New concepts for Backpack](#new-concepts-for-backpack)
    -   [Features](#features)
    -   [Constraints](#constraints)
-   [RTS Configurations](#rts-configurations)
    -   [Combinations](#combinations)
    -   [Other configuration options](#other-configuration-options)
-   [Contracts for Haskell](#contracts-for-haskell)
    -   [Involved](#involved)
    -   [Overview](#overview-1)
    -   [The plan](#the-plan)
    -   [Current status](#current-status)
    -   [Questions](#questions)
    -   [References](#references)
-   [The GHC Commentary: Coding Style Guidelines for RTS C
    code](#the-ghc-commentary-coding-style-guidelines-for-rts-c-code)
    -   [Comments](#comments-1)
    -   [References](#references-1)
    -   [Portability issues](#portability-issues)
        -   [Which C Standard?](#which-c-standard)
        -   [Other portability
            conventions](#other-portability-conventions)
    -   [Debugging/robustness tricks](#debuggingrobustness-tricks)
    -   [Syntactic details](#syntactic-details)
    -   [Inline functions](#inline-functions)
    -   [Source-control issues](#source-control-issues)
-   [Copying GC](#copying-gc)
-   [The type](#the-type)
    -   [Case expressions](#case-expressions)
    -   [Shadowing](#shadowing)
    -   [Human readable Core
        generation](#human-readable-core-generation)
-   [CPS Conversion](#cps-conversion)
    -   [Overview](#overview-2)
    -   [Design Aspects](#design-aspects)
    -   [Simple Design](#simple-design)
    -   [To be worked out](#to-be-worked-out)
    -   [Pipeline](#pipeline)
    -   [TODO](#todo)
    -   [Current Pipeline](#current-pipeline)
        -   [](#section-1)
    -   [Non-CPS Changes](#non-cps-changes)
    -   [Notes](#notes)
    -   [Loopholes](#loopholes)
        -   [GC Blocks](#gc-blocks)
        -   [Update Frames](#update-frames)
        -   [User defined continuations](#user-defined-continuations)
        -   [Branches to continuations](#branches-to-continuations)
    -   [Not in Scope of Current Work](#not-in-scope-of-current-work)
        -   [Static Reference Table
            Handling (SRT)](#static-reference-table-handling-srt)
        -   [Cmm Optimization assumed by
            CPS](#cmm-optimization-assumed-by-cps)
    -   [Notes on future development](#notes-on-future-development)
        -   [Handling GC](#handling-gc)
-   [The GHC Commentary: Data types and data
    constructors](#the-ghc-commentary-data-types-and-data-constructors)
    -   [Data types](#data-types)
-   [The life cycle of a data type](#the-life-cycle-of-a-data-type)
    -   [The constructor wrapper
        functions](#the-constructor-wrapper-functions)
    -   [The constructor worker
        functions](#the-constructor-worker-functions)
    -   [External Core](#external-core)
    -   [Unboxing strict fields](#unboxing-strict-fields)
    -   [Labels and info tables](#labels-and-info-tables)
-   [Demand analyser in GHC](#demand-analyser-in-ghc)
    -   [Demand signatures](#demand-signatures)
        -   [Demand descriptions](#demand-descriptions)
    -   [Worker-Wrapper split](#worker-wrapper-split)
    -   [Relevant compiler parts](#relevant-compiler-parts)
-   [Support for deriving , , and
    instances](#support-for-deriving-and-instances)
    -   [Example](#example)
    -   [Algorithm description](#algorithm-description)
        -   [](#section-2)
        -   [](#section-3)
        -   [](#section-4)
        -   [Covariant and contravariant
            positions](#covariant-and-contravariant-positions)
    -   [Requirements for legal
        instances](#requirements-for-legal-instances)
        -   [Relaxed universality check for
            ](#relaxed-universality-check-for)
    -   [Alternative strategy for deriving \`Foldable\` and
        \`Traversable\`](#alternative-strategy-for-deriving-foldable-and-traversable)
-   [LLVM Back-end Design](#llvm-back-end-design)
-   [Implementation](#implementation)
    -   [Framework](#framework)
    -   [LLVM Code Generation](#llvm-code-generation)
    -   [Register Pinning](#register-pinning)
    -   [Code Generation](#code-generation)
        -   [Unregisterised Vs.
            Registerised](#unregisterised-vs.-registerised)
    -   [!CmmData](#cmmdata)
        -   [1st Pass : Generation](#st-pass-generation)
    -   [!CmmStaticLit](#cmmstaticlit)
        -   [!CmmUninitialised](#cmmuninitialised)
        -   [!CmmAlign & !CmmDataLabel](#cmmalign-cmmdatalabel)
        -   [!CmmString](#cmmstring)
        -   [2nd Pass : Resolution](#nd-pass-resolution)
    -   [!CmmProc](#cmmproc)
-   [Desugaring instance
    declarations](#desugaring-instance-declarations)
    -   [Basic stuff](#basic-stuff)
    -   [Dictionary functions](#dictionary-functions)
    -   [The INLINE strategy](#the-inline-strategy)
    -   [The out-of-line (A) strategy](#the-out-of-line-a-strategy)
    -   [The out-of-line (B) strategy](#the-out-of-line-b-strategy)
    -   [User INLINE pragmas and
        out-of-line (A)](#user-inline-pragmas-and-out-of-line-a)
    -   [Summary](#summary)
-   [Bugs & Other Problems](#bugs-other-problems)
-   [Compiling more than one module at
    once](#compiling-more-than-one-module-at-once)
    -   [The overall driver](#the-overall-driver)
        -   [Dependency analysis](#dependency-analysis-1)
        -   [Recompilation checking and
            stability](#recompilation-checking-and-stability)
        -   [Compilation](#compilation)
-   [Eager Promotion](#eager-promotion)
-   [Eager Version Bumping Strategy](#eager-version-bumping-strategy)
-   [Data types for Haskell entities: , , , , and
    ](#data-types-for-haskell-entities-and)
    -   [Type variables and term
        variables](#type-variables-and-term-variables)
    -   [ and implict Ids](#and-implict-ids)
-   [HC files and the Evil Mangler](#hc-files-and-the-evil-mangler)
-   [Strictness analysis: examples](#strictness-analysis-examples)
-   [System FC: equality constraints and
    coercions](#system-fc-equality-constraints-and-coercions)
    -   [Coercions and Coercion Kinds](#coercions-and-coercion-kinds)
    -   [GADTs](#gadts)
    -   [Representation of coercion
        assumptions](#representation-of-coercion-assumptions)
    -   [Newtypes are coerced types](#newtypes-are-coerced-types)
    -   [Roles](#roles)
    -   [Simplification](#simplification)
-   [GHC Commentary: Runtime aspects of the
    FFI](#ghc-commentary-runtime-aspects-of-the-ffi)
    -   [Foreign Import "wrapper"](#foreign-import-wrapper)
-   [Function Calls](#function-calls)
    -   [Generic apply](#generic-apply)
-   [The Garbage Collector](#the-garbage-collector)
    -   [GC overview](#gc-overview)
    -   [GC data structures](#gc-data-structures)
        -   [generation](#generation)
        -   [nursery](#nursery)
-   [I know kung fu: learning STG by
    example](#i-know-kung-fu-learning-stg-by-example)
    -   [What is STG, exactly?](#what-is-stg-exactly)
    -   [An overview of the STG
        machine](#an-overview-of-the-stg-machine)
        -   [Components of the machine](#components-of-the-machine)
        -   [Important concepts in the
            machine](#important-concepts-in-the-machine)
        -   [Overview of execution model of the
            machine](#overview-of-execution-model-of-the-machine)
    -   [Saturated application to known
        functions](#saturated-application-to-known-functions)
        -   [Example 1: function application with sufficient stack
            space](#example-1-function-application-with-sufficient-stack-space)
        -   [Example 2: function application that needs to grow the
            stack](#example-2-function-application-that-needs-to-grow-the-stack)
    -   [Example 3: Unsaturated applications to known
        functions](#example-3-unsaturated-applications-to-known-functions)
    -   [Example 4: Applications to unknown
        functions](#example-4-applications-to-unknown-functions)
        -   [Dealing with generic
            application](#dealing-with-generic-application)
        -   [Making the call to the generic application
            code](#making-the-call-to-the-generic-application-code)
    -   [Example 5: oversaturated applications to known
        functions](#example-5-oversaturated-applications-to-known-functions)
    -   [Example 6: allocation of thunks and
        data](#example-6-allocation-of-thunks-and-data)
        -   [Checking for sufficient heap
            space](#checking-for-sufficient-heap-space)
        -   [Performing the actual
            allocation](#performing-the-actual-allocation)
        -   [Returning an allocated value to the
            caller](#returning-an-allocated-value-to-the-caller)
    -   [Example 7: \`case\` expressions](#example-7-case-expressions)
        -   [Forcing the scrutinee of the
            \`case\`](#forcing-the-scrutinee-of-the-case)
        -   [Dealing with the forced
            scrutinee](#dealing-with-the-forced-scrutinee)
    -   [Example 8: thunks and thunk
        update](#example-8-thunks-and-thunk-update)
        -   [Thunk entry point](#thunk-entry-point)
        -   [Continuation of the thunk](#continuation-of-the-thunk)
    -   [Conclusion](#conclusion)
-   [Support for generic programming](#support-for-generic-programming)
    -   [Status](#status)
    -   [Main components](#main-components)
    -   [Things that have been removed](#things-that-have-been-removed)
    -   [What already works](#what-already-works)
    -   [Testing](#testing)
-   [Kind polymorphic overhaul](#kind-polymorphic-overhaul)
    -   [Generic representation
        universe](#generic-representation-universe)
    -   [Universe interpretation](#universe-interpretation)
        -   [Names](#names)
    -   [Metadata representation](#metadata-representation)
    -   [Conversion between user datatypes and generic
        representation](#conversion-between-user-datatypes-and-generic-representation)
    -   [Example generic function: \`fmap\` (kind \`\*
        -&gt; \*\`)](#example-generic-function-fmap-kind--)
    -   [Example generic function: \`show\` (kind \`\*\`,
        uses metadata)](#example-generic-function-show-kind-uses-metadata)
    -   [Example datatype encoding: lists (derived by
        the compiler)](#example-datatype-encoding-lists-derived-by-the-compiler)
        -   [Digression](#digression)
    -   [GHC 8.0 and later](#ghc-8.0-and-later)
        -   [Type-level metadata
            encoding](#type-level-metadata-encoding)
        -   [Strictness](#strictness)
    -   [Source Tree Layout](#source-tree-layout)
    -   [Build System Basics](#build-system-basics)
    -   [Coding Style](#coding-style)
-   [The GHC Commentary: GHCi](#the-ghc-commentary-ghci)
    -   [Debugging the interpreter](#debugging-the-interpreter)
    -   [Useful stuff to know about the
        interpreter](#useful-stuff-to-know-about-the-interpreter)
        -   [Stack management](#stack-management)
        -   [Building constructors](#building-constructors)
        -   [Perspective](#perspective)
    -   [case returns between interpreted and compiled
        code](#case-returns-between-interpreted-and-compiled-code)
        -   [Returning to
            interpreted code.](#returning-to-interpreted-code.)
        -   [Returning to compiled code.](#returning-to-compiled-code.)
    -   [Unboxed tuples: a Right Royal Spanner In The
        Works](#unboxed-tuples-a-right-royal-spanner-in-the-works)
-   [Porting GHC using LLVM backend](#porting-ghc-using-llvm-backend)
    -   [Registerised Mode](#registerised-mode)
-   [Packages in GHC](#packages-in-ghc)
    -   [The problem](#the-problem)
    -   [Assumptions](#assumptions)
    -   [The open question](#the-open-question)
    -   [Plan A: GHC's current story](#plan-a-ghcs-current-story)
    -   [Plan B: package mounting](#plan-b-package-mounting)
    -   [Plan C: mention the package in the
        import](#plan-c-mention-the-package-in-the-import)
-   [Problems](#problems)
    -   [Breaking re-installations](#breaking-re-installations)
    -   [Type errors when using packages
        together](#type-errors-when-using-packages-together)
-   [Goals](#goals)
-   [Implementation Plan](#implementation-plan)
    -   [Persistent package store](#persistent-package-store)
    -   [Views](#views)
    -   [Consistent developer
        environment](#consistent-developer-environment)
    -   [Garbage collection](#garbage-collection)
    -   [cabal remove](#cabal-remove)
    -   [cabal upgrade](#cabal-upgrade)
    -   [Current Status](#current-status-1)
        -   [Unique Install Location](#unique-install-location)
        -   [ghc-pkg](#ghc-pkg)
        -   [Adhoc dependency resolution](#adhoc-dependency-resolution)
        -   [Detect whether an overwrite happens and warn about
            it](#detect-whether-an-overwrite-happens-and-warn-about-it)
        -   [Communicate the \`InstalledPackageId\` back to
            cabal-install](#communicate-the-installedpackageid-back-to-cabal-install)
        -   [Garbage Collection](#garbage-collection-1)
        -   [About Shadowing](#about-shadowing)
        -   [About Unique Identifier](#about-unique-identifier)
    -   [Original Plan](#original-plan)
    -   [Hashes and identifiers](#hashes-and-identifiers)
    -   [Install location of installed Cabal
        packages](#install-location-of-installed-cabal-packages)
        -   [Hash](#hash)
        -   [Unique number](#unique-number)
    -   [\`ghc-pkg\`](#ghc-pkg-1)
    -   [Simplistic dependency
        resolution](#simplistic-dependency-resolution)
    -   [Build flavours](#build-flavours)
        -   [The Cabal hash](#the-cabal-hash)
        -   [Released and Unreleased
            packages](#released-and-unreleased-packages)
    -   [Dependency resolution in
        cabal-install](#dependency-resolution-in-cabal-install)
    -   [Garbage Collection](#garbage-collection-2)
    -   [Currently open design
        decisions](#currently-open-design-decisions)
        -   [\`InstalledPackageId\` and install
            path](#installedpackageid-and-install-path)
        -   [Handling of dirty builds](#handling-of-dirty-builds)
        -   [Build flavours](#build-flavours-1)
        -   [\`InstalledPackageInfo\` and solver
            algorithm](#installedpackageinfo-and-solver-algorithm)
        -   [Simplistic dependency
            resolution](#simplistic-dependency-resolution-1)
    -   [Related topics](#related-topics)
        -   [Separating storage and selection of
            packages](#separating-storage-and-selection-of-packages)
        -   [First class environments](#first-class-environments)
    -   [Questions to remember](#questions-to-remember)
-   [The Haskell Execution Model](#the-haskell-execution-model)
-   [HEAP\_ALLOCED](#heap_alloced)
    -   [Speeding up \`HEAP\_ALLOCED()\`](#speeding-up-heap_alloced)
    -   [Eliminating \`HEAP\_ALLOCED\`
        completely](#eliminating-heap_alloced-completely)
        -   [Method 1: put static closures in an aligned
            section](#method-1-put-static-closures-in-an-aligned-section)
        -   [Method 2: copy static closures into a special area at
            startup](#method-2-copy-static-closures-into-a-special-area-at-startup)
-   [Heap and Stack checks](#heap-and-stack-checks)
-   [GHC Commentary: The Layout of Heap
    Objects](#ghc-commentary-the-layout-of-heap-objects)
    -   [Terminology](#terminology)
    -   [Heap Objects](#heap-objects)
    -   [Info Tables](#info-tables)
        -   [](#section-5)
    -   [Types of Payload Layout](#types-of-payload-layout)
        -   [Pointers-first layout](#pointers-first-layout)
        -   [Bitmap layout](#bitmap-layout)
    -   [Dynamic vs. Static objects](#dynamic-vs.-static-objects)
        -   [Dynamic objects](#dynamic-objects)
        -   [Static objects](#static-objects)
    -   [Types of object](#types-of-object)
        -   [Data Constructors](#data-constructors)
        -   [Function Closures](#function-closures)
        -   [Thunks](#thunks)
        -   [Selector thunks](#selector-thunks)
        -   [Partial applications](#partial-applications)
        -   [Generic application](#generic-application)
        -   [Stack application](#stack-application)
        -   [Indirections](#indirections)
        -   [Byte-code objects](#byte-code-objects)
        -   [Black holes](#black-holes)
        -   [Arrays](#arrays)
        -   [MVars](#mvars)
        -   [Weak pointers](#weak-pointers)
        -   [Stable Names](#stable-names)
        -   [Thread State Objects](#thread-state-objects)
        -   [STM objects](#stm-objects)
        -   [Forwarding Pointers](#forwarding-pointers)
    -   [How to add new heap objects](#how-to-add-new-heap-objects)
    -   [Change History](#change-history)
    -   [Speculation and Commentary](#speculation-and-commentary)
    -   [Record of performance improvements made to the Hoopl library
        starting January
        2012](#record-of-performance-improvements-made-to-the-hoopl-library-starting-january-2012)
-   [Haskell Program Coverage](#haskell-program-coverage)
    -   [Binary Tick Boxes](#binary-tick-boxes)
    -   [Machine Generated Haskell](#machine-generated-haskell)
-   [Compiling one module: !HscMain](#compiling-one-module-hscmain)
-   [The Diagram](#the-diagram)
-   [Picture of the main compiler
    pipeline](#picture-of-the-main-compiler-pipeline)
-   [The types](#the-types)
    -   [Decorating \`HsSyn\` with type
        information](#decorating-hssyn-with-type-information)
    -   [Source Locations](#source-locations)
-   [Interface files](#interface-files)
    -   [When is an interface file
        loaded?](#when-is-an-interface-file-loaded)
-   [Immix Garbage Collector](#immix-garbage-collector)
-   [The patches](#the-patches)
    -   [The main patch](#the-main-patch)
    -   [Line before inscreasing block
        size](#line-before-inscreasing-block-size)
    -   [Allocate in lines in minor
        GCs](#allocate-in-lines-in-minor-gcs)
    -   [Remove partial list](#remove-partial-list)
-   [To do](#to-do)
-   [GHC Source Tree Roadmap:
    includes/](#ghc-source-tree-roadmap-includes)
    -   [External APIs](#external-apis)
    -   [Derived Constants](#derived-constants)
    -   [Used when compiling via C](#used-when-compiling-via-c)
    -   [The RTS external APIs](#the-rts-external-apis)
    -   [Included into C-- (\`.cmm\`)
        code](#included-into-c---.cmm-code)
-   [Installing & Using the LLVM
    Back-end](#installing-using-the-llvm-back-end)
    -   [Installing](#installing)
    -   [LLVM Support](#llvm-support)
    -   [Using](#using)
    -   [Supported Platforms &
        Correctness](#supported-platforms-correctness)
    -   [Shared Libraries](#shared-libraries)
    -   [Performance](#performance)
-   [GHC Commentary:
    Libraries/Integer](#ghc-commentary-librariesinteger)
    -   [Selecting an Integer
        implementation](#selecting-an-integer-implementation)
    -   [The Integer interface](#the-integer-interface)
    -   [How Integer is handled inside
        GHC](#how-integer-is-handled-inside-ghc)
-   [An Integrated Code Generator for
    GHC](#an-integrated-code-generator-for-ghc)
    -   [Design elements](#design-elements)
    -   [Design philosophy](#design-philosophy)
    -   [Proposed compilation pipeline](#proposed-compilation-pipeline)
        -   [Convert from STG to control flow
            graph](#convert-from-stg-to-control-flow-graph)
        -   [Instruction selection](#instruction-selection)
        -   [Optimisation](#optimisation)
        -   [Proc-point analysis](#proc-point-analysis)
        -   [Register allocation](#register-allocation)
        -   [Stack layout](#stack-layout)
        -   [Tidy up](#tidy-up)
    -   [Machine-dependence](#machine-dependence)
-   [GHC Commentary: The byte-code interpreter and dynamic
    linker](#ghc-commentary-the-byte-code-interpreter-and-dynamic-linker)
    -   [Linker](#linker)
    -   [Bytecode Interpreter](#bytecode-interpreter)
-   [The I/O Manager](#the-io-manager)
-   [Key data types](#key-data-types)
-   [Kinds](#kinds)
    -   [Representing kinds](#representing-kinds)
    -   [Kind subtyping](#kind-subtyping)
-   [Linearity](#linearity)
-   [Ticky](#ticky)
    -   [Declarations for ticky
        counters](#declarations-for-ticky-counters)
-   [Strictness and let-floating](#strictness-and-let-floating)
-   [Coercions](#coercions)
-   [WARN: arity /](#warn-arity)
-   [Explaining demand transformers](#explaining-demand-transformers)
-   [Nofib stuff](#nofib-stuff)
-   [GHC Commentary: Libraries](#ghc-commentary-libraries)
-   [Building packages that GHC doesn't depend
    on](#building-packages-that-ghc-doesnt-depend-on)
-   [Classifying boot packages](#classifying-boot-packages)
    -   [Required or optional](#required-or-optional)
    -   [Coupling to GHC](#coupling-to-ghc)
    -   [Zero-boot packages](#zero-boot-packages)
    -   [Installation](#installation)
-   [Boot packages dependencies](#boot-packages-dependencies)
    -   [WARNING: Pattern matching in \`ghc-prim\`, \`integer-simple\`,
        and
        \`integer-gmp\`](#warning-pattern-matching-in-ghc-prim-integer-simple-and-integer-gmp)
-   [Repositories](#repositories)
-   [The LLVM backend](#the-llvm-backend)
-   [Loopification](#loopification)
-   [LLVM Mangler](#llvm-mangler)
    -   [TABLES\_NEXT\_TO\_CODE (TNTC)](#tables_next_to_code-tntc)
    -   [Stack Alignment](#stack-alignment)
    -   [SIMD / AVX](#simd-avx)
-   [Migrating Old Commentary](#migrating-old-commentary)
    -   [Before the Show Begins](#before-the-show-begins)
    -   [Genesis](#genesis)
    -   [The Beast Dissected](#the-beast-dissected)
    -   [RTS & Libraries](#rts-libraries)
    -   [Extensions, or Making a Complicated System More
        Complicated](#extensions-or-making-a-complicated-system-more-complicated)
-   [The Marvellous Module Structure of
    GHC](#the-marvellous-module-structure-of-ghc)
    -   [Compilation order is as
        follows:](#compilation-order-is-as-follows)
    -   [Typechecker stuff](#typechecker-stuff)
    -   [!HsSyn stuff](#hssyn-stuff)
    -   [Library stuff: base package](#library-stuff-base-package)
    -   [High-level Dependency Graph](#high-level-dependency-graph)
-   [Module Types](#module-types)
    -   [Module](#module)
    -   [!ModIface](#modiface)
    -   [!ModDetails](#moddetails)
        -   [!ModGuts](#modguts)
    -   [!ModSummary](#modsummary)
    -   [!HomeModInfo](#homemodinfo)
    -   [!HomePackageTable](#homepackagetable)
    -   [!ExternalPackageState](#externalpackagestate)
-   [Multi-instance packages](#multi-instance-packages)
    -   [!ToDo list](#todo-list)
    -   [Next step: dealing with ways](#next-step-dealing-with-ways)
-   [The type](#the-type-1)
    -   [The of a Name](#the-of-a-name)
    -   [Entities and ](#entities-and)
-   [Native Code Generator (NCG)](#native-code-generator-ncg)
    -   [Files, Parts](#files-parts)
    -   [Overview](#overview-3)
        -   [Translation into the Stix
            representation](#translation-into-the-stix-representation)
        -   [Instruction selection](#instruction-selection-1)
        -   [Register allocation](#register-allocation-1)
        -   [Spilling](#spilling)
        -   [Dealing with common cases
            fast](#dealing-with-common-cases-fast)
    -   [Complications, observations, and possible
        improvements](#complications-observations-and-possible-improvements)
        -   [Real vs virtual registers in the instruction
            selectors](#real-vs-virtual-registers-in-the-instruction-selectors)
    -   [Selecting insns for 64-bit values/loads/stores on 32-bit
        platforms](#selecting-insns-for-64-bit-valuesloadsstores-on-32-bit-platforms)
    -   [Shortcomings and inefficiencies in the register
        allocator](#shortcomings-and-inefficiencies-in-the-register-allocator)
        -   [Redundant reconstruction of the control flow
            graph](#redundant-reconstruction-of-the-control-flow-graph)
        -   [Really ridiculous method for doing
            spilling](#really-ridiculous-method-for-doing-spilling)
        -   [Redundant-move support for revised instruction selector
            suggestion](#redundant-move-support-for-revised-instruction-selector-suggestion)
    -   [x86 arcana that you should know
        about](#x86-arcana-that-you-should-know-about)
    -   [Generating code for ccalls](#generating-code-for-ccalls)
    -   [Duplicate implementation for many STG
        macros](#duplicate-implementation-for-many-stg-macros)
    -   [How to debug the NCG without losing your
        sanity/hair/cool](#how-to-debug-the-ncg-without-losing-your-sanityhaircool)
    -   [Historical page](#historical-page-1)
-   [Overview of modules in the new code
    generator](#overview-of-modules-in-the-new-code-generator)
    -   [The new Cmm data type](#the-new-cmm-data-type)
    -   [Module structure of the new code
        generator](#module-structure-of-the-new-code-generator)
        -   [Basic datatypes and
            infrastructure](#basic-datatypes-and-infrastructure)
        -   [Analyses and
            transformations](#analyses-and-transformations)
        -   [Linking the pipeline](#linking-the-pipeline)
        -   [Dead code](#dead-code)
    -   [Historical page](#historical-page-2)
-   [Design of the new code
    generator](#design-of-the-new-code-generator)
    -   [Overview](#overview-4)
    -   [The Cmm pipeline](#the-cmm-pipeline)
        -   [Branches to continuations and the "Adams
            optimisation"](#branches-to-continuations-and-the-adams-optimisation)
    -   [Runtime system](#runtime-system)
-   [NOTE: Historical page](#note-historical-page)
-   [Stupidity in the New Code
    Generator](#stupidity-in-the-new-code-generator)
    -   [Cantankerous Comparisons](#cantankerous-comparisons)
    -   [Dead stack/heap checks](#dead-stackheap-checks)
    -   [Instruction reordering](#instruction-reordering)
    -   [Stack space overuse](#stack-space-overuse)
    -   [Double temp-use means no
        inlinining?](#double-temp-use-means-no-inlinining)
    -   [Stupid spills](#stupid-spills)
    -   [Noppy proc-points](#noppy-proc-points)
    -   [Lots of temporary variables](#lots-of-temporary-variables)
    -   [Double proc points](#double-proc-points)
    -   [Rewriting stacks](#rewriting-stacks)
    -   [Spilling Hp/Sp](#spilling-hpsp)
    -   [Up and Down](#up-and-down)
    -   [Sp is generally stupid](#sp-is-generally-stupid)
    -   [Heap and R1 aliasing](#heap-and-r1-aliasing)
    -   [Historical page](#historical-page-3)
-   [GHC's glorious new code
    generator](#ghcs-glorious-new-code-generator)
    -   [Workflow for the new code generator and
        Hoopl](#workflow-for-the-new-code-generator-and-hoopl)
    -   [Status report April 2011](#status-report-april-2011)
-   [Old Code Generator (prior to
    GHC 7.8)](#old-code-generator-prior-to-ghc-7.8)
    -   [Storage manager
        representations](#storage-manager-representations)
    -   [Generated Cmm Naming
        Convention](#generated-cmm-naming-convention)
    -   [Modules](#modules)
        -   [](#section-6)
        -   [](#section-7)
        -   [](#section-8)
        -   [](#section-9)
        -   [](#section-10)
        -   [Memory and Register
            Management](#memory-and-register-management)
        -   [Function Calls and Parameter
            Passing](#function-calls-and-parameter-passing)
        -   [Misc utilities](#misc-utilities)
        -   [Special runtime support](#special-runtime-support)
-   [Ordering the Core-to-Core optimisation
    passes](#ordering-the-core-to-core-optimisation-passes)
    -   [This ordering obeys all the constraints
        except (5)](#this-ordering-obeys-all-the-constraints-except-5)
    -   [Constraints](#constraints-1)
        -   [1. float-in before strictness](#float-in-before-strictness)
        -   [2. Don't simplify between float-in and
            strictness](#dont-simplify-between-float-in-and-strictness)
        -   [3. Want full-laziness before
            foldr/build](#want-full-laziness-before-foldrbuild)
        -   [4. Want strictness after
            foldr/build](#want-strictness-after-foldrbuild)
        -   [5. Want full laziness after
            strictness](#want-full-laziness-after-strictness)
        -   [6. Want float-in after
            foldr/build](#want-float-in-after-foldrbuild)
        -   [7. Want simplify after
            float-inwards](#want-simplify-after-float-inwards)
        -   [8. If full laziness is ever done after
            strictness](#if-full-laziness-is-ever-done-after-strictness)
        -   [9. Ignore-inline-pragmas flag for final
            simplification](#ignore-inline-pragmas-flag-for-final-simplification)
        -   [10. Run Float Inwards once more after
            strictness-simplify](#run-float-inwards-once-more-after-strictness-simplify)
-   [Overall organisation of GHC](#overall-organisation-of-ghc)
-   [GHC source code](#ghc-source-code)
-   [Updates](#updates)
    -   [Unique](#unique)
    -   [Redesign (2014)](#redesign-2014)
-   [The data type and its friends](#the-data-type-and-its-friends)
    -   [Views of types](#views-of-types)
    -   [The representation of ](#the-representation-of)
    -   [Overloaded types](#overloaded-types)
    -   [Classifying types](#classifying-types)
-   [Package Compatibility](#package-compatibility)
    -   [1. Don't reorganise packages](#dont-reorganise-packages)
    -   [2. Provide older version(s) of base with a new GHC
        release](#provide-older-versions-of-base-with-a-new-ghc-release)
    -   [4. Allow packages to re-export
        modules](#allow-packages-to-re-export-modules)
    -   [4.1 Provide backwards-compatible versions of
        base](#provide-backwards-compatible-versions-of-base)
    -   [4.2 Rename base, and provide a compatibility
        wrapper](#rename-base-and-provide-a-compatibility-wrapper)
    -   [4.3 Don't rename base](#dont-rename-base)
    -   [5. Do some kind of provides/requires interface in
        Cabal](#do-some-kind-of-providesrequires-interface-in-cabal)
        -   [5.1 Make API specifications more
            symmetric](#make-api-specifications-more-symmetric)
        -   [5.2 Make API specifications
            explicit](#make-api-specifications-explicit)
        -   [5.3 Make API specifications more
            specific](#make-api-specifications-more-specific)
    -   [6. Distributions at the Hackage
        level](#distributions-at-the-hackage-level)
    -   [7. Allow package overlaps](#allow-package-overlaps)
    -   [The problem of lax version
        dependencies](#the-problem-of-lax-version-dependencies)
-   [Note about this page](#note-about-this-page)
-   [Explicit package imports](#explicit-package-imports)
    -   [Is the 'from <package>' compulsory?](#is-the-from-compulsory)
    -   [Package versions](#package-versions)
    -   [Importing from the home
        package](#importing-from-the-home-package)
    -   [The 'as P' alias](#the-as-p-alias)
    -   [Qualified names](#qualified-names)
    -   [Exporting modules from other
        packages](#exporting-modules-from-other-packages)
    -   [Syntax](#syntax)
        -   [Syntax formalised and
            summarised](#syntax-formalised-and-summarised)
        -   [Proposal for Package
            Mounting](#proposal-for-package-mounting)
        -   [Evaluation](#evaluation)
        -   [Note on Package Grafting](#note-on-package-grafting)
    -   [Alternative Proposal for Packages (with
        explicit namespaces)](#alternative-proposal-for-packages-with-explicit-namespaces)
    -   [A different, but related,
        problem](#a-different-but-related-problem)
    -   [Proposal](#proposal)
        -   [Naming a namespace](#naming-a-namespace)
        -   [What namespaces are available by
            default?](#what-namespaces-are-available-by-default)
        -   [Namespace resolution](#namespace-resolution)
        -   [Syntax](#syntax-1)
        -   [Exports](#exports-1)
        -   [Implicit imports](#implicit-imports)
        -   [Exposed vs Hidden packages](#exposed-vs-hidden-packages)
        -   [What if you wanted to import A.B.C from P1 and A.B.C from
            P2 into the *same*
            module?](#what-if-you-wanted-to-import-a.b.c-from-p1-and-a.b.c-from-p2-into-the-same-module)
-   [Package Reorg](#package-reorg)
    -   [Goals](#goals-1)
    -   [Proposal](#proposal-1)
        -   [What is in the Core
            Packages?](#what-is-in-the-core-packages)
        -   [Requirements to libraries to be included in core
            set](#requirements-to-libraries-to-be-included-in-core-set)
        -   [The base package](#the-base-package)
        -   [Other packages](#other-packages)
    -   [Testing](#testing-1)
    -   [Implementation-specific notes](#implementation-specific-notes)
        -   [Notes about GHC](#notes-about-ghc)
        -   [Notes about Hugs](#notes-about-hugs)
-   [Commentary: The Package System](#commentary-the-package-system)
    -   [Architecture](#architecture)
    -   [Identifying Packages](#identifying-packages)
    -   [Design constraints](#design-constraints)
    -   [The Plan](#the-plan-1)
        -   [Detecting ABI
            incompatibility](#detecting-abi-incompatibility)
        -   [Allowing ABI compatibilty](#allowing-abi-compatibilty)
-   [The Parser](#the-parser)
    -   [Principles](#principles)
    -   [Avoiding right-recursion](#avoiding-right-recursion)
    -   [Indentation](#indentation)
    -   [Syntax extensions](#syntax-extensions)
-   [Pinned Objects](#pinned-objects)
-   [Overview](#overview-5)
-   [The driver pipeline](#the-driver-pipeline)
-   [The compiler pipeline](#the-compiler-pipeline)
-   [Video](#video)
-   [Platforms](#platforms)
    -   [Limitations](#limitations)
    -   [Macros](#macros)
-   [Pointer Tagging](#pointer-tagging-1)
    -   [Meaning of the tag bits](#meaning-of-the-tag-bits)
    -   [Optimisations enabled by tag
        bits](#optimisations-enabled-by-tag-bits)
    -   [Garbage collection with tagged
        pointers](#garbage-collection-with-tagged-pointers)
    -   [Invariants](#invariants)
    -   [Compacting GC](#compacting-gc)
    -   [Dealing with tags in the code](#dealing-with-tags-in-the-code)
-   [Position-Independent Code and Dynamic
    Linking](#position-independent-code-and-dynamic-linking)
    -   [How to access symbols](#how-to-access-symbols)
    -   [CLabel.labelDynamic](#clabel.labeldynamic)
    -   [Info Tables](#info-tables-1)
    -   [Imported labels in
        SRTs (Windows)](#imported-labels-in-srts-windows)
    -   [PIC and dynamic linking support in the
        NCG](#pic-and-dynamic-linking-support-in-the-ncg)
    -   [How things are done on different
        platforms](#how-things-are-done-on-different-platforms)
        -   [Position dependent code](#position-dependent-code)
        -   [Position independent code](#position-independent-code)
    -   [Linking on ELF](#linking-on-elf)
    -   [Mangling dynamic library
        names](#mangling-dynamic-library-names)
-   [GHC Commentary: The C code
    generator](#ghc-commentary-the-c-code-generator)
    -   [Header files](#header-files)
    -   [Prototypes](#prototypes)
-   [Primitive Operations (!PrimOps)](#primitive-operations-primops)
    -   [The primops.txt.pp file](#the-primops.txt.pp-file)
    -   [Implementation of !PrimOps](#implementation-of-primops)
        -   [Inline !PrimOps](#inline-primops)
        -   [Out-of-line !PrimOps](#out-of-line-primops)
        -   [Foreign out-of-line !PrimOps and \`foreign import
            prim\`](#foreign-out-of-line-primops-and-foreign-import-prim)
    -   [Adding a new !PrimOp](#adding-a-new-primop)
-   [Profiling](#profiling)
    -   [Cost-centre profiling](#cost-centre-profiling)
    -   [Ticky-ticky profiling](#ticky-ticky-profiling)
-   [, , and ](#and)
    -   [The \`Module\` and \`ModuleName\`
        types](#the-module-and-modulename-types)
    -   [The type](#the-type-2)
-   [Recompilation Avoidance](#recompilation-avoidance)
    -   [What is recompilation
        avoidance?](#what-is-recompilation-avoidance)
    -   [Example](#example-2)
    -   [Why do we need recompilation
        avoidance?](#why-do-we-need-recompilation-avoidance)
        -   [GHCi and \`--make\`](#ghci-and---make)
        -   [\`make\`](#make)
    -   [How does it work?](#how-does-it-work)
        -   [Deciding whether to
            recompile](#deciding-whether-to-recompile)
        -   [Example](#example-3)
        -   [How does fingerprinting
            work?](#how-does-fingerprinting-work)
        -   [Mutually recursive groups of
            entities](#mutually-recursive-groups-of-entities)
        -   [Fixities](#fixities)
        -   [Instances](#instances)
        -   [Orphans](#orphans)
        -   [Rules](#rules)
        -   [On ordering](#on-ordering)
        -   [Packages](#packages)
        -   [Package version changes](#package-version-changes)
    -   [Interface stability](#interface-stability)
-   [The Register Allocator](#the-register-allocator-1)
    -   [Overview](#overview-6)
    -   [Code map](#code-map)
    -   [References](#references-2)
    -   [Register pressure in Haskell
        code](#register-pressure-in-haskell-code)
    -   [Hacking/Debugging](#hackingdebugging)
    -   [Runtime performance](#runtime-performance)
    -   [Possible Improvements](#possible-improvements)
-   [Haskell Excecution: Registers](#haskell-excecution-registers)
-   [Relevant GHC parts for Demand Analysis
    results](#relevant-ghc-parts-for-demand-analysis-results)
-   [Remembered Sets](#remembered-sets)
    -   [Remembered set maintenance during
        mutation](#remembered-set-maintenance-during-mutation)
        -   [Thunk Updates](#thunk-updates)
        -   [Mutable objects: MUT\_VAR,
            MVAR](#mutable-objects-mut_var-mvar)
        -   [Arrays: MUT\_ARR\_PTRS](#arrays-mut_arr_ptrs)
        -   [Threads: TSO](#threads-tso)
    -   [Remembered set maintenance during
        GC](#remembered-set-maintenance-during-gc)
-   [The renamer](#the-renamer)
    -   [The global renamer environment,
        ](#the-global-renamer-environment)
    -   [Unused imports](#unused-imports)
    -   [Name Space Management](#name-space-management)
    -   [Rebindable syntax](#rebindable-syntax)
-   [Replacing the Native Code
    Generator](#replacing-the-native-code-generator)
-   [Resource Limits](#resource-limits)
    -   [Code generation changes](#code-generation-changes)
        -   [Dynamic closure allocation](#dynamic-closure-allocation)
        -   [CAF Allocation](#caf-allocation)
        -   [Thunk code](#thunk-code)
        -   [Foreign calls](#foreign-calls)
    -   [Case split](#case-split)
    -   [Front-end changes](#front-end-changes)
-   [Garbage Collection Roots](#garbage-collection-roots)
-   [GHC Source Tree Roadmap: rts/](#ghc-source-tree-roadmap-rts)
    -   [Subdirectories of rts/](#subdirectories-of-rts)
    -   [Haskell Execution](#haskell-execution)
    -   [The \[wiki:Commentary/Rts/Storage Storage
        Manager\]](#the-wikicommentaryrtsstorage-storage-manager)
    -   [Data Structures](#data-structures)
    -   [The \[wiki:Commentary/Rts/Scheduler
        Scheduler\]](#the-wikicommentaryrtsscheduler-scheduler)
    -   [C files: the \[wiki:Commentary/Rts/FFI
        FFI\]](#c-files-the-wikicommentaryrtsffi-ffi)
    -   [The \[wiki:Commentary/Rts/Interpreter Byte-code
        Interpreter\]](#the-wikicommentaryrtsinterpreter-byte-code-interpreter)
    -   [\[wiki:Commentary/Profiling
        Profiling\]](#wikicommentaryprofiling-profiling)
    -   [RTS Debugging](#rts-debugging)
    -   [The Front Panel](#the-front-panel)
    -   [Other](#other)
    -   [OLD stuff](#old-stuff)
-   [Sanity Checking](#sanity-checking)
-   [The Scheduler](#the-scheduler)
    -   [OS Threads](#os-threads)
    -   [Haskell threads](#haskell-threads)
-   [Seq magic](#seq-magic)
    -   [The baseline position](#the-baseline-position)
        -   [Problem 1 (Trac \#1031)](#problem-1-trac-1031)
        -   [Problem 2 (Trac \#2273)](#problem-2-trac-2273)
        -   [Problem 3 (Trac \#5262)](#problem-3-trac-5262)
        -   [Problem 4: seq in the IO
            monad](#problem-4-seq-in-the-io-monad)
        -   [Problem 5: the need for special
            rules](#problem-5-the-need-for-special-rules)
-   [A better way](#a-better-way)
-   [The GHC Commentary: Signals](#the-ghc-commentary-signals)
    -   [Signal handling in the RTS](#signal-handling-in-the-rts)
        -   [The timer signal](#the-timer-signal)
    -   [The interrupt signal](#the-interrupt-signal)
    -   [Signal handling in Haskell
        code](#signal-handling-in-haskell-code)
    -   [RTS Alarm Signals and Foreign
        Libraries](#rts-alarm-signals-and-foreign-libraries)
-   [Slop](#slop)
    -   [Why do we want to avoid slop?](#why-do-we-want-to-avoid-slop)
    -   [How does slop arise?](#how-does-slop-arise)
    -   [What do we do about it?](#what-do-we-do-about-it)
-   [Layout of important files and
    directories](#layout-of-important-files-and-directories)
    -   [Files in \`\$(TOP)\`](#files-in-top)
    -   [\`libraries/\`](#libraries)
    -   [\`compiler/\`, \`docs/\`, \`ghc/\`](#compiler-docs-ghc)
    -   [\`rts/\`](#rts)
    -   [\`includes/\`](#includes)
    -   [\`utils/\`, \`libffi/\`](#utils-libffi)
    -   [\`driver/\`](#driver)
    -   [\`ghc-tarballs/\` (Windows only)](#ghc-tarballs-windows-only)
    -   [\`testsuite/\`, \`nofib/\`](#testsuite-nofib)
    -   [\`mk/\`, \`rules/\`](#mk-rules)
    -   [\`distrib/\`](#distrib)
    -   [Stuff that appears only in a build
        tree](#stuff-that-appears-only-in-a-build-tree)
        -   [\`inplace/\`](#inplace)
        -   [\`.../dist\*/\`](#dist)
    -   [Stack Layout](#stack-layout-1)
        -   [Representing Stack Slots](#representing-stack-slots)
        -   [Laying out the stack](#laying-out-the-stack)
        -   [A greedy algorithm](#a-greedy-algorithm)
-   [Layout of the stack](#layout-of-the-stack)
    -   [Info tables for stack frames](#info-tables-for-stack-frames)
    -   [Layout of the payload](#layout-of-the-payload)
    -   [Kinds of Stack Frame](#kinds-of-stack-frame)
-   [The STG syntax data types](#the-stg-syntax-data-types)
-   [GHC Commentary: Software Transactional
    Memory (STM)](#ghc-commentary-software-transactional-memory-stm)
-   [Background](#background)
    -   [Definitions](#definitions)
        -   [Useful RTS terms](#useful-rts-terms)
        -   [Transactional Memory terms](#transactional-memory-terms)
-   [Overview of Features](#overview-of-features)
    -   [Reading and Writing](#reading-and-writing)
    -   [Blocking](#blocking)
    -   [Choice](#choice)
    -   [Data Invariants](#data-invariants)
    -   [Exceptions](#exceptions)
-   [Overview of the Implementation](#overview-of-the-implementation)
    -   [Transactions that Read
        and Write.](#transactions-that-read-and-write.)
        -   [Transactional Record](#transactional-record)
        -   [Starting](#starting)
        -   [Reading](#reading)
        -   [Writing](#writing)
        -   [Validation](#validation)
        -   [Committing](#committing)
        -   [Aborting](#aborting)
        -   [Exceptions](#exceptions-1)
    -   [Blocking with ](#blocking-with)
    -   [Choice with ](#choice-with)
    -   [Invariants](#invariants-1)
        -   [Details](#details)
        -   [Changes from Choice](#changes-from-choice)
    -   [Other Details](#other-details)
        -   [Detecting Long Running
            Transactions](#detecting-long-running-transactions)
        -   [Transaction State](#transaction-state)
        -   [GC and ABA](#gc-and-aba)
        -   [Management of s](#management-of-s)
        -   [Tokens and Version Numbers.](#tokens-and-version-numbers.)
        -   [Implementation Invariants](#implementation-invariants)
        -   [Fine Grain Locking](#fine-grain-locking)
    -   [Bibliography](#bibliography)
-   [GHC Commentary: Storage](#ghc-commentary-storage)
-   [General overview](#general-overview)
-   [IMPORTANT NOTE](#important-note)
-   [The demand analyzer](#the-demand-analyzer)
    -   [Important datatypes](#important-datatypes)
-   [Symbol Names](#symbol-names)
    -   [Tuples](#tuples)
    -   [Unboxed Tuples](#unboxed-tuples)
    -   [Alphanumeric Characters](#alphanumeric-characters)
    -   [Constructor Characters](#constructor-characters)
    -   [Variable Characters](#variable-characters)
    -   [Other](#other-1)
    -   [Examples](#examples)
-   [The monad for renaming, typechecking,
    desugaring](#the-monad-for-renaming-typechecking-desugaring)
-   [Kirsten's sketchy notes on getting ticky to
    work](#kirstens-sketchy-notes-on-getting-ticky-to-work)
-   [The GHC Commentary: Checking
    Types](#the-ghc-commentary-checking-types)
    -   [The Overall Flow of Things](#the-overall-flow-of-things)
        -   [Entry Points Into the Type
            Checker](#entry-points-into-the-type-checker)
        -   [Renaming and Type Checking a
            Module](#renaming-and-type-checking-a-module)
    -   [Type Checking a Declaration
        Group](#type-checking-a-declaration-group)
    -   [Type checking Type and Class
        Declarations](#type-checking-type-and-class-declarations)
    -   [More Details](#more-details)
        -   [Types Variables and Zonking](#types-variables-and-zonking)
        -   [Type Representation](#type-representation)
        -   [Type Checking Environment](#type-checking-environment)
        -   [Expressions](#expressions-1)
        -   [Handling of Dictionaries and Method
            Instances](#handling-of-dictionaries-and-method-instances)
    -   [Connection with GHC's Constraint
        Solver](#connection-with-ghcs-constraint-solver)
    -   [Generating Evidence](#generating-evidence)
    -   [The Solver](#the-solver)
        -   [Given Constraints](#given-constraints)
        -   [Derived Constraints](#derived-constraints)
        -   [Wanted Constraints](#wanted-constraints)
-   [The data type and its friends](#the-data-type-and-its-friends-1)
    -   [Views of types](#views-of-types-1)
    -   [The representation of ](#the-representation-of-1)
    -   [Overloaded types](#overloaded-types-1)
    -   [Classifying types](#classifying-types-1)
    -   [Unique](#unique-1)
    -   [Current design](#current-design)
        -   [Known-key things](#known-key-things)
        -   [Interface files](#interface-files-1)
    -   [Redesign (2014)](#redesign-2014-1)
-   [Unpacking primitive fields](#unpacking-primitive-fields)
    -   [Goals and non-goals](#goals-and-non-goals)
    -   [Detailed design](#detailed-design)
    -   [Benchmarks](#benchmarks)
-   [Unused imports](#unused-imports-1)
    -   [The current story](#the-current-story)
    -   [Examples](#examples-1)
    -   [Specfication](#specfication)
    -   [Implementation](#implementation-1)
    -   [Algorithm](#algorithm)
-   [Updates](#updates-1)
-   [The user manual](#the-user-manual)
-   [GHC Boot Library Version
    History](#ghc-boot-library-version-history)
-   [GHC Commentary: Weak Pointers and
    Finalizers](#ghc-commentary-weak-pointers-and-finalizers)
-   [Work in Progress on the LLVM
    Backend](#work-in-progress-on-the-llvm-backend)
    -   [LLVM IR Representation](#llvm-ir-representation)
    -   [TABLES\_NEXT\_TO\_CODE](#tables_next_to_code-1)
    -   [LLVM Alias Analysis Pass](#llvm-alias-analysis-pass)
    -   [Optimise LLVM for the type of Code GHC
        produces](#optimise-llvm-for-the-type-of-code-ghc-produces)
    -   [Update the Back-end to use the new Cmm data types / New Code
        Generator](#update-the-back-end-to-use-the-new-cmm-data-types-new-code-generator)
    -   [LLVM's Link Time Optimisations](#llvms-link-time-optimisations)
    -   [LLVM Cross Compiler / Port](#llvm-cross-compiler-port)
    -   [Get rid of Proc Point
        Splitting](#get-rid-of-proc-point-splitting)
    -   [Don't Pass Around Dead STG
        Registers](#dont-pass-around-dead-stg-registers)
-   [Wired-in and known-key things](#wired-in-and-known-key-things)
    -   [Wired-in things](#wired-in-things)
    -   [Known-key things](#known-key-things-1)
    -   [Initialisation](#initialisation)
    -   [\`Orig\` \`RdrName\` things](#orig-rdrname-things)
-   [GHC Commentary: The Word](#ghc-commentary-the-word)

GHC Source Code Abbreviations
=============================

Certain abbreviations are used pervasively throughout the GHC source
code. This page gives a partial list of them and their expansion:

-   **ANF**: A-normal form

<!-- -->

-   **CAF**: Constant Applicative Form

<!-- -->

-   **Class**: Type Class

<!-- -->

-   **Cmm**: The final IR used in GHC, based on the C-- language

<!-- -->

-   **Core**: GHC core language. Based on System FC (variant of
    System F). Represents a type-checked and desugared program in some
    (out of several) intermediate compilation step

<!-- -->

-   **CoreFV**: Free variables in core

<!-- -->

-   **!CoreLint**: Type and sanity-checking of core. (Lint: Jargon for a
    program analysis that looks for bug-suspicious code.)

<!-- -->

-   **!CoreSubst**: Substitution in core

<!-- -->

-   **!CoreSyn**: Core abstract syntax

<!-- -->

-   **!DataCon**: Data constructor

<!-- -->

-   **Ds**: Desugarer

<!-- -->

-   **Gbl**: Global

<!-- -->

-   **Hs**: Haskell Syntax (generally as opposed to Core, for example,
    Expr vs !HsExpr)

<!-- -->

-   **Hsc**: Haskell compiler. Means it Deals with compiling a single
    module and no more.

<!-- -->

-   **!HsSyn**: Haskell abstract syntax

<!-- -->

-   **Id**: Synonym for Var, but indicating a term variable

<!-- -->

-   **Iface**: Interface, as in Haskell interface (.hi) files

<!-- -->

-   **!IfaceSyn**: Interface abstract syntax

<!-- -->

-   **LHs**: Located Haskell something

<!-- -->

-   **Loc**: Location, as in !SrcLoc

<!-- -->

-   **Located**: Something annotated with a !SrcSpan

<!-- -->

-   **Lcl**: Local

<!-- -->

-   **nativeGen**: Native code generator (generates assembly from Cmm)

<!-- -->

-   **Occ**: Occurrence

`*However,inthecontextof`[`OccName`](http://hackage.haskell.org/trac/ghc/wiki/Commentary/Compiler/RdrNameType#TheOccNametype)`,"occurrence"actuallymeans"classified(i.e.asatypename,valuename,etc)butnotqualifiedandnotyetresolved"`

-   **PId**: Package ID

<!-- -->

-   **!PprCore**: Pretty-printing core

<!-- -->

-   **Rdr**: Parser (or reader)

<!-- -->

-   **Rn**: Rename or Renamer

<!-- -->

-   **Rts**: Run Time System

<!-- -->

-   **!SimplCore**: Simplify core (the so-called simplifier belongs to
    this, as does the strictness analyser)

<!-- -->

-   **!SrcLoc**: Source location (filename, line number,
    character position)

<!-- -->

-   **!SrcSpan**: Source location span (filename, start line number and
    character position, end line number and character position)

<!-- -->

-   **STG**: \[Commentary/Compiler/StgSynType Spineless Tagless
    G-machine\]

<!-- -->

-   **Tc**: !TypeCheck{ing,er}

<!-- -->

-   **TSO**: [Thread State
    Object](https://ghc.haskell.org/trac/ghc/wiki/Commentary/Rts/Storage/HeapObjects#ThreadStateObjects)

<!-- -->

-   **!TyCon**: Type constructor

<!-- -->

-   **!TyThing**: Something that is type-checkable

<!-- -->

-   **Ty**: Type

<!-- -->

-   **!TyVar**: Synonym for Var, but indicating a type variable

<!-- -->

-   **Var**: A variable with some information about its type (or kind)

Aging in the generational GC
============================

Aging is an important technique in generational GC: the idea is that
objects that have only recently been allocated have not had sufficient
chance to die, and so promoting them immediately to the next generation
may lead to retention of unnecessary data. The problem is amplified if
the prematurely promoted objects are thunks that are subsequently
updated, leading to retention of an arbitrary amount of live data until
the next collection of the old generation, which may be a long time
coming.

The idea is that instead of promoting live objects directly from
generation 0 into generation 1, they stay in generation 0 for a "while",
and if they live long enough, they get promoted. The simplest way is to
segment the objects in generation 0 by the number of collections they
have survived, up to a maximum. GHC 6.12 used to do this: each
generation had a tunable number of *steps*. Objects were initially
promoted to step 0, copied through each subsequent step on following GC
cycles, and then eventually promoted to the next generation.

Measurement we made showed that the optimal number of steps was
somewhere between 1 and 3 (2 was almost always better than either 1 or
3). In priniciple it is possible to have a fractional number of steps,
although GHC 6.12 only supported integral numbers.

In GHC 6.13 and later, we made the following change: each block now
points to the generation to which objects in that block will be copied
in the next GC (the \`dest\` field of \`bdescr\`). This lets us decide
on a block-by-block basis which objects to promote and which to retain
in a generation, and lets us implement fractional numbers of steps. At
the same time, we dropped the notion of explicit steps, so each
generation just has a single list of blocks. This means that we can no
longer do aging of more than 2 GC cycles, but since the measurements
showed that this was unlikely to be beneficial, and the new structure is
much simpler, we felt it was worthwhile.

Blocks in the nursery have a \`dest\` field pointing to generation 0,
and blocks of live objects in generation 0 have a \`dest\` field
pointing to generation 1. This gives us the same effect as 2 steps did
in the GHC 6.12, except that intermediate generations (e.g. gen 1 in a
3-gen setup) now only have one step rather than 2. We could implement
aging in the intermediate generations too if that turns out to be
beneficial (more than 2 generations is rarely better than 2, according
to our measurements).

Improving LLVM Alias Analysis
=============================

This page tracks the information and progress relevant to improving the
alias analysis pass for the LLVM backend of GHC.

This correspond to bug \#5567.

LLVM Alias Analysis Infrastructure
----------------------------------

Some links to the various documentation on LLVM's AA support:

`*`[`LLVM` `Alias` `Analysis`
`Infrastructure`](http://llvm.org/docs/AliasAnalysis.html)\
`*`[`LLVM's` `Analysis` `and` `Transform`
`Passes`](http://llvm.org/docs/Passes.html)\
`*`[`The` `Often` `Misunderstood` `GEP`
`Instruction`](http://llvm.org/docs/GetElementPtr.html)\
`*`[`LLVM` `Language` `Reference`](http://llvm.org/docs/LangRef.html)\
`*`[`LLVM` `Dev` `List:` `Comparison` `of` `Alias` `Analysis` `in`
`LLVM`](http://groups.google.com/group/llvm-dev/browse_thread/thread/2a5944692508bcc2/363c96bb1c6a506d?show_docid=363c96bb1c6a506d&pli=1)

Max's Work
----------

Max had a crack at writing a custom alias analysis pass for LLVM,
relevant links are:

`*`[`Email` `to` `LLVM`
`dev`](http://lists.cs.uiuc.edu/pipermail/llvmdev/2011-September/043603.html)\
`*`[`Blog` `post` `about`
`results`](http://blog.omega-prime.co.uk/?p=135)\
`*`[`A` `port` `to` `LLVM`
`3.6`](https://github.com/bgamari/ghc-llvm-analyses)

TBAA
----

LLVM as of version 2.9 includes Type Based Alias Analysis. This mean
using metadata you can specify a type hierarchy (with alias properties
between types) and annotate your code with these types to improve the
alias information. This should allow us to improve the alias analysis
without any changes to LLVM itself like Max made.

`*`[`LLVM` `TBBA` `Doc`](http://llvm.org/docs/LangRef.html#tbaa)

STG / Cmm Alias Properties
--------------------------

**Question** (David Terei): What alias properties does the codegen obey?
Sp and Hp never alias? R<n> registers never alias? ....

**Answer** (Simon Marlow): Sp\[\] and Hp\[\] never alias, R\[\] never
aliases with Sp\[\], and that's about it.

*' Simon*': As long as it propagates properly, such that every F(Sp) is
a stack pointer, where F() is any expression context except a
dereference. That is, we better be sure that

is "stack", not "heap".

How to Track TBAA information
-----------------------------

Really to be sound and support Cmm in full we would need to track and
propagate TBAA information. It's Types after all! At the moment we
don't. We simply rely on the fact that the Cmm code generated for loads
and stores is nearly always in the form of:

That is to say, it has the values it depends on for the pointer
derivation in-lined in the load or store expression. It is very rarely
of the form:

And when it is, 'it is' (unconfirmed) always deriving a "heap" pointer,
"stack" pointers are always of the in-line variety. This assumption if
true allows us to look at just a store or load in isolation to properly
Type it.

There are two ways to type this 'properly'.

1\. Do data flow analysis. This is the only proper way to do it but also
annoying. 2. Do block local analysis. Instead of doing full blow data
flow analysis, just track the type of pointers stored to CmmLocal regs
at the block level. This is safe but just may miss some opportunities
when a CmmLocal's value is assigned in another block... My hunch is this
is quite rare so this method should be fairly effective (and easier to
implement and quicker to run that 1.)

LLVM type system
----------------

The above aliasing information can be encoded as follows:

The fact that \`R\[\]\` never aliases with \`Sp\[\]\` is never used as
the one way relation isn't expressible in LLVM.

Stores/loads needs to be annotated with \`!tbaa\` and one of the above
four types e.g.

Problems / Optmisations to Solve
--------------------------------

### LLVM Optimisations

Roman reported that running 'opt -std-compile-opts' gives much better
code than running 'opt -O3'.

**Following is from Roman Leschinskiy**

'-O2 -std-compile-opts' does the trick but it's obviously overkill
because it essentially executes the whole optimisation pipeline twice.
The crucial passes seem to be loop rotation and loop invariant code
motion. These are already executed twice by -O2 but it seems that they
don't have enough information then and that something interesting
happens in later passes which allows them to work much better the third
time.

### Safe Loads (speculative load)

We want to allow LLVM to speculatively hoist loads out of conditional
blocks. Relevant LLVM source code is here:

`*`[`SimplifyCFG` `Source`
`Code`](http://llvm.org/docs/doxygen/html/SimplifyCFG_8cpp_source.html)\
`*`[`llvm::isSafeToSpeculativelyExecute`](http://llvm.org/docs/doxygen/html/namespacellvm.html#a4899ff634bf732c16dd22ecfdafdea7d)\
`*`[`LLVM` `Mailing` `List` `Discussion` `about` `'Safe`
`loads'`](http://lists.cs.uiuc.edu/pipermail/llvmdev/2012-January/046958.html)

**Following is from Roman Leshchinskiy**

I've poked around a bit and things are rather complicated. So far I've
identified two problems. Here is a small example function:

This is the interesting C-- bit:

Look at what indexDoubleArray\# compiles to: F64\[I32\[Sp + 12\] + ((R1
&lt;&lt; 3) + 8)\]. We would very much like LLVM to hoist the
I32\[Sp+12\] bit (i.e., loading the pointer to the ByteArray data) out
of the loop because that might allow all sorts of wonderful optimisation
such as promoting it to a register. But alas, this doesn't happen, LLVM
leaves the load in the loop. Why? Because it assumes that the load might
fail (for instance, if Sp is NULL) and so can't move it past
conditionals. We know, of course, that this particular load can't fail
and so can be executed speculatively but there doesn't seem to be a way
of communicating this to LLVM.

As a quick experiment, I hacked LLVM to accept "safe" annotations on
loads and then manually annotated the LLVM assembly generated by GHC and
that helped quite a bit. I suppose that's the way to go - we'll have to
get this into LLVM in some form and then the backend will have to
generate those annotations for loads which can't fail. I assume they are
loads through the stack pointer and perhaps the heap pointer unless
we're loading newly allocated memory (those loads can't be moved past
heap checks). In any case, the stack pointer is the most important
thing. I can also imagine annotating pointers (such as Sp) rather than
instructions but that doesn't seem to be the LLVM way and it's also less
flexible.

### GHC Heap Check (case merging)

See bug \#1498

**Following is from Roman Leshchinskiy**

I investigated heap check a bit more and it seems to me that it's
largely GHC's fault. LLVM does do loop unswitching which correctly pulls
out loop-invariant heap checks but that happens fairly late in its
pipeline and heap checks interfere with optimisations before that.

However, we really shouldn't be generating those heap checks in the
first place. Here is a small example loop:

This is the C-- that GHC generates:

Note how in each loop iteration, we add 12 to Hp, then do the heap check
and then subtract 12 from Hp again. I really don't think we should be
generating that and then relying on LLVM to optimise it away.

This happens because GHC commons up heap checks for case alternatives
and does just one check before evaluating the case. The relevant comment
from CgCase.lhs is this:

A more interesting situation is this:

where !x! indicates a possible heap-check point. The heap checks in the
alternatives **can** be omitted, in which case the topmost heapcheck
will take their worst case into account.

This certainly makes sense if A allocates. But with vector-based code at
least, a lot of the time neither A nor C will allocate **and** C will
tail-call A again so by pushing the heap check into !A!, we are now
doing it **in** the loop rather than at the end.

It seems to me that we should only do this if A actually allocates and
leave the heap checks in the alternatives if it doesn't (perhaps we
could also use a common heap check if **all** alternatives allocate). I
tried to hack this and see what happens but found the code in CgCase and
friends largely incomprehensible. What would I have to change to
implement this (perhaps controlled by a command line flag) and is it a
good idea at all?

GHC Commentary: The GHC API
===========================

This section of the commentary describes everything between
\[wiki:Commentary/Compiler/HscMain HscMain\] and the front-end; that is,
the parts of GHC that coordinate the compilation of multiple modules.

The GHC API is rather stateful; the state of an interaction with GHC is
stored in an abstract value of type . The only fundamental reason for
this choice is that the models the state of the RTS's linker, which must
be single-threaded.

Although the GHC API apparently supports multiple clients, because each
can be interacting with a different , in fact it only supports one
client that is actually executing code, because the
\[wiki:Commentary/Rts/Interpreter\#Linker RTS linker\] has a single
global symbol table.

This part of the commentary is not a tutorial on *using* the GHC API:
for that, see [Using GHC as a
Library](http://haskell.org/haskellwiki/GHC/As_a_library). Here we are
going to talk about the implementation.

A typical interaction with the GHC API goes something like the
following:

`*Youprobablywanttowrapthewholeprogramin``togeterrormessages`\
`*Createanewsession:`\
`*Settheflags:``,``.`\
`*Addsome`*`targets`*`:``,``,`\
`*Perform`[`ref(Dependency`
`Analysis)`](ref(Dependency_Analysis) "wikilink")`:`\
`*Load(compile)thesourcefiles:`

Warning: Initializing GHC is tricky! Here is a template that seems to
initialize GHC and a session. Derived from ghc's Main.main function.

You must pass the path to as an argument to .

The field of tells the compiler what kind of output to generate from
compilation. There is unfortunately some overlap between this and the
passed to ; we hope to clean this up in the future, but for now it's
probably a good idea to make sure that these two settings are consisent.
That is, if , then , if then .

Targets
-------

The targets specify the source files or modules at the top of the
dependency tree. For a Haskell program there is often just a single
target , but for a library the targets would consist of every visible
module in the library.

The type is defined in
[GhcFile(compiler/main/HscTypes.lhs)](GhcFile(compiler/main/HscTypes.lhs) "wikilink").
Note that a includes not just the file or module name, but also
optionally the complete source text of the module as a : this is to
support an interactive development environment where the source file is
being edited, and the in-memory copy of the source file is to be used in
preference to the version on disk.

Dependency Analysis
-------------------

The dependency analysis phase determines all the Haskell source files
that are to be compiled or loaded in the current session, by traversing
the transitive dependencies of the targets. This process is called the
*downsweep* because we are traversing the dependency tree downwards from
the targets. (The *upsweep*, where we compile all these files happens in
the opposite direction of course).

The function takes the targets and returns a list of consisting of all
the modules to be compiled/loaded.

The !ModSummary type
--------------------

A (defined in
[GhcFile(compiler/main/HscTypes.lhs)](GhcFile(compiler/main/HscTypes.lhs) "wikilink"))
contains various information about a module:

`*Its``,whichincludesthepackagethatitbelongsto`\
`*Its``,whichliststhepathnamesofallthefilesassociatedwiththemodule`\
`*Themodulesthatitimports`\
`*Thetimeitwaslastmodified`\
`*...someotherthings`

We collect information for all the modules we are interested in during
the *downsweep*, below. Extracting the information about the module name
and the imports from a source file is the job of
[GhcFile(compiler/main/HeaderInfo.hs)](GhcFile(compiler/main/HeaderInfo.hs) "wikilink")
which partially parses the source file.

Converting a given module name into a is done by in
[GhcFile(compiler/main/GHC.hs)](GhcFile(compiler/main/GHC.hs) "wikilink").
Similarly, if we have a filename rather than a module name, we generate
a using .

Loading (compiling) the Modules
-------------------------------

When the dependency analysis is complete, we can load these modules by
calling . The same interface is used regardless of whether we are
loading modules into GHCi with the command, or compiling a program with
: we always end up calling .

The process in principle is fairly simple:

`*Visiteachmoduleinthedependencytreefromthebottomup,invoking[wiki:Commentary/Compiler/HscMainHscMain]`\
`tocompileit(the`*`upsweep`*`).`\
`*Finally,linkallthecodetogether.InGHCithisinvolvesloadingalltheobjectcodeintomemoryandlinkingit`\
`withthe[wiki:Commentary/Rts/Interpreter#LinkerRTSlinker],andthenlinkingallthebyte-codetogether.In`\
```modethisinvolvesinvokingtheexternallinkertolinktheobjectcodeintoabinary.`

The process is made more tricky in practice for two reasons:

`*Wemightnotneedtocompilecertainmodules,ifnoneoftheirdependencieshavechanged.GHC's`\
`[wiki:Commentary/Compiler/RecompilationAvoidancerecompilationchecker]determineswhetheramodulereallyneeds`\
`tobecompiledornot.`\
`*InGHCi,wemightjustbereloadingtheprogramaftermakingsomechanges,sowedon'tevenwanttore-link`\
`modulesforwhichnodependencieshavechanged.`

GHC Commentary: Asynchronous Exceptions
=======================================

GHC Commentary: Backends
========================

After \[wiki:Commentary/Compiler/CmmType Cmm\] has been generated, we
have a choice of targets to compile to:

`*[wiki:Commentary/Compiler/Backends/PprCTheCcodegenerator]`\
`*[wiki:Commentary/Compiler/Backends/NCGThenativecodegenerator]`\
`*[wiki:Commentary/Compiler/Backends/LLVMTheLLVMcodegenerator]`\
`*[wiki:Commentary/Compiler/Backends/GHCiTheGHCicodegenerator]`

These backends are completely interchangeable. Our preferred route is
the native code generator. The C code generator is used for portable,
non-optimised, or unregisterised compilation (Note that the LLVM backend
also supports building GHC in unregisterised mode as well as
registerised mode so it is usually the preferred route for porting GHC).

Types in the back end (aka "The \`Rep\` swamp")
===============================================

I have completed a major representation change, affecting both old and
new code generators, of the various \`Rep\` types. It's pervasive in
that it touches a lot of files; and in the native code-gen very many
lines are changed. The new situation is much cleaner.

Here are the highlights of the new design.

\`CmmType\`
-----------

There is a new type \`CmmType\`, defined in module \`CmmExpr\`, which is
just what it sounds like: it's the type of a \`CmmExpr\` or a
\`CmmReg\`.

`` *A`CmmType`is ``*`abstract`*`` :itsrepresentationisprivateto`CmmExpr`.Thatmakesiteasytochangerepresentation. ``\
`` *A`CmmType`isactuallyjustapairofa`Width`andacategory(`CmmCat`). ``\
`` *The`Width`typeisexportedandwidelyusedinpattern-matching,butitdoeswhatitsaysonthetin:widthonly. ``\
`` *Incontrast,the`CmmCat`typeisentirelyprivateto`CmmExpr`.Itisjustanenumerationthatallowsustodistinguish:floats,gcpointers,andother. ``

Other important points are these:

`` *Each`LocalReg`hasa`CmmType`attached;thisreplacesthepreviousunsavourycombinationof`MachRep`and`CmmKind`.Indeed,bothofthelatteraregoneentirely. ``

`` *Noticethata`CmmType`accuratelyknowsaboutgc-pointer-hood.Ultimatelywewillabandonstatic-reference-tablegenerationinSTGsyntax,andinsteadgenerateSRTsfromtheCmmcode.We'llneedtoupdatetheRTS`.cmm`filestodeclarepointer-hood. ``

`` *Thetype`TyCon.PrimRep`remains;itenumeratestherepresentationsthataHaskellvaluecantake.Differencesfrom`CmmType`: ``\
`` *`PrimRep`contains`VoidRep`,but`CmmType`hasnozero-widthform. ``\
`` *`CmmType`includessub-wordwidthvalues(e.g.8-bit)which`PrimRep`doesnot. ``\
`` Thefunction`primRepCmmType`convertsanon-void`PrimRep`toa`CmmType`. ``

`` *`CmmLint`iscomplainsifyouassignagc-ptrtoanon-gc-ptrandviceversa.Ittreats"gc-ptr+constant"asagc-ptr. ``

``*`NB:` `you'd` `better` `not` `make` `an` `interior` `pointer`
`live` `across` `a`
`call`*`,elsewe'llsaveitonthestackandtreatitasaGCroot.It'snotclearhowtoguaranteethisdoesn'thappenastheresultofsomeoptimisation.`

**Parsing \`.cmm\` RTS files.** The global register \`P0\` is a
gc-pointer version of \`R0\`. They both map to the same physical
register, though!

The \`MachOp\` type
-------------------

The \`MachOp\` type enumerates (in machine-independent form) the
available machine instructions. The principle they embody is that
*everything except the width is embodied in the opcode*. In particular,
we have

`` *`MO_S_Lt`,`MO_U_Lt`,and`MO_F_Lt`forcomparison(signed,unsigned,andfloat). ``\
`` *`MO_SS_Conv`,`MO_SF_Conv`etc,forconversion(`SS`issigned-to-signed,`SF`issigned-to-float,etc). ``

These constructor all take \`Width\` arguments.

The \`MachOp\` data type is defined in \`CmmExpr\`, not in a separate
\`MachOp\` module.

Foreign calls and hints
-----------------------

In the new Cmm representation (\`ZipCfgCmmRep\`), but not the old one,
arguments and results to all calls, including foreign ones, are ordinary
\`CmmExpr\` or \`CmmReg\` respectively. The extra information we need
for foreign calls (is this signed? is this an address?) are kept in the
calling convention. Specifically:

`` *`MidUnsafeCall`callsa`MidCallTarget` ``\
`` *`MidCallTarget`iseithera`CallishMachOp`ora`ForeignTarget` ``\
`` *Inthelattercasewesupplya`CmmExpr`(thefunctiontocall)anda`ForeignConvention` ``\
`` *A`ForeignConvention`containstheCcallingconvention(stdcall,ccalletc),andalistof`ForiegnHints`forargumentsandforresults.(Wemightwanttorenamethistype.) ``

This simple change was horribly pervasive. The old Cmm rep (and Michael
Adams's stuff) still has arguments and results being (argument,hint)
pairs, as before.

Native code generation and the \`Size\` type
--------------------------------------------

The native code generator has an instruction data type for each
architecture. Many of the instructions in these data types used to have
a \`MachRep\` argument, but now have a \`Size\` argument instead. In
fact, so far as the native code generators are concerned, these \`Size\`
types (which can be machine-specific) are simply a plug-in replacement
for \`MachRep\`, with one big difference: **\`Size\` is completely local
to the native code generator** and hence can be changed at will without
affecting the rest of the compiler.

\`Size\` is badly named, but I inherited the name from the previous
code.

I rather think that many instructions should have a \`Width\` parameter,
not a \`Size\` parameter. But I didn't feel confident to change this.
Generally speaking the NCG is a huge swamp and needs re-factoring. I'm
working on getting Backtraces in GHC. Progress can be seen here:
<https://github.com/abacathoo/ghc>

The Block Allocator
===================

Source:
[GhcFile(includes/rts/storage/Block.h)](GhcFile(includes/rts/storage/Block.h) "wikilink"),
[GhcFile(rts/sm/BlockAlloc.h)](GhcFile(rts/sm/BlockAlloc.h) "wikilink"),
[GhcFile(rts/sm/BlockAlloc.c)](GhcFile(rts/sm/BlockAlloc.c) "wikilink"),
[GhcFile(includes/rts/storage/MBlock.h)](GhcFile(includes/rts/storage/MBlock.h) "wikilink"),
[GhcFile(rts/sm/MBlock.c)](GhcFile(rts/sm/MBlock.c) "wikilink").

The block allocator is where the storage manager derives much of its
flexibilty. Rather than keep our heap in a single contiguous region of
memory, or one contiguous region per generation, we manage linked lists
of memory blocks. Managing contiguous regions is difficult, especially
when you want to change the size of some of the areas. A
block-structured storage arrangement has several advantages:

`*resizingareasofmemoryiseasy:justchainmoreblocksontothelist.`

`*managinglargeobjectswithoutcopyingiseasy:allocateeachoneacompleteblock,andusetheblocklinkageto`\
`chainthemtogether.`

`*freememorycanberecycledfaster,becauseablockisablock.`

The concept relies on the property that most data objects are
significantly smaller than a block, and only rarely do we need to
allocate objects that approach or exceed the size of a block.

Structure of blocks
-------------------

We want to allocate memory in units of a small block (around 4k, say).
Furthermore, we want each block to have an associated small structure
called a *block descriptor*, which contains information about the block:
its link field, which generation it belongs to, and so on. This is
similar to the well-known "BiBOP" (Big Bag of Pages) technique, where
objects with similar tags are collected together on a page so as to
avoid needing to store an individual tag with each object.

We want a function \`Bdescr(p)\`, that, given an arbitrary pointer into
a block, returns the address of the block descriptor that corresponds to
the block containing that pointer.

There are two options:

`` *Puttheblockdescriptoratthestartoftheblock.`Bdescr(p)=p&~BLOCK_SIZE`.Thisoptionhasproblemsif ``\
`weneedtoallocateacontiguousregionlargerthanasingleblock(GHCdoesthisoccasionallywhenallocating`\
`alargenumberofobjectsinonego).`

`*Allocatememoryinlargerunits(a`*`megablock`*`),dividethemegablockintoblocks,andputalltheblock`\
`descriptorsatthebeginning.Themegablockisaligned,sothattheaddressoftheblockdescriptorfor`\
`ablockisasimplefunctionofitsaddress.The'Bdescr'functionismorecomplicatedthanthefirst`\
`method,butitiseasiertoallocatecontiguousregions(unlessthecontiguousregionislargerthan`\
`amegablock...).`

We adopt the second approach. The following diagram shows a megablock:

[Image(sm-block.png)](Image(sm-block.png) "wikilink")

We currently have megablocks of 1Mb in size (m = 20) with blocks of 4k
in size (k = 12), and these sizes are easy to change
([GhcFile(includes/rts/Constants.h)](GhcFile(includes/rts/Constants.h) "wikilink")).

Block descriptors are currently 32 or 64 bytes depending on the word
size (d = 5 or 6). The block descriptor itself is the structure
\`bdescr\` defined in
[GhcFile(includes/rts/storage/Block.h)](GhcFile(includes/rts/storage/Block.h) "wikilink"),
and that file also defines the \`Bdescr()\` macro.

The block allocator has a the following structure:

`*Atthebottom,talkingtotheOS,isthemegablockallocator(`[`GhcFile(rts/sm/MBlock.c)`](GhcFile(rts/sm/MBlock.c) "wikilink")`,`[`GhcFile(includes/rts/storage/MBlock.h)`](GhcFile(includes/rts/storage/MBlock.h) "wikilink")`).`\
`Itisresponsiblefordeliveringmegablocks,correctlyaligned,totheupperlayers.Itisalsoresponsiblefor`\
`implementing[wiki:Commentary/HeapAllocedHEAP_ALLOCED()]:thepredicatethattestswhetherapointerpointstodynamicallyallocatedmemory`\
`ornot.Thisisimplementedasasimplebitmaplookupona32-bitmachine,andsomethingmorecomplexon`\
`64-bitaddressedmachines.See`[`GhcFile(includes/rts/storage/MBlock.h)`](GhcFile(includes/rts/storage/MBlock.h) "wikilink")`fordetails.`\
``[`br`](br "wikilink")[`br`](br "wikilink")\
`Currently,megablocksareneverfreedbacktotheOS,exceptattheendoftheprogram.Thisisapotential`\
`improvementthatcouldbemade.`

`*Sittingontopofthemegablockallocatoristheblocklayer(`[`GhcFile(includes/rts/storage/Block.h)`](GhcFile(includes/rts/storage/Block.h) "wikilink")`,`[`GhcFile(rts/sm/BlockAlloc.c)`](GhcFile(rts/sm/BlockAlloc.c) "wikilink")`).`\
`Thislayerisresponsibleforproviding:`

`Thesefunctionsallocateanddeallocateablock`*`group`*`:acontiguoussequenceofblocks(thedegenerate,andcommon,case`\
`isasingleblock).Theblockallocatorisresponsibleforkeepingtrackoffreeblocks.Currentlyitdoesthisby`\
`maintaininganordered(byaddress)listoffreeblocks,withcontiguousblockscoallesced.Howeverthisiscertanly`\
`notoptimal,andhasbeenshowntobeabottleneckincertaincases-improvingthisallocationschemewouldbegood.`

GHC Commentary: Garbage Collecting CAFs
=======================================

Files: [GhcFile(rts/sm/GC.c)](GhcFile(rts/sm/GC.c) "wikilink"), function
scavange\_srt in
[GhcFile(rts/sm/Scav.h)](GhcFile(rts/sm/Scav.h) "wikilink")

Constant Applicative Forms, or CAFs for short, are top-level values
defined in a program. Essentially, they are objects that are not
allocated dynamically at run-time but, instead, are part of the static
data of the program. Sometimes, a CAF may refer to many values in the
heap. To avoid memory leaks in such situations, we need to know when a
CAF is never going to be used again, and so we can deallocate the values
that it refers to.

See Note \[CAF management\] in
[GhcFile(rts/sm/Storage.c)](GhcFile(rts/sm/Storage.c) "wikilink") for
more information.

Static Reference Tables
-----------------------

File:
[GhcFile(includes/rts/storage/InfoTables.h)](GhcFile(includes/rts/storage/InfoTables.h) "wikilink")

The info table of various closures may contain information about what
static objects are referenced by the closure. This information is stored
in two parts:

`1.astaticreferencetable(SRT),whichisanarrayofreferencestostaticobjects`\
`2.abitmaskwhichspecifieswhichoftheobjectsareactuallyusedbytheclosure.`

There are two different ways to access this information depending on the
size of the SRT:

`*"small":if``isasmallbitmap,notall1s,thenGET_FUN?_SRTcontainstheSRT.`\
`*"large":if``isall1s,thenGET_FUN?_SRTcontainsalargebitmap,andtheactualSRT.`

Evacuating Static Objects
-------------------------

Files:
[GhcFile(rts/sm/GCThread.h)](GhcFile(rts/sm/GCThread.h) "wikilink"),
[GhcFile(rts/sm/Evac.c)](GhcFile(rts/sm/Evac.c) "wikilink"),
[GhcFile(rts/sm/GC.c)](GhcFile(rts/sm/GC.c) "wikilink")

While scavenging objects, we also process (aka "evacuate") any static
objects that need to be kept alive. When a GC thread discovers a live
static object, it places it on its list. Later, this list is used to
scavange the static objects, potentially finding more live objects. Note
that this process might find more static objects, and thus further
extend the list.

When a static object is scavenged, it is removed from and placed on
another list, called . Later, we use this list to "clean up" the
liveness markers from these static objects, so that we can repeat the
process on the next garbage collection. Note that we can't "clean up"
the liveness markers as we go along because we use them to notice cycles
among the static objects.

Calling Convention
==================

Entry conventions are very conventional: the first N argumements in
registers and the rest on the stack.

Return Convention
=================

All returns are now *direct*; that is, a return is made by jumping to
the code associated with the
\[wiki:Commentary/Rts/Storage/HeapObjects\#InfoTables info table\] of
the topmost \[wiki:Commentary/Rts/Storage/Stack stack frame\].

GHC used to have a more complex return convention called vectored
returns in which some stack frames pointed to vectors of return
addresses; this was dropped in GHC 6.8 after measurements that showed it
was not (any longer) worthwhile.

Historical page
---------------

This page is a bunch of notes on the new code generator. It is outdated
and is here only for historical reasons.It should probably be removed.
See \[wiki:Commentary/Compiler/CodeGen Code Generator\] page for a
description of current code generator.

Cleanup after the new codegen is enabled
========================================

The new codegen was enabled by default in
832077ca5393d298324cb6b0a2cb501e27209768. Now that the switch has been
made, we can remove all the cruft associated with the old code
generator. There are dependencies between some of the components, so we
have to do things in the right order. Here is a list of the cleanup
tasks, and notes about dependencies:

Independent tasks
-----------------

`` *Use`BlockId`or`Label`consistently,currentlyweuseamixtureofthetwo.Maybegetridofthe`BlockId`module. ``

`` *Removelive-varandCAFlistsfrom`StgSyn`,andthencleanup`CoreToStg` ``

`` *DONE:RemovetheSRTpassin`simplStg/SRT.lhs` ``

`*DONE:removeRET_DYNfromtheRTS`

`` *DONE:remove`-fnew-codegen`,related`HscMain`bitsandthe`CodeGen`module. ``

`` *DONE:remove`CmmOpt.cmmMiniInline`,itisnotusedanymore ``

`` *Fixthelayering:`cmm`modulesshouldnotdependon`codeGen/StgCmm*` ``

Towards removing codeGen/Cg\*
-----------------------------

`` *DONE:`CmmParse`shouldproducenew`Cmm`. ``\
`` *Wewillprobablywanttwokindsof`.cmm`file,onethatistobefedthrough`CmmLayoutStack`andonethatisn't. ``\
`` *primopswillbefedthrough`CmmLayoutStack`,andwillusethenativecallingconvention,withthecodegeneratorinsertingthecopyin/copyoutforus. ``

`` *DONE:Removeallthe`Cg*`modules ``

Towards removing \`OldCmm\`
---------------------------

`` *INPROGRESS(SimonM):ChangetheNCGovertoconsumenew`Cmm`.WepossiblyalsowantthegeneratednativecodetousetheHooplBlockrepresentation,althoughthatwillmeanchangingbranchinstructionstohavebothtrueandfalsetargets,ratherthantrueandfallthroughaswehavenow. ``

`` *Remove`cmm/CmmCvt`(thiswillsavesomecompile-timetoo) ``

`` *Remove`cmm/OldCmm*`,`cmm/PprOldCmm`etc. ``

Later
-----

`*DothenewSRTstory(!ToDo:writeawikipageaboutthis)`

Cmm: Implementing Exception Handling
====================================

The IEEE 754 specification for floating point numbers defines exceptions
for certain floating point operations, including:

`*rangeviolation(overflow,underflow);`\
`*roundingerrors(inexact);`\
`` *invalidoperation(invalidoperand,suchascomparisonwitha`NaN`value,thesquarerootofanegativenumberordivisionofzerobyzero);and, ``\
`*zerodivide(aspecialcaseofaninvalidoperation).`

Many architectures support floating point exceptions by including a
special register as an addition to other exception handling registers.
The IBM PPC includes the \`FPSCR\` ("Floating Point Status Control
Register"); the Intel x86 processors use the \`MXCSR\` register. When
the PPC performs a floating point operation it checks for possible
errors and sets the \`FPSCR\`. Some processors allow a flag in the
Foating-Point Unit (FPU) status and control register to be set that will
disable some exceptions or the entire FPU exception handling facility.
Some processors disable the FPU after an exception has occurred while
others, notably Intel's x86 and x87 processors, continue to perform FPU
operations. Depending on whether quiet !NaNs (QNaNs) or signaling !NaNs
(SNaNs) are used by the software, an FPU exception may signal an
interrupt for the software to pass to its own exception handler.

Some higher level languages provide facilities to handle these
exceptions, including Ada, Fortran (F90 and later), C++ and C (C99,
fenv.h, float.h on certain compilers); others may handle such exceptions
without exposing a low-level interface. There are three reasons to
handle FPU exceptions, and these reasons apply similarly to other
exceptions:

`*thefacilitiesprovidegreatercontrol;`\
`*thefacilitiesareefficient--moreefficientthanahigher-levelsoftwaresolution;and,`\
`*FPUexceptionsmaybeunavoidable,especiallyifseveralFPUoperationsareseriallyperformedatthemachinelevelsothehigherlevelsoftwarehasnoopportunitytochecktheresultsinbetweenoperations.`

#### An Integral Exception Example

There has been at least one problem in GHC that would benefit from
exception handling--in some cases, for \`Integral\`s. See bug ticket
\#1042. The bug occurs in \`show\`ing the number, in
\[GhcFile(libraries/base/GHC/Show.lhs) GHC.Show\], \`showSignedInt\`,
before conversion from base\_2 to base\_10, where a negative \`Int\`
(always \`Int32\`) is negated in order to process it as a positive value
when converting it to a string, base\_10, causing an overflow error on
some architectures. (Bear in mind that it would show up here in the
example for \#1042 because the function would be evaluated in GHCi here;
the negation is the problem and the exception shows up in the *next*
instruction on that operand, here \`DIV\`.)

The exception example in \#1042 does not occur on PowerPC machines,
which dutifully print the two's complement of : \`0\`. (\`-2147483648\`
is the minimum bound for signed Ints, so negating it should properly
become, bitwise, a positive \`2147483647\` (all but bit 31 set); once
negated again when divided by \`-1\` this would be \`0\`; \`-0\` is
converted to \`0\`.) On some architectures such as Intel 64 and IA-32,
negating the minimum bound does not wrap around to \`0\` but overflows,
which is reported as a floating point "overflow" (\`\#O\`) exception:
the \`NEG\` instruction modifies the \`OF\` flag (bit 11) in the
\`EFLAGS\` register--curiously enough, the \`DIV\` and \`IDIV\`
instructions have *undefined* effects on the \`OF\` flag.

The workaround was to avoid negating \`minBound\` \`Int\`s; note that no
Intel instructions allow one to modify the \`OF\` flag directly.
Alternative solutions might be to

`` 1.maskthe"exception"byclearingtheinterruptflag,`IF`,usingthe`CLI`instruction;or, ``\
`` 1.conditionallyunsettheflagbyusingthe`PUSHF`instructiononthe`EFLAGS`registertopushitslowerword(bits15-0,includingtheoffendingbit11(`OF`))ontothestack,resetthe`OF`bit,thenpushthatbackontothestackandpopitintoEFLAGSwith`POPF`.Dependingonvariableregisterused,theassembleroutputwouldlooksimilarto: ``

#### A Floating Point Exception Example

There was a long message thread on the Haskell-prime mailing list,
"realToFrac Issues," beginning with [John Meacham's
message](http://www.haskell.org/pipermail/haskell-prime/2006-February/000791.html)
and ending with [Simon Marlow's
message](http://www.haskell.org/pipermail/haskell-prime/2006-March/000840.html).
The following code for converting a Float to a Double will *fail* to
produce a floating point exception or NaN on x86 machines (recall that
0.0/0.0 is NaN *and* a definite FPU exception):

\[in GHCi-6.6 on PowerPC, OS X\]:

This bug is not due to the lack of FPU exceptions in Cmm but bears
mention as the internal conversion performed in 'realToFrac' on 'Float's
would benefit from FPU exceptions: with Haskell-support for FPU
exceptions this realToFrac would be able to issue an exception for NaN,
Infinity or rounding errors when converting a Float to a Double and vice
versa. There is a related problem with rounding errors in the functions
'encodeFloat', 'decodeFloat', 'encodeDouble' and 'decodeDouble', see
\[wiki:ReplacingGMPNotes/TheCurrentGMPImplementation\].

On 5 May 2008, Isaac Dupree asked

`Istheredocumentation(e.g.ontheGHCCommentarysomewhereIcan't`\
`find)anexplanationofwhatC--"kinds"areorhowthey'reuseful/used?`

Probably not. GHC Cmm is a sort of pidgin version of C-- 2.0, and true
C-- kinds are explained in the [C-- specification, section
5.1](http://www.cminusminus.org/code.html).

`WhenIwasportabilizingthatcodeareaawhileagoIhadignorantly`\
`changedsomeoftheusesof"kind"to"hint"forconsistency(bothnames`\
`hadbeenbeingusedforthesamethingviatype-synonym.)andbecauseI`\
`couldguesshowthecodemakesenseifitwas,informally,ahintabout`\
`whattodo.`

Hint was the word used originally, and several people (including
reviewers) objected to it on the grounds that the 'hints' are actually
mandatory to get the compiler to do what you want (e.g., pass arguments
in floating-point registers). So we changed the name to 'kind'.

If you like dense, indigestible academic papers full of formalism,
there's [one I'm quite proud
of](http://www.cs.tufts.edu/~nr/pubs/staged-abstract.html). It explains
in detail how kinds are useful for specifying and implementing procedure
calling conventions, which is the use to which they are put within GHC.

Norman Ramsey

### Note To Reader

This page was written with more detail than usual since you may need to
know how to work with Cmm as a programming language. Cmm is the basis
for the future of GHC, Native Code Generation, and if you are interested
in hacking Cmm at least this page might help reduce your learning curve.
As a finer detail, if you read the \[wiki:Commentary/Compiler/HscMain
Compiler pipeline\] wiki page or glanced at the diagram there you may
have noticed that whether you are working backward from an
\`intermediate C\` (Haskell-C "HC", \`.hc\`) file or an Assembler file
you get to Cmm before you get to the STG language, the Simplifier or
anything else. In other words, for really low-level debugging you may
have an easier time if you know what Cmm is about. Cmm also has
opportunities for implementing small and easy hacks, such as little
optimisations and implementing new Cmm Primitive Operations.

A portion of the \[wiki:Commentary/Rts RTS\] is written in Cmm:
[GhcFile(rts/Apply.cmm)](GhcFile(rts/Apply.cmm) "wikilink"),
[GhcFile(rts/Exception.cmm)](GhcFile(rts/Exception.cmm) "wikilink"),
[GhcFile(rts/HeapStackCheck.cmm)](GhcFile(rts/HeapStackCheck.cmm) "wikilink"),
[GhcFile(rts/PrimOps.cmm)](GhcFile(rts/PrimOps.cmm) "wikilink"),
[GhcFile(rts/StgMiscClosures.cmm)](GhcFile(rts/StgMiscClosures.cmm) "wikilink"),
[GhcFile(rts/StgStartup.cmm)](GhcFile(rts/StgStartup.cmm) "wikilink")
and [GhcFile(StgStdThunks.cmm)](GhcFile(StgStdThunks.cmm) "wikilink").
(For notes related to \`PrimOps.cmm\` see the \[wiki:Commentary/PrimOps
PrimOps\] page; for much of the rest, see the
\[wiki:Commentary/Rts/HaskellExecution HaskellExecution\] page.) Cmm is
optimised before GHC outputs either HC or Assembler. The C compiler
(from HC, pretty printed by
[GhcFile(compiler/cmm/PprC.hs)](GhcFile(compiler/cmm/PprC.hs) "wikilink"))
and the \[wiki:Commentary/Compiler/Backends/NCG Native Code Generator\]
(NCG) \[wiki:Commentary/Compiler/Backends Backends\] are closely tied to
data representations and transformations performed in Cmm. In GHC, Cmm
roughly performs a function similar to the intermediate [Register
Transfer Language (RTL)](http://gcc.gnu.org/onlinedocs/gccint/RTL.html)
in GCC.

Table of Contents
=================

`1.[wiki:Commentary/Compiler/CmmType#AdditionsinCmmAdditionsinCmm]`\
`1.[wiki:Commentary/Compiler/CmmType#CompilingCmmwithGHCCompilingCmmwithGHC]`\
`1.[wiki:Commentary/Compiler/CmmType#BasicCmmBasicCmm]`\
`1.[wiki:Commentary/Compiler/CmmType#CodeBlocksinCmmCodeBlocksinCmm]`\
`*[wiki:Commentary/Compiler/CmmType#BasicBlocksandProceduresBasicBlocksandProcedures]`\
`1.[wiki:Commentary/Compiler/CmmType#VariablesRegistersandTypesVariables,RegistersandTypes]`\
`1.[wiki:Commentary/Compiler/CmmType#LocalRegistersLocalRegisters]`\
`1.[wiki:Commentary/Compiler/CmmType#GlobalRegistersandHintsGlobalRegistersandHints]`\
`1.[wiki:Commentary/Compiler/CmmType#DeclarationandInitialisationDeclarationandInitialisation]`\
`1.[wiki:Commentary/Compiler/CmmType#MemoryAccessMemoryAccess]`\
`1.[wiki:Commentary/Compiler/CmmType#LiteralsandLabelsLiteralsandLabels]`\
`*[wiki:Commentary/Compiler/CmmType#LabelsLabels]`\
`1.[wiki:Commentary/Compiler/CmmType#SectionsandDirectivesSectionsandDirectives]`\
`*[wiki:Commentary/Compiler/CmmType#TargetDirectiveTargetDirective]`\
`1.[wiki:Commentary/Compiler/CmmType#ExpressionsExpressions]`\
`*[wiki:Commentary/Compiler/CmmType#QuasioperatorSyntaxQuasi-operatorSyntax]`\
`1.[wiki:Commentary/Compiler/CmmType#StatementsandCallsStatementsandCalls]`\
`*[wiki:Commentary/Compiler/CmmType#CmmCallsCmmCalls]`\
`1.[wiki:Commentary/Compiler/CmmType#OperatorsandPrimitiveOperationsOperatorsandPrimitiveOperations]`\
`1.[wiki:Commentary/Compiler/CmmType#OperatorsOperators]`\
`1.[wiki:Commentary/Compiler/CmmType#PrimitiveOperationsPrimitiveOperations]`\
`1.[wiki:Commentary/Compiler/CmmType#CmmDesign:ObservationsandAreasforPotentialImprovementCmmDesign:ObservationsandAreasforPotentialImprovement]`

The Cmm language
================

\`Cmm\` is the GHC implementation of the \`C--\` language; it is also
the extension of Cmm source code files: \`.cmm\` (see
\[wiki:Commentary/Rts/Cmm What the hell is a .cmm file?\]). The GHC
\[wiki:Commentary/Compiler/CodeGen Code Generator\] (\`CodeGen\`)
compiles the STG program into \`C--\` code, represented by the \`Cmm\`
data type. This data type follows the [definition of
\`C--\`](http://www.cminusminus.org/) pretty closely but there are some
remarkable differences. For a discussion of the Cmm implementation
noting most of those differences, see the
\[wiki:Commentary/Compiler/CmmType\#BasicCmm Basic Cmm\] section, below.

`*`[`GhcFile(compiler/cmm/Cmm.hs)`](GhcFile(compiler/cmm/Cmm.hs) "wikilink")`:themaindatatypedefinition.`\
`*`[`GhcFile(compiler/cmm/MachOp.hs)`](GhcFile(compiler/cmm/MachOp.hs) "wikilink")`` :datatypesdefiningthemachineoperations(e.g.floatingpointdivide)providedby`Cmm`. ``\
`*`[`GhcFile(compiler/cmm/CLabel.hs)`](GhcFile(compiler/cmm/CLabel.hs) "wikilink")`` :datatypefortop-level`Cmm`labels. ``

`*`[`GhcFile(compiler/cmm/PprCmm.hs)`](GhcFile(compiler/cmm/PprCmm.hs) "wikilink")`` :pretty-printerfor`Cmm`. ``\
`*`[`GhcFile(compiler/cmm/CmmUtils.hs)`](GhcFile(compiler/cmm/CmmUtils.hs) "wikilink")`` :operationsover`Cmm` ``

`*`[`GhcFile(compiler/cmm/CmmLint.hs)`](GhcFile(compiler/cmm/CmmLint.hs) "wikilink")`:aconsistencychecker.`\
`*`[`GhcFile(compiler/cmm/CmmOpt.hs)`](GhcFile(compiler/cmm/CmmOpt.hs) "wikilink")`` :anoptimiserfor`Cmm`. ``

`*`[`GhcFile(compiler/cmm/CmmParse.y)`](GhcFile(compiler/cmm/CmmParse.y) "wikilink")`,`[`GhcFile(compiler/cmm/CmmLex.x)`](GhcFile(compiler/cmm/CmmLex.x) "wikilink")`:parserandlexerfor[wiki:Commentary/Rts/Cmm.cmmfiles].`

`*`[`GhcFile(compiler/cmm/PprC.hs)`](GhcFile(compiler/cmm/PprC.hs) "wikilink")`` :pretty-print`Cmm`inCsyntax,whencompilingviaC. ``

Additions in Cmm
----------------

Although both Cmm and C-- allow foreign calls, the \`.cmm\` syntax
includes the

The \[R2\] part is the (set of) register(s) that you need to save over
the call.

Other additions to C-- are noted throughout the
\[wiki:Commentary/Compiler/CmmType\#BasicCmm Basic Cmm\] section, below.

Compiling Cmm with GHC
----------------------

GHC is able to compile \`.cmm\` files with a minimum of user-effort. To
compile \`.cmm\` files, simply invoke the main GHC driver but remember
to:

`` *addtheoption`-dcmm-lint`ifyouhavehandwrittenCmmcode; ``\
`*addappropriateincludes,especially`[`GhcFile(includes/Cmm.h)`](GhcFile(includes/Cmm.h) "wikilink")`` ifyouareusingCmmmacrosorGHCdefinesforcertaintypes,suchas`W_`for`bits32`or`bits64`(dependingonthemachinewordsize)--`Cmm.h`isinthe`/includes`directoryofeveryGHCdistribution,i.e.,`usr/local/lib/ghc-6.6/includes`;and, ``\
`` *ifyoudoincludeGHCheaderfiles,remembertopassthecodethroughtheCpreprocessorbyaddingthe`-cpp`option. ``

For additional fun, you may pass GHC the \`-keep-s-file\` option to keep
the temporary assembler file in your compile directory. For example:
This will only work with very basic Cmm files. If you noticed that GHC
currently provides no \`-keep-cmm-file\` option and \`-keep-tmp-files\`
does not save a \`.cmm\` file and you are thinking about redirecting
output from \`-ddump-cmm\`, beware. The output from \`-ddump-cmm\`
contains equal-lines and dash-lines separating Cmm Blocks and Basic
Blocks; these are unparseable. The parser also cannot handle \`const\`
sections. For example, the parser will fail on the first \`0\` or
alphabetic token after \`const\`: Although GHC's Cmm pretty printer
outputs C-- standard parenthetical list of arguments after procedure
names, i.e., \`()\`, the Cmm parser will fail at the \`(\` token. For
example: The Cmm procedure names in
[GhcFile(rts/PrimOps.cmm)](GhcFile(rts/PrimOps.cmm) "wikilink") are not
followed by a (possibly empty) parenthetical list of arguments; all
their arguments are Global (STG) Registers, anyway, see
\[wiki:Commentary/Compiler/CmmType\#VariablesRegistersandTypes
Variables, Registers and Types\], below. Don't be confused by the
procedure definitions in other handwritten \`.cmm\` files in the RTS,
such as [GhcFile(rts/Apply.cmm)](GhcFile(rts/Apply.cmm) "wikilink"):
all-uppercase procedure invocations are special reserved tokens in
[GhcFile(compiler/cmm/CmmLex.x)](GhcFile(compiler/cmm/CmmLex.x) "wikilink")
and
[GhcFile(compiler/cmm/CmmParse.y)](GhcFile(compiler/cmm/CmmParse.y) "wikilink").
For example, \`INFO\_TABLE\` is parsed as one of the tokens in the Alex
\`info\` predicate:

GHC's Cmm parser also cannot parse nested code blocks. For example: The
C-- specification example in section 4.6.2, "Procedures as section
contents" also will not parse in Cmm: Note that if \`p (bits32 i) { ...
}\` were written as a Cmm-parseable procedure, as \`p { ... }\`, the
parse error would occur at the closing curly bracket for the \`section
"data" { ... p { ... } }\`&lt;- here.

Basic Cmm
---------

FIXME: The links in this section are dead. But the files can be found
here: [1](http://www.cs.tufts.edu/~nr/c--/index.html). Relevant
discussion about the documentations of C--:
[2](https://mail.haskell.org/pipermail/ghc-devs/2014-September/006301.html)

Cmm is a high level assembler with a syntax style similar to C. This
section describes Cmm by working up from assembler--the C-- papers and
specification work down from C. At the least, you should know what a
"high level" assembler is, see ["What is a High Level
Assembler?"](http://webster.cs.ucr.edu/AsmTools/HLA/HLADoc/HLARef/HLARef3.html#1035157).
Cmm is different than other high level assembler languages in that it
was designed to be a semi-portable intermediate language for compilers;
most other high level assemblers are designed to make the tedium of
assembly language more convenient and intelligible to humans. If you are
completely new to C--, I highly recommend these papers listed on the
[C-- Papers](http://cminusminus.org/papers.html) page:

`*`[`C--:` `A` `Portable` `Assembly` `Language` `that` `Supports`
`Garbage` `Collection`
`(1999)`](http://cminusminus.org/abstracts/ppdp.html)`(PaperpagewithAbstract)`\
`*`[`C--:` `A` `Portable` `Assembly` `Language`
`(1997)`](http://cminusminus.org/abstracts/pal-ifl.html)`(PaperpagewithAbstract)`\
`*`[`A` `Single` `Intermediate` `Language` `That` `Supports` `Multiple`
`Implementations` `of` `Exceptions`
`(2000)`](http://cminusminus.org/abstracts/c--pldi-00.html)`(PaperpagewithAbstract)`\
`*`[`The` `C--` `Language` `Specification` `Version` `2.0` `(CVS`
`Revision` `1.128,` `23` `February`
`2005)`](http://cminusminus.org/extern/man2.pdf)`(PDF)`

Cmm is not a stand alone C-- compiler; it is an implementation of C--
embedded in the GHC compiler. One difference between Cmm and a C--
compiler like [Quick C--](http://cminusminus.org/code.html) is this: Cmm
uses the C preprocessor (cpp). Cpp lets Cmm *integrate* with C code,
especially the C header defines in
[GhcFile(includes)](GhcFile(includes) "wikilink"), and among many other
consequences it makes the C-- \`import\` and \`export\` statements
irrelevant; in fact, according to
[GhcFile(compiler/cmm/CmmParse.y)](GhcFile(compiler/cmm/CmmParse.y) "wikilink")
they are ignored. The most significant action taken by the Cmm modules
in the Compiler is to optimise Cmm, through
[GhcFile(compiler/cmm/CmmOpt.hs)](GhcFile(compiler/cmm/CmmOpt.hs) "wikilink").
The Cmm Optimiser generally runs a few simplification passes over
primitive Cmm operations, inlines simple Cmm expressions that do not
contain global registers (these would be left to one of the
\[wiki:Commentary/Compiler/Backends Backends\], which currently cannot
handle inlines with global registers) and performs a simple loop
optimisation.

### Code Blocks in Cmm

The Haskell representation of Cmm separates contiguous code into:

`*`*`modules`*`` (compilationunits;a`.cmm`file);and ``\
`*`*`basic` `blocks`*

Cmm modules contain static data elements (see
\[wiki:Commentary/Compiler/CmmType\#LiteralsandLabels Literals and
Labels\]) and \[wiki:Commentary/Compiler/CmmType\#BasicBlocks:Procedures
Basic Blocks\], collected together in \`Cmm\`, a type synonym for
\`GenCmm\`, defined in
[GhcFile(compiler/cmm/Cmm.hs)](GhcFile(compiler/cmm/Cmm.hs) "wikilink"):
\`CmmStmt\` is described in
\[wiki:Commentary/Compiler/CmmType\#StatementsandCalls Statements and
Calls\];[BR](BR "wikilink") \`Section\` is described in
\[wiki:Commentary/Compiler/CmmType\#SectionsandDirectives Sections and
Directives\];[BR](BR "wikilink") the static data in \`\[d\]\` is
\[\`CmmStatic\`\] from the type synonym \`Cmm\`;[BR](BR "wikilink")
\`CmmStatic\` is described in
\[wiki:Commentary/Compiler/CmmType\#LiteralsandLabels Literals and
Labels\].

#### Basic Blocks and Procedures

Cmm procedures are represented by the first constructor in \`GenCmmTop d
i\`: For a description of Cmm labels and the \`CLabel\` data type, see
the subsection \[wiki:Commentary/Compiler/CmmType\#LiteralsandLabels
Literals and Labels\], below.

Cmm Basic Blocks are labeled blocks of Cmm code ending in an explicit
jump. Sections (see
\[wiki:Commentary/Compiler/CmmType\#SectionsandDirectives Sections and
Directives\]) have no jumps--in Cmm, Sections cannot contain nested
Procedures (see, e.g.,
\[wiki:Commentary/Compiler/CmmType\#CompilingCmmwithGHC Compiling Cmm
with GHC\]). Basic Blocks encapsulate parts of Procedures. The data type
\`GenBasicBlock\` and the type synonym \`CmmBasicBlock\` encapsulate
Basic Blocks; they are defined in
[GhcFile(compiler/cmm/Cmm.hs)](GhcFile(compiler/cmm/Cmm.hs) "wikilink"):
The \`BlockId\` data type simply carries a \`Unique\` with each Basic
Block. For descriptions of \`Unique\`, see

`*the[wiki:Commentary/Compiler/RenamerRenamer]page;`\
`*the[wiki:Commentary/Compiler/WiredIn#KnownkeythingsKnownKeyThings]sectionofthe[wiki:Commentary/Compiler/WiredInWired-inandKnownKeyThings]page;and,`\
`*the[wiki:Commentary/Compiler/EntityTypes#TypevariablesandtermvariablesTypevariablesandtermvariables]sectionofthe[wiki:Commentary/Compiler/EntityTypesEntityTypes]page.`

### Variables, Registers and Types

Like other high level assembly languages, all variables in C-- are
machine registers, separated into different types according to bit
length (8, 16, 32, 64, 80, 128) and register type (integral or floating
point). The C-- standard specifies little more type information about a
register than its bit length: there are no distinguishing types for
signed or unsigned integrals, or for "pointers" (registers holding a
memory address). A C-- standard compiler supports additional information
on the type of a register value through compiler *hints*. In a foreign
call, a \`"signed" bits8\` would be sign-extended and may be passed as a
32-bit value. Cmm diverges from the C-- specification on this point
somewhat (see below). C-- and Cmm do not represent special registers,
such as a Condition Register (\`CR\`) or floating point unit (FPU)
status and control register (\`FPSCR\` on the PowerPC, \`MXCSR\` on
Intel x86 processors), as these are a matter for the
\[wiki:Commentary/Compiler/Backends Backends\].

C-- and Cmm hide the actual number of registers available on a
particular machine by assuming an "infinite" supply of registers. A
backend, such as the NCG or C compiler on GHC, will later optimise the
number of registers used and assign the Cmm variables to actual machine
registers; the NCG temporarily stores any overflow in a small memory
stack called the *spill stack*, while the C compiler relies on C's own
runtime system. Haskell handles Cmm registers with three data types:
\`LocalReg\`, \`GlobalReg\` and \`CmmReg\`. \`LocalReg\`s and
\`GlobalRegs\` are collected together in a single \`Cmm\` data type:

#### Local Registers

Local Registers exist within the scope of a Procedure: For a list of
references with information on \`Unique\`, see the
\[wiki:Commentary/Compiler/CmmType\#BasicBlocksandProcedures Basic
Blocks and Procedures\] section, above.

A \`MachRep\`, the type of a machine register, is defined in
[GhcFile(compiler/cmm/MachOp.hs)](GhcFile(compiler/cmm/MachOp.hs) "wikilink"):
There is currently no register for floating point vectors, such as
\`F128\`. The types of Cmm variables are defined in the Happy parser
file
[GhcFile(compiler/cmm/CmmParse.y)](GhcFile(compiler/cmm/CmmParse.y) "wikilink")
and the Alex lexer file
[GhcFile(compiler/cmm/CmmLex.x)](GhcFile(compiler/cmm/CmmLex.x) "wikilink").
(Happy and Alex will compile these into \`CmmParse.hs\` and
\`CmmLex.hs\`, respectively.) Cmm recognises the following \`C--\` types
as parseable tokens, listed next to their corresponding s in
[GhcFile(includes/Cmm.h)](GhcFile(includes/Cmm.h) "wikilink") and their
STG types: || **Cmm Token** || **Cmm.h \#define** || **STG type** || ||
\`bits8\` || \`I8\` || \`StgChar\` or \`StgWord8\` || || \`bits16\` ||
\`I16\` || \`StgWord16\` || || \`bits32\` || \`I32\`, \`CInt\`,
\`CLong\` || \`StgWord32\`; \`StgWord\` (depending on architecture) ||
|| \`bits64\` || \`I64\`, \`CInt\`, \`CLong\`, \`L\_\` || \`StgWord64\`;
\`StgWord\` (depending on architecture) || || \`float32\` || \`F\_\` ||
\`StgFloat\` || || \`float64\` || \`D\_\` || \`StgDouble\` ||

[GhcFile(includes/Cmm.h)](GhcFile(includes/Cmm.h) "wikilink") also
defines \`L\_\` for \`bits64\`, so \`F\_\`, \`D\_\` and \`L\_\`
correspond to the \`GlobalReg\` data type constructors \`FloatReg\`,
\`DoubleReg\` and \`LongReg\`. Note that although GHC may generate other
register types supported by the \`MachRep\` data type, such as \`I128\`,
they are not parseable tokens. That is, they are internal to GHC. The
special defines \`CInt\` and \`CLong\` are used for compatibility with C
on the target architecture, typically for making \`foreign "C"\` calls.

**Note**: Even Cmm types that are not explicit variables (Cmm literals
and results of Cmm expressions) have implicit \`MachRep\`s, in the same
way as you would use temporary registers to hold labelled constants or
intermediate values in assembler functions. See:

`` *[wiki:Commentary/Compiler/CmmType#LiteralsandLabelsLiteralsandLabels]forinformationrelatedtotheCmmliterals`CmmInt`and`CmmFloat`;and, ``\
`` *[wiki:Commentary/Compiler/CmmType#ExpressionsExpressions],regardingthe`cmmExprRep`functiondefinedin ``[`GhcFile(compiler/cmm/Cmm.hs)`](GhcFile(compiler/cmm/Cmm.hs) "wikilink")`.`

#### Global Registers and Hints

These are universal both to a Cmm module and to the whole compiled
program. Variables are global if they are declared at the top-level of a
compilation unit (outside any procedure). Global Variables are marked as
external symbols with the \`.globl\` assembler directive. In Cmm, global
registers are used for special STG registers and specific registers for
passing arguments and returning values. The Haskell representation of
Global Variables (Registers) is the \`GlobalReg\` data type, defined in
[GhcFile(compiler/cmm/Cmm.hs)](GhcFile(compiler/cmm/Cmm.hs) "wikilink"):
For a description of the \`Hp\` and \`Sp\` *virtual registers*, see
\[wiki:Commentary/Rts/HaskellExecution The Haskell Execution Model\]
page. General \`GlobalReg\`s are clearly visible in Cmm code according
to the following syntax defined in
[GhcFile(compiler/cmm/CmmLex.x)](GhcFile(compiler/cmm/CmmLex.x) "wikilink"):
|| **\`GlobalReg\` Constructor** || **Syntax** || **Examples** || ||
\`VanillaReg Int\` || \`R ++ Int\` || \`R1\`, \`R10\` || || \`FloatReg
Int\` || \`F ++ Int\` || \`F1\`, \`F10\` || || \`DoubleReg Int\` || \`D
++ Int\` || \`D1\`, \`D10\` || || \`LongReg Int\` || \`L ++ Int\` ||
\`L1\`, \`L10\` || General \`GlobalRegs\` numbers are decimal integers,
see the \`parseInteger\` function in
[GhcFile(compiler/utils/StringBuffer.lhs)](GhcFile(compiler/utils/StringBuffer.lhs) "wikilink").
The remainder of the \`GlobalReg\` constructors, from \`Sp\` to
\`BaseReg\` are lexical tokens exactly like their name in the data type;
\`PicBaseReg\` does not have a lexical token since it is used only
inside the NCG. See \[wiki:Commentary/PositionIndependentCode Position
Independent Code and Dynamic Linking\] for an in-depth description of
PIC implementations in the NCG.

\`GlobalRegs\` are a very special case in Cmm, partly because they must
conform to the STG register convention and the target C calling
convention. That the Cmm parser recognises \`R1\` and \`F3\` as
\`GlobalRegs\` is only the first step. The main files to look at for
more information on this delicate topic are:

`*`[`GhcFile(compiler/codeGen/CgCallConv.hs)`](GhcFile(compiler/codeGen/CgCallConv.hs) "wikilink")`(thesectionon"Registerassignment")`\
`*`[`GhcFile(includes/stg/Regs.h)`](GhcFile(includes/stg/Regs.h) "wikilink")`(definingSTGregisters)`\
`*`[`GhcFile(includes/stg/MachRegs.h)`](GhcFile(includes/stg/MachRegs.h) "wikilink")`(target-specificmappingofmachineregistersfor`*`registerised`*`buildsofGHC)`\
`*`[`GhcFile(rts/PrimOps.cmm)`](GhcFile(rts/PrimOps.cmm) "wikilink")`` (examplesof`GlobalReg`registerusageforout-of-lineprimops) ``

All arguments to out-of-line !PrimOps in
[GhcFile(rts/PrimOps.cmm)](GhcFile(rts/PrimOps.cmm) "wikilink") are STG
registers.

Cmm recognises all C-- syntax with regard to *hints*. For example: Hints
are represented in Haskell as \`MachHint\`s, defined near \`MachRep\` in
[GhcFile(compiler/cmm/MachOp.hs)](GhcFile(compiler/cmm/MachOp.hs) "wikilink"):

Although the C-- specification does not allow the C-- type system to
statically distinguish between floats, signed ints, unsigned ints or
pointers, Cmm does. Cmm \`MachRep\`s carry the float or int kind of a
variable, either within a local block or in a global register.
\`GlobalReg\` includes separate constructors for \`Vanilla\`, \`Float\`,
\`Double\` and \`Long\`. Cmm still does not distinguish between signed
ints, unsigned ints and pointers (addresses) at the register level, as
these are given *hint* pseudo-types or their real type is determined as
they run through primitive operations. \`MachHint\`s still follow the
C-- specification and carry kind information as an aide to the backend
optimisers.

Global Registers in Cmm currently have a problem with inlining: because
neither
[GhcFile(compiler/cmm/PprC.hs)](GhcFile(compiler/cmm/PprC.hs) "wikilink")
nor the NCG are able to keep Global Registers from clashing with C
argument passing registers, Cmm expressions that contain Global
Registers cannot be inlined into an argument position of a foreign call.
For more thorough notes on inlining, see the comments in
[GhcFile(compiler/cmm/CmmOpt.hs)](GhcFile(compiler/cmm/CmmOpt.hs) "wikilink").

#### Declaration and Initialisation

Cmm variables hold the same values registers do in assembly languages
but may be declared in a similar way to variables in C. As in C--, they
may actually be declared anywhere in the scope for which they are
visible (a block or file)--for Cmm, this is done by the \`loopDecls\`
function in
[GhcFile(compiler/cmm/CmmParse.y)](GhcFile(compiler/cmm/CmmParse.y) "wikilink").
In
[GhcFile(compiler/rts/PrimOps.cmm)](GhcFile(compiler/rts/PrimOps.cmm) "wikilink"),
you will see Cmm variable declarations like this one: Remember that Cmm
code is run through the C preprocessor. \`W\_\` will be transformed into
\`bits32\`, \`bits64\` or whatever is the \`bits\`*size* of the machine
word, as defined in
[GhcFile(includes/Cmm.h)](GhcFile(includes/Cmm.h) "wikilink"). In
Haskell code, you may use the
[GhcFile(compiler/cmm/MachOp.hs)](GhcFile(compiler/cmm/MachOp.hs) "wikilink")
functions \`wordRep\` and \`halfWordRep\` to dynamically determine the
machine word size. For a description of word sizes in GHC, see the
\[wiki:Commentary/Rts/Word Word\] page.

The variables \`w\`, \`code\` and \`val\` should be real registers. With
the above declaration the variables are uninitialised. Initialisation
requires an assignment *statement*. Cmm does not recognise C-- "\`{\`
*literal*, ... \`}\`" initialisation syntax, such as \`bits32{10}\` or
\`bits32\[3\] {1, 2, 3}\`. Cmm does recognise initialisation with a
literal: The typical method seems to be to declare variables and then
initialise them just before their first use. (Remember that you may
declare a variable anywhere in a procedure and use it in an expression
before it is initialised but you must initialise it before using it
anywhere else--statements, for example.)

#### Memory Access

If the value in \`w\` were the address of a memory location, you would
obtain the value at that location similar to Intel assembler syntax. In
Cmm, you would write: compare the above statement to indirect addressing
in Intel assembler:

The code between the brackets (\`w\` in \`\[w\]\`, above) is an
*expression*. See the \[wiki:Commentary/Compiler/CmmType\#Expressions
Expressions\] section. For now, consider the similarity between the
Cmm-version of indexed memory addressing syntax, here: and the
corresponding Intel assembler indexed memory addressing syntax, here:
You will generally not see this type of syntax in either handwritten or
GHC-produced Cmm code, although it is allowed; it simply shows up in
macros. C-- also allows the \`\*\` (multiplication) operator in
addressing expressions, for an approximation of *scaled* addressing
(\`\[base \* (2\^n)\]\`); for example, \`n\` (the "scale") must be
\`0\`, \`1\`, \`2\` or \`4\`. C-- itself would not enforce alignment or
limits on the scale. Cmm, however, could not process it: since the NCG
currently outputs GNU Assembler syntax, the Cmm or NCG optimisers would
have to reduce \`n\` in (\`\* n\`) to an absolute address or relative
offset, or to an expression using only \`+\` or \`-\`. This is not
currently the case and would be difficult to implement where one of the
operands to the \`\*\` is a relative address not visible in the code
block. [GhcFile(includes/Cmm.h)](GhcFile(includes/Cmm.h) "wikilink")
defines macros to perform the calculation with a constant. For example:
is used in: The function \`cmmMachOpFold\` in
[GhcFile(compiler/cmm/CmmOpt.hs)](GhcFile(compiler/cmm/CmmOpt.hs) "wikilink")
will reduce the resulting expression \`Sp + (n \* SIZEOF\_W)\` to \`Sp +
N\`, where \`N\` is a constant. A very large number of macros for
accessing STG struct fields and the like are produced by
[GhcFile(includes/mkDerivedConstants.c)](GhcFile(includes/mkDerivedConstants.c) "wikilink")
and output into the file \`includes/DerivedConstants.h\` when GHC is
compiled.

Of course, all this also holds true for the reverse (when an assignment
is made to a memory address): or, for an example of a macro from
\`DerivedConstants.h\`: this will be transformed to:

### Literals and Labels

Cmm literals are exactly like C-- literals, including the Haskell-style
type syntax, for example: \`0x00000001::bits32\`. Cmm literals may be
used for initialisation by assignment or in expressions. The \`CmmLit\`
and \`CmmStatic\` data types, defined in
[GhcFile(compiler/cmm/Cmm.hs)](GhcFile(compiler/cmm/Cmm.hs) "wikilink")
together represent Cmm literals, static information and Cmm labels: Note
how the \`CmmLit\` constructor \`CmmInt Integer MachRep\` contains sign
information in the \`Integer\`, the representation of the literal
itself: this conforms to the C-- specification, where integral literals
contain sign information. For an example of a function using \`CmmInt\`
sign information, see \`cmmMachOpFold\` in
[GhcFile(compiler/cmm/CmmOpt.hs)](GhcFile(compiler/cmm/CmmOpt.hs) "wikilink"),
where sign-operations are performed on the \`Integer\`.

The \`MachRep\` of a literal, such as \`CmmInt Integer MachRep\` or
\`CmmFloat Rational MachRep\` may not always require the size defined by
\`MachRep\`. The NCG optimiser,
[GhcFile(compiler/nativeGen/MachCodeGen.hs)](GhcFile(compiler/nativeGen/MachCodeGen.hs) "wikilink"),
will test a literal such as \`1::bits32\` (in Haskell, \`CmmInt
(1::Integer) I32\`) for whether it would fit into the bit-size of
Assembler instruction literals on that particular architecture with a
function defined in
[GhcFile(compiler/nativeGen/MachRegs.lhs)](GhcFile(compiler/nativeGen/MachRegs.lhs) "wikilink"),
such as \`fits16Bits\` on the PPC. If the Integer literal fits, the
function \`makeImmediate\` will truncate it to the specified size if
possible and store it in a NCG data type, \`Imm\`, specifically \`Maybe
Imm\`. (These are also defined in
[GhcFile(compiler/nativeGen/MachRegs.lhs)](GhcFile(compiler/nativeGen/MachRegs.lhs) "wikilink").)

The Haskell representation of Cmm separates unchangeable Cmm values into
a separate data type, \`CmmStatic\`, defined in
[GhcFile(compiler/cmm/Cmm.hs)](GhcFile(compiler/cmm/Cmm.hs) "wikilink"):
Note the \`CmmAlign\` constructor: this maps to the assembler directive
\`.align N\` to set alignment for a data item (hopefully one you
remembered to label). This is the same as the \`align\` directive noted
in Section 4.5 of the [C-- specification
(PDF)](http://cminusminus.org/extern/man2.pdf). In the current
implementation of Cmm the \`align\` directive seems superfluous because
[GhcFile(compiler/nativeGen/PprMach.hs)](GhcFile(compiler/nativeGen/PprMach.hs) "wikilink")
translates \`Section\`s to assembler with alignment directives
corresponding to the target architecture (see
\[wiki:Commentary/Compiler/CmmType\#SectionsandDirectives Sections and
Directives\], below).

#### Labels

Remember that C--/Cmm names consist of a string where the first
character is:

`*ASCIIalphabetic(uppercaseorlowercase);`\
`` *anunderscore:`_`; ``\
`` *aperiod:`.`; ``\
`` *adollarsign:`$`;or, ``\
`` *acommercialat:`@`. ``

Cmm labels conform to the C-- specification. C--/Cmm uses labels to
refer to memory locations in code--if you use a data directive but do
not give it a label, you will have no means of referring to the memory!
For \`GlobalReg\`s (transformed to assembler \`.globl\`), labels serve
as both symbols and labels (in the assembler meaning of the terms). The
Haskell representation of Cmm Labels is contained in the \`CmmLit\` data
type, see \[wiki:Commentary/Compiler/CmmType\#Literals Literals\]
section, above. Note how Cmm Labels are \`CLabel\`s with address
information. The \`Clabel\` data type, defined in
[GhcFile(compiler/cmm/CLabel.hs)](GhcFile(compiler/cmm/CLabel.hs) "wikilink"),
is used throughout the Compiler for symbol information in binary files.
Here it is:

### Sections and Directives

The Haskell representation of Cmm Section directives, in
[GhcFile(compiler/cmm/Cmm.hs)](GhcFile(compiler/cmm/Cmm.hs) "wikilink")
as the first part of the "Static Data" section, is: Cmm supports the
following directives, corresponding to the assembler directives
pretty-printed by the \`pprSectionHeader\` function in
[GhcFile(compiler/nativeGen/PprMach.hs)](GhcFile(compiler/nativeGen/PprMach.hs) "wikilink"):
|| **\`Section\` Constructor** || **Cmm section directive** ||
**Assembler Directive** || || \`Text\` || \`"text"\` || \`.text\` || ||
\`Data\` || \`"data"\` || \`.data\` || || \`ReadOnlyData\` ||
\`"rodata"\` || \`.rodata\`[BR](BR "wikilink")(generally; varies by
arch,OS) || || \`RelocatableReadOnlyData\` || no parse (GHC internal),
output: \`"relreadonly"\` ||
\`.const\_data\`[BR](BR "wikilink")\`.section
.rodata\`[BR](BR "wikilink")(generally; varies by arch,OS) || ||
\`UninitialisedData\` || \`"bss"\`, output: \`"uninitialised"\` ||
\`.bss\` || || \`ReadOnlyData16\` || no parse (GHC internal), output:
none || \`.const\`[BR](BR "wikilink")\`.section
.rodata\`[BR](BR "wikilink")(generally; on
x86\_64:[BR](BR "wikilink")\`.section .rodata.cst16\`) || You probably
already noticed I omitted the alignment directives (for clarity). For
example, \`pprSectionHeader\` would pretty-print \`ReadOnlyData\` as on
an i386 with the Darwin OS. If you are really on the ball you might have
noticed that the \`PprMach.hs\` output of "\`.section .data\`" and the
like is really playing it safe since on most OS's, using GNU Assembler,
the \`.data\` directive is equivalent to \`.section \_\_DATA .data\`, or
simply \`.section .data\`. Note that \`OtherSection String\` is not a
catch-all for the Cmm parser. If you wrote: The Cmm parser (through GHC)
would panic, complaining, "\`PprMach.pprSectionHeader: unknown
section\`."

While the C-- specification allows a bare \`data\` keyword directive,
Cmm does not:

Cmm does not recognise the C-- "\`stack\`" declaration for allocating
memory on the system stack.

GHC-produced Cmm code is replete with \`data\` sections, each of which
is stored in \`.data\` section of the binary code. This contributes
significantly to the large binary size for GHC-compiled code.

`====TargetDirective====`

The C-- specification defines a special \`target\` directive, in section
4.7. The \`target\` directive is essentially a code block defining the
properties of the target architecture: This is essentially a
custom-coded version of the GNU Assembler (\`as\`) \`.machine\`
directive, which is essentially the same as passing the \`-arch
\[cpu\_type\]\` option to \`as\`.

Cmm does not support the \`target\` directive. This is partly due GHC
generally lacking cross-compiler capabilities. Should GHC move toward
adding cross-compilation capabilities, the \`target\` might not be a bad
thing to add. Target architecture parameters are currently handled
through the \[wiki:Attic/Building/BuildSystem Build System\], which
partly sets such architectural parameters through
[GhcFile(includes/mkDerivedConstants.c)](GhcFile(includes/mkDerivedConstants.c) "wikilink")
and
[GhcFile(includes/ghcconfig.h)](GhcFile(includes/ghcconfig.h) "wikilink").

### Expressions

Expressions in Cmm follow the C-- specification. They have:

`*noside-effects;and,`\
`*oneresult:`\
`*a`*`k`*`-bitvalue`[`BR`](BR "wikilink")`` --theseexpressionsmaptothe`MachOp`datatype,definedin ``[`GhcFile(compiler/cmm/MachOp.hs)`](GhcFile(compiler/cmm/MachOp.hs) "wikilink")`,see[wiki:Commentary/Compiler/CmmType#OperatorsandPrimitiveOperationsOperatorsandPrimitiveOperations],the`*`k`*`-bitvaluemaybe:`\
`` *aCmmliteral(`CmmLit`);or, ``\
`` *aCmmvariable(`CmmReg`,see[wiki:Commentary/Compiler/CmmType#VariablesRegistersandTypesVariables,RegistersandTypes]); ``[`BRor`](BR "wikilink")`,`\
`*abooleancondition.`

Cmm expressions may include

`` *aliteraloraname(`CmmLit`containsboth,see[wiki:Commentary/Compiler/CmmType#LiteralsandLabelsLiteralsandLabels],above); ``\
`` *amemoryreference(`CmmLoad`and`CmmReg`,see[wiki:Commentary/Compiler/CmmType#MemoryAccessMemoryAccess],above); ``\
`` *anoperator(a`MachOp`,in`CmmMachOp`,below);or, ``\
`` *anotherexpression(a`[CmmExpr]`,in`CmmMachOp`,below). ``

These are all included as constructors in the \`CmmExpr\` data type,
defined in
[GhcFile(compiler/cmm/Cmm.hs)](GhcFile(compiler/cmm/Cmm.hs) "wikilink"):
Note that \`CmmRegOff reg i\` is only shorthand for a specific
\`CmmMachOp\` application: The function \`cmmRegRep\` is described
below. Note: the original comment following \`CmmExpr\` in
[GhcFile(compiler/cmm/Cmm.hs)](GhcFile(compiler/cmm/Cmm.hs) "wikilink")
is erroneous (cf., \`mangleIndexTree\` in
[GhcFile(compiler/nativeGen/MachCodeGen.hs)](GhcFile(compiler/nativeGen/MachCodeGen.hs) "wikilink"))
but makes the same point described here. The offset, \`(CmmLit (CmmInt i
rep))\`, is a literal (\`CmmLit\`), not a name (\`CLabel\`). A
\`CmmExpr\` for an offset must be reducible to a \`CmmInt\` *in
Haskell*; in other words, offsets in Cmm expressions may not be external
symbols whose addresses are not resolvable in the current context.

Boolean comparisons are not boolean conditions. Boolean comparisons
involve relational operators, such as \`&gt;\`, \`&lt;\` and \`==\`, and
map to \`MachOp\`s that are converted to comparison followed by branch
instructions. For example, \`&lt;\` would map to \`MO\_S\_Lt\` for
signed operands,
[GhcFile(compiler/nativeGen/MachCodeGen.hs)](GhcFile(compiler/nativeGen/MachCodeGen.hs) "wikilink")
would transform \`MO\_S\_Lt\` into the \`LTT\` constructor of the
\`Cond\` union data type defined in
[GhcFile(compiler/nativeGen/MachInstrs.hs)](GhcFile(compiler/nativeGen/MachInstrs.hs) "wikilink")
and
[GhcFile(compiler/nativeGen/PprMach.hs)](GhcFile(compiler/nativeGen/PprMach.hs) "wikilink")
would transform \`LTT\` to the distinguishing comparison type for an
assembler comparison instruction. You already know that the result of a
comparison instruction is actually a change in the state of the
Condition Register (CR), so Cmm boolean expressions do have a kind of
side-effect but that is to be expected. In fact, it is necessary since
at the least a conditional expression becomes two assembler
instructions, in PPC Assembler: This condition mapping does have an
unfortunate consequence: conditional expressions do not fold into single
instructions. In Cmm, as in C--, expressions with relational operators
may evaluate to an integral (\`0\`, nonzero) instead of evaluating to a
boolean type. For certain cases, such as an arithmetic operation
immediately followed by a comparison, extended mnemonics such as
\`addi.\` might eliminate the comparison instruction. See
\[wiki:Commentary/Compiler/CmmType\#CmmDesignObservationsandAreasforPotentialImprovement
Cmm Design: Observations and Areas for Potential Improvement\] for more
discussion and potential solutions to this situation.

Boolean conditions include: \`&&\`, \`||\`, \`!\` and parenthetical
combinations of boolean conditions. The \`if expr { }\` and \`if expr {
} else { }\` statements contain boolean conditions. The C-- type
produced by conditional expressions is \`bool\`, in Cmm, type
\`BoolExpr\` in
[GhcFile(compiler/cmm/CmmParse.y)](GhcFile(compiler/cmm/CmmParse.y) "wikilink"):
The type \`BoolExpr\` maps to the \`CmmCondBranch\` or \`CmmBranch\`
constructors of type \`CmmStmt\`, defined in
[GhcFile(compiler/cmm/Cmm.hs)](GhcFile(compiler/cmm/Cmm.hs) "wikilink"),
see \[wiki:Commentary/Compiler/CmmType\#StatementsandCalls Statements
and Calls\].

The \`CmmExpr\` constructor \`CmmMachOp MachOp \[CmmExpr\]\` is the core
of every operator-based expression; the key here is \`MachOp\`, which in
turn depends on the type of \`MachRep\` for each operand. See
\[wiki:Commentary/Compiler/CmmType\#FundamentalandPrimitiveOperators
Fundamental and PrimitiveOperators\]. In order to process \`CmmExpr\`s,
the data type comes with a deconstructor function to obtain the relevant
\`MachRep\`s, defined in
[GhcFile(compiler/cmm/Cmm.hs)](GhcFile(compiler/cmm/Cmm.hs) "wikilink"):
The deconstructors \`cmmLitRep\` and \`cmmRegRep\` (with its supporting
deconstructor \`localRegRep\`) are also defined in
[GhcFile(compiler/cmm/Cmm.hs)](GhcFile(compiler/cmm/Cmm.hs) "wikilink").

In PPC Assembler you might add two 32-bit integrals by: while in Cmm you
might write: Remember that the assignment operator, \`=\`, is a
statement since it has the "side effect" of modifying the value in
\`res\`. The \`+\` expression in the above statement, for a 32-bit
architecture, would be represented in Haskell as: The \`expr\`
production rule in the Cmm Parser
[GhcFile(compiler/cmm/CmmParse.y)](GhcFile(compiler/cmm/CmmParse.y) "wikilink")
maps tokens to "values", such as \`+\` to an addition operation,
\`MO\_Add\`. The \`mkMachOp\` function in the Parser determines the
\`MachOp\` type in \`CmmMachOp MachOp \[CmmExpr\]\` from the token value
and the \`MachRep\` type of the \`head\` variable. Notice that the
simple \`+\` operator did not contain sign information, only the
\`MachRep\`. For \`expr\`, signed and other \`MachOps\`, see the
\`machOps\` function in
[GhcFile(compiler/cmm/CmmParse.y)](GhcFile(compiler/cmm/CmmParse.y) "wikilink").
Here is a table of operators and the corresponding \`MachOp\`s
recognised by Cmm (listed in order of precedence): || **Operator** ||
**\`MachOp\`** || || \`/\` || \`MO\_U\_Quot\` || || \`\*\` ||
\`MO\_Mul\` || || \`%\` || \`MO\_U\_Rem\` || || \`-\` || \`MO\_Sub\` ||
|| \`+\` || \`MO\_Add\` || || \`&gt;&gt;\` || \`MO\_U\_Shr\` || ||
\`&lt;&lt;\` || \`MO\_Shl\` || || \`&\` || \`MO\_And\` || || \`\^\` ||
\`MO\_Xor\` || || \`|\` || \`MO\_Or\` || || \`&gt;=\` || \`MO\_U\_Ge\`
|| || \`&gt;\` || \`MO\_U\_Gt\` || || \`&lt;=\` || \`MO\_U\_Le\` || ||
\`&lt;\` || \`MO\_U\_Lt\` || || \`!=\` || \`MO\_Ne\` || || \`==\` ||
\`MO\_Eq\` || || \`\~\` || \`MO\_Not\` || || \`-\` || \`MO\_S\_Neg\` ||

#### Quasi-operator Syntax

If you read to the end of \`expr\` in
[GhcFile(compiler/cmm/CmmParse.y)](GhcFile(compiler/cmm/CmmParse.y) "wikilink"),
in the next production rule, \`expr0\`, you will notice that Cmm
expressions also recognise a set of name (not symbol) based operators
that would probably be better understood as *quasi-operators*. The
syntax for these quasi-operators is in some cases similar to syntax for
Cmm statements and generally conform to the C-- specification, sections
3.3.2 (\`expr\`) and 7.4.1 (syntax of primitive operators), *except
that* 3. *and, by the equivalence of the two,* 1. *may return*
**multiple** '' arguments''. In Cmm, quasi-operators may have side
effects. The syntax for quasi-operators may be:

`` 1.`expr0` ```` `expr0` ``[`BR`](BR "wikilink")`(justlikeinfix-functionsinHaskell);`\
`` 1.`type[expression]` ``[`BR`](BR "wikilink")`` (thememoryaccessquasi-expressiondescribedin[wiki:Commentary/Compiler/CmmType#MemoryAccessMemoryAccess];theHaskellrepresentationofthissyntaxis`CmmLoadCmmExprMachRep`); ``\
`` 1.`%name(exprs0)` ``[`BR`](BR "wikilink")`(standardprefixform,similartoC--`*`statement`*`` syntaxforproceduresbutwiththedistinguishingprefix`%`;inCmmthisis ``*`also`
`used` `as` `statement` `syntax` `for` `calls,` `which` `are` `really`
`built-in`
`procedures`*`,see[wiki:Commentary/Compiler/CmmType#CmmCallsCmmCalls])`

A \`expr0\` may be a literal (\`CmmLit\`) integral, floating point,
string or a \`CmmReg\` (the production rule \`reg\`: a \`name\` for a
local register (\`LocalReg\`) or a \`GlobalReg\`).

Note that the \`name\` in \`expr0\` syntax types 1. and 3. must be a
known *primitive* (primitive operation), see
\[wiki:Commentary/Compiler/CmmType\#OperatorsandPrimitiveOperations
Operators and Primitive Operations\]. The first and third syntax types
are interchangeable: The primitive operations allowed by Cmm are listed
in the \`machOps\` production rule, in
[GhcFile(compiler/cmm/CmmParse.y)](GhcFile(compiler/cmm/CmmParse.y) "wikilink"),
and largely correspond to \`MachOp\` data type constructors, in
[GhcFile(compiler/cmm/MachOp.hs)](GhcFile(compiler/cmm/MachOp.hs) "wikilink"),
with a few additions. The primitive operations distinguish between
signed, unsigned and floating point types.

Cmm adds some expression macros that map to Haskell Cmm functions. They
are listed under \`exprMacros\` in
[GhcFile(compiler/cmm/CmmParse.y)](GhcFile(compiler/cmm/CmmParse.y) "wikilink")
and include:

`` *`ENTRY_CODE` ``\
`` *`INFO_PTR` ``\
`` *`STD_INFO` ``\
`` *`FUN_INFO` ``\
`` *`GET_ENTRY` ``\
`` *`GET_STD_INFO` ``\
`` *`GET_FUN_INFO` ``\
`` *`INFO_TYPE` ``\
`` *`INFO_PTRS` ``\
`` *`INFO_NPTRS` ``\
`` *`RET_VEC` ``

### Statements and Calls

Cmm Statements generally conform to the C-- specification, with a few
exceptions noted below. Cmm Statements implement:

`` *no-op;theemptystatement:`;` ``\
`` *C--(C99/C++style)comments:`//...\n`and`/*...*/` ``\
`` *theassignmentoperator:`=` ``\
`` *storeoperation(assignmenttoamemorylocation):`type[expr]=` ``\
`` *controlflowwithinprocedures(`goto`)andbetweenprocedures(`jump`,returns)(note:returnsare ``*`only`*`Cmmmacros)`\
`` *foreigncalls(`foreign"C"...`)andcallstoCmmPrimitiveOperations(`%`) ``\
`*procedurecallsandtailcalls`\
`` *conditionalstatement(`if...{...}else{...}`) ``\
`` *tabledconditional(`switch`) ``

Cmm does not implement the C-- specification for Spans (sec. 6.1) or
Continuations (sec. 6.7).[BR](BR "wikilink") Although Cmm supports
primitive operations that may have side effects (see
\[wiki:Commentary/Compiler/CmmType\#PrimitiveOperations Primitive
Operations\], below), it does not parse the syntax \`%%\` form mentioned
in section 6.3 of the C-- specification. Use the \`%name(arg1,arg2)\`
expression-syntax instead. [BR](BR "wikilink") Cmm does not implement
the \`return\` statement (C-- spec, sec. 6.8.2) but provides a set of
macros that return a list of tuples of a \`CgRep\` and a \`CmmExpr\`:
\`\[(CgRep,CmmExpr)\]\`. For a description of \`CgRep\`, see comments in
[GhcFile(compiler/codeGen/SMRep.lhs)](GhcFile(compiler/codeGen/SMRep.lhs) "wikilink").
The return macros are defined at the end of the production rule
\`stmtMacros\` in
[GhcFile(compiler/cmm/CmmParse.y)](GhcFile(compiler/cmm/CmmParse.y) "wikilink"):

`` *`RET_P` ``\
`` *`RET_N` ``\
`` *`RET_PP` ``\
`` *`RET_NN` ``\
`` *`RET_NP` ``\
`` *`RET_PPP` ``\
`` *`RET_NNP` ``\
`` *`RET_NNNP` ``\
`` *`RET_NPNP` ``

In the above macros, \`P\` stands for \`PtrArg\` and \`N\` stands for
\`NonPtrArg\`; both are \`CgRep\` constructors. These return macros
provide greater control for the \[wiki:Commentary/Compiler/CodeGen
CodeGen\] and integrate with the RTS but limit the number and type of
return arguments in Cmm: you may only return according to these macros!
The returns are processed by the \`emitRetUT\` function in
[GhcFile(compiler/cmm/CmmParse.y)](GhcFile(compiler/cmm/CmmParse.y) "wikilink"),
which in turn calls several functions from
[GhcFile(compiler/codeGen/CgMonad.lhs)](GhcFile(compiler/codeGen/CgMonad.lhs) "wikilink"),
notably \`emitStmts\`, which is the core Code Generator function for
emitting \`CmmStmt\` data.

The Haskell representation of Cmm Statements is the data type
\`CmmStmt\`, defined in
[GhcFile(compiler/cmm/Cmm.hs)](GhcFile(compiler/cmm/Cmm.hs) "wikilink"):
Note how the constructor \`CmmJump\` contains \`\[LocalReg\]\`: this is
the Cmm implementation of the C-- \`jump\` statement for calling another
procedure where the parameters are the arguments passed to the other
procedure. None of the parameters contain the address--in assembler, a
label--of the caller, to return control to the caller. The \`CmmCall\`
constructor also lacks a parameter to store the caller's address. Cmm
implements C-- jump nesting and matching returns by *tail calls*, as
described in section 6.8 of the C-- specification. Tail calls are
managed through the \[wiki:Commentary/Compiler/CodeGen CodeGen\], see
[GhcFile(compiler/codeGen/CgTailCall.lhs)](GhcFile(compiler/codeGen/CgTailCall.lhs) "wikilink").
You may have already noticed that the call target of the \`CmmJump\` is
a \`CmmExpr\`: this is the Cmm implementation of computed procedure
addresses, for example: The computed procedure address, in this case
\`(bits32\[x+4\])\`, should always be the first instruction of a \`Cmm\`
procedure. You cannot obtain the address of a code block *within* a
procedure and \`jump\` to it, as an alternative way of computing a
*continuation*.

\`CmmBranch BlockId\` represents an unconditional branch to another
\[wiki:Commentary/Compiler/CmmType\#BasicBlocksandProcedures Basic
Block\] in the same procedure. There are two unconditional branches in
Cmm/C--:

`` 1.`goto`statement;and ``\
`` 1.abranchfromthe`else`portionofan`if-then-else`statement. ``

\`CmmCondBranch CmmExpr BlockId\` represents a conditional branch to
another \[wiki:Commentary/Compiler/CmmType\#BasicBlocksandProcedures
Basic Block\] in the same procedure. This is the \`if expr\` statement
where \`expr\` is a \`CmmExpr\`, used in both the unary \`if\` and
\`if-then-else\` statements. \`CmmCondBranch\` maps to more complex
Assembler instruction sets or HC code
([GhcFile(compiler/cmm/PprC.hs)](GhcFile(compiler/cmm/PprC.hs) "wikilink")).
For assembler, labels are created for each new Basic Block. During
parsing, conditional statements map to the \`BoolExpr\` data type which
guides the encoding of assembler instruction sets.

\`CmmSwitch\` represents the \`switch\` statement. It is parsed and
created as with the \`doSwitch\` function in
[GhcFile(compiler/cmm/CmmParse.y)](GhcFile(compiler/cmm/CmmParse.y) "wikilink")
or created from \`case\` expressions with the \`emitSwitch\` and
\`mk\_switch\` functions in
[GhcFile(compiler/codeGen/CgUtils.hs)](GhcFile(compiler/codeGen/CgUtils.hs) "wikilink").
In the NCG, a \`CmmSwitch\` is generated as a jump table using the
\`genSwitch\` function in
[GhcFile(compiler/nativeGen/MachCodeGen.hs)](GhcFile(compiler/nativeGen/MachCodeGen.hs) "wikilink").
There is currently no implementation of any optimisations, such as a
cascade of comparisons for switches with a wide deviation in values or
binary search for very wide value ranges--for output to HC, earlier
versions of GCC could not handle large if-trees, anyway.

#### Cmm Calls

Cmm calls include both calls to foreign functions and calls to Cmm
quasi-operators using expression syntax (see
\[wiki:Commentary/Compiler/CmmType\#QuasioperatorSyntax Quasi-operator
Syntax\]). Although Cmm does not implement any of the control flow
statements of C-- specification (section 6.8.1), foreign calls from Cmm
are one of the most complex components of the system due to various
differences between the Cmm and C calling conventions.

The data type, \`CmmCallTarget\` is defined in
[GhcFile(compiler/cmm/Cmm.hs)](GhcFile(compiler/cmm/Cmm.hs) "wikilink")
as: \`CCallConv\` is defined in
[GhcFile(compiler/prelude/ForeignCall.lhs)](GhcFile(compiler/prelude/ForeignCall.lhs) "wikilink");
for information on register assignments, see comments in
[GhcFile(compiler/codeGen/CgCallConv.hs)](GhcFile(compiler/codeGen/CgCallConv.hs) "wikilink").

\`CallishMachOp\` is defined in
[GhcFile(compiler/cmm/MachOp.hs)](GhcFile(compiler/cmm/MachOp.hs) "wikilink");
see, also, below \[wiki:Commentary/Compiler/CmmType\#PrimitiveOperations
Primitive Operations\]. \`CallishMachOp\`s are generally used for
floating point computations (without implementing any floating point
exceptions). Here is an example of using a \`CallishMachOp\` (not yet
implemented):

### Operators and Primitive Operations

Cmm generally conforms to the C-- specification for operators and
"primitive operations". The C-- specification, in section 7.4, refers to
both of these as "primitive operations" but there are really two
different types:

`*`*`operators`*`,asIrefertothem,are:`\
`` *parseabletokens,suchas`+`,`-`,`*`or`/`; ``\
`*generallymaptoasinglemachineinstructionorpartofamachineinstruction;`\
`*havenosideeffects;and,`\
`` *arerepresentedinHaskellusingthe`MachOp`datatype; ``\
`*`*`primitive`
`operations`*`(Cmm`*`quasi-operators`*`` )arespecial,usuallyinlined,procedures,representedinHaskellusingthe`CallishMachOp`datatype;primitiveoperationsmayhavesideeffects. ``

The \`MachOp\` and \`CallishMachOp\` data types are defined in
[GhcFile(compiler/cmm/MachOp.hs)](GhcFile(compiler/cmm/MachOp.hs) "wikilink").

Both Cmm Operators and Primitive Operations are handled in Haskell as
\[wiki:Commentary/PrimOps\#InlinePrimOps Inline PrimOps\], though what I
am calling Cmm *primitive operations* may be implemented as out-of-line
foreign calls.

#### Operators

 Each \`MachOp\` generally corresponds to a machine instruction but may
have its value precomputed in the Cmm, NCG or HC optimisers.

#### Primitive Operations

Primitive Operations generally involve more than one machine instruction
and may not always be inlined.

 For an example, the floating point sine function, \`sinFloat\#\` in
[GhcFile(compiler/prelude/primops.txt.pp)](GhcFile(compiler/prelude/primops.txt.pp) "wikilink")
is piped through the \`callishOp\` function in
[GhcFile(compiler/codeGen/CgPrimOp.hs)](GhcFile(compiler/codeGen/CgPrimOp.hs) "wikilink")
to become \`Just MO\_F32\_Sin\`. The \`CallishMachOp\` constructor
\`MO\_F32\_Sin\` is piped through a platform specific function such as
[GhcFile(compiler/nativeGen/X86/CodeGen.hs)](GhcFile(compiler/nativeGen/X86/CodeGen.hs) "wikilink")
on X86, where the function \`genCCall\` will call \`outOfLineFloatOp\`
to issue a call to a C function such as \`sin\`.

Cmm Design: Observations and Areas for Potential Improvement
------------------------------------------------------------

"If the application of a primitive operator causes a system exception,
such as division by zero, this is an unchecked run-time error. (A future
version of this specification may provide a way for a program to recover
from such an exception.)" C-- spec, Section 7.4. Cmm may be able to
implement a partial solution to this problem, following the paper: [A
Single Intermediate Language That Supports Multiple Implementations of
Exceptions (2000)](http://cminusminus.org/abstracts/c--pldi-00.html).
(TODO: write notes to wiki and test fix.)

The IEEE 754 specification for floating point numbers defines exceptions
for certain floating point operations, including:

`*rangeviolation(overflow,underflow);`\
`*roundingerrors(inexact);`\
`` *invalidoperation(invalidoperand,suchascomparisonwitha`NaN`value,thesquarerootofanegativenumberordivisionofzerobyzero);and, ``\
`*zerodivide(aspecialcaseofaninvalidoperation).`

Many architectures support floating point exceptions by including a
special register as an addition to other exception handling registers.
The IBM PPC includes the \`FPSCR\` ("Floating Point Status Control
Register"); the Intel x86 processors use the \`MXCSR\` register. When
the PPC performs a floating point operation it checks for possible
errors and sets the \`FPSCR\`. Some processors allow a flag in the
Foating-Point Unit (FPU) status and control register to be set that will
disable some exceptions or the entire FPU exception handling facility.
Some processors disable the FPU after an exception has occurred while
others, notably Intel's x86 and x87 processors, continue to perform FPU
operations. Depending on whether quiet !NaNs (QNaNs) or signaling !NaNs
(SNaNs) are used by the software, an FPU exception may signal an
interrupt for the software to pass to its own exception handler.

Some higher level languages provide facilities to handle these
exceptions, including Ada, Fortran (F90 and later), C++ and C (C99,
fenv.h, float.h on certain compilers); others may handle such exceptions
without exposing a low-level interface. There are three reasons to
handle FPU exceptions, and these reasons apply similarly to other
exceptions:

`*thefacilitiesprovidegreatercontrol;`\
`*thefacilitiesareefficient--moreefficientthanahigher-levelsoftwaresolution;and,`\
`*FPUexceptionsmaybeunavoidable,especiallyifseveralFPUoperationsareseriallyperformedatthemachinelevelsothehigherlevelsoftwarehasnoopportunitytochecktheresultsinbetweenoperations.`

A potential solution to the problem of implementing Cmm exceptions,
especially for floating point operations, is at
\[wiki:Commentary/CmmExceptions Cmm: Implementing Exception Handling\].

The C-- Language Specification mentions over 75 primitive operators. The
Specification lists separate operators for integral and floating point
(signed) arithmetic (including carry, borrow and overflow checking),
logical comparisons and conversions (from one size float to another,
from float to integral and vice versa, etc.). C-- also includes special
operators for floating point number values, such as \`NaN\`,
\`mzero\`*k* and \`pzero\`*k*, and rounding modes; integral kinds also
include bitwise operators, unsigned variants, and bit extraction for
width changing and sign or zero-extension. A C-- implementation may
conveniently map each of these operators to a machine instruction, or to
a simulated operation on architectures that do not support a single
instruction. There seem to be two main problems with the current
GHC-implementation of Cmm:

`1.notenoughoperators`\
`` 1.noimplementationofvector(SIMD)registers(thoughthereisa`I128``MachRep`) ``

If a particular architecture supports it, assembler includes
instructions such as mnemonics with the \`.\` ("dot") suffix (\`add.,
fsub.\`), which set the Condition Register (CR) thereby saving you at
least one instruction. (Extended mnemonics can save you even more.)
Extended mnemonics with side effects may be implemented as new
\`CallishMachOps\`, see
\[wiki:Commentary/Compiler/CmmType\#PrimitiveOperations Primitive
Operations\] and \[wiki:Commentary/Compiler/CmmType\#CmmCalls Cmm
Calls\]. Assembler also supports machine exceptions, especially
exceptions for floating-point operations, invalid storage access or
misalignment (effective address alignment). The current implementation
of Cmm cannot model such exceptions through flow control because no flow
control is implemented, see \[wiki:Commentary/Compiler/CmmType\#CmmCalls
Cmm Calls\].

Hiding the kinds of registers on a machine eliminates the ability to
handle floating point exceptions at the Cmm level and to explicitly
vectorize (use SIMD extensions). The argument for exposing vector types
may be a special case since such low-level operations are exposed at the
C-level, as new types of variables or "intrinsics," that are C-language
extensions provided by special header files and compiler support
(\`vector unsigned int\` or \`\_\_m128i\`, \`vector float\` or
\`\_\_m128\`) and operations (\`vec\_add()\`, \`+\` (with at least one
vector operand), \`\_mm\_add\_epi32()\`).

GHC Commentary: What the hell is a \`.cmm\` file?
=================================================

A \`.cmm\` file is rather like C--. The syntax is almost C-- (a few
constructs are missing), and it is augmented with some macros that are
expanded by GHC's code generator (eg. \`INFO\_TABLE()\`). A \`.cmm\`
file is compiled by GHC itself: the syntax is parsed by
[GhcFile(compiler/cmm/CmmParse.y)](GhcFile(compiler/cmm/CmmParse.y) "wikilink")
and
[GhcFile(compiler/cmm/CmmLex.x)](GhcFile(compiler/cmm/CmmLex.x) "wikilink")
into the \[wiki:Commentary/Compiler/CmmType Cmm\] data type, where it is
then passed through one of the \[wiki:Commentary/Compiler/Backends
back-ends\].

We use the C preprocessor on \`.cmm\` files, making extensive use of
macros to make writing this low-level code a bit less tedious and
error-prone. Most of our C-- macros are in
[GhcFile(includes/Cmm.h)](GhcFile(includes/Cmm.h) "wikilink"). One
useful fact about the macros is \`P\_\` is an alias for \`gcptr\`, and
you should not use it for non-garbage-collected pointers.

Reading references
------------------

Reading material for learning Cmm is somewhat scattered, so I (Arash)
have created a list of useful links. Since the Cmm language is changing
as GHC changes, I have prioritized resources that are not too old.
(*Feel free to add/remove/modify this list! :)*)

`*AnoverviewofCmmisgivenin`[`David` `Terei's` `bachelor`
`thesis`](https://davidterei.com/downloads/papers/terei:2009:honours_thesis.pdf)`(chapter2.4.3).`\
`*Thecommentsinthebeginningof`[`GhcFile(compiler/cmm/CmmParse.y)`](GhcFile(compiler/cmm/CmmParse.y) "wikilink")`issuper-usefulandkeptuptodate.Therestofthefilecontainsthe`*`grammar`*`ofthelanguage.Afraidofgrammars?EdwardYangwrotethisfantastic`[`blog`
`post`](http://blog.ezyang.com/2013/07/no-grammar-no-problem/)`onhowtounderstandtheconstructsofCmmbyusingthegrammar.`\
`*CmmhasapreprocessorliketheoneinCandmanyofthemacrosaredefinedin`[`GhcFile(includes/Cmm.h)`](GhcFile(includes/Cmm.h) "wikilink")`.`\
`*In2012,SimonMarlowextendedtheCmmlanguagebyaddinganewhigh-levelsyntaxwhichcanbeusedwhenyoudon'tneedlow-levelaccess(likeregisters).The`[`commit`](https://github.com/ghc/ghc/commit/a7c0387d20c1c9994d1100b14fbb8fb4e28a259e)`explainsthedetails.`\
`*Cmmisalsodescribed[wiki:Commentary/Compiler/CmmTypeonthiswiki],butitiswrittenbeforethenewsyntaxwasintroduced.`\
`` *Stackframetypesarecreatedusing`INFO_TABLE_RET`,thesyntaxcanbeconfusingsincethereareboth ``*`arguments`*`and`*`fields`*`,I(Arash)havenotseenanythinglikeitinotherprogramminglanguages.Itriedtoexplainitinmy`[`master`
`thesis`](http://arashrouhani.com/papers/master-thesis.pdf)`(sections4.2and4.2.1).`

Other information
-----------------

It can take time to learn Cmm. One unintuitive thing to watch out for is
that there are no function calls in low-level cmm code. The new syntax
from 2012 allows function calls but you should know that they are kind
of magical.

We say that **Cmm** is GHC's implementation of **C--**. This naming
scheme is not done consistently everywhere, unfortunately. If you are
interested in C-- (which have diverged from Cmm), you can check out the
[website](http://www.cminusminus.org/) and the
[specification](http://www.cs.tufts.edu/~nr/c--/extern/man2.pdf).

Code Generator
==============

This page describes code generator ("codegen") in GHC. It is meant to
reflect current state of the implementation. If you notice any
inaccuracies please update the page (if you know how) or complain on
ghc-devs.

A brief history of code generator
---------------------------------

You might occasionally hear about "old" and "new" code generator. GHC
7.6 and earlier used the old code generator. New code generator was
being developed since 2007 and it was
\[changeset:832077ca5393d298324cb6b0a2cb501e27209768/ghc enabled by
default on 31 August 2012\] after the release of GHC 7.6.1. The first
stable GHC to use the new code generator is 7.8.1 released in early
2014. The commentary on the old code generator can be found
\[wiki:Commentary/Compiler/OldCodeGen here\]. Notes from the development
process of the new code generator are located in a couple of pages on
the wiki - to find them go to \[wiki:TitleIndex Index\] and look for
pages starting with "!NewCodeGen".

There are some plans for the future development of code generator. One
plan is to expand the capability of the pipeline so that it does native
code generation too so that existing backends can be discarded - see
\[wiki:Commentary/Compiler/IntegratedCodeGen IntegratedCodeGen\] for
discussion of the design. It is hard to say if this will ever happen as
currently there is no work being done on that subject and in the
meanwhile there was an alternative proposal to
\[wiki:Commentary/Compiler/Backends/LLVM/ReplacingNCG replace native
code generator with LLVM\].

Overview
--------

The goal of the code generator is to convert program from
\[wiki:Commentary/Compiler/GeneratedCode STG\] representation to
\[wiki:Commentary/Compiler/CmmType Cmm\] representation. STG is a
functional language with explicit stack. Cmm is a low-level imperative
language - something between C and assembly - that is suitable for
machine code generation. Note that terminology might be a bit confusing
here: the term "code generator" can refer both to STG-&gt;Cmm pass and
the whole STG-&gt;Cmm-&gt;assembly pass. The Cmm-&gt;assembly conversion
is performed by one the backends, eg. NCG (Native Code Generator or
LLVM.

The top-most entry point to the codegen is located in
[GhcFile(compiler/main/HscMain.hs)](GhcFile(compiler/main/HscMain.hs) "wikilink")
in the \`tryNewCodegen\` function. Code generation is done in two
stages:

`1.ConvertSTGtoCmmwithimplicitstack,andnativeCmmcalls.Thiswholestagelivesin`[`GhcFile(compiler/codeGen)`](GhcFile(compiler/codeGen) "wikilink")`` directorywiththeentrypointbeing`codeGen`functionin ``[`GhcFile(compiler/codeGen/StgCmm.hs)`](GhcFile(compiler/codeGen/StgCmm.hs) "wikilink")`module.`\
`2.OptimisetheCmm,andCPS-convertittohaveanexplicitstack,andnonativecalls.Thislivesin`[`GhcFile(compiler/cmm)`](GhcFile(compiler/cmm) "wikilink")`` directorywiththe`cmmPipeline`functionfrom ``[`GhcFile(compiler/cmm/CmmPipeline.hs)`](GhcFile(compiler/cmm/CmmPipeline.hs) "wikilink")`modulebeingtheentrypoint.`

The CPS-converted Cmm is fed to one of the backends. This is done by
\`codeOutput\` function
([GhcFile(compiler/main/CodeOutput.lhs)](GhcFile(compiler/main/CodeOutput.lhs) "wikilink")
called from \`hscGenHardCode\` after returning from \`tryNewCodegen\`.

First stage: STG to Cmm conversion
----------------------------------

`*`**`Code`
`generator`**`` convertsSTGto`CmmGraph`.Implementedin`StgCmm*`modules(indirectory`codeGen`). ``\
`` *`Cmm.CmmGraph`isprettymuchaHooplgraphof`CmmNode.CmmNode`nodes.Controltransferinstructionsarealwaysthelastnodeofabasicblock. ``\
`` *Parameterpassingismadeexplicit;thecallingconventiondependsonthetargetarchitecture.Thekeyfunctionis`CmmCallConv.assignArgumentsPos`. ``\
`*ParametersarepassedinvirtualregistersR1,R2etc.[Thesemap1-1torealregisters.]`\
`*Overflowparametersarepassedonthestackusingexplicitmemorystores,tolocationsdescribedabstractlyusingthe[wiki:Commentary/Compiler/StackAreas`*`Stack`
`Area`*`abstraction].`\
`` *Makingthecallingconventionexplicitincludesanexplicitstoreinstructionofthereturnaddress,whichisstoredexplicitlyonthestackinthesamewayasoverflowparameters.Thisisdone(obscurely)in`StgCmmMonad.mkCall`. ``

Second stage: the Cmm pipeline
------------------------------

The core of the Cmm pipeline is implemented by the \`cpsTop\` function
in
[GhcFile(compiler/cmm/CmmPipeline.hs)](GhcFile(compiler/cmm/CmmPipeline.hs) "wikilink")
module. Below is a high-level overview of the pipeline. See source code
comments in respective modules for a more in-depth explanation of each
pass.

`*`**`Control` `Flow`
`Optimisations`**`` ,implementedin`CmmContFlowOpt`,simplifiesthecontrolflowgraphby: ``\
`*Eliminatingblocksthathaveonlyonepredecessorbyconcatenatingthemwiththatpredecessor`\
`*Shortcutingtargetsofbranchesandcalls(seeNote[Whatisshortcutting])`\
``\
`Ifablockbecomesunreachablebecauseofshortcuttingitiseliminatedfromthegraph.However,`**`it`
`is` `theoretically` `possible` `that` `this` `pass` `will` `produce`
`unreachable`
`blocks`**`.Thereasonisthelabelrenamingpassperformedafterblockconcatenationhasbeencompleted.`

`Thispassmightbeoptionallycalledforthesecondtimeattheendofthepipeline.`

`*`**`Common` `Block`
`Elimination`**`` ,implementedin`CmmCommonBlockElim`,eliminatesblocksthatareidentical(exceptforthelabelontheirfirstnode).Sincethispasstraversesblocksindepth-firstorderanyunreachableblocksintroducedbyControlFlowOptimisationsareeliminated. ``**`This`
`pass` `is` `optional.`**

`*`**`Determine`
`proc-points`**`` ,implementedin`CmmProcPoint`.Theideabehindthe"proc-pointsplitting"isthatwefirstdetermineproc-points,ie.blocksinthegraphthatcanbeturnedintoentrypointsofprocedures,andthensplitalargerfunctionintomanysmallerones,eachhavingaproc-pointasitsentrypoint.ThisisrequiredfortheLLVMbackend.Theproc-pointsplittingitselfisdonelaterinthepipeline,buthereweonlydeterminethesetofproc-points.Wefirstcall`callProcPoints`,whichassumesthatentrypointtoaCmmgraphandeverycontinuationofacallisaprocpoint.Ifwearesplittingproc-pointsweupdatethelistofproc-pointsbycalling`minimalProcPointSet`,whichaddsallblocksreachablefrommorethanoneblockinthegraph.Thesetofproc-pointsisrequiredbythestacklayoutpass. ``

`*`**`Figure` `out` `the` `stack`
`layout`**`` ,implementedin`CmmStackLayout`.Thejobofthispassisto: ``\
`*replacereferencestoabstractstackAreaswithfixedoffsetsfromSp.`\
`*replacethe!CmmHighStackMarkconstantusedinthestackcheckwith`\
`themaximumstackusageoftheproc.`\
`*saveanyvariablesthatareliveacrossacall,andreloadthemas`\
`necessary.`\
**`Important`**`:Itmayhappenthatstacklayoutwillinvalidatethecomputedsetofproc-pointsbymakingaproc-pointunreachable.Thisunreachableblockiseliminatedbyoneofsubsequentpassesthatperformsdepth-firsttraversalofagraph:sinkingpass(ifoptimisationsareenabled),proc-pointanalysis(ifoptimisationsaredisabledandwe'redoingproc-pointsplitting)orattheveryendofthepipeline(ifoptimisationsaredisabledandwe'renotdoingproc-pointsplitting).Thismeansthatstartingfromthispointinthepipelinewehaveinconsistentdataandsubsequentstepsmustbepreparedforit.`\
\
`*`**`Sinking`
`assignments`**`` ,implementedin`CmmSink`,performstheseoptimizations: ``\
`*movesassignmentsclosertotheiruses,toreduceregisterpressure`\
`*pushesassignmentsintoasinglebranchofaconditionalifpossible`\
`*inlinesassignmentstoregistersthatarementionedonlyonce`\
`*discardsdeadassignments`\
**`This` `pass` `is`
`optional.`**`Itcurrentlydoesnoteliminatedeadcodeinloops(#8327)andhassomeotherminordeficiencies(eg.#8336).`

`*`**`CAF`
`analysis`**`` ,implementedin`CmmBuildInfoTables`.ComputedCAFinformationisreturnedfrom`cmmPipeline`andusedtocreateStaticReferenceTables(SRT).See[wiki:Commentary/Rts/Storage/GC/CAFshere]forsomemoredetailonCAFsandSRTs.ThispassisimplementedusingHoopl(seebelow). ``

`*`**`Proc-point` `analysis` `and`
`splitting`**`` (onlywhensplittingproc-points),implementedby`procPointAnalysis`in`CmmProcPoint`,takesalistofproc-pointsandforeachblockanddeterminesfromwhichproc-pointtheblockisreachable.ThisisimplementedusingHoopl. ``\
`` Thenthecallto`splitAtProcPoints`splitstheCmmgraphintomultipleCmmgraphs(eachrepresentsasinglefunction)andbuildinfotablestoeachofthem. ``\
`Whendoingthiswemustbepreparedforthefactthataproc-pointdoesnotactuallyexistinthegraphsinceitwasremovedbystacklayoutpass(see#8205).`

`*`**`Attach` `continuations'` `info`
`tables`**`` (onlywhenNOTsplittingproc-points),implementedby`attachContInfoTables`in`CmmProcPoint`attachesinfotablesforthecontinuationsofcallsinthegraph. ``*`[PLEASE`
`WRITE` `MORE` `IF` `YOU` `KNOW` `WHY` `THIS` `IS` `!DONE]`*

`*`**`Update` `info` `tables` `to` `include` `stack`
`liveness`**`` ,implementedby`setInfoTableStackMap`in`CmmLayoutStack`.PopulatesinfotablesofeachCmmfunctionwithstackusageinformation.Usesstackmapscreatedbythestacklayoutpass. ``

`*`**`Control` `Flow`
`Optimisations`**`` ,sameasthebeginningofthepipeline,butthispassrunsonlywith`-O1`and`-O2`.Sincethispassmightproduceunreachableblocksitisfollowedbyacallto`removeUnreachableBlocksProc`(alsoin`CmmContFlowOpt.hs`) ``

Dumping and debugging Cmm
-------------------------

You can dump the generated Cmm code using \`-ddump-cmm\` flag. This is
helpful for debugging Cmm problems. Cmm dump is divided into several
sections:

"Cmm produced by new codegen" is emited in \`HscMain\` module after
converting STG to Cmm. This Cmm has not been processed in any way by the
Cmm pipeline. If you see that something is incorrect in that dump it
means that the problem is located in the STG-&gt;Cmm pass. The last
section, "Output Cmm", is also dumped in \`HscMain\` but this is done
after the Cmm has been processed by the whole Cmm pipeline. All other
sections are dumped by the Cmm pipeline. You can dump only selected
passes with more specific flags. For example, if you know (or suspect)
that the sinking pass is performing some incorrect transformations you
can make the dump shorter by adding \`-ddump-cmm-sp -ddump-cmm-sink\`
flags. This will produce only the "Layout Stack" dump (just before
sinking pass) and "Sink assignments" dump (just after the sinking pass)
allowing you to focus on the changes introduced by the sinking pass.

Register Allocator Code
-----------------------

The register allocator code is split into two main sections, the
register allocator proper and a generic graph coloring library. The
graph coloring library is also used by the Stg-&gt;Cmm converter.

### The register allocator

`*`[`GhcFile(compiler/nativeGen/RegLiveness.hs)`](GhcFile(compiler/nativeGen/RegLiveness.hs) "wikilink")``[`BR`](BR "wikilink")\
`Defines``and``whichcarrynativemachineinstructionsannotatedwithregisterlivenessinformation.Italsoprovidesfunctionstoannotatenativecode(``)withthislivenessinformation,andtoslurpoutsetsofregisterconflictsforfeedingintothecoloringallocator.`

`*`[`GhcFile(compiler/nativeGen/RegAllocColor.hs)`](GhcFile(compiler/nativeGen/RegAllocColor.hs) "wikilink")``[`BR`](BR "wikilink")\
`Defines``,themaindriverfunctionforthegraphcoloringallocator.Thedriveraccepts``swhichusevirtualregs,andproduces``whichuserealmachineregs.Thismodulealsoprovidesfunctionstohelpbuildanddeepseqtheregisterconflictgraph.`

`*`[`GhcFile(compiler/nativeGen/RegAllocLinear.hs)`](GhcFile(compiler/nativeGen/RegAllocLinear.hs) "wikilink")``[`BR`](BR "wikilink")\
`Definesthelinearscanallocator.Itsinterfaceisidenticaltothecoloringallocator.`

`*`[`GhcFile(compiler/nativeGen/RegAllocInfo.hs)`](GhcFile(compiler/nativeGen/RegAllocInfo.hs) "wikilink")``[`BR`](BR "wikilink")\
`Definestheregisterinformationfunction,``,whichtakesasetofrealandvirtualregistersandreturnstheactualregistersusedbyaparticular``;registerallocationisinAT&Tsyntaxorder(source,destination),inaninternalfunction,``;definesthe``datatype`[`BR`](BR "wikilink")[`BR`](BR "wikilink")

`*`[`GhcFile(compiler/nativeGen/RegSpillCost.hs)`](GhcFile(compiler/nativeGen/RegSpillCost.hs) "wikilink")``[`BR`](BR "wikilink")\
`Defines``whichisresponsibleforselectingavirtualregtospilltothestackwhennotenoughrealregsareavailable.`

`*`[`GhcFile(compiler/nativeGen/RegSpill.hs)`](GhcFile(compiler/nativeGen/RegSpill.hs) "wikilink")``[`BR`](BR "wikilink")\
`Defines``whichtakes``sandinsertsspill/reloadinstructionsvirtualregsthatwouldn'tfitinrealregs.``'sstrategyistosimplyinsertsspill/reloadsforeveryuse/defofaparticularvirtualreg.Thisinefficientcodeiscleanedupbythespillcleanerafterallocation.`\
\
`*`[`GhcFile(compiler/nativeGen/RegSpillClean.hs)`](GhcFile(compiler/nativeGen/RegSpillClean.hs) "wikilink")``[`BR`](BR "wikilink")\
`Thespillcleanerisrunafterrealregshavebeenallocated.Iterasesspill/reloadinstructionsinsertedby``thatweren'tstrictlynessesary.`

`*`[`GhcFile(compiler/nativeGen/RegAllocStats.hs)`](GhcFile(compiler/nativeGen/RegAllocStats.hs) "wikilink")``[`BR`](BR "wikilink")\
`Definesdatatypesandprettyprintersusedforcollectingstatisticsanddebugginginfofromthecoloringallocator.`

### Graph coloring

`*`[`GhcFile(compiler/utils/GraphBase.hs)`](GhcFile(compiler/utils/GraphBase.hs) "wikilink")``[`BR`](BR "wikilink")\
`Definesthebasic``,``and``typesusedbythecoloringalgorithm.`

`*`[`GhcFile(compiler/utils/GraphColor.hs)`](GhcFile(compiler/utils/GraphColor.hs) "wikilink")``[`BR`](BR "wikilink")\
`Definesthefunction``whichisresponsibleforassigningcolors(realregs)tonodes(virtualregs)intheregisterconflictgraph.`

`*`[`GhcFile(compiler/utils/GraphOps.hs)`](GhcFile(compiler/utils/GraphOps.hs) "wikilink")``[`BR`](BR "wikilink")\
`Definesfunctionstoperformbasicoperationsonthegraphssuchasadding,deleting,andcoalescingnodes.`

`*`[`GhcFile(compiler/utils/GraphPps.hs)`](GhcFile(compiler/utils/GraphPps.hs) "wikilink")``[`BR`](BR "wikilink")\
`Definesfunctionsforprettyprintgraphsinhumanreadable-ishandgraphvizformat.`

### Miscellanea

`*`[`GhcFile(compiler/nativeGen/RegCoalesce.hs)`](GhcFile(compiler/nativeGen/RegCoalesce.hs) "wikilink")``[`BR`](BR "wikilink")\
`Definesafunction``thatdoesaggressivecoalescingdirectlyon``,withoutusingthegraph.Thisisn'tusedatthemomentbuthasbeenleftinincasewewanttorejigtheallocatorwhenthenewCPSconvertercomesonline.`

`*`[`GhcFile(compiler/nativeGen/RegArchBase.hs)`](GhcFile(compiler/nativeGen/RegArchBase.hs) "wikilink")``[`BR`](BR "wikilink")\
`Definesutilsforcalculatingwhetheraregisterintheconflictgraphistriviallycolorable,inagenericwaywhichhandlesaliasingbetweenregisterclasses.ThismoduleisnotuseddirectlybyGHC.`

`*`[`GhcFile(compiler/nativeGen/RegArchX86.hs)`](GhcFile(compiler/nativeGen/RegArchX86.hs) "wikilink")``[`BR`](BR "wikilink")\
`Containsadescriptionofthealiasingconstraintsbetweentheregistersetsonx86.ThismoduleisnotuseddirectlybyGHC.`

[PageOutline](PageOutline "wikilink")

The GHC Commentary - Coding Style Guidelines for the compiler
=============================================================

This is a rough description of some of the coding practices and style
that we use for Haskell code inside . For run-time system code see the
\[wiki:Commentary/Rts/Conventions Coding Style Guidelines for RTS C
code\]. Also see the wiki page on \[wiki:WorkingConventions Working
Conventions\] for issues related to version control, workflow, testing,
bug tracking and other miscellany.

General Style
-------------

The general rule is to stick to the same coding style as is already used
in the file you're editing. If you must make stylistic changes, commit
them separately from functional changes, so that someone looking back
through the change logs can easily distinguish them.

It's much better to write code that is transparent than to write code
that is short.

Often it's better to write out the code longhand than to reuse a generic
abstraction (not always, of course). Sometimes it's better to duplicate
some similar code than to try to construct an elaborate generalisation
with only two instances. Remember: other people have to be able to
quickly understand what you've done, and overuse of abstractions just
serves to obscure the *really* tricky stuff, and there's no shortage of
that in GHC.

Comments
--------

There are two kinds of comments in source code, comments that describe
the interface (i.e. how is this supposed to be used) and comments that
describe the implementation (e.g. subtle gotchas).

### Comments on top-level entities

Every top-level entity should have a Haddock comment that describes what
it does and, if needed, why it's there. Example:

We use Haddock so that the comment is included in the generated HTML
documentation.

There's a bit of a broken window effect going on, but please try to
follow this rule for new functions you add.

### Comments in the source code

Commenting is good but

`*longcomments`*`interleaved` `with` `the`
`code`*`canmakethecodeitselfincrediblyhardtoread,and`\
`*longcomments`*`detached` `from` `the`
`code`*`areeasytomisswhenyouareeditingthecodeitself,andsoonbecomeoutofdateorevenmisleading.`

We have adopted a style that seems to help. Here's an example: Notice
that

`*`**`Interleaved` `with` `the`
`code`**`` isashortlink`Note[Floatcoercions]`.Youcan'tmissitwhenyouareeditingthecode,butyoucanstillseethecodeitself. ``\
`*`**`Detached` `from` `the`
`code`**`` isthelinkedcomment,startingwiththesamestring`Note[Floatcoercions]`.Itcanbelong,andoftenincludesexamples. ``

The standard format "\`Note \[Float coercions\]\`" serves like an URL,
to point to an out-of-line comment. Usually the target is in the same
module, but not always. Sometimes we say

Please use this technique. It's robust, and survives successive changes
to the same lines of code. When you are changing code, it draws
attention to non-obvious things you might want to bear in mind. When you
encounter the note itself you can search for the string to find the code
that implements the thoughts contained in the comment.

### Comments and examples

When writing a comment to explain a subtle point, consider including an
example code snippet that illustrates the point. For example, the above
\`Note \[Float coercions\]\` continues thus: These kind of code snippets
are extremely helpful to illustrate the point in a concrete way. Other
ways of making the comment concrete are:

`*CiteaparticularTracticketthatthisbitofcodedealswith`\
`*Citeatestcaseinthetestsuitethatillustratesit`

### Longer comments or architectural commentary

Comments with a broad scope, describing the architecture or workings of
more than one module, belong here in the commentary rather than in the
code. Put the URL for the relevant commentary page in a comment in the
code itself, and also put URLs for all relevant commentary pages in a
comment at the top of each module.

### Commit messages

Please do not use commit messages to describe how something works, or
give examples, *even if the patch is devoted to a single change*. The
information is harder to find in a commit message, and (much worse)
there is no explicit indication in the code that there is
carefully-written information available about that particular line of
code. Instead, you can refer to the Note from the commit message.

Commit messages can nevertheless contain substantial information, but it
is usually of a global nature. E.g. "This patch modifies 20 files to
implement a new form of inlining pragma". They are also a useful place
to say which ticket is fixed by the commit, summarise the changes
embodied in the commit etc.

In short, commit messages describe *changes*, whereas comment explain
the code *as it now is*.

Warnings
--------

We are aiming to make the GHC code warning-free, for all warnings turned
on by The build automatically sets these flags for all source files (see
\`mk/warnings.mk\`).

The \[wiki:TestingPatches validate script\], which is used to test the
build before commiting, additionally sets the \`-Werror\` flag, so that
the code **must** be warning-free to pass validation. The \`-Werror\`
flag is not set during normal builds, so warnings will be printed but
won't halt the build.

Currently we are some way from our goal, so some modules have a pragma;
you are encouraged to remove this pragma and fix any warnings when
working on a module.

Exports and Imports
-------------------

### Exports

 We usually (99% of the time) include an export list. The only
exceptions are perhaps where the export list would list absolutely
everything in the module, and even then sometimes we do it anyway.

It's helpful to give type signatures inside comments in the export list,
but hard to keep them consistent, so we don't always do that.

### Imports

List imports in the following order:

`*Localtothissubsystem(ordirectory)first`\
`*Compilerimports,generallyorderedfromspecifictogeneric(ie.modulesfromutils/andbasicTypes/usuallycomelast)`\
`*Libraryimports`\
`*StandardHaskell98importslast`

Import library modules from the \[wiki:Commentary/Libraries boot
packages\] only (boot packages are those packages in the file
\[source:packages\] that have a '-' in the "tag" column). Use
\`\#defines \`in \`HsVersions.h\` when the modules names differ between
versions of GHC. For code inside \`\#ifdef GHCI\`, don't worry about GHC
versioning issues, because this code is only ever compiled by the this
very version of GHC.

**Do not use explicit import lists**, except to resolve name clashes.
There are several reasons for this:

`*Theyslowdowndevelopment:almosteverychangeisaccompaniedbyanimportlistchange.`

`*Theycausespuriousconflictsbetweendevelopers.`

`*Theyleadtouselesswarningsaboutunusedimports,andtimewastedtryingto`\
`keeptheimportdeclarations"minimal".`

`` *GHC'swarningsareusefulfordetectingunnecessaryimports:see`-fwarn-unused-imports`. ``

`` *TAGSisagoodwaytofindoutwhereanidentifierisdefined(use`maketags`in`ghc/compiler`, ``\
`` andhit`M-.`inemacs). ``

If the module can be compiled multiple ways (eg. GHCI vs. non-GHCI),
make sure the imports are properly \`\#ifdefed\` too, so as to avoid
spurious unused import warnings.

Compiler versions and language extensions
-----------------------------------------

GHC must be compilable and validate by the previous two major GHC
releases, and itself. It isn't necessary for it to be compilable by
every intermediate development version.

To maintain compatibility, use
\[wiki:Commentary/CodingStyle\#HsVersions.h HsVersions.h\] (see below)
where possible, and try to avoid using \#ifdef in the source itself.

### 

 is a CPP header file containing a number of macros that help smooth out
the differences between compiler versions. It defines, for example,
macros for library module names which have moved between versions. Take
a look
[GhcFile(compiler/HsVersions.h)](GhcFile(compiler/HsVersions.h) "wikilink").

### Literate Haskell

In GHC we use a mixture of literate () and non-literate () source. I
(Simon M.) prefer to use non-literate style, because I think the }
clutter up the source too much, and I like to use Haddock-style comments
(we haven't tried processing the whole of GHC with Haddock yet, though).

### The C Preprocessor (CPP)

Whenever possible we try to avoid using CPP, as it can hide code from
the compiler (which means changes that work on one platform can break
the build on another) and code using CPP can be harder to understand.

The following CPP symbols are used throughout the compiler:

**`DEBUG`**`::`\
`Usedtoenablesextrachecksanddebuggingoutputinthecompiler.TheASSERTmacro(see``)providesassertionswhichdisappearwhenDEBUGisnotdefined.`

`` However,wheneverpossible,itisbettertouse`debugIsOn`fromthe`Util`module,whichisdefinedtobe`True`when`DEBUG`isdefinedand`False`otherwise.TheidealwaytoprovidedebuggingoutputistouseaHaskellexpression"`whendebugIsOn$...`"toarrangethatthecompilerwillbesilentwhen`DEBUG`isoff(unlessofcoursesomethinggoeswrongortheverbositylevelisnonzero).Whenoption`-O`isused,GHCwilleasilysweepawaytheunreachablecode. ``

`` Asalastresort,debuggingcodecanbeplacedinside`#ifdefDEBUG`,butsincethisstrategyguaranteesthatonlyafractionofthecodeisseenbethecompileronanyonecompilation,itistobeavoidedwhenpossible. ``

`` Regardingperformance,agoodruleofthumbisthat`DEBUG`shouldn'taddmorethanabout10-20%tothecompilationtime.Thisisthecaseatthemoment.Ifitgetstooexpensive,wewon'tuseit.Formoreexpensiveruntimechecks,consideraddingaflag-seeforexample`-dcore-lint`. ``

**Trap, pitfall for using the ASSERT macro**:

The ASSERT macro uses CPP, and if you are unwise enough to try to write
assertions using primed variables (), one possible outcome is that CPP
silently fails to expand the ASSERT, and you get this very baffling
error message: Now you can Google for this error message :-)

**`GHCI`**`::`\
`EnablesGHCisupport,includingthebytecodegeneratorandinteractiveuserinterface.Thisisn'tthedefault,becausethecompilerneedstobebootstrappedwithitselfinorderforGHCitoworkproperly.Thereasonisthatthebyte-codecompilerandlinkerarequitecloselytiedtotheruntimesystem,soitisessentialthatGHCiislinkedwiththemostup-to-dateRTS.AnotherreasonisthattherepresentationofcertaindatatypesmustbeconsistentbetweenGHCianditslibraries,andifthesewereinconsistentthendisastercouldfollow.`

### Platform tests

Please refer to \[wiki:Commentary/PlatformNaming Platforms and
Conventions\] wiki page for an overview of how to handle target specific
code in GHC.

Tabs vs Spaces
--------------

GHCs source code is indented with a mixture of tabs and spaces, and is
standardised on a tabstop of 8.

Most of the Haskell source code in GHC is free of tabs. We'd like to
move away from tabs in the long term, and so a git hook on
darcs.haskell.org will reject series of commits that add tabs to a file
that is currently tab-free. (Note that there are no restrictions on
adding tabs to a file already containing them.)

In order to avoid angering this git hook, you should set your editor to
indent using spaces rather than tabs:

`` *InEmacs,add`(setq-defaultindent-tabs-modenil)`toyour`.emacs`file( ``[`more`
`discussion`](http://cscs.umich.edu/~rlr/Misc/emacs_tabs.htm)`)`\
`` *InSublimeText,savethefollowingtofilesat`Packages/User/Haskell.sublime-settings`and`Packages/User/LiterateHaskell.sublime-settings`: ``

`*In!TextMate,inthetabspop-upmenuatthebottomofthewindow,select"SoftTabs",asshowinthefollowingscreenshotwherethebluerectangleis:`

``[`Image(TextMate-tabs-menu.png)`](Image(TextMate-tabs-menu.png) "wikilink")``

`Alternatively,opentheBundleEditorandaddanewPreferencecalledIndentationtothebundleeditor.Giveitthefollowingcontents:`

Coercions in GHC's core language
================================

Ever since coercions were introduced into GHC's Core language I have
treated

`*Coercionsliketypes`\
`*Coercionvariablesliketypevariables`

In particular, casts, coercion applications, and coercion abstractoins
are all erased before we generate code.

I now think that this is the wrong approach. This note describes why.

Difficulties with the current approach
--------------------------------------

Ther are two problems with the current approach

`*Equalityevidencevariables("typevariables")aretreateddifferentlytodictionaryevidencevariables("termvaraibles").Thisleadstolotsoftiresomenon-uniformities.`\
`` *Inanabstraction`/\a\x:a.e`thetypevariable`a`canappearinthetypeofaterm-variablebinder`x`.Incontrast`x`can'tappearinthetypeofanotherbinder.Coercionbindersbehaveexactlyliketermbindersinthisway,andquiteunliketypebinders. ``\
`*Moreseriously,wedon'thaveadecentwaytohandlesuperclassequalities.`

The last problem is the one that triggered this note, and needs a bit
more explanation. Consider The dictionary for C looks like this: Now
imagine typechecking a function like this The Core program we generate
looks something like this: The \`nd\` binding extracts the \`Num\`
superclass dictionary from the \`C\` dictionary; the case expression is
called a *superclass selector*.

Now suppose that we needed to use the equality superclass rather than
the \`Num\` superclass: The obvious translation would look like this:
But Core doesn't (currently) have a let-binding form that binds a
coercion variable, and whose right-hand side is a term (in this example,
a case expression) rather than a literal coercion! So the current plan
is to generate this instead: This non-uniformity of equality and
dictionary evidence is extremely awkward in the desugarer. Moreover, it
means that we can't abstract the superclass selector; we'd really like
to have: And it interacts poorly with the class-op rules that GHC uses
to simplify dictinary selectors. Imagine the call ...unfinished...

Main proposal
-------------

Recall our basic types Note that

`` *`Var`canbeatypevariable,coercionvariable,ortermvariable.Youcantellwhichwithadynamictest(e.g.`isId::Var->Bool`). ``

`` *`Lam`isusedfortypeabstractions,coercionabstractions,andvalueabstractions.The`Var`cantellyouwhich. ``

`` *Typeapplications(inaterm)looklike`(Appf(Typet))`.The`(Typet)`partmustliterallyappearthere,withnointerveningjunk.Thisisnotstaticallyenforced,butitturnsouttobemuchmoreconvenientthanhavingaconstructor`TyAppCoreExprType`. ``

OK now the new proposal is to *treat equality evidence just like any
other sort of evidence*.

`*Acoercionvariableistreatedliketerm-levelidentifier,notatype-levelidentifier.(Moreonwhatthatmeansbelow.)`

`` *Acoercionisan`CoreExpr`,ofform`Coerciong`,whosetypeis`(s~t)`,ofform`PredTy(EqPredst)`. ``

`` *Unliketypeapplications,coercionapplicationsarenotrequiredtohavea`(Coerciong)`astheargument.Forexample,supposewehave ``

`` Thentheterm`(fx(id(x~Int)c))`wouldbefine.Noticethatthecoercionargumentisanappplicationoftheidentityfunction.(Yesit'sabitcontrived.)In`CoreExpr`formitwouldlooklike: ``

`*Similarlyalet-bindingcanbindacoercion`

`*Coercionapplicationiscall-byvalue.Dittolet-bindings.Youmusthavetheevidencebeforecallingthefunction.`\
\
`*Soitdoesn'tmakesensetohaverecursivecoercionbindings.`

`` *Ifwesee`Let(NonRecc(Coerciong))e`wecansubstitute`(Coerciong)`foranyterm-leveloccurrencesof`c`intheterm`e`,and`g`for`c`inanyoccurrencesof`c`incoercionsinside`e`.(Thisseemsabitmessy.) ``

Parsing of command line arguments
=================================

GHC's many flavours of command line flags make the code interpreting
them rather involved. The following provides a brief overview of the
processing of these options. Since the addition of the interactive
front-end to GHC, there are two kinds of flags: static and dynamic.
Static flags can only be set once on the command line. They remain the
same throughout the whole GHC session (so for example you cannot change
them within GHCi using \`:set\` or with \`OPTIONS\_GHC\` pragma in the
source code). Dynamic flags are the opposite: they can be changed in
GHCi sessions using \`:set\` command or \`OPTIONS\_GHC\` pragma in the
source code. There are few static flags and it is likely that in the
future there will be even less. Thus, you won't see many static flag
references in the source code, but you will see a lot of functions that
use dynamic flags.

Command line flags are described by Flag data type defined in
[GhcFile(compiler/main/CmdLineParser.hs)](GhcFile(compiler/main/CmdLineParser.hs) "wikilink"):

This file contains functions that actually parse the command line
parameters.

Static flags
------------

Static flags are managed by functions in
[GhcFile(compiler/main/StaticFlags.hs)](GhcFile(compiler/main/StaticFlags.hs) "wikilink").

Function \`parseStaticFlags ::\` is an entry point for parsing static
flags. It is called by the \`main :: IO ()\` function of GHC in
[GhcFile(ghc/Main.hs)](GhcFile(ghc/Main.hs) "wikilink"). Two global
IORefs are used to parse static flags: \`v\_opt\_C\_ready\` and
\`v\_opt\_C\`. These are defined using \`GLOBAL\_VAR\` macro from
[GhcFile(compiler/HsVersions.h)](GhcFile(compiler/HsVersions.h) "wikilink").
First IORef is a flag that checks whether the static flags are parsed at
the right time. Initialized to \`False\`, it is set to \`True\` after
the parsing is done. \`v\_opt\_C\` is a \`\[String\]\` used to store
parsed flags (see \`addOpt\` and \`removeOpt\` functions).

In
[GhcFile(compiler/main/StaticFlags.hs)](GhcFile(compiler/main/StaticFlags.hs) "wikilink"),
\`flagsStatic :: \[Flag IO\]\` defines a list of static flags and what
actions should be taken when these flags are encountered (see \`Flag\`
data type above). It also contains some helper functions to check
whether particular flags have been set. Functions \`staticFlags ::
\[String\]\` and \`packed\_staticFlags :: \[FastString\]\` return a list
of parsed command line static flags, provided that parsing has been done
(checking the value of \`v\_opt\_C\_ready\`).

Dynamic flags
-------------

They are managed by functions in
[GhcFile(compiler/main/DynFlags.hs)](GhcFile(compiler/main/DynFlags.hs) "wikilink")
file. Looking from the top you will find data types used to described
enabled dynamic flags: \`DumpFlag\`, \`GeneralFlag\`, \`WarningFlag\`,
\`Language\`, \`SafeHaskellMode\`, \`ExtensionFlag\` and finally
\`DynFlags\`. Function \`defaultDynFlags :: Settings -&gt; DynFlags\`
initializes some of the flags to default values. Available dynamic flags
and their respective actions are defined by \`dynamic\_flags :: \[Flag
(CmdLineP DynFlags)\]\`. Also, \`fWarningFlags :: \[FlagSpec
WarningFlag\]\`, \`fFlags :: \[FlagSpec GeneralFlag\]\`, \`xFlags ::
\[FlagSpec ExtensionFlag\]\` and a few more smaller functions define
even more flags needed for example for language extensions, warnings and
other things. These flags are descibred by the data type \`FlagSpec f\`:

 Flags described by \`FlagSpec\` can be reversed, e.g. flags that start
with \`-f\` prefix are reversed by using \`-fno-\` prefix instead.

The GHC Commentary
==================

This tree of wiki pages is a "commentary" on the GHC source code. It
contains all the explanatory material that doesn't belong in comments in
the source code itself, because the material is wide-ranging, usually
covers multiple source files, and is more architectural in nature. The
commentary can also be considered a design document for GHC.

For the beginners there is \[wiki:Newcomers a short getting started
guide\].

For the dedicated, there are \[wiki:AboutVideos videos of Simon and
Simon giving an overview of GHC\], at the 2006 \[wiki:Hackathon GHC
Hackathon\].

Also check out the \[wiki:ReadingList GHC Reading List\], which gives
lots of background reading that will help you understand the actual
implementation. Here's [another reading
list](http://www.stephendiehl.com/posts/essential_compilers.html) from
Stephen Diehl.

Editing the Commentary
----------------------

Please feel free to add material to the rest of the wiki: don't worry
too much about accuracy (in due course someone will edit your
contribution). When unsure though please indicate this and its best to
ask on the GHC mailing list so you can correct the commentary. Please
give some thought to where in the commentary your contribution belongs.
GHC has an older commentary (non wiki based) that read like a single
coherent narrative, made sure to define terms before using them, and
introduced concepts in the order which made them easiest to understand.
Please do try to preserve those properties in this wiki commentary. If
you're unsure or in a hurry, consider creating a wiki page outside the
commentary and linking to it from the commentary (or the "contributed
documentation" section below).

Try to link to source files as much as possible by using this macro: .
Also try to add appropriate links to other parts of the commentary.

Contents
--------

`*[wiki:Commentary/GettingStartedGettingStarted]`\
`*[wiki:Commentary/SourceTreeSourceTreeRoadmap]`\
`*[wiki:Commentary/ModuleStructureModuleStructure]`\
`*[wiki:Commentary/CodingStyleCodingStyle]`\
`*[wiki:Commentary/AbbreviationsAbbreviationsinGHC]`\
`*[wiki:Commentary/PlatformNamingPlatformsandtheirNamingConvention]`

`*[wiki:Commentary/CompilerTheCompiler]`

`*[wiki:Commentary/LibrariesTheLibrariesonwhichGHCdepends]`\
`` *[wiki:Commentary/Libraries/IntegerTheIntegerlibraries(`integer-gmp`and`integer-simple`)] ``

`*[wiki:Commentary/RtsTheRuntimeSystem(RTS)]`\
`*[wiki:Commentary/Rts/ConventionsRTSCodingConventions]`\
`*[wiki:Commentary/Rts/HaskellExecutionTheHaskellExecutionModel]`\
`*[wiki:Commentary/Rts/StorageThememorylayoutofheapandstackobjects]`\
\
`*Cross-cuttingconcerns:topicswhichspanboththecompilerandtheruntimesystem`\
`*[wiki:Commentary/ProfilingProfiling]`\
`*[wiki:Commentary/Compiler/WiredInWired-inandknown-keythings]`\
`*[wiki:Commentary/PrimOpsPrimitiveOperations(PrimOps)]`\
`*[wiki:Commentary/PackagesThePackageSystem]`\
\
`*[wiki:Commentary/UserManualTheUserManual](formattingguidelinesetc)`

Contributed Documentation
-------------------------

The above commentary covers the source code of GHC. For material that
doesn't concern this topic (such as proposals, work-in-progress and
status reports) or that don't fit into the existing structure, you will
find them below. Feel free to add new material here but please
categorise it correctly.

`*GeneralNotesontheGHCcompiler`\
`*EdwardYang'sblogpostabout`[`the` `entire` `complilation`
`pipeline` `for`
`` `factorial` ``](http://blog.ezyang.com/2011/04/tracing-the-compilation-of-hello-factorial/)\
`*[wiki:AddingNewPrimitiveOperationsNewPrimOps]:HowtoaddnewprimitiveoperationstoGHCHaskell.`\
`*[wiki:ReplacingGMPNotesReplacingGMP]:NotesfromanefforttoreplaceGMPwithanotherBignumlibrary.`\
`*[wiki:ExternalCoreExternalCore]:DescribestheprocessofbringingExternalCoreuptospeed.Oncefinished,thiswillsimplydescribewhatExternalCoreis,andhowitworks.`\
`*`[`The` `Scrap` `your` `boilerplate`
`homepage`](http://sourceforge.net/apps/mediawiki/developers/index.php?title=ScrapYourBoilerplate)`.`\
`*[wiki:Commentary/Compiler/OptOrderingOptimisationOrdering]Describetheorderingandinteractionofoptimisationpasses(Old).`\
`*`[`GHC`
`Illustrated`](https://github.com/takenobu-hs/haskell-ghc-illustrated)`(followthePDFlink),averyinsightfultutorialonGHC'sinternals.`\
`*`[`Ollie` `Charles's` `24` `days` `of` `GHC`
`Extensions`](https://ocharles.org.uk/blog/pages/2014-12-01-24-days-of-ghc-extensions.html)`,and`[`Lennart`
`Augstsson's`
`commentary`](http://augustss.blogspot.com/2014/12/a-commentary-on-24-days-of-ghc.html)\
`*`[`Welcome`](https://ocharles.org.uk/blog/posts/2014-12-01-24-days-of-ghc-extensions.html)\
`*`[`Static`
`Pointers`](https://ocharles.org.uk/blog/guest-posts/2014-12-23-static-pointers.html)\
`*`[`Template`
`Haskell`](https://ocharles.org.uk/blog/guest-posts/2014-12-22-template-haskell.html)\
`*`[`Arrows`](https://ocharles.org.uk/blog/guest-posts/2014-12-21-arrows.html)\
`*`[`Scoped` `Type`
`Variables`](https://ocharles.org.uk/blog/guest-posts/2014-12-20-scoped-type-variables.html)\
`*`[`Existential`
`Quantification`](https://ocharles.org.uk/blog/guest-posts/2014-12-19-existential-quantification.html)\
`*`[`Rank` `N`
`Types`](https://ocharles.org.uk/blog/guest-posts/2014-12-18-rank-n-types.html)\
`*`[`Overloaded`
`Strings`](https://ocharles.org.uk/blog/posts/2014-12-17-overloaded-strings.html)\
`*`[`DeriveGeneric`](https://ocharles.org.uk/blog/posts/2014-12-16-derive-generic.html)\
`*`[`Deriving`](https://ocharles.org.uk/blog/guest-posts/2014-12-15-deriving.html)\
`*`[`Functional`
`Dependencies`](https://ocharles.org.uk/blog/posts/2014-12-14-functional-dependencies.html)\
`*`[`Multi-parameter` `Type`
`Classes`](https://ocharles.org.uk/blog/posts/2014-12-13-multi-param-type-classes.html)\
`*`[`Type`
`Families`](https://ocharles.org.uk/blog/posts/2014-12-12-type-families.html)\
`*`[`Implicit`
`Parameters`](https://ocharles.org.uk/blog/posts/2014-12-11-implicit-params.html)\
`*`[`Nullary` `Type`
`Classes`](https://ocharles.org.uk/blog/posts/2014-12-10-nullary-type-classes.html)\
`*`[`Recursive`
`Do`](https://ocharles.org.uk/blog/posts/2014-12-09-recursive-do.html)\
`*`[`Type`
`Operators`](https://ocharles.org.uk/blog/posts/2014-12-08-type-operators.html)\
`*`[`List`
`Comprehensions`](https://ocharles.org.uk/blog/guest-posts/2014-12-07-list-comprehensions.html)\
`*`[`Rebindable`
`Syntax`](https://ocharles.org.uk/blog/guest-posts/2014-12-06-rebindable-syntax.html)\
`*`[`Bang`
`Patterns`](https://ocharles.org.uk/blog/posts/2014-12-05-bang-patterns.html)\
`*`[`Record`
`Wildcards`](https://ocharles.org.uk/blog/posts/2014-12-04-record-wildcards.html)\
`*`[`Pattern`
`Synonyms`](https://ocharles.org.uk/blog/posts/2014-12-03-pattern-synonyms.html)\
`*`[`View`
`Patterns`](https://ocharles.org.uk/blog/posts/2014-12-02-view-patterns.html)\
`*`[`Thanks`](https://ocharles.org.uk/blog/posts/2014-12-24-conclusion.html)\
`*[wiki:Commentary/Rts/CompilerWays]:Compiler`*`ways`*`inGHC,what,how,andwhere`

`*NotesonimplementedGHCfeatures:`\
`*`[`Evaluation` `order` `and` `state`
`tokens`](https://www.fpcomplete.com/tutorial-preview/4431/z0KpB0ai2R)`:noteswrittenbyMichaelSnoyberginresponseto#9390.`\
`*[wiki:FoldrBuildNotesNotesonfusion](egfoldr/build)`\
`*[wiki:OverloadedListsOverloadedlistsyntax]allowsyoutouselistnotationforthingsotherthanlists.`\
`*[wiki:GhcKindsKindpolymorphismanddatatypepromotion]`\
`*[wiki:KindFactAkindforclassconstraints.ImplementedasConstraintKinds]`\
`*[wiki:Commentary/Compiler/Backends/LLVMLLVMbackend]`\
`*[wiki:Commentary/Compiler/GenericDerivingSupportforgenericprogramming]`\
`*[wiki:TemplateHaskellNotesaboutTemplateHaskell]`\
`*[wiki:RewriteRulesRewriteRules]:NotesabouttheimplementationofRULEsinGHC`\
`*[wiki:MonadComprehensionsMonadComprehensions]:Translationrulesandsomeimplementationdetails`\
`*[wiki:HaddockCommentsHaddock]:SomenotesabouthowtheHaddockcommentsupportisimplemented.`\
`*[wiki:IntermediateTypesIntermediateTypes]:NotesaboutthetypesystemofGHC'snewintermediatelanguage(intheHEADsinceICFP'06)`\
`*[wiki:TypeFunctionsTypefamilies/typefunctions]:Notesconcerningtheimplementationoftypefamilies,associatedtypes,andequalityconstraintsaswellastheextensionofthetypecheckerwithacontraintsolverforequalityconstraints.`\
`` *[wiki:Commentary/Compiler/SeqMagicMagictodowith`seq`andfriends] ``\
`*[wiki:NewPluginsCompilerplug-ins]`\
`*[wiki:MemcpyOptimizationsmemcpy/memmove/memsetoptimizations]`\
`*[wiki:BackEndNotesBackendIdeas]:Someideasandnotesaboutthebackend.`\
`*[wiki:Commentary/Compiler/NewCodeGenNotesaboutthenewcodegenerator]`\
`*[wiki:Commentary/Compiler/HooplPerformanceArecordofimprovementsmadetotheperformanceoftheHoopllibraryfordataflowoptimisation]`\
`*[wiki:DataParallelDPH]:NotesabouttheimplementationofDataParallelHaskell`\
`*[wiki:SafeHaskellSafeHaskell]:ThedesignoftheGHCSafeHaskellextension`\
`*[wiki:SQLLikeComprehensionsSQL-LikeComprehensions]:NotesonSPJs"ComprehensiveComprehensions"(!TransformComprehensions)`\
`` *[wiki:DeferErrorsToRuntimeDeferringcompilationtypeerrorstoruntime(`-fdefer-type-errors`)] ``\
`*[wiki:Commentary/Compiler/DemandDemandanalyser]Notesonthemeanings,worker-wrappersplittingofdemandsignaturesandrelevantcomponentsofthecompiler`\
`*[wiki:NewAxiomsClosedtypefamilies]`\
`` *[wiki:OneShot]Themagic`oneShot`function. ``\
`*[wiki:Commentary/Compiler/DeriveFunctorDerivingFunctor,Foldable,andTraversable]`

`*Notesonproposedorinprogress(butoutoftree)GHCcompilerfeatures:`\
`*[wiki:LanguageStrictMakingHaskellstrict]`\
`*[wiki:PatternMatchCheckImprovingpattern-matchoverlapandexhaustivenesschecks]`\
`*[wiki:GhcAstAnnotationsSource-locationsonHsSyn]`\
`*[wiki:CabalDependencyHowGHCinter-operateswithCabal]and[wiki:Backpack]`\
`*[wiki:StaticValues]andticket#7015`\
`*[wiki:PartialTypeSignaturesPartialtypesignatures]anditsticket#9478`\
`*[wiki:LateLamLiftLatelambda-lifting],anditsticket#9476`\
`*[wiki:RolesRolesinHaskell]`\
`*[wiki:DependentHaskellDependenttypesinHaskell]`\
`*[wiki:NestedCPRNestedCPRanalysis]`\
`*[wiki:TemplateHaskell/AnnotationsGivingTemplateHaskellfullaccesstoannotations]`\
`*[wiki:FunDepsCheckingconsistencyoffunctionaldependencies]`\
`*[wiki:Commentary/GSoCMultipleInstancesAllowingmultipleinstancesofthesamepackagetobeinstalled],eachinstancehavingdifferentdependencies`\
`*[wiki:Commentary/ContractsContractsinHaskell]`\
`*[wiki:HolesAgda-styleholesinterms]whichsupportswritingpartialprograms.`\
`*[wiki:RecordsRecords]`\
`*`[`Cloud`
`Haskell`](http://haskell.org/haskellwiki/GHC/CouldAndHPCHaskell)\
`*[wiki:PackageLanguageAmodularpackagelanguageforHaskell]ScottKilpatrickandDerekDreyeraredesigninganew`

Compiler and runtime system ways in GHC
=======================================

GHC can compile programs in different *ways*. For instance, a program
might be compiled with profiling enabled (\`-prof\`), or for
multithreaded execution (\`-threaded\`), or maybe making some debugging
tools available (\`-debug\`, see Debugging/RuntimeSystem for a
description).

There are two types of GHC ways, RTS-only ways and full ways.

-   **Runtime system (RTS) ways** affect the way that the runtime system
    is built. As an example, \`-threaded\` is a runtime system way. When
    you compile a program with \`-threaded\`, it will be linked to
    a (precompiled) version of the RTS with multithreading enabled.

Obviously, the compiler's RTS must have been built for this way (the
threaded RTS is activated by default BTW). In customised builds, an RTS
way can be added in the build configuration \`mk/build.mk\` (see
[GhcFile(mk/build.mk.sample)](GhcFile(mk/build.mk.sample) "wikilink")),
by adding its *short name* to the variable \`GhcRTSWays\`.

-   **Full ways**

Full compiler ways are ways which affect both the generated code and the
runtime system that runs it.

The profiling way \`-prof\` is such a way. The machine code of a program
compiled for profiling differs from a normal version's code by all code
that gathers the profiling information, and the runtime system has
additional functionality to access and report this information.
Therefore, all libraries used in a profiling-enabled program need to
also have profiling enabled, i.e. a separate library version for
profiling needs to be installed to compile the program with \`prof\`.
(If the library was installed without this profiling version, the
program cannot be linked).

In customised builds, a full way is added in the build configuration
\`mk/build.mk\` by adding its tag to the variable \`GhcLibWays\`.

Available ways in a standard GHC
--------------------------------

Ways are identified internally by a way name, and enabled by specific
compilation flags. In addition, there are short names (tags) for the
available ways, mainly used by the build system.

Here is a table of available ways in a standard GHC, as of May 2015.

||=Way flag =||= Way name =||= Tag =||= Type =||= Description =|| ||= -
=|| - || \`v\` || Full || (vanilla way) default || ||=\`-threaded\` =||
WayThreaded || \`thr\` || RTS || multithreaded runtime system ||
||=\`-debug\` =|| WayDebug || \`debug\` || RTS || debugging, enables
trace messages and extra checks || ||=\`-prof\` =|| WayProf || \`p\` ||
Full || profiling, enables cost centre stacks and profiling reports ||
||=\`-eventlog\` =|| WayEventLog || \`l\` || RTS || Event logging (for
ghc-events, threadscope, and EdenTV) || ||=\`-dyn\` =|| WayDyn ||
\`dyn\` || Full || Dynamic linking ||

The standard (*vanilla*) way of GHC has a name (*vanilla*), but it could
(probably?) even be switched off in a custom build if desired.
Obviously, the libraries would still need to be built in the vanilla way
for all RTS-only ways, so one would need \`GhcLibWays=v\` when building
any other RTS-only way.

The code (see below) contains another way, for Glasgow parallel Haskell,
which is currently unmaintained (\`WayPar\`).

### Ways for parallel execution on clusters and multicores

The parallel Haskell runtime system for Eden (available from
<http://github.com/jberthold/ghc>) defines several RTS-only ways for
Eden. All these ways execute the RTS in multiple instances with
distributed heaps, they differ in the communication substrate (and
consequently in the platform).

||=Way flag =||= Way name =||= Tag =||= Type =||= communication (OS) =||
||=\`-parpvm\` =|| WayParPvm ||\`pp\`|| RTS || PVM (Linux) ||
||=\`-parmpi\` =|| WayParMPI ||\`pm\`|| RTS || MPI (Linux) ||
||=\`-parcp\` =|| WayParCp ||\`pc\`|| RTS || OS-native shared memory
(Windows/Linux) || ||=\`-parms\` =|| WayParMSlot ||\`ms\`|| RTS ||
Windows mail slots (Windows) ||

Combining ways
--------------

The alert reader might have noticed that combinations like "threaded
with dynamic linking" or "profiled with eventlog" are not covered in the
table. Some ways can be used together (most prominently, debugging can
be used together with any other way), others are mutually excluding each
other (like profiling with eventlog).

The allowed combinations are defined inside the compiler, in
[GhcFile(compiler/main/DynFlags.hs)](GhcFile(compiler/main/DynFlags.hs) "wikilink").
Which brings us to discussing some of the internals.

Internals
=========

Ways are defined in
[GhcFile(compiler/main/DynFlags.hs)](GhcFile(compiler/main/DynFlags.hs) "wikilink")
as a Haskell data structure \`Way\`.

Function \`dynamic\_flags\` defines the actual flag strings for the ghc
invocation (like \`-prof\`, \`-threaded\`), which activate the
respective \`Way\`.

The short name tags for ways are defined in \`wayTag\`. The tags are
used in the suffixes of \*.o and \*.a files for RTS and libraries, for
instance \`\*.p\_o\` for profiling, \`\*.l\_o\` for eventlog.

A number of other functions in there customise behaviour depending on
the ways. Note \`wayOptc\` which sets some options for the C compiler,
like \`-DTRACING\` for the \`-eventlog\` way.

However, this is not the full truth. For instance, there is no
\`-DDEBUG\` for the debug way here, but the RTS is full of \`\#ifdef
DEBUG\`.

In [GhcFile(mk/ways.mk)](GhcFile(mk/ways.mk) "wikilink"), we find all
the short names and all combinations enumerated, and some more options
are defined here (\`WAY\_\*\_HC\_OPTS\`). These definitions are for the
driver script, and pass on the right (long-name) options to the Haskell
compiler to activate what is inside DynFlags (like -prof for
WAY\_p\_HC\_OPTS). Here we find \`\`\`WAY\_debug\_HC\_OPTS= -static
-optc-DDEBUG -ticky -DTICKY\_TICKY\`\`\` so we can learn that ticky
profiling is activated by compiling with \`debug\`.

(TODO be more precise on where the options from ways.mk are used.)

GHC Commentary: The Compiler
============================

The compiler itself is written entirely in Haskell, and lives in the
many sub-directories of the
[GhcFile(compiler)](GhcFile(compiler) "wikilink") directory.

`*[wiki:ModuleDependenciesCompilerModuleDependencies](dealswiththearcanemutualrecursionsamongGHC'smanydatatypes)`\
`*[wiki:Commentary/CodingStyleCodingguidelines]`

`*[wiki:Commentary/Compiler/CommandLineArgsCommandlinearguments]`\
`*[wiki:Commentary/PipelineThecompilationpipeline]`

`*`**`Compiling` `one` `module:` `!HscMain`**\
`*[wiki:Commentary/Compiler/HscMainOverview]givesthebigpicture.`\
`*Somedetailsofthe[wiki:Commentary/Compiler/Parserparser]`\
`*Somedetailsofthe[wiki:Commentary/Compiler/Renamerrenamer]`\
`*Somedetailsofthe[wiki:Commentary/Compiler/TypeCheckertypechecker]`\
`*Somedetailsofthe[wiki:Commentary/Compiler/Core2CorePipelinesimplifier]`\
`*Somedetailsofthe[wiki:Commentary/Compiler/CodeGencodegenerator]convertsSTGtoCmm`\
`*[wiki:Commentary/Compiler/BackendsBackends]convertCmmtonativecode:`\
`*[wiki:Commentary/Compiler/Backends/PprCCcodegenerator]`\
`*[wiki:Commentary/Compiler/Backends/NCGNativecodegenerator]`\
`*[wiki:Commentary/Compiler/Backends/LLVMLLVMbackend]`\
`*[wiki:Commentary/Compiler/Backends/GHCiGHCibackend]`\
`*Aguidetothe[wiki:Commentary/Compiler/GeneratedCodegeneratedassemblycode]`

`*[wiki:Commentary/Compiler/KeyDataTypesKeydatatypes]`\
`*[wiki:Commentary/Compiler/HsSynTypeThesourcelanguage:HsSyn]`\
`*[wiki:Commentary/Compiler/RdrNameTypeRdrNames,Modules,andOccNames]`\
`*[wiki:Commentary/Compiler/ModuleTypesModIface,ModDetails,ModGuts]`\
`*[wiki:Commentary/Compiler/NameTypeNames]`\
`*[wiki:Commentary/Compiler/EntityTypesEntities]:variables,typeconstructors,dataconstructors,andclasses.`\
`*Types:`\
`*[wiki:Commentary/Compiler/TypeTypeTypes]`\
`*[wiki:Commentary/Compiler/KindsKinds]`\
`*[wiki:Commentary/Compiler/FCEqualitytypesandcoercions]`\
`*[wiki:Commentary/Compiler/CoreSynTypeThecorelanguage]`\
`*[wiki:Commentary/Compiler/StgSynTypeTheSTGlanguage]`\
`*[wiki:Commentary/Compiler/CmmTypeTheCmmlanguage]`\
`*[wiki:Commentary/Compiler/BackEndTypesBackendtypes]`\
\
`*[wiki:Commentary/Compiler/DriverCompilingmorethanonemoduleatonce]`\
`*[wiki:Commentary/Compiler/DataTypesHowdatatypedeclarationsarecompiled]`\
`*[wiki:Commentary/Compiler/APITheGHCAPI]`\
`*[wiki:Commentary/Compiler/SymbolNamesSymbolnamesandtheZ-encoding]`\
`*[wiki:TemplateHaskell/ConversionsTemplateHaskell]`\
`*[wiki:Commentary/Compiler/WiredInWired-inandknown-keythings]`\
`*[wiki:Commentary/Compiler/PackagesPackages]`\
`*[wiki:Commentary/Compiler/RecompilationAvoidanceRecompilationAvoidance]`

Case studies:

`*[wiki:Commentary/Compiler/CaseStudies/BoolImplementationofwired-inBooldatatype]`

Overall Structure
-----------------

Here is a block diagram of its top-level structure:

[Image(ghc-top.png)](Image(ghc-top.png) "wikilink")

The part called \[wiki:Commentary/Compiler/HscMain HscMain\] deals with
compiling a single module. On top of this is built the **compilation
manager** (in blue) that manages the compilation of multiple modules. It
exports an interface called the **GHC API**. On top of this API are four
small front ends:

`*GHCi,theinteractiveenvironment,isimplementedin`[`GhcFile(ghc/InteractiveUI.hs)`](GhcFile(ghc/InteractiveUI.hs) "wikilink")`and`[`GhcFile(compiler/main/InteractiveEval.hs)`](GhcFile(compiler/main/InteractiveEval.hs) "wikilink")`.ItsitssquarelyontopoftheGHCAPI.`\
\
`*``isalmostatrivialclientoftheGHCAPI,andisimplementedin`[`GhcFile(compiler/main/GhcMake.hs)`](GhcFile(compiler/main/GhcMake.hs) "wikilink")`.`

`*``,theMakefiledependencygenerator,isalsoaclientoftheGHCAPIandisimplementedin`[`GhcFile(compiler/main/DriverMkDepend.hs)`](GhcFile(compiler/main/DriverMkDepend.hs) "wikilink")`.`

`*The"one-shot"mode,whereGHCcompileseachfileonthecommandlineseparately(eg.``).ThismodebypassestheGHCAPI,andisimplemented`\
`directlyontopof[wiki:Commentary/Compiler/HscMainHscMain],sinceitcompilesonlyonefileatatime.Infact,thisisallthat`\
`` GHCconsistedofpriortoversion5.00whenGHCiand`--make`wereintroduced. ``

GHC is packaged as a single binary in which all of these front-ends are
present, selected by the command-line flags indicated above. There is a
single command-line interface implemented in
[GhcFile(ghc/Main.hs)](GhcFile(ghc/Main.hs) "wikilink").

In addition, GHC is compiled, without its front ends, as a *library*
which can be imported by any Haskell program; see
\[wiki:Commentary/Compiler/API the GHC API\]. Package keys, installed
package IDs, ABI hashes, package names and versions, Nix-style hashes,
... there's so many different identifiers, what do they all mean? I
think the biggest source of confusion (for myself included) is keeping
straight not only what these terms mean, but also what people want them
to mean in the future, and what we //actually// care about. So I want to
help clarify this a bit, by clearly separating the //problem you are
trying to solve// from //how you are solving the problem//.

The content here overlaps with wiki:Commentary/Packages but is looking
at the latest iteration of the multi-instances and Backpack work.

See also \`Note \[The identifier lexicon\]\` in
\`compiler/basicTypes/Module.hs\`.

Some relevant tickets: \#10622

What problems do we need to solve?
----------------------------------

When we come up with identification schemes for packages, we are trying
to solve a few problems:

`[SYMBOL]::`\
`Whatsymbolnamesshouldweputinthebinary?(e.g.,the"foozm0zi1"in"foozm0zi1_A_DZCF_closure")`\
`-Itmustbeuniqueenoughthatforalllibrarieswewould`\
`liketobeabletolinktogether,thereshouldnotbe`\
`conflicts.`\
`-HOWEVER,itmustbestableenoughthatifwemakeaminor`\
`sourcecodechange,wedon'thavetogratuitouslyrecompile`\
`everydependency.`

`[ABI]::`\
`WhencanIswapoutonecompiledpackagewithanotherWITHOUTrecompiling,i.e.whatistheABIofthepackage?EqualABIsimpliesequalsymbols,thoughnotviceversa.ABIisusuallycomputedaftercompilationiscomplete.`\
`-ABIcanserveascorrectnesscondition:ifwelinkagainstaspecificABI,wecanbesurethatanythingwithanequivalentABIwon'tcauseourpackagetosegfault.`\
`-ABIcanalsoserveasanindirection:welinkedagainstanABI,anythingthatiscompatiblecanbehotswappedinwithoutcompilation.Inpractice,thiscapabilityisrarelyusedbyusersbecauseit'squitehardtocompileapackagemultipletimeswiththesameABI,because(1)compilationisnondeterministic,and(2)evenifnotypeschange,achangeinimplementationcancauseadifferentexportedunfolding,whichisABIrelevant.`

`[SOURCE]::`\
`Whatistheunitofdistribution?Inotherwords,whenamaintaineruploadsansdisttoHackage,howdoyouidentifythatsourcetarball?`\
`-OnHackage,apackagenameplusversionuniquelyidentifiesan`\
`sdist.Thisisenforcedbycommunitystandards;inalocal`\
`developmentenvironment,thismaynotholdsincedevswilledit`\
`codewithoutupdatingtheversionnumber.Callthis[WEAKSOURCE].`\
`-Alternately,acryptographichashofthesourcecodeuniquely`\
`identifiesthestreamofbytes.Thisisenforcedbymath.Callthis[STRONGSOURCE].`

`[LIBRARY]::`\
`` Whenyoubuildalibrary,yougetan`libfoo.so`file.WhatidentifiesanOSlevellibrary? ``

`[NIX]::`\
`WhatisthefullsetofsourcewhichIcanusetoreproduceablybuildabuildproduct?`\
`-Intoday'sCabal,youcouldapproximatethisbytaking[WEAKSOURCE]ofapackage,aswellasallofitstransitivedependencies.Callthis[WEAKNIX].`\
`-TheNixapproachistoensuredeterministicbuildsbytakingthehashofthesource[STRONGSOURCE]andalsorecursivelyincludingthe[NIX]ofeachdirectdependency.Callthis[STRONGNIX].`\
`-Notethat[ABI]doesNOTimply[NIX];apackagemightbebinarycompatiblebutdosomethingdifferent,andinaNixmodeltheyshouldberecordeddifferently.`

`[TYPES]::`\
`Whenaretwotypesthesame?Iftherearefromdifferingpackages,theyareobviouslydifferent;iftheyarefromthesamepackage,theymightstillbedifferentifthedependenciesweredifferentineachcase.`\
`-Typesshowupinerrormessage,sothisisaUSERVISIBLE`\
`notion.Manypeoplehave(cogently)arguedthatthisshould`\
`beASSIMPLEaspossible,becausethere'snothingworse`\
`thanbeingtoldthatData.ByteString.ByteStringisnot`\
`equaltoData.ByteString.ByteString(becausetheywerefrom`\
`differentpackages.)`

Current mechanisms
------------------

Today, we have a lot of different MECHANISMS for identifying these:

`PackageName::`\
`Somethinglike"lens"`

`PackageVersion::`\
`Somethinglike"0.1.2"`

`(Source)PackageID::`\
`Packagenameplusversion.WithHackagetoday,thisidentifiesaunitofdistribution:givenapackageIDyoucandownloadasourcetarball[SOURCE]ofapackage(butnotbuildit).Pre-GHC7.10,thepackageIDwasusedforlibraryidentification,symbolsandtype-checking([LIBRARY],[SYMBOL]and[TYPES]),butthisisnolongerthecase.`

`InstalledPackageID::`\
`Packagename,packageversion,andtheoutputofghc--abi-hash.Thisiscurrentlyusedtouniquelyidentifyabuiltpackage,althoughtechnicallyitonlyidentifies[ABI].`

`PackageKey(newin7.10)::`\
`Hashofpackagename,packageversion,thepackagekeysofall`\
`textualdependenciesthepackageincluded,andinBackpack`\
`amappingfromholenametomodulebypackagekey.`\
`InGHC7.10thisisusedforlibraryidentification,symbolsandtype-checking([LIBRARY],[SYMBOL]and[TYPES]).Becauseitincludespackagekeysoftextualdependencies,italsodistinguishesbetweendifferentdependencyresolutions,ala[WEAKNIX].`

New concepts for Backpack
-------------------------

First, we have to take the concept of an InstalledPackageId and make it
more precise, having it identity components rather than packages.

`ComponentID::`\
`` Thepackagename,thepackageversion,thenameofthecomponent(blankinthecaseofthedefaultlibrarycomponent),andthehashofsourcecodesdisttarball,selectedCabalflags(notthecommandlineflags),GHCflags,hashesofdirectdependenciesofthecomponent(the`build-depends`ofthelibraryintheCabalfile). ``

Then in Backpack we have these concepts:

`Indefinite/definiteunit::`\
`Anindefiniteunitisasingleunitwhichhasn'tbeeninstantiated;adefiniteunitisonethathasaninstantiationofitsholes.Unitswithoutholesarebothdefiniteandindefinite(theycanbeusedforbothcontexts).`

`Indefiniteunitrecord(in"logical"indefiniteunitdatabase)::`\
`Anindefiniteunitrecordisthemostgeneralresultoftype-checkingaunitwithoutanyofitsholesinstantiated.Itconsistsofthetypesofthemodulesintheunit(ModIfaces)aswellasthesourcecodeoftheunit(sothatitcanberecompiledintoadefiniteunit).Indefiniteunitrecordscanbeinstalledinthe"indefiniteunitdatabase."`

`Definiteunitrecord(previouslyinstalledpackagerecord,inthedefiniteunitdatabase,previouslytheinstalledpackagedatabase)::`\
`Adefiniteunitrecordisafully-instantiatedunitwithitsassociatedlibrary.Itconsistsofthetypesandobjectsofthecompiledunit;theyalsocontainmetadatafortheirassociatedpackage.Definiteunitrecordscanbeinstalledinthe"definiteunitdatabase"(previouslyknownasthe"installedpackagedatabase.")`

To handle these, we need some new identifiers:

`UnitId(previouslynamedPackageKey)::`\
`ForBackpackunits,theunitIDisthecomponentIDplusamappingfromholestomodules(unitkeyplusmodulename).Fornon-Backpackunits,theunitIDisequivalenttothecomponentsourcehash(theholemappingisempty).Theseservetheroleof[SYMBOL,LIBRARY,TYPES].(Partiallydefiniteunitkeyscanoccuron-the-flyduringtypechecking.)Whenalloftherequirementsarefilled(sothereisnooccurrenceofHOLE),theunitkeyservesastheprimarykeyfortheinstalledunitdatabase.(Wemightcallthisan"installedunitID"inthiscontext)TheunitID"HOLE"isadistinguishedunitID,whichisforthe"holepackage",representingmoduleswhicharenotyetimplemented(thereisnotactuallyaunitnamedhole,it'sjustanotationalconvention).`

`Module::`\
`AunitIDplusamodulename.`

Features
--------

There are a number of enhancements proposed for how Cabal handles
packages, which have often been conflated together. I want to clearly
separate them out here:

`Non-destructiveinstalls::`\
`IfIhavepackagefoo-0.2compiledagainstbar-0.1,andadifferentbuildcompiledagainstbar-0.2,Ishouldbeabletoputtheminthesameinstalledpackagedatabase.THISISHIGHPRIORITY.`

`Views::`\
`IfIhavepackagefoocompiledagainstbar-0.1,andbazcompiledagainstbar-0.2,thesetwopackagesaren'tusabletogether(moduloprivatedependencies,seebelow).ViewsareaUIparadigmmakingiteasierforuserstoworkinauniversewherefooisavailable,orauniversewherebazisavailable,butnotbothsimultaneously.Cabalsandboxesareviewsbutwithoutasharedinstalledpackagedatabase.Thisislowerpriority,becauseifyouusecabal-installtogetacoherentdependencyset,you'llneverseebothfooandbazatthesametime;theprimarybenefitofthisistoassistwithdirectuseofGHC/GHCi,however,itisgenerallybelievedthatnon-destructiveinstallswillmakeitdifficulttouseGHC/GHCibyitself.`

`Privatedependencies::`\
`IfIhaveapackagefoo-0.2whichdependsonalibrarybar-0.1,butnotinanyexternallyvisibleway,itshouldbeallowedforaclienttoseparatelyusebar-0.2.ThisisLOWpriority;amusingly,in7.10,thisisalreadysupportedbyGHC,butnotbyCabal.`

`Hotswappablelibraries::`\
`IfIinstallalibraryandit'sassignedABIhash123abc,andthenIinstallanumberoflibrariesthatdependonit,hotswappablelibrarymeansthatIcanreplacethatinstalledlibrarywithanotherversionwiththesameABIhash,andeverythingwillkeepworking.ThisfeatureisaccidentallysupportedbyGHCtoday,butnooneusesit(becauseABIsarenotstableenough);wearewillingtobreakthismodeofusetosupportotherfeatures.`

Constraints
-----------

For an implementer, it is best if each problem is solved separately.
However, Simon has argued strongly it is best if we REDUCE the amount of
package naming concepts. You can see this in pre-7.10 GHC, where the
package ID (package name + version) was used fulfill many functions:
linker symbols, type identity as well as being a unit of distribution.

So the way I want to go about arguing for the necessity of a given
identifier is by showing that it is IMPOSSIBLE (by the intended
functions) for a single identifier to serve both roles. Here are the
main constraints:

`-[SYMBOL]and[STRONGNIX]/[STRONGSOURCE]don'tplaynicelytogether.Ifyoumodifyyoursourcecode,a[STRONGNIX/SOURCE]identifiermustchange;ifthismeans[SYMBOL]changestoo,youwillhavetorecompileeverything.However,youcanworkaroundthisproblembyusingfakeidentifiersduringdevelopmenttoavoidrecompilation,recompilingwiththecorrectNIXidentifierwhenit'sfinallytimetoinstall.`

`-[SOURCE]and[TYPES]areincompatibleundernon-destructiveinstallsandprivatedependencies.Withprivatedependencies(whichGHCsupports!),Imaylinkagainstthemultipleinstancesofthesamesourcebutcompiledagainstdifferentdependencies;weMUSTNOTconsiderthesetypestobethesame.Note:GHCusedtousepackageIDforbothofthese;socoherencewasguaranteedbyrequiringdestructiveinstalls.`

`-[NIX]and[TYPES]areincompatibleunderBackpack.InBackpack,alibraryauthormaydistributeapackagewiththeexplicitintentthatitmaybeusedinthesameclientmultipletimeswithdifferentinstantiationsofitsholes;thesetypesmustbekeptdistinct.`

RTS Configurations
==================

The RTS can be built in several different ways, corresponding to global
CPP defines. The flavour of the RTS is chosen by GHC when compiling a
Haskell program, in response to certain command-line options: , , etc.

The CPP symbols and their corresponding command-line flags are:

`::`\
`Enablesprofiling.`[`br`](br "wikilink")\
`GHCoption:`[`br`](br "wikilink")\
`RTSsuffix:`

`::`\
`EnablesmultithreadingintheRTS,boundthreads,andSMPexecution.`[`br`](br "wikilink")\
`GHCoption:`[`br`](br "wikilink")\
`RTSsuffix:`

`::`\
`Enablesextradebuggingcode,assertions,traces,andthe``options.`[`br`](br "wikilink")\
`GHCoption:`[`br`](br "wikilink")\
`RTSsuffix:`

`::`\
`EnablesRTStracingandeventlogging,see`[`GhcFile(rts/Trace.c)`](GhcFile(rts/Trace.c) "wikilink")`` .Impliedby`DEBUG`. ``[`br`](br "wikilink")\
`GHCoption:`[`br`](br "wikilink")\
`RTSsuffix:`

So for example, is the version of the runtime compiled with and , and
will be linked in if you use the and options to GHC.

The ways that the RTS is built in are controlled by the Makefile
variable.

Combinations
------------

All combinations are allowed. Only some are built by default though; see
\[source:mk/config.mk.in\] to see how the \`GhcRTSWays\` variable is
set.

Other configuration options
---------------------------

`::`\
`` Disabledtheuseofhardwareregistersforthestackpointer(`Sp`),heappointer(`Hp`),etc.Thisis ``\
`` enabledwhenbuilding"unregisterised"code,whichiscontrolledbythe`GhcUnregisterised`buildoption. ``\
`TypicallythisisnecessarywhenbuildingGHConaplatformforwhichthereisnonativecodegenerator`\
`andLLVMdoesnothaveaGHCcallingconvention.`

`::`\
`EnablestheuseoftheRTS"mini-interpreter",whichsimulatestail-calls.Again,thisisenabledby`\
`` `GhcUnregisterised`inthebuildsystem. ``

`::`\
`Controlswhethertheinfotableisplaceddirectlybeforetheentrycodeforaclosureorreturncontinuation.`\
`` Thisisnormallyturnedoniftheplatformsupportsit,butisturnedoffby`GhcUnregisterised`. ``

Contracts for Haskell
=====================

Involved
--------

`*SimonPeyton-Jones`\
`*DimitriosVytiniotis`\
`*KoenClaessen`\
`*Charles-PierreAstolfi`

Overview
--------

Contracts, just as types, give a specification of the arguments and
return values of a function. For example we can give to head the
following contract:

Where Ok means that the result of head is not an error/exception as long
as the argument isn't.

Any Haskell boolean expression can be used in a contract, for example is
a contract that means that for every a which is an actual integer (not
an error), then fac a &gt;= a

We can also use a higher-order contracts: This contract means that if we
apply map to a non-empty list with a function that takes a non-negative
integer and returns an positive integer then map returns a list of
values without errors.

For a formal introduction, one can read \[1\].

The plan
--------

Verifying that a function satisfies a given contract is obviously
undecidable, but that does not mean that we can't prove anything
interesting. Our plan is to translate Haskell programs to first-order
logic (with equality) and then use Koen's automated theorem prover to
check contract satisfaction. Given that first-order logic is only
semi-decidable, the theorem prover can (and in fact does) hang when fed
with contracts that are in contradiction with the function definition.

Current status
--------------

The current status is described in \[3\] and some code and examples can
be found in \[2\]. Note that given it's just a prototype the input
syntax is slightly different from Haskell. In the end, we should get a
ghc extension for contracts.

Questions
---------

`*Doweneedcfnesspredicateanymore?ItwasimportantinthePOPLpaperbutisstillrelevant?`\
`*UNRshouldberenamedtoalessconfusingname.`\
`*Hoarelogicvsliquidtypes`\
`*Semantics&domaintheorytoprovethecorrectnessofthetranslation`\
`*Unfoldingforprovingcontractsonrecursivefunctions`

References
----------

\[1\] :
<http://research.microsoft.com/en-us/um/people/simonpj/papers/verify/index.htm>
[BR](BR "wikilink") \[2\] : <https://github.com/cpa/haskellcontracts>
and <https://github.com/cpa/haskellcontracts-examples>
[BR](BR "wikilink") \[3\] :
<https://github.com/cpa/haskellcontracts/blob/master/draft2.pdf>

The GHC Commentary: Coding Style Guidelines for RTS C code
==========================================================

Comments
--------

These coding style guidelines are mainly intended for use in and . See
\[wiki:Commentary/CodingStyle Coding Style Guidelines\] for code in .

These are just suggestions. They're not set in stone. Some of them are
probably misguided. If you disagree with them, feel free to modify this
document (and make your commit message reasonably informative) or mail
someone (eg. [The GHC mailing
list](mailto:glasgow-haskell-users@haskell.org))

References
----------

If you haven't read them already, you might like to check the following.
Where they conflict with our suggestions, they're probably right.

`*TheC99standard.Onereasonablereferenceis`[`here`](http://home.tiscalinet.ch/t_wolf/tw/c/c9x_changes.html)`.`

`*WritingSolidCode,MicrosoftPress.(Highlyrecommended.)`

`*Autoconfdocumentation.Seealso`\
``[`The` `autoconf` `macro`
`archive`](http://peti.gmd.de/autoconf-archive/)\
`and`[`Cyclic` `Software's`
`description`](http://www.cyclic.com/cyclic-pages/autoconf.html)`.`

`*`[`Indian` `Hill` `C` `Style` `and` `Coding`
`Standards`](http://www.cs.arizona.edu/~mccann/cstyle.html)

`*`[`A` `list` `of` `C` `programming` `style`
`links`](http://www.cs.umd.edu/users/cml/cstyle/)

`*`[`A` `very` `large` `list` `of` `C` `programming`
`links`](http://www.lysator.liu.se/c/c-www.html)

Portability issues
------------------

### Which C Standard?

We try to stick to C99 where possible. We use the following C99 features
relative to C89, some of which were previously GCC extensions (possibly
with different syntax):

`*Variablelengtharraysasthelastfieldofastruct.GCChas`\
`asimilarextension,butthesyntaxisslightlydifferent:inGCCyou`\
`woulddeclarethearrayas``,whereasinC99itis`\
`declaredas``.`

`*Inlineannotationsonfunctions(seelater)`

`*Labeledelementsininitialisers.Again,GCChasaslightly`\
`differentsyntaxfromC99here,andwestickwiththeGCCsyntax`\
`untilGCCimplementstheC99proposal.`

`*C++-stylecomments.ThesearepartoftheC99standard,andwe`\
`prefertousethemwheneverpossible.`

In addition we use ANSI-C-style function declarations and prototypes
exclusively. Every function should have a prototype; static function
prototypes may be placed near the top of the file in which they are
declared, and external prototypes are usually placed in a header file
with the same basename as the source file (although there are exceptions
to this rule, particularly when several source files together implement
a subsystem which is described by a single external header file).

`*WeusethefollowingGCCextensions,butsurroundthemwith``:`\
`*Functionattributes(mostlyjust``and``)`\
`*Inlineassembly.`

### Other portability conventions

`*charcanbesignedorunsigned-alwayssaywhichyoumean`

`*OurPOSIXpolicy:trytowritecodethatonlyusesPOSIX`\
`(`[`IEEE` `Std`
`1003.1`](http://www.opengroup.org/onlinepubs/009695399/toc.htm)`)`\
`interfacesandAPIs.Weusedtodefine``by`\
`default,butfoundthatthiscausedmoreproblemsthanitsolved,so`\
`nowwerequireanycodethatisPOSIX-complianttoexplicitlysayso`\
`byhaving``atthetop.Trytodothis`\
`wheneverpossible.`

`*Somearchitectureshavememoryalignmentconstraints.Othersdon't`\
`haveanyconstraintsbutgofasterifyoualignthings.These`\
`macros(from``)tellyouwhichalignmenttouse`

`*Use``,``and``when`\
`reading/writingintsandptrstothestackorheap.Notethat,by`\
`definition,``,``and``arethe`\
`samesizeandhavethesamealignmentconstraintsevenif`\
```onthatplatform.`

`*Use``,``,etcwhenyouneedacertain`\
`minimumnumberofbitsinatype.Use``and``when`\
`there'snoparticularconstraint.ANSIConlyguaranteesthatints`\
`areatleast16bitsbutwithinGHCweassumetheyare32bits.`

`*Use``and``forfloatingpointvalues`\
`whichwillgoon/havecomefromthestackorheap.Notethat`\
```mayoccupymorethanone``,butitwill`\
`alwaysbeawholenumbermultiple.`

`*Use``,``toread`\
`and``valuesfromthestack/heap,and`\
```/``toassign`\
`StgFloat/StgDoublevaluestoheap/stacklocations.Thesemacros`\
`takecareofalignmentrestrictions.`

`*Heap/Stacklocationsarealways``aligned;the`\
`alignmentrequirementsofan``maybemorethanthat`\
`of``,butwedon'tpadmisaligned`\
`becausedoingsowouldbetoomuchhassle(see``&co`\
`above).`

`*Avoidconditionalcodelikethis:`

`Instead,addanappropriatetesttotheconfigure.acscriptanduse`\
`theresultofthattestinstead.`

`TheproblemisthatthingschangefromoneversionofanOSto`\
`another-thingsgetadded,thingsgetdeleted,thingsgetbroken,`\
`somethingsareoptionalextras.Using"featuretests"insteadof`\
`"systemtests"makesthingsalotlessbrittle.Thingsalsotendto`\
`getdocumentedbetter.`

Debugging/robustness tricks
---------------------------

Anyone who has tried to debug a garbage collector or code generator will
tell you: "If a program is going to crash, it should crash as soon, as
noisily and as often as possible." There's nothing worse than trying to
find a bug which only shows up when running GHC on itself and doesn't
manifest itself until 10 seconds after the actual cause of the problem.

We put all our debugging code inside . The general policy is we don't
ship code with debugging checks and assertions in it, but we do run with
those checks in place when developing and testing. Anything inside
should not slow down the code by more than a factor of 2.

We also have more expensive "sanity checking" code for hardcore
debugging - this can slow down the code by a large factor, but is only
enabled on demand by a command-line flag. General sanity checking in the
RTS is currently enabled with the RTS flag.

There are a number of RTS flags which control debugging output and
sanity checking in various parts of the system when is defined. For
example, to get the scheduler to be verbose about what it is doing, you
would say . See and for the full set of debugging flags. To check one of
these flags in the code, write: would check the flag before generating
the output (and the code is removed altogether if is not defined).

All debugging output should go to .

Particular guidelines for writing robust code:

`*Useassertions.Uselotsofassertions.Ifyouwriteacomment`\
`thatsays"takesa+venumber"addanassertion.Ifyou'recasting`\
`aninttoanat,addanassertion.Ifyou'recastinganinttoa`\
`char,addanassertion.Weusethe``macroforwriting`\
`assertions;itgoesawaywhen``isnotdefined.`

`*Writespecialdebuggingcodetochecktheintegrityofyourdata`\
`structures.(Mostoftheruntimecheckingcodeisin`\
```)Addextraassertionswhichcallthiscodeat`\
`thestartandendofanycodethatoperatesonyourdata`\
`structures.`

`*Whenyoufindahard-to-spotbug,trytothinkofsomeassertions,`\
`sanitychecksorwhateverthatwouldhavemadethebugeasierto`\
`find.`

`*Whendefininganenumeration,it'sagoodideanottouse0for`\
`normalvalues.Instead,make0raiseaninternalerror.Theidea`\
`hereistomakeiteasiertodetectpointer-relatederrorsonthe`\
`assumptionthatrandompointersaremorelikelytopointtoa0`\
`thantoanythingelse.`

`*Use``or``wheneveryouwriteapieceof`\
`incomplete/brokencode.`

`*Whentesting,trytomakeinfrequentthingshappenoften.For`\
`example,makeacontextswitch/gc/etchappeneverytimeacontext`\
`switch/gc/etccanhappen.Thesystemwillrunlikeapigbutit'll`\
`catchalotofbugs.`

Syntactic details
-----------------

`*Pleasekeepto80columns:thelinehastobedrawnsomewhere,and`\
`bykeepingitto80columnswecanensurethatcodelooksOKon`\
`everyone'sscreen.Longlinesarehardtoread,andasignthat`\
`thecodeneedstoberestructuredanyway.`

`*Anindentationwidthof4ispreferred(don'tuseactualtabcharacters,usespaces).`

`*`**`Important:`**`Put"redundant"bracesorparensinyourcode.`\
`Omittingbracesandparensleadstoveryhardtospotbugs-`\
`especiallyifyouusemacros(andyoumighthavenoticedthatGHC`\
`doesthisalot!)`

`Inparticular,putbracesroundthebodyofforloops,whileloops,`\
`ifstatements,etc.evenifthey"aren'tneeded"becauseit's`\
`reallyhardtofindtheresultingbugifyoumessup.Indentthem`\
`anywayyoulikebutputtheminthere!`

`*Whendefiningamacro,alwaysputparensroundargs-justincase.`\
`Forexample,write:`

`insteadof`

`*Don'tdeclareandinitializevariablesatthesametime.`\
`Separatingthedeclarationandinitializationtakesmorelines,but`\
`makethecodeclearer.`

`*Don'tdefinemacrosthatexpandtoalistofstatements.Youcould`\
`justusebracesasin:`

`(butit'susuallybettertouseaninlinefunctioninstead-seeabove).`

`*Don'tevenwritemacrosthatexpandto0statements-theycanmess`\
`youupaswell.Usethe``macroinstead.`

`*Thiscode`

`lookslikeitdeclarestwopointersbut,infact,onlypisapointer.`\
`It'ssafertowritethis:`

`Youcouldalsowritethis:`

`butitispreferrabletosplitthedeclarations.`

`*TrytouseANSIC'senumfeaturewhendefininglistsofconstants`\
`ofthesametype.Amongotherbenefits,you'llnoticethatgdb`\
`usesthenameinsteadofits(usuallyinscrutable)numberwhen`\
`printingvalueswithenumtypesandgdbwillletyouusethename`\
`inexpressionsyoutype.`

`Examples:`

`insteadof`

`and`

`insteadof`

`*Whencommentingoutlargechunksofcode,use```\
`ratherthan``becauseCdoesn'thave`\
`nestedcomments.`

`*Whendeclaringatypedefforastruct,givethestructanameas`\
`well,sothatotherheaderscanforward-referencethestructname`\
`anditbecomespossibletohaveopaquepointerstothestruct.Our`\
`conventionistonamethestructthesameasthetypedef,butadda`\
`leadingunderscore.Forexample:`

`*Donotuse``insteadofexplicitcomparisonagainst`\
`or``;thelatterismuchclearer.`

`*PleasewritecommentsinEnglish.EspeciallyavoidKlingon.`

Inline functions
----------------

Use inline functions instead of macros if possible - they're a lot less
tricky to get right and don't suffer from the usual problems of side
effects, evaluation order, multiple evaluation, etc.

`*Inlinefunctionsgetthenamingissueright.E.g.they`\
`canhavelocalvariableswhich(inanexpressioncontext)`\
`macroscan't.`

`*Inlinefunctionshavecall-by-valuesemanticswhereasmacrosare`\
`call-by-name.Youcanbebittenbyduplicatedcomputationifyou`\
`aren'tcareful.`

`*Youcanuseinlinefunctionsfrominsidegdbifyoucompilewith`\
`-O0or-fkeep-inline-functions.Ifyouusemacros,you'dbetterknow`\
`whattheyexpandto.`

`However,notethatmacroscanserveasbothl-valuesandr-valuesand`\
`canbe"polymorphic"astheseexamplesshow:`

There are three macros to do inline portably. Don't use \`inline\`
directly, use these instead:

\`INLINE\_HEADER\`

`Aninlinefunctioninaheaderfile.Thisisjustlikeamacro.Weneveremit`\
`astandalonecopyofthefunction,soit`*`must`*`beinlinedeverywhere.`

\`STATIC\_INLINE\`

`AninlinefunctioninaCsourcefile.Again,itisalwaysinlined,andwenever`\
`emitastandalonecopy.`

\`EXTERN\_INLINE\`

`Afunctionwhichisoptionallyinlined.TheCcompileristoldtoinlineifpossible,`\
`butwealsogeneratedastandalonecopyofthefunctionjustincase(seesource:rts/Inlines.c).`

Source-control issues
---------------------

`*Don'tbetemptedtore-indentorre-organiselargechunksofcode-`\
`itgenerateslargediffsinwhichit'shardtoseewhetheranything`\
`elsewaschanged,andcausesextraconflictswhenmovingpatchesto`\
`anotherbranch.`\
``[`BR`](BR "wikilink")[`BR`](BR "wikilink")\
`Ifyoumustre-indentorre-organise,don'tincludeanyfunctional`\
`changesthatcommitandgiveadvancewarningthatyou'reabouttodo`\
`itincaseanyoneelseischangingthatfile.Formoredetailson`\
`sourcecontrolconventions,see[wiki:WorkingConventions/Git].`

for file in \*; do

`iconv-fascii-tutf-8"$file"-o"${file%.txt}.wiki"`

done

Copying GC
==========

GHC uses copying GC by default, while it requires more memory than
\[wiki:Commentary/Rts/Storage/GC/Compaction mark/compact\], it is
faster.

The basic copying scheme is [Cheney's
Algorithm](http://en.wikipedia.org/wiki/Cheney%27s_algorithm). Starting
from the \[wiki:Commentary/Rts/Storage/GC/Roots roots\], we visit each
live object:

`*Theobjectis`*`evacuated`*`` (copied)toitsdestinationgeneration.Thedestinationisgivenby`bd->dest`pointerinthe`bdescr`ofthe ``\
`blockinwhichitlives;typicallyanobjectispromotedtothenexthighestgeneration,butthebasicpolicyisaffectedby[wiki:Commentary/Rts/Storage/GC/Agingaging]and[wiki:Commentary/Rts/Storage/GC/EagerPromotioneagerpromotion].`

`*Theheaderwordoftheoriginalobjectisreplacedbya`*`forwarding`
`pointer`*`.Theforwardingpointerisjustthepointertothenewcopy,withtheleastsignificantbitsetto1sothatforwardingpointerscanbedistinguishedfrominfotablepointers.`

`*Wescanobjectsthathavebeenevacuated,and`*`scavenge`*`eachone.Scavenginginvolvesevacuatingeachofthepointers`\
`intheobject,replacingeachpointerwithapointertotheevacuatedcopy.`

`*Whentherearenomoreobjectstobescavenged,thealgorithmiscomplete.Thememorycontainingtheevacuatedobjectsisretained,allthememorycontainingtheoldobjectsandforwardingpointersisdiscarded.`

Evacuation is implemented in the file
[GhcFile(rts/sm/Evac.c)](GhcFile(rts/sm/Evac.c) "wikilink").[br](br "wikilink")
Scavenging is implemented in the file
[GhcFile(rts/sm/Scav.c)](GhcFile(rts/sm/Scav.c) "wikilink").[br](br "wikilink")

The principle APIs are

`` `voidevacuate(StgClosure**p)`:: ``\
`` whichevacuatestheobjectpointedtobythepointerat`p`,andupdates`p`topointtothenewlocation. ``

`` `voidscavenge_block(bdescr*bd)`:: ``\
`` whichscavengesalltheobjectsintheblock`bd`(objectsbetween`bd->u.scan`and`bd->free`areassumedto ``\
`beunscavengedsofar).`

= Core-to-Core optimization pipeline

After the source program has been \[wiki:Commentary/Compiler/TypeChecker
typechecked\] it is desugared into GHC's intermediate language
\[wiki:Commentary/Compiler/CoreSynType Core\]. The Core representation
of a program is then optimized by a series of correctness preserving
Core-to-Core passes. This page describes the overall structure of the
Core-to-Core optimization pipeline. Detailed descriptions of
optimizations are available
\[wiki:Commentary/Compiler/Core2CorePipeline\#Furtherreading in the
published papers\]. An overview of the whole compiler pipeline is
available \[wiki:Commentary/Compiler/HscMain here\].

== Optimizations during desugaring

At the end of desugaring we run the \`simpleOptPgm\` function that
performs some simple optimizations: eliminating dead bindings, and
inlining non-recursive bindings that are used only once or where the RHS
is trivial. The rest of Core optimisations is performed by the
Core-to-Core pipeline.

== The pipeline

The structure of the Core-to-Core pipeline is determined in the
\`getCoreToDo\` function in the
[GhcFile(compiler/simplCore/SimplCore.lhs)](GhcFile(compiler/simplCore/SimplCore.lhs) "wikilink")
module. Below is an ordered list of performed optimisations. These are
enabled by default with \`-O1\` and \`-O2\` unless the description says
a specific flag is required. The simplifier, which the pipeline
description below often refers to, is described in detail in
\[wiki:Commentary/Compiler/Core2CorePipeline\#Simplifier the next
section\].

`*`**`Static` `Argument`
`Transformation`**`` :triestoremoveredundantargumentstorecursivecalls,turningthemintofreevariablesinthosecalls.Onlyenabledwith`-fstatic-argument-transformation`.Ifrunthispassisprecededwitha"gentle"runofthesimplifier. ``

`*`**`Vectorisation`**`` :runthe[wiki:DataParallelDataParallelHaskell][wiki:DataParallel/Vectorisationvectoriser].Onlyenabledwith`-fvectorise`.TODO:does`-Odph`imply`fvectorise`? ``

`*`**`Simplifier,` `gentle` `run`**

`*`**`Specialisation`**`:specialisationattemptstoeliminateoverloading.Moredetailscanbefoundinthecommentsin`[`GhcFile(compiler/specialise/Specialise.lhs)`](GhcFile(compiler/specialise/Specialise.lhs) "wikilink")`.`

`*`**`Full` `laziness,` `1st`
`pass`**`:floatslet-bindingsoutsideoflambdas.Thispassincludesannotatingbindingswithlevelinformationandthenrunningthefloat-outpass.Inthisfirstpassofthefulllazinesswedon'tfloatpartialapplicationsandbindingsthatcontainfreevariables-thiswillbedonebythesecondpasslaterinthepipeline.See"FurtherReading"sectionbelowforpointerswheretofindthedescriptionofthefulllazinessalgorithm.`

`*`**`Float` `in,` `1st`
`pass`**`:theoppositeoffulllaziness,thispassfloatslet-bindingsasclosetotheirusesitesaspossible.Itwillnotundothefulllazinessbysinkingbindingsinsidealambda,unlessthelambdaisone-shot.Atthisstagewehavenotyetrunthedemandanalysis,soweonlyhavedemandinformationforthingsthatweimported.`

`*`**`Simplifier,` `main`
`run`**`:runthemainpassesofthesimplifier(phases2,1and0).Phase0isrunwithatleast3iterations.`

`*`**`Call`
`arity`**`:attemptstoeta-expandlocalfunctionsbasedonhowtheyareused.Ifrun,thispassisfollowedbya0phaseofthesimplifier.SeeNotesin`[`GhcFile(compiler/simplCore/CallArity.hs)`](GhcFile(compiler/simplCore/CallArity.hs) "wikilink")`andtherelevantpaper.`

`*`**`Demand` `analysis,` `1st`
`pass`**`(a.k.a.strictnessanalysis):runsthedemandanalyserfollowedbyworker-wrappertransformationand0phaseofthesimplifier.Thispasstriestodetermineifsomeexpressionsarecertaintobeusedandwhethertheywillbeusedonceormanytimes(cardinalityanalysis).Wecurrentlydon'thavemeansofsayingthatabindingiscertaintobeusedmanytimes.Wecanonlydeterminethatitiscertaintobeone-shot(ie.usedonlyonce)orprobabletobeoneshot.DemandanalysispassonlyannotatesCorewithstrictnessinformation.Thisinformationislaterusedbyworker/wrapperpasstoperformtransformations.CPRanalysisisalsodoneduringdemandanalysis.`

`*`**`Full` `laziness,` `2nd`
`pass`**`:anotherfull-lazinesspass.Thistimepartialapplicationsandfunctionswithfreevariablesarefloatedout.`

`*`**`Common`
`Sub-expression-elimination`**`:eliminatesexpressionsthatareidentical.`

`*`**`Float` `in,` `2nd` `pass`**

`*`**`Check` `rules,` `1st`
`pass`**`` :thispassisnotforoptimisationbutfortroubleshootingtherules.Itisonlyenabledwith`-frule-check`flagthatacceptsastringpattern.Thispasslooksforrulesbeginningwiththatstringpatternthatcouldhavefiredbutdidn'tandprintsthemtostdout. ``

`*`**`Liberate`
`case`**`` :unrollsrecursivefunctionsonceintheirownRHS,toavoidrepeatedcaseanalysisoffreevariables.It'sabitlikethecall-patternspecialisationbutforfreevariablesratherthanarguments.Followedbyaphase0simplifierrun.Onlyenabledwith`-fliberate-case`flag. ``

`*`**`Call-pattern`
`specialisation`**`` :Onlyenabledwith`-fspec-constr`flag.TODO:explainwhatitdoes. ``

`*`**`Check` `rules,` `2nd` `pass`**

`*`**`Simplifier,` `final`**`:final0phaseofthesimplifier.`

`*`**`Damand` `analysis,` `2nd`
`pass`**`` (a.k.a.latedemandanalysis):thispassconsistsofdemandanalysisfollowedbyworker-wrappertransformationandphase0ofthesimplifier.Thereasonforthispassisthatsomeopportunitiesfordiscoveringstrictnesswerenotvisibleearlier;andoptimisationslikecall-patternspecialisationcancreatefunctionswithunusedargumentswhichareeliminatedbylatedemandanalysis.Onlyrunwith`-flate-dmd-anal`.FIXME:butthecardinalitypapersayssomethingelse,namelythatthelatepassismeanttodetectsingleentrythunks.Isitstillthecaseinthecurrentimplementation? ``

`*`**`Check` `rules,` `3rd` `pass`**

The plugin mechanism allows to modify the above pipeline dynamically.

== Simplifier

Simplifier is the workhorse of the Core-to-Core optimisation pipeline.
It performs all the local transformations: (TODO: this list is most
likely not comprehensive)

`-constantfolding`\
`-applyingtherewriterules`\
`-inlining`\
`-caseofcase`\
`-caseofknownconstructor`\
`-etaexpansionandetareduction`\
`-combiningadjacentcasts`\
`-pushingacastoutofthewayofanapplicatione.g.`

Video: [GHC Core
language](http://www.youtube.com/watch?v=EQA69dvkQIk&list=PLBkRCigjPwyeCSD_DFxpd246YIF7_RDDI)
(14'04")

The  type
========

The Core language is GHC's central data types. Core is a very small,
explicitly-typed, variant of System F. The exact variant is called
\[wiki:Commentary/Compiler/FC System FC\], which embodies equality
constraints and coercions.

The type, and the functions that operate over it, gets an entire
directory
[GhcFile(compiler/coreSyn)](GhcFile(compiler/coreSyn) "wikilink"):

`*`[`GhcFile(compiler/coreSyn/CoreSyn.hs)`](GhcFile(compiler/coreSyn/CoreSyn.hs) "wikilink")`:thedatatypeitself.`

`*`[`GhcFile(compiler/coreSyn/PprCore.hs)`](GhcFile(compiler/coreSyn/PprCore.hs) "wikilink")`:pretty-printing.`\
`*`[`GhcFile(compiler/coreSyn/CoreFVs.hs)`](GhcFile(compiler/coreSyn/CoreFVs.hs) "wikilink")`:findingfreevariables.`\
`*`[`GhcFile(compiler/coreSyn/CoreSubst.hs)`](GhcFile(compiler/coreSyn/CoreSubst.hs) "wikilink")`:substitution.`\
`*`[`GhcFile(compiler/coreSyn/CoreUtils.hs)`](GhcFile(compiler/coreSyn/CoreUtils.hs) "wikilink")`:avarietyofotherusefulfunctionsoverCore.`

`*`[`GhcFile(compiler/coreSyn/CoreUnfold.hs)`](GhcFile(compiler/coreSyn/CoreUnfold.hs) "wikilink")`:dealingwith"unfoldings".`

`*`[`GhcFile(compiler/coreSyn/CoreLint.hs)`](GhcFile(compiler/coreSyn/CoreLint.hs) "wikilink")`:type-checktheCoreprogram.Thisisanincredibly-valuableconsistencycheck,enabledbytheflag``.`

`*`[`GhcFile(compiler/coreSyn/CoreTidy.hs)`](GhcFile(compiler/coreSyn/CoreTidy.hs) "wikilink")`:partofthe[wiki:Commentary/Compiler/HscMaintheCoreTidypass](therestisin`[`GhcFile(compiler/main/TidyPgm.hs)`](GhcFile(compiler/main/TidyPgm.hs) "wikilink")`).`\
`*`[`GhcFile(compiler/coreSyn/CorePrep.hs)`](GhcFile(compiler/coreSyn/CorePrep.hs) "wikilink")`:[wiki:Commentary/Compiler/HscMaintheCorePreppass]`

Here is the entire Core type
[GhcFile(compiler/coreSyn/CoreSyn.hs)](GhcFile(compiler/coreSyn/CoreSyn.hs) "wikilink"):
That's it. All of Haskell gets compiled through this tiny core.

 is parameterised over the type of its *binders*, . This facility is
used only rarely, and always temporarily; for example, the let-floater
pass attaches a binding level to every binder. By far the most important
type is , which is with binders. If you want to learn more about such
AST-parametrization, I encourage you to read a blog post about it:
<http://blog.ezyang.com/2013/05/the-ast-typing-problem> .

Binder is used (as the name suggest) to bind a variable to an
expression. The data type is parametrized by the binder type. The most
common one is the where comes from
[GhcFile(compiler/basicTypes/Var.hs)](GhcFile(compiler/basicTypes/Var.hs) "wikilink"),
which in fact is a with some extra informations attached (like types).

Here are some notes about the individual constructors of .

`*``representsvariables.The``itcontainsisessentiallyan[wiki:Commentary/Compiler/RdrNameType#TheOccNametypeOccName]plusa``;however,equality``on``sisbasedonlyontheir``'s,so`*`two`
`s` `with` `different` `types` `may` `be` `-equal`*`.`

`*``isusedforbothtermandtypeabstraction(smallandbiglambdas).`

`*``appearsonlyintype-argumentpositions(e.g.``).Toemphasisethis,thetypesynonym``isusedasdocumentationwhenweexpectthata``constructormayshowup.Anythingnotcalled``shouldnotusea``constructor.AdditionalGHCCoreusessocalledtype-lambdas,theyarelikelambdas,butinsteadoftakingarealargument,theytakeatypeinstead.YoushouldnotconfusethemwithTypeFamilies,becausetype-lambdasareworkingonavaluelevel,whiletypefamiliesarefunctionsonthetypelevel.Thesimpliesexampleforatype-lambdausageisapolymorphicone:``.ItwillberepresentedinCoreas``,where``isa*typeargument*,sowhenspecyfyingtheargumentof``wecanreferto``.ThisishowpolymorphismisrepresentedinCore.`

`*``handlesbothrecursiveandnon-recursivelet-bindings;seethethetwoconstructorsfor``.TheLetconstructorcontainsbothbindersaswellastheresultingexpression.Theresultingexpressionisthe``inexpression``.`

`*``expressionsneed[wiki:Commentary/Compiler/CoreSynType#Caseexpressionsmoreexplanation].`

`*``isusedforan[wiki:Commentary/Compiler/FCFCcastexpression].``isasynonymfor``.`

`*``isusedtorepresentallthekindsofsourceannotationwesupport:profilingSCCs,HPCticks,andGHCibreakpoints.Wasnamed``sometimeago.`

Case expressions
----------------

Case expressions are the most complicated bit of . In the term :

`*``isthescrutinee`\
`*``isthe`**`case` `binder`**`(seenotesbelow)`\
`*``isthetypeoftheentirecaseexpression(redundantonce[wiki:Commentary/Compiler/FCFC]isinHEAD--wasforGADTs)`\
`*``isalistofthecasealternatives`

A case expression can scrutinise

`*`**`a` `data` `type`**`(thealternativesare``s),or`\
`*`**`a` `primitive` `literal`
`type`**`(thealternativesare``s),or`\
`*`**`a` `value` `of` `any` `type` `at`
`all`**`(ifthereisone``alternative).`

A case expression is **always strict**, even if there is only one
alternative, and it is . (This differs from Haskell!) So will call ,
rather then returning .

The field, called the **case binder**, is an unusual feature of GHC's
case expressions. The idea is that *in any right-hand side, the case
binder is bound to the value of the scrutinee*. If the scrutinee was
always atomic nothing would be gained, but real expressiveness is added
when the scrutinee is not atomic. Here is a slightly contrived example:
(Here, "" is the case binder; at least that is the syntax used by the
Core pretty printer.) This expression evaluates ; if the result is , it
returns , otherwise it returns the reversed list appended to itself.
Since the returned value of is present in the implementation, it makes
sense to have a name for it!

The most common application is to model call-by-value, by using instead
of . For example, here is how we might compile the call if we knew that
was strict:

Case expressions have several invariants

`*The``typeisthesameasthetypeofanyoftheright-handsides(uptorefiningunification--coreRefineTysin`[`GhcFile(compiler/types/Unify.hs)`](GhcFile(compiler/types/Unify.hs) "wikilink")`--inpre-[wiki:Commentary/Compiler/FCFC]).`\
\
`*Ifthereisa``alternative,itmustappearfirst.Thismakesfindinga``alternativeeasy,whenitexists.`

`*Theremainingnon-DEFAULTalternativesmustappearinorderof`\
`*tag,for``s`\
`*lit,for``s`\
`Thismakesfindingtherelevantconstructoreasy,andmakescomparisoneasiertoo.`

`*Thelistofalternativesis`**`always`
`exhaustive`**`,meaningthatitcovers`**`all` `reachable`
`cases`**`.Note,however,thatan"exhausive"casedoesnotnecessarilymentionallconstructors:`

`Theinnercasedoesnotneeda``alternative,becausexcan'tbe``atthatprogrampoint.Furthermore,GADTtype-refinementmightmeanthatsomealternativesarenotreachable,andhencecanbediscarded.`

Shadowing
---------

One of the important things when working with Core is that variable
shadowing is allowed. In other words, it is possible to come across a
definition of a variable that has the same name (\`realUnique\`) as some
other one that is already in scope. One of the possible ways to deal
with that is to use \`Subst\` (substitution environment from
[GhcFile(compiler/coreSyn/CoreSubst.hs)](GhcFile(compiler/coreSyn/CoreSubst.hs) "wikilink")),
which maintains the list of variables in scope and makes it possible to
clone (i.e. rename) only the variables that actually capture names of
some earlier ones. For some more explanations about this approach see
[Secrets of the Glasgow Haskell Compiler inliner
(JFP'02)](http://research.microsoft.com/%7Esimonpj/Papers/inlining/index.htm)
(section 4 on name capture).

Human readable Core generation
------------------------------

If you are interested in the way Core is translated into human readable
form, you should check the sources for
[GhcFile(compiler/coreSyn/PprCore.hs)](GhcFile(compiler/coreSyn/PprCore.hs) "wikilink").
It is especially usefull if you want to see how the Core data types are
being build, especially when there is no Show instance defined for them.

CPS Conversion
==============

This part of the compiler is now merged in ghc-HEAD.

Overview
--------

This pass takes Cmm with native proceedure calls and an implicit stack
and produces Cmm with only tail calls implemented as jumps and an
explicit stack. In a word, it does CPS conversion. (All right, so that's
two words.)

Design Aspects
--------------

`*Proc-PointAnalysis`\
`*CallingConventions`\
`*LiveValueAnalysis`\
`*StackLayout`

Simple Design
-------------

`*Splitblocksintomultipleblocksatfunctioncalls`\
`*TODO:eliminateextrajumpatblockendswhenthereisalreadyajumpattheendofthecall`\
`*Dolivenessanalysis`\
`*Spliteveryblockintoaseparatefunction`\
`*Passalllivevaluesasparameters(probablyslow)`\
`*Mustarrangeforboththecallerandcalleetoknowargumentorder`\
`*Simpledesign:calleejustchoosessomeorderandallcallersmustcomply`\
`*Eventuallycouldbepassedimplicitlybutkeepingthingsexplicitmakesthingseasier`\
`*Evantuallycoulduseacustomcallingconvention`\
`*Actualsyntaxisprobablyvirtual.(I.e.inanexternaltable,notinactualsyntaxbecausethatwouldrequirechangestothetypeforCmmcode)`\
`*Inputcode:`\
``\
`*Outputcode:`\
``\
`*Savelivevaluesbeforeacallinthecontinuation`\
`*Mustarrangeforboththecallerandcalleetoknowfieldorder`\
`*Simpledesign:calleejustchoosessomeorderandallcallersmustcomply`\
`*Eventuallyneedstobeoptimizedtoreducecontinuationshuffling`\
`*Canregisterallocationalgorithmsbeunifiedwiththisintooneframework?`

To be worked out
----------------

`*Thecontinuationsfor``and``aredifferent.`\
``\
`*Couldmakeaforeachthatshufflestheargumentsintoacommonformat.`\
`*Couldmakeonebranchprimaryandshuffletheothertomatchit,butthatmightentailunnecessarymemorywrites.`

Pipeline
--------

`*CPS`\
`*Makeclosuresandstacksmanifest`\
`*Makesallcallsaretailcalls`\
`*ParameterElimination`\
`*Makescallingconventionexplicit`\
`*Forexternallyvisiblefunctionscallingconventionsismachinespecific,butnotbackendspecificbecausefunctionscompiledfromdifferentbackendsmustbebeabletocalleachother`\
`*Forlocalfunctionscallingconventioncanbeleftuptothebackendbecauseitcantakeadvantageofregisterallocation.`\
`*However,thefirstfirstdraftwillspecifythestandardcallingconventionforallfunctionsevenlocalonesbecause:`\
`*It'ssimpler`\
`*TheCcodegeneratorcan'thandlefunctionparametersbecauseoftheEvilMangler`\
`*TheNCGdoesn'tyetunderstandparameters`

TODO
----

`*Downstream`\
`*Argumentpassingconvention`\
`*Stackcheck`\
`*Needssomewaytosynchronizethebranchlabelwiththeheapcheck`\
`*Midstream`\
`*Support``(neededbyrts/Apply.cmm)`\
`*Morefactoringandcleanup/documentation`\
`*Wikidocumentthedesignedchoosen`\
`*Betterstackslotselection`\
`*Foreignfunctioncalls`\
`*Garbagecollector`\
`*Procpoints`\
`*Maycausenewblocks`\
`*Maycausenewfunctions`\
`*Livescouldbepasseseitheronstackorinarguments`\
`*Procgroupingofblocks`\
`*Upstream`\
`*Have``emitC--withfunctions.`

Current Pipeline
----------------

### 

The / pipeline and the / pipeline can each independantly use the CPS
pass. However, they currently bypass it untill the CPS code becomes
stablized, but they must both use the pass. This pass converts the
header on each function from a to a .

Non-CPS Changes
---------------

`*CmmSyntaxChanges`\
`*Thereturnsparametersofafunctioncallmustbesurroundedbyparenthesis.`\
`Forexample`

`Thisissimplytoavoidshift-reduceconflictswithassignment.`\
`Futurerevisionstotheparsermayeliminatetheneedforthis.`

`*Variabledeclarationsmayareannotatedtoindicate`\
`whethertheyareGCfollowablepointers.`

`*Thebitmapofa``isnowspecifiedusing`\
`aparameterlikesyntax.`

`Notethatthesearenotrealparameters,theyarethestacklayout`\
`ofthecontinuation.Also,untiltheCPSalgorithm`\
`getsproperlyhookedintothe``paththeparameternamesarenotused.`\
`*Thereturnvaluesofafunctioncallmayonlybe``.`\
`Thisisduetochangesinthe``datatype.`

`*CmmDataTypeChanges`\
`*Thereturnparametersofa``are``insteadof``.`\
`Thisisbecausea``doesn'thaveawelldefinedpointerhood,`\
`andthereturnvalueswillbecomeparameterstocontinuationswhere`\
`theirpointerhoodwillbeneeded.`\
`*Thetypeofinfotablesisnowaseparateparameterto`\
`*Before`

`*After`

`Thisistosupportusingeither``or`\
`astheheaderofa``.`\
`*Beforeinfotableconversionuse`

`*Afterinfotableconversionuse`

`Samefor``and``.`\
`*Newtypealiases``,``and``.`\
`Respectivelythesearetheactualparametersofafunctioncall,`\
`theformalparametersofafunction,andthe`\
`returnresultsofafunctioncallwithpointerhoodannotation`\
`(CPSmayconvertthesetoformalparameterofthecall'scontinuation).`

Notes
-----

`*Changedtheparametertoa``tobe``insteadof`\
`*``are`\
`*Thisfieldseemstonothavebeenbeingused;itonlyrequireatypechange`\
`*GCcanbecleanedupb/coftheCPS`\
`*Before`

`*After`

`*WeneedtheNCGtodoaliasinganalysis.AtpresenttheCPSpasswillgeneratethefollowing,andwillassumethattheNCGcanfigureoutwhentheloadsandstorescanbeeliminated.(Theglobalsavespartofa``isdeadb/cofthis.)`

`*Simplecalls`\
`*Before`

`*OutputofCPS`

`*OptimizationbytheNCG`

Loopholes
---------

There are a number of deviations from what one might expect from a CPS
algorithm due to the need to encode existing optimizations and idioms.

### GC Blocks

For obvious reasons, the stack used by GC blocks does not count tward
the maximum amount of stack used by the function.

This loophole is overloaded by the GC **functions** so they don't create
their own infinite loop. The main block is marked as being the GC block
so its stack usage doesn't get checked.

### Update Frames

Update frame have to be pushed onto the stack at the begining of an
update function. We could do this by wrapping the update function inside
another function that just does the work of calling that other function,
but since updates are so common we don't want to pay the cost of that
extra jump. Thus a function can be annotated with a frame that should be
pushed on entry.

Note that while the frame is equivalent to a tail call at the end of the
function, the frame must be pushed at the beginning of the function
because parts of the blackhole code look for these update frames to
determine what thunks are under evaluation.

### User defined continuations

Pushing an update frame on the stack requires the ability to define a
function that will pull that frame from the stack and have access to any
values within the frame. This is done with user-defined continuations.

### Branches to continuations

A GC block for a heap check after a call should only take one or two
instructions. However the natural code: would generate a trivial
continuation for the call as well as a trivial continuation for the call
that just calls the proc point .

We solve this by changing the syntax to

Now the call has the same return signature as and can use the same
continuation. (A call followed by a thus gets optimized down to just the
call.)

Not in Scope of Current Work
----------------------------

Improvements that could be made but that will not be implemented durring
the curent effort.

### Static Reference Table Handling (SRT)

As it stands, each function and thus each call site must be annotated
with a bitmap and a pointer or offset to the SRT shared by the function.
This does not interact with the stack in any way so it ought to be
outside the scope of the CPS algorithm. However there is some level of
interaction because

`1.theSRTinformationoneachcallsiteneedstobeattachedtotheresultingcontinuationand`\
`2.functionsreadfromaCmmfilemightneedtobeannotatedwiththatSRTinfo.`

The first is a concern for correctness but may be handled by treating
the SRT info as opaque data. The second is a concern for ease of use and
thus the likelyhood of mistakes in hand written C-- code. At the moment
it appears that all of the C-- functions in the runtime system (RTS) use
a null SRT so for now we'll just have the CPS algorithm treat the SRT
info as opaque.

In the future it would be nice to have a more satisfactory way to handle
both these issues.

### Cmm Optimization assumed by CPS

In order to simplify the CPS pass, it makes some assumptions about the
optimizer.

`*TheCPSpassmaygeneratemoreblocksthanstrictlynecessary.Inparticular,`\
`itmightbepossibletojointogethertwoblockswhenthesecondblockisonly`\
`enteredbythefirstblock.Thisisasimpleoptimizationthatneedstobeimplemented.`\
`*TheCPSpassmaygeneratemoreloadsandstoresthanstrictlynecessary.Inparticular,`\
`itmayloadalocalregisteronlytostoreitbacktothesamestacklocationafew`\
`statementslater.Theremaybeinterveningbranches.Theoptimizer`\
`needstobeextendedtoeliminatetheseloadstorepairs.`

Notes on future development
---------------------------

### Handling GC

The GHC Commentary: Data types and data constructors
====================================================

This chapter was thoroughly changed Feb 2003. If you are interested in
how a particular data type is implemented take a look at
\[wiki:Commentary/Compiler/CaseStudies/Bool this case study\].

Data types
----------

Consider the following data type declaration: The user's source program
mentions only the constructors \`MkT\` and \`Nil\`. However, these
constructors actually *do* something in addition to building a data
value. For a start, \`MkT\` evaluates its arguments. Secondly, with the
flag \`-funbox-strict-fields\` GHC will flatten (or unbox) the strict
fields. So we may imagine that there's the *source* constructor \`MkT\`
and the *representation* constructor \`MkT\`, and things start to get
pretty confusing.

GHC now generates three unique \`Name\`s for each data constructor:
Recall that each occurrence name (OccName) is a pair of a string and a
name space (see \[wiki:Commentary/Compiler/RdrNameType\#TheOccNametype
RdrNames, Modules, and OccNames\]), and two OccNames are considered the
same only if both components match. That is what distinguishes the name
of the name of the DataCon from the name of its worker Id. To keep
things unambiguous, in what follows we'll write "MkT{d}" for the source
data con, and "MkT{v}" for the worker Id. (Indeed, when you dump stuff
with "-ddumpXXX", if you also add "-dppr-debug" you'll get stuff like
"Foo {- d rMv -}". The "d" part is the name space; the "rMv" is the
unique key.)

Each of these three names gets a distinct unique key in GHC's name
cache.

The life cycle of a data type
=============================

Suppose the Haskell source looks like this: When the parser reads it in,
it decides which name space each lexeme comes from, thus: Notice that in
the Haskell source *all data contructors are named via the "source data
con" MkT{d}*, whether in pattern matching or in expressions.

In the translated source produced by the type checker (-ddump-tc), the
program looks like this:

Notice that the type checker replaces the occurrence of MkT by the
*wrapper*, but the occurrence of Nil by the *worker*. Reason: Nil
doesn't have a wrapper because there is nothing to do in the wrapper
(this is the vastly common case).

Though they are not printed out by "-ddump-tc", behind the scenes, there
are also the following: the data type declaration and the wrapper
function for MkT. Here, the *wrapper* \$WMkT evaluates and takes apart
the argument p, evaluates the argument t, and builds a three-field data
value with the *worker* constructor MkT{v}. (There are more notes below
about the unboxing of strict fields.) The worker \$WMkT is called an
*implicit binding*, because it's introduced implicitly by the data type
declaration (record selectors are also implicit bindings, for example).
Implicit bindings are injected into the code just before emitting code
or External Core.

After desugaring into Core (-ddump-ds), the definition of f looks like
this: Notice the way that pattern matching has been desugared to take
account of the fact that the "real" data constructor MkT has three
fields.

By the time the simplifier has had a go at it, f will be transformed to:
Which is highly cool.

The constructor wrapper functions
---------------------------------

The wrapper functions are automatically generated by GHC, and are really
emitted into the result code (albeit only after CorePre; see
\`CorePrep.mkImplicitBinds\`). The wrapper functions are inlined very
vigorously, so you will not see many occurrences of the wrapper
functions in an optimised program, but you may see some. For example, if
your Haskell source has then \`\$WMkT\` will not be inlined (because it
is not applied to anything). That is why we generate real top-level
bindings for the wrapper functions, and generate code for them.

The constructor worker functions
--------------------------------

Saturated applications of the constructor worker function MkT{v} are
treated specially by the code generator; they really do allocation.
However, we do want a single, shared, top-level definition for top-level
nullary constructors (like True and False). Furthermore, what if the
code generator encounters a non-saturated application of a worker? E.g.
(\`map Just xs\`). We could declare that to be an error (CorePrep should
saturate them). But instead we currently generate a top-level defintion
for each constructor worker, whether nullary or not. It takes the form:
This is a real hack. The occurrence on the RHS is saturated, so the code
generator (both the one that generates abstract C and the byte-code
generator) treats it as a special case and allocates a MkT; it does not
make a recursive call! So now there's a top-level curried version of the
worker which is available to anyone who wants it.

This strange definition is not emitted into External Core. Indeed, you
might argue that we should instead pass the list of \`TyCon\`s to the
code generator and have it generate magic bindings directly. As it
stands, it's a real hack: see the code in CorePrep.mkImplicitBinds.

External Core
-------------

When emitting External Core, we should see this for our running example:
Notice that it makes perfect sense as a program all by itself.
Constructors look like constructors (albeit not identical to the
original Haskell ones).

When reading in External Core, the parser is careful to read it back in
just as it was before it was spat out, namely:

Unboxing strict fields
----------------------

If GHC unboxes strict fields (as in the first argument of MkT above), it
also transforms source-language case expressions. Suppose you write this
in your Haskell source: GHC will desugar this to the following Core
code: The local let-binding reboxes the pair because it may be mentioned
in the case alternative. This may well be a bad idea, which is why
\`-funbox-strict-fields\` is an experimental feature.

It's essential that when importing a type \`T\` defined in some external
module \`M\`, GHC knows what representation was used for that type, and
that in turn depends on whether module M was compiled with
\`-funbox-strict-fields\`. So when writing an interface file, GHC
therefore records with each data type whether its strict fields (if any)
should be unboxed.

Labels and info tables
----------------------

*Quick rough notes: SLPJ March 2003.*

Every data constructor \`C\` has two info tables:

`` *Thestaticinfotable(label`C_static_info`),usedforstatically-allocatedconstructors. ``\
`` *Thedynamicinfotable(label`C_con_info`),usedfordynamically-allocatedconstructors. ``

Statically-allocated constructors are not moved by the garbage
collector, and therefore have a different closure type from
dynamically-allocated constructors; hence they need a distinct info
table. Both info tables share the same entry code, but since the entry
code is physically juxtaposed with the info table, it must be duplicated
(\`C\_static\_entry\` and \`C\_con\_entry\` respectively).

[PageOutline](PageOutline "wikilink")

Demand analyser in GHC
======================

This page explains basics of the so-called demand analysis in GHC,
comprising strictness and absence analyses. Meanings of demand
signatures are explained and examples are provided. Also, components of
the compiler possibly affected by the results of the demand analysis are
listed with explanations provided.

`*The`[`demand-analyser` `draft`
`paper`](http://research.microsoft.com/en-us/um/people/simonpj/papers/demand-anal/demand.ps)`isasyetunpublished,butgivesthemostaccurateoverviewofthewayGHC'sdemandanalyserworks.`

------------------------------------------------------------------------

Demand signatures
-----------------

Let us compile the following program with \`-O2 -ddump-stranal\` flags:

The resulting demand signature for function \`f\` will be the following
one:

This should be read as "\`f\` puts stricts demands on both its arguments
(hence, \`S\`); \`f\` might use its first and second arguments. but in
the second argument (which is a product), the second component is
ignored". The suffix \`m\` in the demand signature indicates that the
function returns **CPR**, a constructed product result (for more
information on CPR see the JFP paper [Constructed Product Result
Analysis for
Haskell](http://research.microsoft.com/en-us/um/people/simonpj/Papers/cpr/index.htm)).

Current implementation of demand analysis in Haskell performs annotation
of all binders with demands, put on them in the context of their use.
For functions, it is assumed, that the result of the function is used
strictly. The analysis infers strictness and usage information
separately, as two components of a cartesian product domain. The same
analysis also performs inference CPR and bottoming properties for
functions, which can be read from the suffix of the signature. Demand
signatures of inner definitions may also include *demand environments*
that indicate demands, which a closure puts to its free variables, once
strictly used, e.g. the signature

indicates that the function has one parameter, which is used lazily
(hence \`<L,U>\`), however, when its result is used strictly, the free
variable \`skY\` in its body is also used strictly.

### Demand descriptions

Strictness demands

`` *`B`--a ``*`hyperstrict`*`` demand.Theexpression`e`putsthisdemandonitsargument`x`ifeveryevaluationof`e`isguaranteedtodiverge,regardlessofthevalueoftheargument.Wecallthisdemand ``*`hyperstrict`*`` becauseitissafetoevaluate`x`toarbitrarydepthbeforeevaluating`e`.Thisdemandispolymorphicwithrespecttofunctioncallsandcanbeseenas`B=C(B)=C(C(B))=...`foranarbitrarydepth. ``\
``\
`` *`L`--a ``*`lazy`*`` demand.Ifanexpression`e`placesdemand`L`onavariable`x`,wecandeducenothingabouthow`e`uses`x`.`L`isthecompletelyuninformativedemand,thetopelementofthelattice. ``

`` *`S`--a ``*`head-strict`*`` demand.If`e`placesdemand`S`on`x`then`e`evaluates`x`toatleasthead-normalform;thatis,totheoutermostconstructorof`x`.Thisdemandistypicallyplacedbythe`seq`functiononitsfirstargument.Thedemand`S(L...L)`placesalazydemandonallthecomponents,andsoisequivalentto`S`;hencetheidentity`S=S(L...L)`.Anotheridentityisforfunctions,whichstatesthat`S=C(L)`.Indeed,ifafunctioniscertainlycalled,itisevaluatedatlestuptotheheadnormalform,i.e., ``*`strictly`*`.However,itsresultmaybeusedlazily.`

`` *`S(s1...sn)`--astructuredstrictnessdemandonaproduct.Itisatleasthead-strict,andperhapsmore. ``

`` *`C(s)`--a ``*`call-demand`*`` ,whenplacedonabinder`x`,indicatesthatthevalueisafunction,whichisalwayscalledanditsresultisusedaccordingtothedemand`s`. ``

Absence/usage demands

`` *`A`--whenplacedonabinder`x`itmeansthat`x`isdefinitelyunused. ``

`` *`U`--thevalueisusedonsomeexecutionpath.Thisdemandisatopofusagedomain. ``

`` *`H`--a ``*`head-used`*`` demand.Indicatesthataproductvalueisuseditself,howeveritscomponentsarecertainlyignored.Thisdemandistypicallyplacedbythe`seq`functiononitsfirstargument.Thisdemandispolymorphicwithrespecttoproductsandfunctions.Foraproduct,thehead-useddemandisexpandedas`U(A,...,A)`andforfunctionsitcanbereadas`C(A)`,asthefunctioniscalled(i.e.,evaluatedtoatleastahead-normalform),butitsresultisignored. ``

`` *`U(u1...un)`--astructuredusagedemandonaproduct.Itisatleasthead-used,andperhapsmore. ``

`` *`C(u)`--a ``*`call-demand`*`` forusageinformation.Whenputonabinder`x`,indicatesthat`x`inallexecutionspathswhere`x`isused,itis ``*`applied`*`` tosomeargument,andtheresultoftheapplicationisusedwithademand`u`. ``

Additional information (demand signature suffix)

`` *`m`--afunctionreturnsa ``[`constructed` `product`
`result`](http://research.microsoft.com/en-us/um/people/simonpj/Papers/cpr/index.htm)

`` *`b`--thefunctionisa ``*`bottoming`*`` one,i.e.,somedecorationof`error`andfriends. ``

Worker-Wrapper split
--------------------

Demand analysis in GHC drives the *worker-wrapper transformation*, which
exposes specialised calling conventions to the rest of the compiler. In
particular, the worker-wrapper transformation implements the unboxing
optimisation.

The worker-wrapper transformation splits each function \`f\` into a
*wrapper*, with the ordinary calling convention, and a *worker*, with a
specialised calling convention. The wrapper serves as an
impedance-matcher to the worker; it simply calls the worker using the
specialised calling convention. The transformation can be expressed
directly in GHC's intermediate language. Suppose that \`f\` is defined
thus: and that we know that \`f\` is strict in its argument (the pair,
that is), and uses its components. What worker-wrapper split shall we
make? Here is one possibility: Now the wrapper, \`f\`, can be inlined at
every call site, so that the caller evaluates \`p\`, passing only the
components to the worker \`\$wf\`, thereby implementing the unboxing
transformation.

But what if \`f\` did not use \`a\`, or \`b\`? Then it would be silly to
pass them to the worker \`\$wf\`. Hence the need for absence analysis.
Suppose, then, that we know that \`b\` is not needed. Then we can
transform to: Since \`b\` is not needed, we can avoid passing it from
the wrapper to the worker; while in the worker, we can use \`error
"abs"\` instead of \`b\`.

In short, the worker-wrapper transformation allows the knowledge gained
from strictness and absence analysis to be exposed to the rest of the
compiler simply by performing a local transformation on the function
definition. Then ordinary inlining and case elimination will do the
rest, transformations the compiler does anyway.

Relevant compiler parts
-----------------------

Multiple parts of GHC are sensitive to changes in the nature of demand
signatures and results of the demand analysis, which might cause
unexpected errors when hacking into demands.
\[wiki:Commentary/Compiler/Demand/RelevantParts This list\] enumerates
the parts of the compiler that are sensitive to demand, with brief
summaries of how so.

Support for deriving , , and  instances
======================================

[PageOutline](PageOutline "wikilink")

GHC 6.12.1 introduces an extension to the mechanism allowing for
automatic derivation of , , and instances using the , , and extensions,
respectively. Twan van Laarhoven [first proposed this
feature](https://mail.haskell.org/pipermail/haskell-prime/2007-March/002137.html)
in 2007, and [opened a related GHC Trac
ticket](https://ghc.haskell.org/trac/ghc/ticket/2953) in 2009.

Example
-------

The derived code would look something like this:

Algorithm description
---------------------

, , and all operate using the same underlying mechanism. GHC inspects
the arguments of each constructor and derives some operation to perform
on each argument, which depends of the type of the argument itself. In a
instance, for example would be applied to occurrences of the last type
parameter, but would be applied to other type parameters. Typically,
there are five cases to consider. (Suppose we have a data type .)

1\. Terms whose type does not mention 2. Terms whose type mentions 3.
Occurrences of 4. Tuple values 5. Function values

After this is done, the new terms are combined in some way. For
instance, instances combine terms in a derived definition by applying
the appropriate constructor to all terms, whereas in instances, a
derived definition would the terms together.

### 

A comment in
[TcGenDeriv.hs](http://git.haskell.org/ghc.git/blob/9f968e97a0de9c2509da00f6337b612dd72a0389:/compiler/typecheck/TcGenDeriv.hs#l1476)
lays out the basic structure of , which derives an implementation for .

 is special in that it can recurse into function types, whereas and
cannot (see the section on covariant and contravariant positions).

### 

Another comment in
[TcGenDeriv.hs](http://git.haskell.org/ghc.git/blob/9f968e97a0de9c2509da00f6337b612dd72a0389:/compiler/typecheck/TcGenDeriv.hs#l1725)
reveals the underlying mechanism behind :

In addition to , also generates a definition for as of GHC 7.8.1
(addressing [\#7436](https://ghc.haskell.org/trac/ghc/ticket/7436)). The
pseudo-definition for would look something like this:

### 

From
[TcGenDeriv.hs](http://git.haskell.org/ghc.git/blob/9f968e97a0de9c2509da00f6337b612dd72a0389:/compiler/typecheck/TcGenDeriv.hs#l1800):

### Covariant and contravariant positions

One challenge of deriving instances for arbitrary data types is handling
function types. To illustrate this, note that these all can have derived
instances:

but none of these can:

In , , and , all occurrences of the type variable are in *covariant*
positions (i.e., the values are produced), whereas in , , and , all
occurrences of are in *contravariant* positions (i.e., the values are
consumed). If we have a function , we can't apply to an value in a
contravariant position, which precludes a instance.

Most type variables appear in covariant positions. Functions are special
in that the lefthand side of a function arrow reverses variance. If a
function type appears in a covariant position (e.g., above), then is in
a contravariant position and is in a covariant position. Similarly, if
appears in a contravariant position (e.g., above), then is in a
covariant position and is in a contravariant position.

If we annotate covariant positions with (for positive) and contravariant
positions with (for negative), then we can examine the above examples
with the following pseudo-type signatures:

Since , , and all use the last type parameter in at least one position,
GHC would reject a derived instance for each of them.

Requirements for legal instances
--------------------------------

This mechanism cannot derive , , or instances for all data types.
Currently, GHC checks if a data type meets the following criteria:

1\. The data type has at least one type parameter. (For example, cannot
have a instance.) 2. The data type's last type parameter cannot be used
contravariantly. (see the section on covariant and contravariant
positions.) 3. The data type's last type parameter cannot be used in the
"wrong place" in any constructor's data arguments. For example, in , the
type parameter is only ever used as the last type argument in and , so
both and values can be ped. However, in , the type variable appears in a
position other than the last, so trying to an value would not typecheck.

`Notethattherearetwoexceptionstothisrule:tupleandfunctiontypes.`

4\. The data type's last type variable cannot used in a constraint. For
example, would be rejected.

In addition, GHC performs checks for certain classes only:

1\. For derived and instances, a data type cannot use function types.
This restriction does not apply to derived instances, however. 2. For
derived and instances, the data type's last type variable must be truly
universally quantified, i.e., it must not have any class or equality
constraints. This means that the following is legal:

`butthefollowingisnotlegal:`

`Thisrestrictiondoesnotapplytoderived``instances.Seethefollowingsectionformoredetails.`

### Relaxed universality check for 

 and cannot be used with data types that use existential constraints,
since the type signatures of and make this impossible. However,
instances are unique in that they do not produce constraints, but only
consume them. Therefore, it is permissible to derive instances for
constrained data types (e.g., GADTs).

For example, consider the following GADT:

In the type signatures for and , the parameter appears both in an
argument and the result type, so pattern-matching on a value of must not
impose any constraints, as neither nor would typecheck.

, however, only mentions in argument types:

Therefore, a derived instance for typechecks:

Deriving instances for GADTs with equality constraints could become
murky, however. Consider this GADT:

All four constructors have the same "shape" in that they all take an
argument of type (or , to which is constrained to be equal). Does that
mean all four constructors would have their arguments folded over? While
it is possible to derive perfectly valid code which would do so:

it is much harder to determine which arguments are equivalent to . Also
consider this case:

For all we know, it may be that . Does this mean that the argument in
should be folded over?

To avoid these thorny edge cases, we only consider constructor arguments
(1) whose types are *syntactically* equivalent to the last type
parameter and (2) in cases when the last type parameter is a truly
universally polymorphic. In the above example, only fits the bill, so
the derived instance is actually:

To expound more on the meaning of criterion (2), we want not only to
avoid cases like , but also something like this:

In this example, the last type variable is instantiated with , which
contains one type variable applied to another type variable . We would
*not* fold over the argument of type in this case, because the last type
variable should be *simple*, i.e., contain only a single variable
without any application.

For the original discussion on this proposal, see
[\#10447](https://ghc.haskell.org/trac/ghc/ticket/10447).

Alternative strategy for deriving \`Foldable\` and \`Traversable\`
------------------------------------------------------------------

We adapt the algorithms for \`-XDeriveFoldable\` and
\`-XDeriveTraversable\` based on that of \`-XDeriveFunctor\`. However,
there is an important difference between deriving the former two
typeclasses and the latter one (as of GHC 8.2, addressing [Trac
\#11174](https://ghc.haskell.org/trac/ghc/ticket/11174)), which is best
illustrated by the following scenario:

The generated code for the \`Functor\` instance is straightforward:

But if we use too similar of a strategy for deriving the \`Foldable\`
and \`Traversable\` instances, we end up with this code:

This is unsatisfying for two reasons:

1\. The \`Traversable\` instance doesn't typecheck! \`Int\#\` is of kind
\`\#\`, but \`pure\` expects an argument whose type is of kind \`\*\`.
This effectively prevents \`Traversable\` from being derived for any
datatype with an unlifted argument type (see [Trac
\#11174](https://ghc.haskell.org/trac/ghc/ticket/11174)).

2\. The generated code contains superfluous expressions. By the
\`Monoid\` laws, we can reduce \`f a &lt;&gt; mempty\` to \`f a\`, and
by the \`Applicative\` laws, we can reduce \`fmap WithInt (f a)
&lt;\*&gt; pure i\` to \`fmap (\\b -&gt; WithInt b i) (f a)\`.

We can fix both of these issues by incorporating a slight twist to the
usual algorithm that we use for \`-XDeriveFunctor\`. The differences can
be summarized as follows:

1\. In the generated expression, we only fold over arguments whose types
mention the last type parameter. Any other argument types will simply
produce useless \`mempty\`s or \`pure\`s, so they can be safely ignored.

2\. In the case of \`-XDeriveTraversable\`, instead of applying
\`ConName\`, we apply \`\\b\_i ... b\_k -&gt; ConName a\_1 ... a\_n\`,
where

-   \`ConName\` has \`n\` arguments
-   \`{b\_i, ..., b\_k}\` is a subset of \`{a\_1, ..., a\_n}\` whose
    indices correspond to the arguments whose types mention the last
    type parameter. As a consequence, taking the difference of \`{a\_1,
    ..., a\_n}\` and \`{b\_i, ..., b\_k}\` yields the all the argument
    values of \`ConName\` whose types do not mention the last
    type parameter. Note that \`\[i, ..., k\]\` is a strictly increasing

[PageOutline](PageOutline "wikilink")

LLVM Back-end Design
====================

The current design tries to fit into GHC's pipeline stages as an
alternative to the C and NCG back-ends as seamlessly as possible. This
allows for quicker development and focus on the core task of LLVM code
generation.

The LLVM pipeline works as follows:

`*NewpathforLLVMgeneration,separatefromCandNCG.(pathforksatcompiler/main/CodeOutput.lhs,sameplacewhereCandNCGfork).`\
`*LLVMcodegenerationwilloutputLLVMassemblycode.`\
`*TheLLVMassemblycodeistranslatedtoanobjectfileasfollows`\
`*TheLLVMoptimizerisrunwhichisaseriesofbitcodetobitcodeoptimizationpasses(usingthe``tool).`\
`*FinallyanobjectfileiscreatedfromtheLLVMbitcode(usingthe``tool)`\
`*ThisbringstheLLVMpathbacktotheotherback-ends.`\
`*ThefinalstateistheLinkstage,whichusesthesystemlinkeraswiththeotherback-ends.`

Here is a diagram of the pipeline:

This approach was the easiest and thus quickest way to initially
implement the LLVM back-end. Now that it is working, there is some room
for additional optimisations. A potential optimisation would be to add a
new linker phase for LLVM. Instead of each module just being compiled to
native object code ASAP, it would be better to keep them in the LLVM
bitcode format and link all the modules together using the LLVM linker.
This enable all of LLVM's link time optimisations. All the user program
LLVM bitcode will then be compiled to a native object file and linked
with the runtime using the native system linker.

Implementation
==============

Framework
---------

`*New`**`-fllvm`**`codegenerationpipeline,involvedmodifying:`\
`*``-Selectsappropriateback-endforcodegeneration(C,NCG,LLVM).`\
`*``` -StoresGHCconfiguration(commandlineoptions,compiletimeoptions...ect).Added`HscLlvm`targettype. ``\
`*``` -Storesmodules/filestocompileforghc.AddednewLLVMfilesanddirectorystoredunder`llvmGen`,andnewCPPflagtoenabletheLLVMcodegenerator(`-DLLVM`). ``\
`*``` -Addednew`GhcWithLlvmCodeGen`optionwhichcanbesetin`build.mk`to`YES`toenabletheLLVMcodegenerator. ``\
`*``` -Added`LlvmAs`phasetoinvokethecompilationofLLVMbitcode/IRtoanobjectfile.Afterthisphaselinkingcanoccur. ``\
`*``` -Addedcodefornew`LlvmAs`,`LlvmOpt`and`LlvmLlc`phases. ``\
`*``` -Invokes`llvm-as`tooltocompileallvmassemblyfile('.ll')toabitcodefile(`.bc`). ``\
`*``` -Invokesthellvm`opt`tooltooptimisethemodule.Justusethellvmstandardoptimisationgroupsof`O1`,`O2`,`O3`,dependingontheoptimisationlevelpassedto'ghc'bytheuser. ``\
`*``` -Invokesthellvm`llc`tooltogeneratethemachinecode('.s'file)fromtheoptimisedbitcode.'As'stagerunsnext,partofexisting'ghc'pipeline. ``\
`*``` -Storesthepathanddefaultsettingsofthesystemtoolsneeded,soforLLVMback-endthisis`llvm-as`,`opt`and`llc`. ``

The LLVM pipeline works as specified above. Code generation phase
occurs, using the option data the appropriate generator is selected
(which is the Llvm back-end is \`-fllvm\` has been specified on the
command line). After code generation, the next phase is determined, this
is done from the \`HscLlvm\` target data constructor which is selected
at ghc startup by . The next phase is \`LlvmAs\` which will compile the
text IR to an LLVM bitcode file (equivalent to \`llvm-as\` tool). After
this the \`LlvmLlc\` phase is run, which produces a native object file
from the llvm bitcode file (equivalanet to the \`llc\` tool). At this
stage, the output from all three back-ends should be 'equivalent'. After
this phase, the \`StopLn\`, or linking phase occurs which should result
in the end result. Compiling some Haskell code with the c-backend and
some with the llvm-backend and linking them together is supported.

LLVM Code Generation
--------------------

For LLVM code generation we need a method for representing and
generating LLVM code. The [LLVM
FAQ](http://llvm.org/docs/FAQ.html#langirgen) suggest the following
possible approaches:

`*CallintoLLVMLibrariesusingFFI(canprobablyuse`[`Haskell`
`LLVM` `Bindings`](http://hackage.haskell.org/package/llvm)`)`\
`*EmitLLVMAssembly(approachtakenby`[`EHC's`](http://www.cs.uu.nl/wiki/Ehc/WebHome)`LLVMBack-end,canusethe`[`module`](https://subversion.cs.uu.nl/repos/project.UHC.pub/trunk/EHC/src/ehc/LLVM.cag)`developedbythemforthis)`\
`*EmitLLVMBitcode(can'tseeanyreasontodothis)`

The approach taken was to use the LLVM module from
[EHC](http://www.cs.uu.nl/wiki/Ehc/WebHome). This module contains an
abstract syntax representation of LLVM Assembly and the ability to
pretty print it. It has been heavily modified to increase its language
coverage as it was missing several LLVM constructs which were needed.
Ideally we would like to add a second pretty printer which calls into
the LLVM C++ API to generate LLVM Bitcode. This should hopefully
decrease the compile times and make the back-end more resilient to
future changes to LLVM Assembly. The LLVM Haskell binding (first option)
wasn't used as it represents LLVM at a very high level, which isn't
appropriate for the back-end.

Register Pinning
----------------

The new back-end supports a custom calling convention to place the STG
virtual registers into specific hardware registers. The current approach
taken by the C back-end and NCG of having a fixed assignment of STG
virtual registers to hardware registers for performance gains is not
implemented in the LLVM back-end. Instead, it uses a custom calling
convention to support something semantically equivalent to register
pinning. The custom calling convention passes the first N variables in
specific hardware registers, thus guaranteeing on all function entries
that the STG virtual registers can be found in the expected hardware
registers. This approach is believed to provide better performance than
the register pinning used by NCG/C back-ends as it keeps the STG virtual
registers mostly in hardware registers but allows the register allocator
more flexibility and access to all machine registers.

For some more information about the use of a custom calling convention
see [here (Discussion between Chris Lattner and David
Terei)](http://www.nondot.org/sabre/LLVMNotes/GlobalRegisterVariables.txt)

Code Generation
---------------

Code generation consists of translating a list of \`GenCmmTop\` data
types to LLVM code. \`GenCmmTop\` has the following form:

That is, it consists of two types, static data and functions. Each can
largely be handled separately. Just enough information is needed such
that pointers can be constructed to them and in many cases this
information can be gathered from assumptions and constraints on Cmm.

After all the polymorphic types are bound we get this:

The code generator lives in \`llvmGen\` with the driver being
\`llvmGen/LlvmCodeGen.lhs\`.

A large part of the code generation is keeping track of defined
variables/functions and their type. An \`LlvmEnv\` construct is used for
this. It is simply a dictionary storing function/variable names with
their corresponding type information. This is used to create correct
references/pointers between variables and functions.

### Unregisterised Vs. Registerised

Code generation can take place in two general modes, \`unregisterised\`
and \`registerised\`. There are two major differences from a back-end
code generation point of view. Firstly, in unregisterised mode a
optimisation feature called is disabled. This means that the \`h\` field
of \`CmmProc\` is empty. In registerised mode it instead contains the
\`CmmStatic\` data for the procedures info table which must be placed
just before the procedure in the generated code so that both the info
table and procedure can be accessed through one pointer. This
optimisation can be disabled separately though in \`registerised\` mode.

The other major change is the use of pinned global registers. The
\`Cmm\` language includes a concept called registers. These are used
like machine registers or variables in C to store the result of
expressions. Unlike \`LLVM\` they are mutable. \`Cmm\` includes two
types of registers as you can see below:

A \`LocalReg\` is a temporary general purpose register used in a
procedure with scope of a single procedure. A \`GlobalReg\` on the other
hand has global scope and a specific use. They are used just like
machine registers, with a Stack Pointer and Heap Pointer registers
creating a virtual machine (\`STG\`). \`GlobalReg\` is of the form:

In unregisterised mode these global registers are all just stored in
memory in the heap. A specific pass operating on Cmm that takes place
just before code generation thus transforms code such as:

into the following unregisterised form for code generation:

Where \`MainCapability\` is a label to the start of a RTS defined
structure storing all the global registers.

In registerised mode as many of these global registers are assigned
permanently to fixed hardware registers. This is done as it greatly
improves performance. As these registers are accessed very frequently
needing to load and store to memory for accessing adds a great cost. So
for example on \`x86\` the following map between \`Cmm\` global
registers and \`x86\` hardware registers exists:

These are all the available \`callee save\` registers on x86. \`callee
save\` are used as in ghc generated code now saving and restoring of
these registers are needed due to there new special use and because GHC
uses continuation passing style, so a \`'ret'\` statement is never
actually generated. And since they are \`callee save\`, foreign code can
also be called without any need to handle the \`Cmm\` registers.

!CmmData
--------

\`CmmData\` takes the following form:

Code generation takes place mainly in , driven by the main Llvm compiler
driver, }.

The code generation for data occurs in two phases, firstly the types and
all data is generated except for address values. Then the address values
are resolved. This two step method is used as in the first pass, we
don't know if a address refers to an external address or a
procedure/data structure in the current LLVM module. We also need the
type information in LLVM to create a pointer.

### 1st Pass : Generation

All \`CmmStatic\` is translated to LLVM structures.

!CmmStaticLit
-------------

These are translated when possible as follows:

`` *`CmmInt`->ReducedtoIntandthenanappropriate`LMInt`ofcorrectsizeiscreated.AsLLVMsupportsanybitsize,thisisverystraightforward. ``\
`` *`CmmFloat`->Translatedtoadouble,detectingNANandINFINITYcorrectly.ThencorrectLLVMtype(`float`,`double`,`float80`,`float128`)isselected. ``\
`` *`CmmLabel`->Leftuntranslatedatfirst,laterresolvedoncewehavedeterminedtypes.Aspointersarecasttowordsizeints,wecanstilldeterminetypes. ``\
`` *`CmmLabelOff`->Asabove. ``\
`` *`CmmLabelDiffOff`->Asabove. ``\
`` *`CmmBlock`->`BlockId`ischangedtoa`CLabel`andthentreatedasa`CmmLabel`statictype. ``\
`` *`CmmHighStackMark`->Panicoccursifthistypeisencountered. ``

#### !CmmUninitialised

For this, a zeroed array of \`8bit\` values is created of correct size.

#### !CmmAlign & !CmmDataLabel

The LLVM back-end can't handle \`CmmAlign\` or \`CmmDataLabel\`. A panic
occurs if either is encountered. A \`CmmDataLabel\` is expected at the
very start of each list of \`CmmStatic\`. It is removed and used as the
name for the structure and constant instance.

#### !CmmString

This is translated into a LLVM string. Ascii characters are used when
they are printable, escaped hex values otherwise. A null termination is
added.

### 2nd Pass : Resolution

After the first pass, all types have been determined and all data
translated except for address values (CLabel's). All generated llvm data
is added to a Map of string to \`LlvmType\`, string being the data
structure name. All \`CmmProc's\` are added to the map as well, they
don't need to be properly passed though, just their names retrieved as
they have a constant type of void return and no parameters.

Now appropriate pointers can be generated using the type information
from the map and LLVM's \`getelementptr\` instruction. These are then
all passed to int's to allow the types of structures to be determined in
advance. If a pointer doesn't have a match in the Map, it is assumed to
refer to an external (outside of this module) address. An external
reference is declared for this address as:

Where i32 is the pointer size. (i64 if on 64 bit).

!CmmProc
--------

A Cmm procedure is made up of a list of basic blocks, with each basic
block being comprised of a list of CmmStmt

Desugaring instance declarations
================================

These notes compare various ways of desugaring Haskell instance
declarations. The tradeoffs are more complicated than I thought!

Basic stuff
-----------

 These desugar to the following Core: (Notation: I am omitting foralls,
big lambdas, and type arguments. I'm also using \`f x = e\` rather than
\`f = \\x.e\`.)

Points worth noting:

`*Theclassgivesrisetoaneponymousdatatype(inGHCitisactually`\
`` called`:TC`),thedictionary. ``

`*Thereisaneponymoustop-levelselectorfunctionforeachclassmethod,`\
`` `opF`and`opG`inthiscase. ``

`` *Thedefaultmethodfor`opG`becomesatop-levelfunction`$dmopG`. ``\
`` Ittakesthe`(Ca)`dictionaryaargumentbecausetheRHSisallowedtocall ``\
`othermethodsofC.`

`` *Theinstancedeclarationdefinesadictionary`dCInt`.Notice ``\
`` thatit'srecursive,becausewemustpass`dCInt`to`opGI`. ``

`` *Crucially,thesimplifieriscarefulnottochoose`dCInt`as ``\
`` aloopbreaker,andhenceifitsees`casedCIntof...`it ``\
`` cansimplifythe`case`. ``

`` *If`$dmopG`isinlined,therecursionisbrokenanyway. ``

Dictionary functions
--------------------

Now consider an instance declaration that has a context: Here is one way
to desugar it. Notice that

`` *Ifweinlinetheselector`opF`in`opFd_as`,then ``\
`` wecansimplify`opfl`togiveadirectly-recursivefunction: ``

`Thisisimportant.`

`` *TheBADTHINGisthat`dCList`isbig,andhencewon'tbeinlined. ``\
`That'sbadbecauseitmeansthatifwesee`

`` wedon'tgettocall`opfl`directly.Insteadwe'llcall`dCList`,build ``\
`thedictionary,dotheselection,etc.Sospecialiationwon'thappen,`\
`evenwhenallthetypesarefixed.`

The INLINE strategy
-------------------

An obvious suggestion, which GHC implemented for a long time, is to give
\`dCList\` an INLINE pragma. Then it'll inline at every call site, the
dictionary will be visible to the selectors, and good things happen.

But it leads to a huge code blow-up in some cases. We call these
dictionary functions a lot, often in a nested way, and we know programs
for which the INLINE-all-dfuns approach generates gigantic code.
(Example: Serge's !DoCon.)

The out-of-line (A) strategy
----------------------------

The INLINE strategy would make sense if \`dCList\` could be guaranteed
small. Suppose the original instance declaration had been like this:
This is exactly what GHC 6.10 now does, behind the scenes. Desugaring
just as above, we'd get the following: Notice that

`` *`dCList`isguaranteedsmall,andcouldreasonablybeINLINEd ``\
`ateverycallsite.Thisgoodbecauseitexposesthedictionary`\
`structuretoselectors.`

`` *`dCList`and`opF_aux`aremutuallyrecursive.Butifwe ``\
`` avoidchoosing`dCList`astheloopbreakerwecaninline ``\
`` `dCList`into`opF_aux`,andthenthe`opF`selector ``\
`` can"see"thedictionarystructure,and`opF_aux`simplifies,thus: ``

`` Good!Now`opF_aux`isself-recursiveasitshouldbe. ``\
`Thesamethinghappenswithtwomutuallyrecursivemethods`

`` *BUTnoticethatwereconstructthe`(C[a])`dictionaryon ``\
`eachiterationoftheloop.AsGaneshpointsoutin#3073,that`\
`issometimesbad.`

The out-of-line (B) strategy
----------------------------

We can avoid reconstructing the dictionary by passing it to
\`opF\_aux\`, by recasting latter thus: Notice the extra \`C \[a\]\` in
the context of \`opF\_aux\`. (Remember this is all internal to GHC.) Now
the same desugaring does this: The two definitions aren't even
recursive. BUT now that \`d\_as\` is an *argument* of \`opF\_aux\`, the
latter can't "see" that it's always a dictionary! Sigh. As a result, the
recursion in \`opF\_aux\` always indirects through the (higher order)
dictionary argument, using a so-called "unknown" call, which is *far*
less efficient than direct recursion.

Note also that

`` *Typechecking`opF_aux`isabitfragile;see#3018.Troubleisthat ``\
`` whenaconstraint`(C[a])`arisesinitsRHStherearetwoways ``\
`` ofdischargingit:byusingtheargument`d_as`directly,orby ``\
`` calling`(dCListd_a)`.As#3018shows,it'shardtoguaranteethat ``\
`we'lldotheformer.`

User INLINE pragmas and out-of-line (A)
---------------------------------------

There is another difficulty with the out-of-line(A) strategy, that is
currently unsolved. Consider something like this: Then we'll desugar to
something like this: The INLINE on \`dCT\` is added by the compiler; the
INLINE on \`opF\_aux\` is just propagated from the users's INLINE
pragma... maybe the RHS is big.

Now the difficulty is that we GHC currently doesn't inline into the RHS
of an INLINE function (else you'd get terrible code blowup). So the
recursion between \`dCT\` and \`opF\_aux\` is not broken. One of the two
must be chosen as loop breaker, and the simplifier chooses \`opF\_aux\`.
Ironcially, therefore the user INLINE pragma has served only to
guarantee that it *won't* be inlined!!

(This issue doesn't arise with out-of-line(B) because (B) doesn't make
\`dCT\` and \`opF\_aux\` mutually recursive.)

Summary
-------

Here are the current (realistic) options:

`*Out-of-line(A):GHC6.10doesthis.`\
`*Good:recursivemethodsbecomedirectlymutually-recursive`\
`*Bad:lackofmemoisation`\
`*Bad:difficultywithuserINLINEpragmas`

`*Out-of-line(B)`\
`*Good:memoisationworks`\
`*Verybad:recursivemethodsiterateonlyvia"unknown"calls.`\
`*Good:nodifficultywithuserINLINEpragmas`

My current difficulty is that I see no way to get all the good things at
once.

PS: see also the comments at the start of
\`compiler/typecheck/TcInstDcls.lhs\`, which cover some of the same
ground.

Bugs & Other Problems
=====================

I've moved all known bugs into the trac bug database, the can be found
[here](http://hackage.haskell.org/trac/ghc/query?status=infoneeded&status=merge&status=new&status=patch&component=Compiler+%28LLVM%29&order=priority&col=id&col=summary&col=status&col=type&col=priority&col=milestone&col=component)

Compiling more than one module at once
======================================

When compiling a single module, we can assume that all of our
dependencies have already been compiled, and query the environment as
necessary when we need to do things like look up interfaces to find out
what the types in our dependencies are. When we compile more than module
at once, as in \`--make\`, things get a bit more complicated:

1\. We have to analyze the dependency structure of the program in
question, and come up with a plan for how to compile the various
modules, and

2\. We have an opportunity to cache and reuse information from interface
files which we may load from the environment. This is why, for example,
\`ghc --make\` outperforms parallel one-shot compilation on one core.

This discussion is going to omit concerns related to dynamic code
loading in GHC (as would be the case in GHCi).

The overall driver
------------------

The meat of this logic is in
[GhcFile(compiler/main/GhcMake.hs)](GhcFile(compiler/main/GhcMake.hs) "wikilink"),
with primary entry point the function \`load\` (in the case of
\`--make\`, this function is called with \`LoadAllTargets\`, instructing
all target modules to be compiled, which is stored in \`hsc\_targets\`).

### Dependency analysis

Dependency analysis is carried out by the \`depanal\` function; the
resulting \`ModuleGraph\` is stored into \`hsc\_mod\_graph\`.
Essentially, this pass looks at all of the imports of the target modules
(\`hsc\_targets\`), and recursively pulls in all of their dependencies
(stopping at package boundaries.) The resulting module graph consists of
a list of \`ModSummary\` (defined in
[GhcFile(compiler/main/HscTypes.lhs)](GhcFile(compiler/main/HscTypes.lhs) "wikilink")),
which record various information about modules prior to compilation
(recompilation checking, even), such as their module identity (the
current package name plus the module name), whether or not the file is a
boot file, where the source file lives. Dependency analysis inside GHC
is often referred to as \*\*downsweep\*\*.

ToDo: say something about how hs-boot files are

The dependency analysis is cached (in \`hsc\_mod\_graph\`), so later
calls to \`depanal\` can reuse this information. (This is not germane
for \`--make\`, which only calls \`depanal\` once.) \`discardProg\`
deletes this information entirely, while \`invalidateModSummaryCache\`
simply "touches" the timestamp associated with the file so that we
resummarize it.

The result of dependency analysis is topologically sorted in \`load\` by
\`topSortModuleGraph\`.

### Recompilation checking and stability

See also the page on \[wiki:Commentary/Compiler/RecompilationAvoidance
recompilation avoidance\].

ToDo: say something about stability; it's per SCC

### Compilation

Compilation, also known as \*\*upsweep\*\*, walks the module graph in
topological order and compiles everything. Depending on whether or not
we are doing parallel compilation, this implemented by \`upsweep\` or by
\`parUpsweep\`. In this section, we'll talk about the sequential
upsweep.

The key data structure which we are filling in as we perform compilation
is the \*\*home package table\*\* or HPT (\`hsc\_HPT\`, defined in
[GhcFile(compiler/main/HscTypes.lhs)](GhcFile(compiler/main/HscTypes.lhs) "wikilink")).
As its name suggests, it contains informations from the \*home
package\*, i.e. the package we are currently compiling. Its entries,
\`HomeModInfo\`, contain the sum total knowledge of a module after
compilation: both its pre-linking interface \`ModIface\` as well as the
post-linking details \`ModDetails\`.

We \*clear\* out the home package table in the session (for \`--make\`,
this was empty anyway), but we pass in the old HPT.

ToDo: talk about how we fix up loops after we finish the loop

Finally, when the module is completely done being compiled, it is
registered in the home package table

ToDo: Talk about what happens when we fail while in the middle of
compiling a module cycle

Eager Promotion
===============

Eager promotion is a technique we use in GHC to improve the performance
of generational GC. It is somewhat specific to the characteristics of
lazy evaluation, since it takes advantage of the fact that we have some
objects that are mutated just once (i.e. thunks).

The key observation is this: when an object P contains a pointer to an
object Q in a younger generation, and P is not mutable, then we know
that Q cannot be garbage collected until the generation in which P
resides is collected. Hence, we might as well promote Q to this
generation immediately, rather than
\[wiki:Commentary/Rts/Storage/GC/Aging aging\] it or promoting it to an
intermediate generation. Furthermore, if eager promotion is successful,
then the object containing the old-to-new pointers will no longer need
to be in the \[wiki:Commentary/Rts/Storage/GC/RememberedSets remembered
set\] for the generation it resides in.

We gave some performance results for this technique in [Runtime Support
for Multicore
Haskell](http://www.haskell.org/~simonmar/papers/multicore-ghc.pdf); the
upshot is that it's worth 10% or so.

Eager promotion works like this. To do eager promtion, the scavenger
sets the flag \`gct-&gt;eager\_promotion\` (it can leave the flag set
when scavenging multiple objects, this is the usual way), and
\`gct-&gt;evac\_gen\` is set to the generation to which to eagerly
promote objects. The \`evacuate\` function will try to move each live
object into \`gct-&gt;evac\_gen\` or a higher generation if possible,
and set \`gct-&gt;failed\_to\_evac\` if it fails (see
\[wiki:Commentary/Rts/Storage/GC/RememberedSets\]). It may fail if the
target object has already been moved: we can't move an object twice
during GC, because there may be other pointers already updated to point
to the new location. It may also fail if the object is in a generation
that is not being collected during this cycle.

Objects which are repeatedly mutable should not be subject to eager
promotion, because the object may be mutated again, so eagerly promoting
the objects it points to may lead to retaining garbage unnecessarily.
Hence, when we are scavenging a mutable object (see
[GhcFile(rts/sm/Scav.c)](GhcFile(rts/sm/Scav.c) "wikilink")), we
temporarily turn off \`gct-&gt;eager\_promotion\`.

Eager Version Bumping Strategy
==============================

Versioning of GHC core/boot libraries adheres to Haskell's [Package
Versioning Policy](https://wiki.haskell.org/Package_versioning_policy)
whose scope is considered to apply to \*\*released artifacts\*\* (and
therefore doesn't prescribe when to //actually// perform version
increments during development)

However, in the spirit of continuous integration, GHC releases snapshot
artifacts, and therefore it becomes important for early
testers/evaluators/package-authors to be presented with accurate
PVP-adhering versioning, especially for those who want adapt to upcoming
API changes in new major GHC releases early (rather than being hit
suddenly by a disruptive version-bump-wave occurring at GHC release
time).

So while the usual scheme is to update a package version in the VCS
right before a release (and reviewing at that point whether a
patchlevel, minor or major version bump is mandated by the PVP), for GHC
bundled core/boot packages, the \*\*eager version bumping\*\* scheme is
preferred, which basically means:

This becomes particularly easy when also maintaining a \`changelog\`
file during development highlighting the changes for releases, as then
one easily keeps track of the last released version, as well as becoming
aware more easily of minor/major version increment-worthy API changes.

Video: [Types and
Classes](http://www.youtube.com/watch?v=pN9rhQHcfCo&list=PLBkRCigjPwyeCSD_DFxpd246YIF7_RDDI)
(23'53")

Data types for Haskell entities: , , , , and 
=============================================

For each kind of Haskell entity (identifier, type variable, type
constructor, data constructor, class) GHC has a data type to represent
it. Here they are:

`*`**`Type`
`constructors`**`arerepresentedbythe``type(`[`GhcFile(compiler/types/TyCon.hs)`](GhcFile(compiler/types/TyCon.hs) "wikilink")`).`\
`*`**`Classes`**`arerepresentedbythe``type(`[`GhcFile(compiler/types/Class.hs)`](GhcFile(compiler/types/Class.hs) "wikilink")`).`\
`*`**`Data`
`constructors`**`arerepresentedbythe``type(`[`GhcFile(compiler/basicTypes/DataCon.hs)`](GhcFile(compiler/basicTypes/DataCon.hs) "wikilink")`).`\
`*`**`Pattern`
`synonyms`**`arerepresentedbythe``type(`[`GhcFile(compiler/basicTypes/PatSyn.hs)`](GhcFile(compiler/basicTypes/PatSyn.hs) "wikilink")`).`\
`*`**`Term` `variables`**```and`**`type`
`variables`**```arebothrepresentedbythe``type(`[`GhcFile(compiler/basicTypes/Var.hs)`](GhcFile(compiler/basicTypes/Var.hs) "wikilink")`).`

All of these entities have a , but that's about all they have in common.
However they are sometimes treated uniformly:

`*A`**`` `TyThing` ``**`(`[`GhcFile(compiler/types/TypeRep.hs)`](GhcFile(compiler/types/TypeRep.hs) "wikilink")`)issimplythesumofallfour:`

`Forexample,atypeenvironmentisamapfrom``to``.(Thefactthata``tellswhatnamespaceitbelongstoallow,forexample,identicallynamedvaluesandtypestositinasinglemap.)`

All these data types are implemented as a big record of information that
tells you everything about the entity. For example, a contains a list of
its data constructors; a contains its type (which mentions its ); a
contains the s of all its method selectors; and an contains its type
(which mentions type constructors and classes).

So you can see that the GHC data structures for entities is a *graph*
not tree: everything points to everything else. This makes it very
convenient for the consumer, because there are accessor functions with
simple types, such as . But it means that there has to be some tricky
almost-circular programming ("knot-tying") in the type checker, which
constructs the entities.

Type variables and term variables
---------------------------------

Type variables and term variables are represented by a single data type,
, thus
([GhcFile(compiler/basicTypes/Var.hs)](GhcFile(compiler/basicTypes/Var.hs) "wikilink")):
It's incredibly convenient to use a single data type for both, rather
than using one data type for term variables and one for type variables.
For example:

`*Findingthefreevariablesofatermgivesasetofvariables(bothtypeandtermvariables):``.`\
`*WeonlyneedonelambdaconstructorinCore:``.`

The type distinguishes the two sorts of variable; indeed, it makes
somewhat finer distinctions
([GhcFile(compiler/basicTypes/Var.hs)](GhcFile(compiler/basicTypes/Var.hs) "wikilink")):
Every has fields and a . The latter is identical to the in the former,
but is cached in the for fast comparison.

Here are some per-flavour notes:

`::isselfexplanatory.`

`::isusedduringtype-checkingonly.Oncetypecheckingisfinished,therearenomore``s.`

`::isusedfortermvariablesbound`*`in` `the` `module` `being`
`compiled`*`.Morespecifically,a``isboundeither`*`within`*`anexpression(lambda,case,locallet),oratthetoplevelofthemodulebeingcompiled.`\
`*The``ofa``maychangeasthesimplifierrepeatedlybashesonit.`\
`*A``carriesaflagsayingwhetherit'sexported.Thisisusefulforknowingwhetherwecandiscarditifitisnotused.`

`::isusedforfixed,immutable,top-leveltermvariables,notablyonesthatareimportedfromothermodules.Thismeansthat,forexample,theoptimizerwon'tchangeitsproperties.`\
`*Alwayshasan``or``[wiki:Commentary/Compiler/NameTypeName],andhencehasa``thatisgloballyuniqueacrossthewholeofaGHCinvocation.`\
`*Alwaysboundattoplevel.`\
`*The``ofa``iscompletelyfixed.`\
`*AllimplicitIds(dataconstructors,classmethodselectors,recordselectorsandthelike)areare``sfrombirth,eventheonesdefinedinthemodulebeingcompiled.`\
`*Whenfindingthefreevariablesofanexpression(``),weonlycollect``andignore``.`

All the value bindings in the module being compiled (whether top level
or not) are s until the !CoreTidy phase. In the !CoreTidy phase, all
top-level bindings are made into s. This is the point when a becomes
"frozen" and becomes a fixed, immutable .

 and implict Ids
----------------

s are further classified by their . This type is defined in
[GhcFile(compiler/basicTypes/IdInfo.hs)](GhcFile(compiler/basicTypes/IdInfo.hs) "wikilink"),
because it mentions other structured types such as . Unfortunately it is
*used* in Var.hs so there's a hi-boot knot to get it there. Anyway,
here's the declaration (elided a little): Some s are called **implicit
s**. These are s that are defined by a declaration of some other entity
(not just an ordinary variable binding). For example:

`*Theselectorsofarecordtype`\
`*Themethodselectorsofaclass`\
`*TheworkerandwrapperIdforadataconstructor`

It's easy to distinguish these Ids, because the field says what kind of
thing it is: .

HC files and the Evil Mangler
=============================

GHC no longer has an evil mangler.

Strictness analysis: examples
=============================

Consider:

We want to make sure to figure out that f's argument is demanded with
type L1X(L1X(LMX)) -- that is, it may or may not be demanded, but if it
is, it's always applied to two arguments. This shows why shouldn't just
throw away the argument info: in this case, the expression has a
nonstrict demand placed on it, yet we still care about the arguments.

On the other hand, in: we want to say that if the result of has demand
placed on it (i.e., not a call demand), the body of has demand placed on
it, not . So this case needs to be treated differently from the one
above.

System FC: equality constraints and coercions
=============================================

For many years, GHC's intermediate language was essentially:

`*SystemFw,plus`\
`*algebraicdatatypes(includingexistentials)`

But that is inadequate to describe GADTs and associated types. So in
2006 we extended GHC to support System FC, which adds

`*equalityconstraintsandcoercions`

You can find a full description of FC in the paper
[3](http://research.microsoft.com/~simonpj/papers/ext-f); note that GHC
uses the system described in post-publication Appendix C, not the system
in the main body of the paper. The notes that follow sketch the
implementation of FC in GHC, but without duplicating the contents of the
paper.

A coercion \`c\`, is a type-level term, with a kind of the form \`T1 :=:
T2\`. (\`c :: T1 :=: T2\`) is a proof that a term of type \`T1\` can be
coerced to type \`T2\`. Coercions are classified by a new sort of kind
(with the form ). Most of the coercion construction and manipulation
functions are found in the module,
[GhcFile(compiler/types/Coercion.hs)](GhcFile(compiler/types/Coercion.hs) "wikilink").

Coercions appear in Core in the form of expressions: if \`t :: T1\` and
\`c :: T1:=:T2\`, then . See \[wiki:Commentary/Compiler/CoreSynType\].

Coercions and Coercion Kinds
----------------------------

The syntax of coercions extends the syntax of types (and the type
\`Coercion\` is just a synonym for \`Type\`). By representing coercion
evidence on the type level, we can take advantage of the existing
erasure mechanism and keep non-termination out of coercion proofs (which
is necessary to keep the system sound). The syntax of coercions and
types also overlaps a lot. A normal type is evidence for the reflexive
coercion, i.e., Coercion variables are used to abstract over evidence of
type equality, as in

There are also coercion constants that are introduced by the compiler to
implement some source language features (newtypes for now, associated
types soon and probably more in the future). Coercion constants are
represented as \`TyCon\`s made with the constructor \`CoercionTyCon\`.

Coercions are type level terms and can have normal type constructors
applied to them. The action of type constructors on coercions is much
like in a logical relation. So if \`c1 :: T1 :=: T2\` then

and if \`c2 :: S1 :=: S2\` then The sharing of syntax means that a
normal type can be looked at as either a type or as coercion evidence,
so we use two different kinding relations, one to find type-kinds
(implemented in Type as \`typeKind :: Type -&gt; Kind\`) and one to find
coercion-kinds (implemented in Coercion as \`coercionKind :: Coercion
-&gt; Kind\`).

Coercion variables are distinguished from type variables, and
non-coercion type variables (just like any normal type) can be used as
the reflexive coercion, while coercion variables have a particular
coercion kind which need not be reflexive.

GADTs
-----

The internal representation of GADTs is as regular algebraic datatypes
that carry coercion evidence as arguments. A declaration like would
result in a data constructor with type This means that (unlike in the
previous intermediate language) all data constructor return types have
the form \`T a1 ... an\` where \`a1\` through \`an\` are the parameters
of the datatype.

However, we also generate wrappers for GADT data constructors which have
the expected user-defined type, in this case Where the 4th and 5th
arguments given to \`T1\` are the reflexive coercions

Representation of coercion assumptions
--------------------------------------

In most of the compiler, as in the FC paper, coercions are abstracted
using \`ForAllTy cv ty\` where \`cv\` is a coercion variable, with a
kind of the form \`PredTy (EqPred T1 T2)\`. However, during type
inference it is convenient to treat such coercion qualifiers in the same
way other class membership or implicit parameter qualifiers are treated.
So functions like \`tcSplitForAllTy\` and \`tcSplitPhiTy\` and
\`tcSplitSigmaTy\`, treat \`ForAllTy cv ty\` as if it were \`FunTy
(PredTy (EqPred T1 T2)) ty\` (where \`PredTy (EqPred T1 T2)\` is the
kind of \`cv\`). Also, several of the \`dataCon\`XXX functions treat
coercion members of the data constructor as if they were dictionary
predicates (i.e. they return the \`PredTy (EqPred T1 T2)\` with the
theta).

Newtypes are coerced types
--------------------------

The implementation of newtypes has changed to include explicit type
coercions in the place of the previously used ad-hoc mechanism. For a
newtype declared by the \`NewTyCon\` for \`T\` will contain n\`t\_co =
CoT\` where: This \`TyCon\` is a \`CoercionTyCon\`, so it does not have
a kind on its own; it basically has its own typing rule for the
fully-applied version. If the newtype \`T\` has k type variables, then
\`CoT\` has arity at most k. In the case that the right hand side is a
type application ending with the same type variables as the left hand
side, we "eta-contract" the coercion. So if we had then we would
generate the arity 0 coercion \`CoS : S :=: \[\]\`. The primary reason
we do this is to make newtype deriving cleaner. If the coercion cannot
be reduced in this fashion, then it has the same arity as the tycon.

In the paper we'd write and then when we used \`CoT\` at a particular
type, \`s\`, we'd say which encodes as \`(TyConApp instCoercionTyCon
\[TyConApp CoT \[\], s\])\`

But in GHC we instead make \`CoT\` into a new piece of type syntax (like
\`instCoercionTyCon\`, \`symCoercionTyCon\` etc), which must always be
saturated, but which encodes as In the vocabulary of the paper it's as
if we had axiom declarations like The newtype coercion is used to wrap
and unwrap newtypes whenever the constructor or case is used in the
Haskell source code.

Such coercions are always used when the newtype is recursive and are
optional for non-recursive newtypes. Whether or not they are used can be
easily changed by altering the function mkNewTyConRhs in
iface/BuildTyCl.lhs.

Roles
-----

Roles specify what nature of equality a coercion is proving. See
\[wiki:Roles\] and RolesImplementation.

Simplification
--------------

`*exprIsConApp_maybe`

`*simplExpr`

GHC Commentary: Runtime aspects of the FFI
==========================================

Foreign Import "wrapper"
------------------------

Files [GhcFile(rts/Adjustor.c)](GhcFile(rts/Adjustor.c) "wikilink")
[GhcFile(rts/AdjustorAsm.S)](GhcFile(rts/AdjustorAsm.S) "wikilink").

Occasionally, it is convenient to treat Haskell closures as C function
pointers. This is useful, for example, if we want to install Haskell
callbacks in an existing C library. This functionality is implemented
with the aid of adjustor thunks.

An adjustor thunk is a dynamically allocated code snippet that allows
Haskell closures to be viewed as C function pointers.

Stable pointers provide a way for the outside world to get access to,
and evaluate, Haskell heap objects, with the RTS providing a small range
of ops for doing so. So, assuming we've got a stable pointer in our hand
in C, we can jump into the Haskell world and evaluate a callback
procedure, say. This works OK in some cases where callbacks are used,
but does require the external code to know about stable pointers and how
to deal with them. We'd like to hide the Haskell-nature of a callback
and have it be invoked just like any other C function pointer.

Enter adjustor thunks. An adjustor thunk is a little piece of code
that's generated on-the-fly (one per Haskell closure being exported)
that, when entered using some 'universal' calling convention (e.g., the
C calling convention on platform X), pushes an implicit stable pointer
(to the Haskell callback) before calling another (static) C function
stub which takes care of entering the Haskell code via its stable
pointer.

An adjustor thunk is allocated on the C heap, and is called from within
Haskell just before handing out the function pointer to the Haskell (IO)
action. User code should never have to invoke it explicitly.

An adjustor thunk differs from a C function pointer in one respect: when
the code is through with it, it has to be freed in order to release
Haskell and C resources. Failure to do so will result in memory leaks on
both the C and Haskell side.

------------------------------------------------------------------------

CategoryStub

Function Calls
==============

Source files: [GhcFile(rts/Apply.h)](GhcFile(rts/Apply.h) "wikilink"),
[GhcFile(rts/Apply.cmm)](GhcFile(rts/Apply.cmm) "wikilink")

Dealing with calls is by far the most complicated bit of the execution
model, and hence of the code generator. GHC uses an *eval/apply*
strategy for compiling function calls; all the details of the design are
in the paper [Making a fast curry: push/enter vs. eval/apply for
higher-order
languages](http://www.haskell.org/~simonmar/papers/eval-apply.pdf).

First, we need some terminology:

`*The`**`arity`**`ofafunctionisthenumberoflambdasstaticallyusedin[wiki:Commentary/Compiler/StgSynTypethelambda-formofitsdefinition].Notethatarityisnotdeduciblefromthetype.Example:`

`` Here,`f`hasarity1,eventhoughitstypesuggestsittakestwoarguments.Thepointisthatthecompiledcodefor`f`willexpecttobepassedjustoneargument,`x`. ``

`*The`**`entry` `point`**`(sometimescalledthe`**`fast` `entry`
`point`**`)ofafunctionofarityNexpectsitsfirstNargumentstobepassedinaccordancewiththestandard[wiki:Commentary/Rts/HaskellExecution/CallingConventioncallingconventions].`

`*A`**`known`
`call`**`isacallofafunctionwhosebindingsiteisstaticallyvisible:`\
`*Thefunctionisboundattoplevelinthismodule;or,`\
`*Thefunctionisboundattoplevelinanothermodule,andoptimistionison,sowecanseethedetails(notablyarity)ofthefunctioninthemodule'sinterfacefile;or,`\
`` *Thefunctionisboundbyan`let`bindingthatenclosesthecall. ``

When compiling a call, there are several cases to consider, which are
treated separately.

`*`**`Unknown`
`function`**`;acallinwhichwedonotstaticallyknowwhatthefunctionis.Inthatcasewemustdoa"genericapply".Thisissoexcitingthatitdeservesits[wiki:Commentary/Rts/HaskellExecution/FunctionCalls#Genericapplyownsection].`

`*`**`Known` `function,` `saturated`
`call`**`.Thefunctionisappliedtoexactlytherightnumberofargumentstosatisfyitsarity.Inthatcase,wesimplyloadtheargumentsaccordingtothestandardentryconvention,andtail-call(jumpto)thefunction'sentrypoint.Onaverage,about80%ofallcallsfallintothiscategory(seetheeval/applypaperformeasurements).`

`*`**`Known` `function,` `too` `few`
`arguments`**`.Inthiscase,wewanttobuildapartialapplication(PAP),andreturnwithapointertothePAPinthereturnregister.SincebuildingaPAPisacomplicatedbusiness,insteadwejustbehaveasforanunknownfunctioncall,whichwillendupcallingintothe`[`ref(Generic`
`apply)`](ref(Generic_apply) "wikilink")`code,whichwillbuildthePAPforus.`

`*`**`Known` `function,` `too` `many`
`arguments`**`.Wewanttosavetheextraargumentsonthestack,pushareturnaddress,andthenbehavejustlikeasaturatedcall.Whentheresultcomesback,weshouldbehavelike"unknowncall".However,toavoidneedingtogeneratecodeforanewcontinuationhere,thereturnaddressthatwepushonthestackisthatofanappropriate`[`ref(Generic`
`apply)`](ref(Generic_apply) "wikilink")`function,whichwillperformtheapplicationoftheextraargumentstothe(unknown)functionreturnedbythesaturatedcall.`

Generic apply
-------------

Files: [GhcFile(utils/genapply)](GhcFile(utils/genapply) "wikilink")

When compiling a call that has an unknown function, we must generate
code to

`*Evaluatethefunction`\
`*Scrutinisethefunctionvaluereturnedtoseeitsarity,anddispatchintothesamethreecasesasinthecaseofknowncalls:`\
`*Exactlytherightnumberofarguments:loadthemintothestandardlocationsandtail-callthefunction'sentrypoint`\
`*Toofewarguments:buildaPAP`\
`*Toomanyarguments:savetheexcessarguments,andtailcallthefunctionasforasaturatedcal.`

All of this takes quite a lot of code, so we pre-generate a whole bunch
of generic-apply code sequencues, one for each combination of arguments.
This code is generated by the tool
[GhcFile(utils/genapply)](GhcFile(utils/genapply) "wikilink"), and the
generated code appears in \`rts/AutoApply.cmm\`.

For example, if we find a call to an unknown function applied to two
(boxed) \`Int\` arguments, load the function and its two arguments as
for the standard entry convention and jump to \`stg\_ap\_pp\_fast\`.
This latter code is in \`rts/AutoApply.cmm\`, generated by the
\`genapply\` tool. The "\`pp\`" part is the bit that says the code is
specialised for two pointer arguments.

In addition to the family of \`stg\_ap\_<pattern>\_fast\` functions for
making calls to unknown functions with various argument patterns, there
is a corresponding family of return addresses
\`stg\_ap\_<pattern>\_info\`. The idea is that you can push a
continuation that will make a call to the function that is returned to
it. For example, to push a continuation that will apply a single pointer
argument, we would push the following words on the stack:

|| arg || || \`stg\_ap\_p\_info\` ||

The Garbage Collector
=====================

GC concepts:

`*[wiki:Commentary/Rts/Storage/GC/AgingAging]`\
`*[wiki:Commentary/Rts/Storage/GC/PinnedPinnedobjects]`\
`*[wiki:Commentary/Rts/Storage/GC/RootsRoots]`\
`*[wiki:Commentary/Rts/Storage/GC/EagerPromotionEagerpromotion]`\
`*[wiki:Commentary/Rts/Storage/GC/RememberedSetsRememberedsets]`\
`*[wiki:Commentary/Rts/Storage/GC/WeakWeakpointersandfinalizers]`\
`*[wiki:Commentary/Rts/Storage/GC/CAFsCAFs]`

GC algorithms supported:

`*[wiki:Commentary/Rts/Storage/GC/CopyingCopyingGC]`\
`*[wiki:Commentary/Rts/Storage/GC/ParallelParallelGC]`\
`*[wiki:Commentary/Rts/Storage/GC/MarkingMarking](forcompactionorsweeping)`\
`*[wiki:Commentary/Rts/Storage/GC/CompactionCompaction]`\
`*[wiki:Commentary/Rts/Storage/GC/SweepingSweeping](formark-regionGC)`\
`*[wiki:Commentary/Rts/Storage/GC/ImmixImmix](notsupportedyet)`

GC overview
-----------

The GC is designed to be flexible, supporting lots of ways to tune its
behaviour. Here's an overview of the techniques we use:

`` *GenerationalGC,witharuntime-selectablenumberofgenerations(`+RTS-G ``<n>`` -RTS`,where`n>=1`).Currentlyitisa ``\
`traditionalgenerationalcollectorwhereeachcollectioncollectsaparticulargenerationandallyoungergenerations.`\
`Generalizingthissuchthatanysubsetofgenerationscanbecollectedisapossiblefutureextension.`

`*Theheapgrowsondemand.Thisisstraightforwardlyimplementedbybasingthewholestoragemanagerona[wiki:Commentary/Rts/Storage/BlockAllocblockallocator].`

`*Aging:objectscanbeagedwithinageneration,toavoidprematurepromotion.See[wiki:Commentary/Rts/Storage/GC/Aging].`

`` *Theheapcollectionpolicyisruntime-tunable.Youselecthowlargeagenerationgetsbeforeitiscollectedusingthe`+RTS-F ``<n>`` -RTS`option,where` ``<n>`` `isafactorofthegeneration'ssizethelasttimeitwascollected.Thedefaultvalueis2,thatisagenerationisallowedtodoubleinsizebeforebeingcollected. ``

GC data structures
------------------

[GhcFile(includes/rts/storage/GC.h)](GhcFile(includes/rts/storage/GC.h) "wikilink")

### generation

The main data structure is \`generation\`, which contains:

`` `blocks`:: ``\
`apointertoalistofblocks`

`` `large_objects`:: ``\
`apointertoalistofblockscontaininglargeobjects`

`` `threads`:: ``\
`alistofthreadsinthisgeneration`

`` `mut_list`:: ``\
`the[wiki:Commentary/Rts/Storage/GC/RememberedSetsrememberedset],alistofblockscontainingpointerstoobjectsin`*`this`*`generationthatpointtoobjectsin`*`younger`*`generations`

and various other administrative fields (see
[GhcFile(includes/rts/storage/GC.h)](GhcFile(includes/rts/storage/GC.h) "wikilink")
for the details).

Generations are kept in the array \`generations\[\]\`, indexed by the
generation number.

### nursery

A \`nursery\` is a list of blocks into which the mutator allocates new
(small) objects. For reasons of locality, we want to re-use the list of
blocks for the nursery after each GC, so we keep the nursery blocks
rather than freeing and re-allocating a new nursery after GC.

The struct \`nursery\` contains only two fields

`` `blocks`:: ``\
`thelistofblocksinthisnursery`\
`` `n_blocks`:: ``\
`thenumberofblocksintheabovelist`

In the threaded RTS, there is one nursery per Capability, as each
Capability allocates independently into its own allocation area.
Nurseries are therefore stored in an array \`nurseries\[\]\`, indexed by
Capability number.

The blocks of the nursery notionally logically to generation 0, although
they are not kept on the list \`generations\[0\].blocks\`. The reason is
that we want to keep the actual nursery blocks separate from any blocks
containing live data in generation 0. Generation 0 may contain live data
for two reasons:

`*objectsliveinthenurseryarenotpromotedtogeneration1immediately,insteadtheyare[wiki:Commentary/Rts/Storage/GC/Agingaged],firstbeingcopiedtogeneration0,andthenbeingpromotedtogeneration1inthenextGCcycleiftheyarestillalive.`

`*Ifthereisonlyonegeneration(generation0),thenliveobjectsingeneration0areretainedingeneration0afteraGC.`

I know kung fu: learning STG by example
=======================================

The STG machine is an essential part of GHC, the world's leading Haskell
compiler. It defines how the Haskell evaluation model should be
efficiently implemented on standard hardware. Despite this key role, it
is generally poorly understood amongst GHC users. This document aims to
provide an overview of the STG machine in its modern, eval/apply-based,
pointer-tagged incarnation by a series of simple examples showing how
Haskell source code is compiled.

What is STG, exactly?
---------------------

Haskell code being sucked through GHC has a complex lifecycle. Broadly
speaking, it transitions between five representations:

The path from C-- to assembly varies: the three possible backends are C
(\`-fvia-c\`), LLVM (\`-fllvm\`), and the default backend -- the native
code genarator (or NCG), which generates assembly directly from the
GHC-internal C-- data type.

STG is a simple functional language, rather like the more famous Core
language. It differs in the following main respects:

`1.Initscurrentincarnation,itisn'ttypedintheHaskellsense,`\
`thoughitdoesknowabout`*`representation`*`types`\
`2.Itisinadministrativenormalform(ANF),whichiswhereevery`\
`subexpressionisgivenaname`\
`3.Every$\lambda$,constructorapplication,andprimitiveoperator`\
`is$\eta$-expanded`\
`4.Itisannotatedwithatonofinformationthatthecode`\
`generatorisinterestedinknowing`

STG expressions can be one of the following:

`1.Atoms(i.e.literalsandvariables)`\
`` 2.`let`-bindings(bothrecursiveandnon-recursive)overanother ``\
`expression,wherelet-boundthingsareoneof:`\
`*Afunctionvaluewithexplicitlambdas`\
`*Anunsaturatedapplication`\
`*Aconstructorappliedtoatoms`\
`*Athunk(i.e.anyexpressionnotfittingintooneoftheabove`\
`categories)`

`3.Saturatedprimitiveapplicationofaprimitivetovariables`\
`4.Applicationofavariabletooneormoreatoms`\
`5.Casedeconstructionofanexpression,whereeachbranchmayalso`\
`beanexpression`

The job of the *STG machine* is to evaluate these expressions in a way
which is efficiently implementable on standard hardware. This document
will look at how exactly this is achieved by looking at real examples of
the C-- code GHC generates for various Haskell expressions.

This document will take a very low-level view of the machine, so if you
want to get comfortable with how the STG machine executes at a more
abstract level before reading this document, you might want to read the
paper ["How to make a fast curry: push/enter vs.
eval/apply"](http://research.microsoft.com/en-us/um/people/simonpj/papers/eval-apply/).
It presents the STG machine without reference to an explicit stack or
registers, but instead as a transition system. This transition system
has also been implemented as a Haskell program called
[ministg](http://hackage.haskell.org/package/ministg) by [Bernie
Pope](http://ww2.cs.mu.oz.au/~bjpop/), for those who wish to see it in
action on some simple examples.

An overview of the STG machine
------------------------------

Before we dive in, a note: this document will describe the STG machine
as it is implemented on x86-style architectures. I will use the terms
"the STG machine" and "the STG machine as implemented on x86 by GHC"
interchangeably. The implementation is somewhat different on x64, not
least due to the greater number of available registers.

This overview section is rather bare. Readers might be able to fill in
any gaps in my explanation by using some of the following sources:

`*`[`The` `Haskell` `Execution`
`Model`](http://hackage.haskell.org/trac/ghc/wiki/Commentary/Rts/HaskellExecution)\
`*`[`Storage`](http://hackage.haskell.org/trac/ghc/wiki/Commentary/Rts/Storage)\
`*`[`The` `Spineless` `Tagless`
`G-machine`](http://research.microsoft.com/en-us/um/people/simonpj/Papers/spineless-tagless-gmachine.ps.gz)\
`-nowsadlyratheroutofdate`\
`*`[`Faster` `laziness` `through` `dynamic` `pointer`
`tagging`](http://research.microsoft.com/en-us/um/people/simonpj/papers/ptr-tag/ptr-tagging.pdf)

### Components of the machine

In its bare essentials, the STG machine consists of three parts:

`1.TheSTGregisters:`\
`*Thereareratheralotofregistershere:morethancanbe`\
`practicablystoredinactualavailableprocessorregistersonmost`\
`architectures.`\
`*Todealwiththelackofprocessorregisters,mostoftheSTG`\
`registersareactuallykeptonthestackinablockofmemory`\
`pointedtobyaspecialSTGregistercalledthe"baseregister"(or`\
`` `BaseReg`).Togetorsetvaluesofregisterswhicharenotkeptin ``\
`processorregisters,theSTGmachinegeneratesaninstructionto`\
`` loadorstorefromanaddressrelativetothe`BaseReg`. ``\
`` *Themostimportantfourregistersarethe`BaseReg`,thestack ``\
`` pointer(`Sp`),theheappointer(`Hp`),andthegeneralpurpose ``\
`` register`R1`whichisusedforintermediatevalues,aswellasfor ``\
`returningevaluatedvalueswhenunwindingthestack.Thesearethe`\
`fourregisterswhichareassignedactualprocessorregisterswhen`\
`implementingtheSTGmachineonx86.`\
`2.TheSTGstack:`\
`*Storesfunctionargumentsandcontinuations(i.e.thestack`\
`frameswhichareexecutedwhenafunctionreturns)`\
`*Growsdownwardsinmemory`\
`` *ThetopofthestackispointedtobytheSTGregister`Sp`,and ``\
`` themaximumavailablestackpointerisstoredin`SpLim`.Thereis ``\
`noframepointer.`

`3.Theheap:`\
`*Usedtostoremanydifferentsortsofheapobject:notably`\
`functions,thunksanddataconstructors`\
`*Growsupwardsinmemory,towardsthestack`\
`*Allallocationoccursusingabump-allocator:theheappointeris`\
`simplyincrementedbythenumberofbytesdesired(subjecttotoa`\
`checkthatthisdoesnotexhaustavailablememory).Thegarbage`\
`collectorisresponsibleformovingobjectsoutoftheareaofthe`\
`heapmanagedbythebumpallocatorandintothecareofits`\
`generationalcollector.`\
`*Thelastaddressinthebump-allocatedpartoftheheapthathas`\
`` beenusedispointedtobytheSTGregister`Hp`,with`HpLim` ``\
`holdingthemaximumaddressavailableforbump-allocation.`

### Important concepts in the machine

Some of the key concepts in the STG machine include *closures*, *info
tables* and *entry code*. We tackle them in reverse order:

`Entrycode::`\
`TheactualmachinecodethattheSTGmachinewillexecuteupon`\
`"entry".Entrymeansdifferentthingsfordifferentheapobjects.`

`*For`*`thunks`*`,entryiswhenthethunkisforcedbysomedemand`\
`` foritsvalue,suchasa`case`expressionscrutinisingit ``\
`*For`*`functions`*`,entryiswhenthefunctionisappliedtoas`\
`manyargumentsasaredemandedbythearityrecordedinitsinfo`\
`table`\
`*For`*`continuations`*`,entryoccurswhenavalueisreturnedfrom`\
`anestedcall,andhencetheneedarisestoconsumethevalueand`\
`continueevaluation`

`Infotable::`\
`Ablockofmemoryallocatedstatically,whichcontainsmetadata`\
`aboutaclosure.Themostimportantfieldsforourpurposesarethe`\
`entrycodepointerandthearityinformation(ifthisistheinfo`\
`tableforathunk,functionorpartialapplication)`

`Closure::`\
`Essentiallyaheap-allocatedpairofthefreevariablesofsome`\
`code,andapointertoitsinfotable(i.e.itsinfopointer).`

For an example of how these parts work together, consider the following
code

The nested lambda will give rise to all of the above objects.

The closure will store a pointer to \`x\`'s closure (as it is a free
variable of the lambda), along with a pointer to an info table. That
info table will contain information relevant to a function value,
recording information such as the fact that it has an arity of 1 (i.e.
the binding for \`y\`), and the pointer to the entry code for the
function \`\\y -&gt; y + x\` itself. This entry code will implement the
addition by combining the closure for the free variable \`x\` (taken
from the closure) with the stack-passed \`y\` variable's closure.

Upon entry to some code, pointers to closures are made available in
\`R1\`. That is to say, before entry code is jumped to, \`R1\` is set up
to point to the associated closure, so that the entry code can access
free variables (if any).

Closures for code which contain no free variables (such as the closure
for \`True\` and \`False\`, and functions applied to no arguments such
as \`(:)\` and \`id\`) are allocated statically by the compiler in the
same manner as info tables are.

### Overview of execution model of the machine

This will be covered in more detail in the examples below, so I will use
this section to make some general points.

The goal of the STG machine is to reduce the current expression to a
value. When it has done so, it:

`1.StoresataggedpointertoevaluatedclosureintheSTGregister`\
`` `R1` ``\
`2.Jumpstotheentrycodeoftheinfotablepointedtobythe`\
`valueatthetopoftheSTGstack`\
`*Thismayalsobecalledtheinfotableofthe`*`continuation`*`of`\
`theexpression`

The continuation code is responsible for popping its info pointer (and
stack-allocated free variables, if any) from the stack before returning.

Arguments are passed on the stack, and are popped by the callee. Upon a
jump to the entry code for a function, there are always precisely as
many arguments on the stack as the (statically known) arity of that
function, and those arguments will be followed by the info pointer of a
continuation.

Saturated application to known functions
----------------------------------------

Handling application in the STG machine is a big topic, and so in this
first section we only look at the case of *saturated* applications to
*known* functions - i.e. those functions that the compiler statically
knows information such as the entry code pointer and arity for.

### Example 1: function application with sufficient stack space

Application of functions is the bread and butter of the STG machine.
Correspondingly, this first Haskell program

compiles to very simple C-- code

The STG machine passes arguments to functions on the STG stack, and a
pointer to the stack top is stored in the STG register \`Sp\`.
Furthermore, because GHC currently uses the eval/apply variant of the
STG machine, exactly as many arguments as the function expects to
receive are guaranteed to present on the stack.

Therefore, upon entry to the \`known\_app\` function, we are guaranteed
that the STG stack has a pointer to a closure of type \`()\` on top of
it. In order to call \`known\_fun\`, we just modify the top of the stack
to replace that pointer with a pointer to the statically allocated
closure for the literal \`10\`, and then tail-call into the entry code
of \`known\_fun\`.

### Example 2: function application that needs to grow the stack

This Haskell code is apparently little more complicated than the
previous example

however, it generates radically different C-- code:

As before, upon entry the STG stack is guaranteed to have a single
closure pointer at its top. However, in order to call into known\_fun\_2
we need at least two free stack slots at the top for arguments, which
means that we have to grow the stack by one word before we can make the
call.

#### Checking for sufficient stack space

First, we check to see if growing the stack would overflow allocated
stack space, by comparing the STG stack pointer register \`Sp\` with the
stack limit register \`SpLim\`:

(The stack grows downwards, hence the *subtraction* of 4 from the
current \`Sp\`). If the stack check fails, we branch to \`clH\`:

This stores the closure of the current function in \`R1\`, and then
jumps into the hand-written garbage collector code to force it to grow
the stack. After the stack has been grown, the collector will call back
into \`Main\_knownzuappzu2\_entry\` by using the information stored in
the (statically-allocated) \`Main\_knownzuappzu2\_closure\` closure
pointed to by \`R1\`, and the stack check will be run again - hopefully
succeeding this time!

#### Making the known call

Given that the stack check succeeds, it is easy to make the actual call
we are after. We simply grow the stack by the required amount, and write
the two arguments to \`known\_fun\_2\` into the top two stack slots
(overwriting our own first argument in the process, of course):

A simple tail call to the new function finishes us off:

Example 3: Unsaturated applications to known functions
------------------------------------------------------

Despite describing an undersaturated call, this Haskell code

compiles to straightforward C-- as follows

The reason that there is no special magic to deal with undersaturated
applications to known functions is simple: GHC simply gives
\`known\_undersaturated\_app\` an arity of 2, so by the time we jump to
the entry code the stack must already contain any arguments required by
\`known\_fun\_2\`.

Example 4: Applications to unknown functions
--------------------------------------------

We aren't going to tackle oversaturated calls to known functions until
we've considered happens to calls to statically-unknown functions. To
see what these look like, we are going to use the following Haskell code

Which compiles to this C-- function

Unlike the previous cases we have looked at, we are compiling an
application where we don't statically know either the arity or the info
pointer of the function being applied. To deal with such cases, the STG
machine uses several pre-compiled "generic apply" functions which
inspect the info-table for the function in question and decide how the
available arguments should be applied to it.

### Dealing with generic application

There are three cases the generic apply functions have to deal with:

`1.Thefunction'sarity(recorderinthefunctionclosure'sinfo`\
`table)exactlymatchesthenumberofargumentsavailableonthe`\
`stack`\
`*Thisisthebestcase.Inthiscase,thegenericapplyfunction`\
`simplymakesatailcallintothefunction'sentrycode`

`2.Thefunction'sarityisgreaterthanthenumberofarguments`\
`availableonthestack`\
`*Inthiscase,thegenericapplycodeallocatesaPAP(partial`\
`application)closurewhichclosesoverboththenewargumentsand`\
`thefunctionpointer,andreturnsthatvalue,inthenormalSTGish`\
`way,tothecontinuationonthetopofthestack`

`3.Thefunction'sarityislessthanthenumberofarguments`\
`availableonthestack`\
`*Inthiscase,anumberofargumentsmatchingthearityarepushed`\
`ontopofthestack,followedbyacontinuationwhichusesanother`\
`ofthegenericapplyfunctionstoapplytheremainingarguments.`\
`Thecodefortheoriginalfunctionisthenentered`\
`*Eventuallythecodeforthecontinuationisenteredandanother`\
`genericapplyfunctionwillbetail-calledtodealwiththe`\
`result`

Potentially, one generic apply function is required for every "argument
pattern". Some example argument patterns are:

Because the number of patterns is large (actually unbounded, because
functions might be of any arity), GHC only generates generic apply
functions for enough patterns so that 99.9% of all calls observed in
practice have a generic apply function. Generic apply functions for
calls of larger arity can be simulated by chaining together several
smaller generic apply functions, in a similar manner as when dealing
with oversaturated function applications.

### Making the call to the generic application code

Let's remind ourselves of the original code:

Knowing about generic apply functions, the call itself is easy to
understand. We pop the top of the stack (the function argument) into
\`R1\` and then jump into the generic application code for the case
where the stack contains a single pointer argument, which deals with all
the cases for \`f\` described above.

Example 5: oversaturated applications to known functions
--------------------------------------------------------

This Haskell code

compiles to the following C-- function

As you might see, despite being a call to a known function, this code
makes use of the generic apply functions we discussed in the last
section. Let's pick the function apart and see how it works.

First, we do the usual stack check. What differs from the last time we
saw this check is that we are not only allocating space for arguments on
the stack, but also for a *continuation*. We set up these new stack
entries as follows:

i.e. the final stack looks as follows (note that the code overwrites the
old pointer to a closure of type ()):

Because \`known\_fun\_2\` is of arity 2, when we jump to its entry code,
it will only consume the top two arguments from the stack: i.e. the two
pointers to \`base\_GHCziBase\_id\_closure\`. It will then evaluate to
some sort of value and transfer control to the entry code for
\`stg\_ap\_p\_info\`.

This is where the magic happens: the entry code for \`stg\_ap\_p\_info\`
will apply the function value that was returned from \`known\_fun\_2\`
to the (pointer) argument in the "free variable" of its (stack
allocated) closure -- and we have arranged that that is
\`stg\_INTLIKE\_closure+209\`, i.e. the closure for the \`Int\` literal
\`10\`. This code is shared with the generic application functions for
calls to unknown functions, so this will make use of the
\`stg\_ap\_p\_fast\` function we saw before.

Finally, control will be transferred back to the caller for
\`known\_oversat\_app\`, and all will be well.

Example 6: allocation of thunks and data
----------------------------------------

Something that happens all the time in Haskell is allocation. There are
three principal types of thing that get allocated: function closures,
thunks, and data. These are all treated pretty much the same in the STG
machine for the simple reason that they share many common
characteristics:

`*EntrycodewhichtheSTGmachinejumpsto,inordertoevaluate`\
`them`\
`*Notethatforconstructors,theentrycodeistrivial,asthey`\
`arealwaysalreadyevaluated!Inthiscase,controlwillbe`\
`transferreddirectlybacktothecaller'scontinuation.`

`*Freevariablesstoredinaclosure`\
`*Fordata,these"freevariables"willbethevaluesinthefields`\
`oftheparticulardataconstructor`

`*Info-tablescontainingvariousmiscellaneousmetadataaboutthe`\
`heapobject,suchasfunctionarity`

Let us look at how a thunk and a data constructor get allocated in a
simple setting:

This compiles into the following C--:

Let's break this function down slowly.

### Checking for sufficient heap space

Any function that needs to allocate memory might find that the heap has
been exhausted. If that happens, it needs to call into the garbage
collector in order to get the heap cleaned up and (possibly) enlarged.

Hence, the first thing any such function does is check to see if enough
memory is available for its purposes:

This is simple enough. The function needs to allocate 20 bytes (the data
constructor takes up 2 words, and the thunk will take up 3), so it
speculatively increments Hp and then checks the STG registers \`Hp\` and
\`HpLim\` (the pointer to the top of the available heap space) against
each other.

If memory is insufficient (i.e. we have moved \`Hp\` past the top of the
available heap), the code deals with it by setting the \`HpAlloc\`
register to the number of bytes needed and \`R1\` to the closure for the
function in which the heap check failed, before jumping into the
hand-written garbage collector code for the cleanup. The garbage
collector will resume execution of the code by using the information
from \`R1\`, after it has freed up enough memory.

Side note: I believe that the line setting \`R1\` is unnecessary here,
because \`R1\` should anyway always be set to the address of the closure
when executing the closure entry code. I could be wrong, though.

### Performing the actual allocation

Once the heap check succeeds, we will be able to enter the body of the
function proper. Since the \`Hp\` has already been incremented, we can
just construct the new heap objects directly:

So we get something like this:

The bottom two words are the allocated \`Just\` value, and the three
above that correspond to the \`x + 1\` closure.

### Returning an allocated value to the caller

Now that we have allocated the data we entered the function in order to
construct, we need to return it to the caller. This is achieved by the
following code:

To return, the STG machine:

`` 1.Sets`R1`tothepointertotheresultofevaluation ``\
`2.Popsalltheargumentstothefunctionfromthestack`\
`3.Jumpstotheentrycodeforthecontinuation.Thisisalways`\
`foundatthetopoftheSTGstack,logicallybelowanyarguments`\
`thatwerepushedtomakethecall.`

This is indeed exactly what happens here, with two interesting points:
pointer tagging, and the double-deference of the stack pointer. These
will be discussed in the next two subsections.

#### Pointer tagging

One exciting feature is that the code setting \`R1\`, i.e. \`R1 = Hp -
2\`. This is setting \`R1\` to point to the \`Just\`, we just allocated,
but simultaneously tagging that pointer with the value 2. The fact that
the tag is non-zero indicates to users of the pointer that the thing
pointed to is already evaluated. Furthermore, because \`Maybe\` has only
two constructors, we are able to use the pointer tags to record which
constructor it evaluated to: in this case, the 2 indicates the \`Just\`
constructor.

It is compulsory to tag pointers before jumping to the address of the
continuation entry code: the entry code can and will rely on those tags
being present!

#### \`TABLES\_NEXT\_TO\_CODE\`

Because I have compiled GHC without \`TABLES\_NEXT\_TO\_CODE\`, the
entry code for the continuation is found by dereferencing the pointer to
the info table we found at the top of the STG stack - i.e. a
double-dereference.

The layout of heap objects without \`TABLES\_NEXT\_TO\_CODE\` is as
follows:

With \`TABLES\_NEXT\_TO\_CODE\` on, the situation looks more like this:

The \`TABLES\_NEXT\_TO\_CODE\` optimisation removes the need for that
second dereference during the return, because the entry code is always
right next to the info table. However, it requires special support from
the backend for ensuring that data (i.e. the info table) and code are
contiguous in memory, so it cannot always be used.

Example 7: \`case\` expressions
-------------------------------

Let us now examine how \`case\` expressions are handled. Compiling the
following Haskell

Produces this C-- code

Notice that GHC has generated *two* functions:
\`Main\_casezuscrut\_entry\` and \`scj\_ret\` correspond to the code for
forcing the argument to the \`case\`, and for the *continuation* of the
\`case\` respectively. Let's pick them apart and see how they work!

### Forcing the scrutinee of the \`case\`

When we first call the \`case\_scrut\` function, its entry code begins
executing:

This is a function of arity 1 (i.e. with a single argument), so upon
entry the machine state looks like this:

Because this is a top level function, the closure is statically
allocated and contains no free variables. However, as discussed
previously, the single argument to the function is guaranteed to be
present at the top of the stack.

The code starts off by saving this argument (the \`x\`) temporarily into
\`R1\`:

The next thing the code does is overwrites this argument on the stack
with a pointer to the info-table of the continuation code. This is the
code that will be invoked after \`x\` has been evaluated into WHNF, and
which will do the test to decide whether to continue as the \`Nothing\`
or as the \`Just\` branch of the case:

As we saw earlier, any time that the STG machine decides that it has a
value in its hand, it will continue evaluation by tail-calling the entry
code found by dereferencing the info-table pointer at the top of the
stack. So by putting the address of our continuation in here, we ensure
that the entry code for \`scj\_info\` is executed after \`x\` becomes a
value.

Now, what we need to do is to start the evaluation of \`x\`. We could
just jump into \`x\`'s entry code and hope for the best, but thanks to
GHC's pointer tagging we can sometimes avoid doing this indirect branch.

So, instead, we test to see if the \`x\` pointer has a tag. If it is
tagged, then we know that it is already evaluated and hence jump
directly to the code for the continuation. If it is not tagged, we are
forced to make the jump into the entry code for \`x\`. This choice is
embodied by the following code:

Note the test \`R1 & 3 != 0\`: this reflects the fact that pointer tags
are stored in the lower 2 bits of the pointer on 32 bit machines.
Another interesting feature is how the \`jump\` instructions find the
entry code: again, we see a deference of the info pointer because
\`TABLES\_NEXT\_TO\_CODE\` is turned off.

As we saw, the \`case\` scrutinisation code ended with one of two things
happening: 1. A direct call into the continuation code \`scj\_ret\` if
the scrutinee was already evaluated 2. A call into the entry code for
the scrutinee, if the scrutinee was not evaluated (or it *was*
evaluated, but the pointer was somehow not tagged with that information)
- Because we pushed \`scj\_info\` onto the STG stack, control will
eventually return to \`scj\_ret\` after the evaluation of \`x\` has
finished

It is now time to examine the continuation code to see what happens
after \`x\` becomes a value.

### Dealing with the forced scrutinee

The continuation code is a little more complicated:

Whenever the STG machine evaluates to a value it will return the value
by jumping to the entry point at the top of the stack. In this case,
\`R1\` is guaranteed to be a (tagged) pointer to the thing that was just
evaluated. Because we are scrutinising a \`Maybe\` type (which has fewer
than 4 constructors) the code for the \`case\` continuation is able to
use the tag bits on the returned pointer to decide which of the two
branches to take:

If we were scrutinising a data type with more constructors, the tag bits
would only tell us that the thing was evaluated, not which constructor
it was evaluated to. In this case, we would have to read the constructor
tag by dereferencing \`R1\` and testing the resulting info table pointer
against all possibilities.

If the tag was greater than or equal to 2, we go to the \`ccv\` branch,
which deals with what happens if we had a \`Just\`. In this case, we
need to continue by forcing the thunk inside the \`Just\` and returning
that value to our caller, which is what these lines are doing:

To access the thing inside the \`Just\`, the code assumes that the
\`R1\` pointer is tagged with the 2 that indicates a \`Just\`
constructor, and hence finds the first free variable (stored 4 bytes
into the closure) using \`I32\[R1 + 2\]\`, which is then saved into
\`R1\`. It pops the address of \`scj\_info\` that was pushed onto the
stack in \`Main\_casezuscrut\_entry\` by moving \`Sp\` up 4 bytes
(remember that the STG stack grows downwards) and then untags and jumps
into the entry code for the \`R1\` thunk, using the same
double-dereference pattern discussed earlier.

There seems to be a small missed opportunity here: the code could check
the pointer tag on \`R1\`, and then return directly if it is set. I
imagine that this isn't being done in order to reduce possible code
bloat.

Example 8: thunks and thunk update
----------------------------------

You might be wondering how the \`x + 1\` thunk we saw allocated in a
previous section will behave when it is actually forced. To remind you,
the thunk we saw was constructed by the following Haskell code:

So how does the \`x + 1\` thunk work? An excellent question! Let's take
a look at the C-- for its entry code and find out:

The original Haskell code read \`x + 1\`, but GHC has inlined the actual
code for the addition operation on \`Int\`s, which looks something like:

The second pattern match (to get \`b\`) has been performed statically by
GHC, obtaining the machine literal 1, which shows up directly in the
generated code. Therefore, the code only need to evaluate and
case-decompose the unknown free variable \`x\` of our closure, to get
the \`a\` argument to \`plusInt\`.

### Thunk entry point

This evaluation is what is being done by the thunk entry code
\`slk\_entry\`. Ignoring the stack check, the C-- begins thusly:

Remembering that upon entry to the thunk entry code, \`R1\` points to
the thunk's closure, the new stack looks as follows:

The C-- statement \`R1 = I32\[R1 + 8\]\` is pulling out the pointer to
the free variable of the thunk (which was set up in
\`Main\_buildzudata\_entry\`) into \`R1\`.

Finally, the entry code evaluates that free variable (checking the tag
bits of the pointer first, as usual):

Because we put \`soN\_info\` at the top of the stack, when evaluation of
\`x\` is complete the STG machine will continue by executing the
\`soN\_ret\` code.

The most interesting feature of this code is the extra stuff that has
been pushed onto the stack below \`soN\_ret\`: an info pointer called
\`stg\_upd\_frame\_info\`, and a pointer to the thunk currently being
evaluated.

This is all part of the STG machine's thunk update mechanism. When the
\`soN\_ret\` continuation returns, it will transfer control *not* to the
code forcing the thunk, but to some code which overwrites the contents
of the current thunk closure with a closure representing an
"indirection". The entry code for such an indirection closure is
trivial: it immediately returns a pointer to the thing that was returned
from the \`soN\_ret\` continuation in \`R1\`.

These indirections are the mechanism which ensures that the STG machine
never repeats the work of evaluating a thunk more than once: after the
first evaluation, any code forcing the thunk jumps into the indirection
entry code rather than \`slk\_entry\`.

That being said, let us look at how the continuation responsible for
actually finding the value of \`x + 1\` works:

### Continuation of the thunk

Upon entry to the continuation code, we have the evaluated \`x\` in
\`R1\`: it now needs to do the addition and allocate a \`I\#\`
constructor to hold the result of the addition. Because of the
allocation, \`soN\_ret\` begins with a heap check. Ignoring that check,
we have the following code:

This is mostly standard stuff. Because the \`R1\` pointer is guaranteed
tagged, and there is only one possible constructor, the tag must be 1
and so the \`Int\#\` value inside the \`Int\` is pulled out using
\`I32\[R1 + 3\]\`. This is then put into a newly heap-allocated \`I\#\`
constructor, which is returned in \`R1\` after we pop the \`soN\_info\`
pointer from the stack.

The only interesting point is where we return to: rather than
dereference \`Sp\` to find the info pointer at the top of the STG stack,
GHC has generated code that takes advantage of the fact that the \`Sp\`
is guaranteed to point to \`stg\_upd\_frame\_info\`. This avoids one
pointer dereference.

Conclusion
----------

This document has left much of the detail of how STG is implemented out:
notable omissions include CAFs, and the precise behaviour of the garbage
collector. Nonetheless, my hope is that it has helped you to gain some
more insight into the weird and wonderful way the Haskell evaluation
model is implemented.

Support for generic programming
===============================

[PageOutline](PageOutline "wikilink")

GHC includes a new (in 2010) mechanism to let you write generic
functions. It is described in paper [A generic deriving mechanism for
Haskell](http://www.dreixel.net/research/pdf/gdmh_nocolor.pdf). This
page sketches the specifics of the implementation; we assume you have
read the paper. The [HaskellWiki
page](http://www.haskell.org/haskellwiki/Generics) gives a more general
overview.

This mechanism replaces the [previous generic classes
implementation](http://www.haskell.org/ghc/docs/6.12.2/html/users_guide/generic-classes.html).
What we describe until the "Kind polymorphic overhaul" section is
implemented and released in GHC 7.2.1.

Status
------

Use **Keyword** = \`Generics\` to ensure that a ticket ends up on this
auto-generated list

Open Tickets:
[patch|infoneeded,keywords=\~Generics)](TicketQuery(status=infoneeded,status=new "wikilink")

Closed Tickets:
[TicketQuery(status=infoneeded,status=closed,keywords=\~Generics)](TicketQuery(status=infoneeded,status=closed,keywords=~Generics) "wikilink")

Main components
---------------

`` *`TcDeriv.tcDeriving`nowallowsderiving`Generic`instances. ``

`` *Therepresentationtypesandcorefunctionalityofthelibraryliveon`GHC.Generics`(onthe`ghc-prim`package). ``

`` *Manynameshavebeenaddedasknownin`prelude/PrelNames` ``

`` *Mostofthecodegenerationishandledby`types/Generics` ``

Things that have been removed
-----------------------------

`*Allofthe`[`generic` `classes`
`stuff`](http://www.haskell.org/ghc/docs/6.12.2/html/users_guide/generic-classes.html)`.Inparticular,thefollowinghavebeenremoved:`\
`` *`hasGenerics`fieldfrom`TyCon`; ``\
`` *`HsNumTy`constructorfrom`HsType`; ``\
`` *`TypePat`constructorfrom`Pat`. ``

`` *The`-XGenerics`flagisnowdeprecated. ``

What already works
------------------

`` *`Generic`and`Generic1`instancescanbederivedwhen`-XDeriveGeneric`isenabled. ``

`` *The`default`keywordcanusedforgenericdefaultmethodsignatureswhen`-XDefaultSignatures`isenabled. ``

`*Genericdefaultsareproperlyinstantiatedwhengivinganinstancewithoutdefiningthegenericdefaultmethod.`

`` *Basetypeslike`[]`,`Maybe`,tuples,comewithGenericinstances. ``

Testing
-------

`` *Testsareavailableunderthe`generics`directoryofthetestsuite. ``

Kind polymorphic overhaul
=========================

With the new \`-XPolyKinds\` functionality we can make the support for
generic programming better typed. The basic idea is to define the
universe codes (\`M1\`, \`:+:\`, etc.) as constructors of a datatype.
Promotion then lifts these constructors to types, which we can use as
before, only that now we have them all classified under a new kind. The
overhaul of the main module is explained below; for easier comparison
with the current approach, names are kept the same whenever possible.

Generic representation universe
-------------------------------

\`m\` is the only real parameter here. \`f\` and \`x\` are there because
we can't write kinds directly, since \`Universe\` is also a datatype
(even if we're only interested in its promoted version). So we pass
\`f\` and \`x\` only to set them to \`\* -&gt; \*\` and \`\*\`,
respectively, in \`Interprt\`. \`m\` is different: it stands for the
kind of metadata representation types, and we really want to be
polymorphic over that, since each user datatype will introduce a new
metadata kind.

Universe interpretation
-----------------------

As promised, we set \`f\` to \`\* -&gt; \*\` and \`x\` to \`\*\`.
Unfortunately we don't have \[GhcKinds\#Explicitkindvariables explicit
kind variable annotations\] yet, so we cannot leave \`m\` polymorphic!
So this code doesn't compile:

### Names

As an aside, note that we have to come up with names like \`UU\` and
\`KK\` for the \`Universe\` even though we really just wanted to use
\`U1\` and \`K1\`, like before. Then we would have a type and a
constructor with the same name, but that's ok. However, \`Universe\`
defines both a type (with constructors) and a kind (with types). So if
we were to use \`U1\` in the \`Universe\` constructors, then we could no
longer use that name in the \`Interprt\` constructors. It's a bit
annoying, because we are never really interested in the type
\`Universe\` and its constructors: we're only interested in its promoted
variant. This is a slight annoyance of automatic promotion: when you
define a "singleton type" (like our GADT \`Interprt\` for \`Universe\`)
you cannot reuse the constructor names.

Metadata representation
-----------------------

 There's more of these, but they don't add any new concerns.

Conversion between user datatypes and generic representation
------------------------------------------------------------

We now get a more precise kind for \`Rep\`:

Example generic function: \`fmap\` (kind \`\* -&gt; \*\`)
---------------------------------------------------------

User-visible class, exported:

Defined by the generic programmer, not exported:

Note that previously \`Functor\` and \`GFunctor\` had exactly the same
types. Now we can make clear what the difference between them is.

Example generic function: \`show\` (kind \`\*\`, uses metadata)
---------------------------------------------------------------

User-visible class, exported:

Defined by the generic programmer, not exported:

The other cases do not add any further complexity.

Example datatype encoding: lists (derived by the compiler)
----------------------------------------------------------

Note that we use only one datatype; more correct would be to use 3, one
for \`DList\`, another for the constructors, and yet another for the
selectors (or maybe even n datatypes for the selectors, one for each
constructor?) But we don't do that because \`Universe\` is polymorphic
only over \`m\`, so a single metadata representation type. If we want a
more fine-grained distinction then we would need more parameters in
\`Universe\`, and also to split the \`MM\` case.

### Digression

Even better would be to index the metadata representation types over the
type they refer to. Something like: But now we are basically asking for
promotion of data families, since we want to use promoted \`DList\`.
Also, the case for \`MM\` in \`Universe\` would then be something like:
But I'm not entirely sure about this.

GHC 8.0 and later
-----------------

### Type-level metadata encoding

Because what we've described so far is rather backwards-incompatible, we
wanted to at least try to improve the encoding of metadata, which was
currently rather clunky prior to GHC 8.0 (giving rise to lots of empty,
compiler-generated datatypes and respective instances). We can
accomplished that by changing \`M1\` to keep the meta-information *at
the type level*:

Why did we need to add \`FixityI\`? Because \`Fixity\` does not promote.
Yet, we wanted to expose \`Fixity\` to the user, not \`FixityI\`. Note
that the meta-data classes remained mostly unchanged (aside from some
enhancements to
[Datatype](https://ghc.haskell.org/trac/ghc/ticket/10030) and
[Selector](https://ghc.haskell.org/trac/ghc/ticket/10716)):

But now, using the magic of singletons, we give *one single instance*
for each of these classes, instead of having to instantiate them each
time a user derives \`Generic\`:

Naturally, we require singletons for \`Bool\`, \`Maybe\`, \`FixityI\`,
\`Associativity\`, \`SourceUnpackedness\`, \`SourceStrictness\`, and
\`DecidedStrictness\`, but that is one time boilerplate code, and is not
visible for the user. (In particular, this is where we encode that the
demotion of (the kind) \`FixityI\` is (the type) \`Fixity\`.)

I believe this change is almost fully backwards-compatible, and lets us
simplify the code for \`deriving Generic\` in GHC. Furthermore, I
suspect it will be useful to writers of generic functions, who can now
match at the type-level on things such as whether a constructor is a
record or not.

I say "almost fully backwards-compatible" because handwritten
\`Generic\` instances might break with this change. But we've never
recommended doing this, and I think users who do this are more than
aware that they shouldn't rely on it working across different versions
of GHC.

#### Example

Before GHC 8.0, the following declaration:

Would have generated all of this:

But on GHC 8.0 and later, this is all that is generated (assuming it was
compiled with no strictness optimizations):

Not bad!

### Strictness

The \`Selector\` class now looks like this:

This design draws much inspiration from the way Template Haskell handles
strictness as of GHC 8.0 (see
[here](https://ghc.haskell.org/trac/ghc/ticket/10697) for what motivated
the change). We make a distinction between the *source* strictness
annotations and the strictness GHC actually *decides* during
compilation. To illustrate the difference, consider the following data
type:

If we were to encode the source unpackedness and strictness of each of
\`T\`'s fields, they were be \`SourceUnpack\`/\`SourceStrict\`,
\`NoSourceUnpackedness\`/\`SourceStrict\`, and
\`NoSourceUnpackedness\`/\`NoSourceStrictness\`, no matter what. Source
unpackedness/strictness is a purely syntactic property.

The strictness that the user writes, however, may be different from the
strictness that GHC decides during compilation. For instance, if we were
to compile \`T\` with no optimizations, the decided strictness of each
field would be \`DecidedStrict\`, \`DecidedStrict\`, and
\`DecidedLazy\`. If we enabled \`-O2\`, however, they would be
\`DecidedUnpack\`, \`DecidedStrict\`, and \`DecidedLazy\`.

Things become even more interesting when \`-XStrict\` and \`-O2\` are
enabled. Then the strictness that GHC would decided is
\`DecidedUnpack\`, \`DecidedStrict\`, and \`DecidedStrict\`. And if you
enable \`-XStrict\`, \`-O2\`, *and* \`-funbox-strict-fields\`, then the
decided strictness is \`DecidedUnpack\`, \`DecidedUnpack\`, and
\`DecidedUnpack\`.

The variety of possible \`DecidedStrictness\` combinations demonstrates
that strictness is more just annotation

Source Tree Layout
------------------

An overview of the source tree may be found \[wiki:Commentary/SourceTree
here\].

Build System Basics
-------------------

Detailed information about the build system may be found \[wiki:Building
here\]; what follows is a quick overview, highlighting the areas where
GHC's build system diverges substantially from the way is used in most
other projects.

Most projects keep the parts of their build machinery in files called
found in many/most subdirectories of the source tree. GHC uses the
filename instead; you'll find a file with this name in quite a number of
subdirectories.

Other build system files are in and .

Coding Style
------------

The \[wiki:WorkingConventions Coding style guidelines\] may be found on
the wiki.

The GHC Commentary: GHCi
========================

This isn't a coherent description of how GHCi works, sorry. What it is
(currently) is a dumping ground for various bits of info pertaining to
GHCi, which ought to be recorded somewhere.

Debugging the interpreter
-------------------------

The usual symptom is that some expression / program crashes when running
on the interpreter (commonly), or gets wierd results (rarely).
Unfortunately, finding out what the problem really is has proven to be
extremely difficult. In retrospect it may be argued a design flaw that
GHC's implementation of the STG execution mechanism provides only the
weakest of support for automated internal consistency checks. This makes
it hard to debug.

Execution failures in the interactive system can be due to problems with
the bytecode interpreter, problems with the bytecode generator, or
problems elsewhere. From the bugs seen so far, the bytecode generator is
often the culprit, with the interpreter usually being correct.

Here are some tips for tracking down interactive nonsense:

`*Findthesmallestsourcefragmentwhichcausestheproblem.`

`` *UsinganRTScompiledwith`-DDEBUG`,runwith`+RTS-Di`togetalistingingreatdetailfromtheinterpreter.Notethatthelistingissovoluminousthatthisisimpracticalunlessyouhavebeendiligentinthepreviousstep. ``

`*Atleastinprinciple,usingthetraceandabitofGDBpokingaroundatthetimeofdeath(Seealso[wiki:Debugging]),youcanfigureoutwhattheproblemis.Inpracticeyouquicklygetdepressedatthehopelessnessofevermakingsenseofthemassofdetails.Well,Ido,anyway.`

`` *`+RTS-Di`trieshardtoprintusefuldescriptionsofwhat'sonthestack,andoftensucceeds.However,ithasnowaytomapaddressestonamesincode/dataloadedbyourruntimelinker.SotheCfunction`ghci_enquire`isprovided.Givenanaddress,itsearchestheloadedsymboltablesforsymbolsclosetothataddress.YoucanrunitfrominsideGDB: ``

`` Inthiscasetheenquired-aboutaddressis`PrelBase_ZMZN_static_entry`.Ifnosymbolsareclosetothegivenaddr,nothingisprinted.Notagreatmechanism,butbetterthannothing. ``

`` *Wehavehadvariousproblemsinthepastduetothebytecodegenerator(compiler/ghci/ByteCodeGen.lhs)beingconfusedaboutthetruesetoffreevariablesofanexpression.Thecompilationschemefor`let`sappliestheBCOfortheRHSofthe`let`toitsfreevariables,soifthefree-varannotationiswrongormisleading,youendupwithcodewhichhaswrongstackoffsets,whichisusuallyfatal. ``

`*Followingthetracesisoftenproblematicbecauseexecutionhopsbackandforthbetweentheinterpreter,whichistraced,andcompiledcode,whichyoucan'tsee.ParticularlyannoyingiswhenthestacklooksOKintheinterpreter,thencompiledcoderunsforawhile,andlaterwearrivebackintheinterpreter,withthestackcorrupted,andusuallyinacompletelydifferentplacefromwhereweleftoff.`

`Ifthisisbitingyoubaaaad,itmaybeworthcopyingsourcesforthecompiledfunctionscausingtheproblem,intoyourinterpretedmodule,inthehopethatyoustayintheinterpretermoreofthetime.`

`` *Therearevariouscommented-outpiecesofcodeinInterpreter.cwhichcanbeusedtogetthestacksanity-checkedaftereveryentry,andevenafteraftereverybytecodeinstructionexecuted.Notethatsomebytecodes(`PUSH_UBX`)leavethestackinanunwalkablestate,sothe`do_print_stack`localvariableisusedtosuppressthestackwalkafterthem. ``

Useful stuff to know about the interpreter
------------------------------------------

The code generation scheme is straightforward (naive, in fact).
\`-ddump-bcos\` prints each BCO along with the Core it was generated
from, which is very handy.

`` *Simple`let`sarecompiledin-line.Forthegeneralcase,`letv=Ein...`,theexpression`E`iscompiledintoanewBCOwhichtakesasargsitsfreevariables,and`v`isboundto`AP(thenewBCO,freevarsofE)`. ``

`` *`case`sasusual,become:pushthereturncontinuation,enterthescrutinee.Thereissomemagictomakeallcombinationsofcompiled/interpretedcallsandreturnswork,describedbelow.Intheinterpretedcase,all`case`altsarecompiledintoasinglebigreturnBCO,whichcommenceswithinstructionsimplementingaswitchtree. ``

### Stack management

There isn't any attempt to stub the stack, minimise its growth, or
generally remove unused pointers ahead of time. This is really due to
laziness on my part, although it does have the minor advantage that
doing something cleverer would almost certainly increase the number of
bytecodes that would have to be executed. Of course we \`SLIDE\` out
redundant stuff, to get the stack back to the sequel depth, before
returning a HNF, but that's all. As usual this is probably a cause of
major space leaks.

### Building constructors

Constructors are built on the stack and then dumped into the heap with a
single \`PACK\` instruction, which simply copies the top N words of the
stack verbatim into the heap, adds an info table, and zaps N words from
the stack. The constructor args are pushed onto the stack one at a time.
One upshot of this is that unboxed values get pushed untaggedly onto the
stack (via \`PUSH\_UBX\`), because that's how they will be in the heap.
That in turn means that the stack is not always walkable at arbitrary
points in BCO execution, although naturally it is whenever GC might
occur.

Function closures created by the interpreter use the AP-node (tagged)
format, so although their fields are similarly constructed on the stack,
there is never a stack walkability problem.

### Perspective

I designed the bytecode mechanism with the experience of both STG hugs
and Classic Hugs in mind. The latter has an small set of bytecodes, a
small interpreter loop, and runs amazingly fast considering the cruddy
code it has to interpret. The former had a large interpretative loop
with many different opcodes, including multiple minor variants of the
same thing, which made it difficult to optimise and maintain, yet it
performed more or less comparably with Classic Hugs.

My design aims were therefore to minimise the interpreter's complexity
whilst maximising performance. This means reducing the number of opcodes
implemented, whilst reducing the number of insns despatched. In
particular, very few (TODO: How many? Which?) opcodes which deal with
tags. STG Hugs had dozens of opcodes for dealing with tagged data.
Finally, the number of insns executed is reduced a little by merging
multiple pushes, giving \`PUSH\_LL\` and \`PUSH\_LLL\`. These opcode
pairings were determined by using the opcode-pair frequency profiling
stuff which is ifdef-d out in Interpreter.c. These significantly improve
performance without having much effect on the ugliness or complexity of
the interpreter.

Overall, the interpreter design is something which turned out well, and
I was pleased with it. Unfortunately I cannot say the same of the
bytecode generator.

case returns between interpreted and compiled code
--------------------------------------------------

Variants of the following scheme have been drifting around in GHC RTS
documentation for several years. Since what follows is actually what is
implemented, I guess it supersedes all other documentation. Beware; the
following may make your brain melt. In all the pictures below, the stack
grows downwards.

### Returning to interpreted code.

Interpreted returns employ a set of polymorphic return infotables. Each
element in the set corresponds to one of the possible return registers
(R1, D1, F1) that compiled code will place the returned value in. In
fact this is a bit misleading, since R1 can be used to return either a
pointer or an int, and we need to distinguish these cases. So, supposing
the set of return registers is {R1p, R1n, D1, F1}, there would be four
corresponding infotables, stg\_ctoi\_ret\_R1p\_info, etc. In the
pictures below we call them stg\_ctoi\_ret\_REP\_info.

These return itbls are polymorphic, meaning that all 8 vectored return
codes and the direct return code are identical.

Before the scrutinee is entered, the stack is arranged like this:

On entry, the interpreted contination BCO expects the stack to look like
this:

A machine code return will park the returned value in R1/F1/D1, and
enter the itbl on the top of the stack. Since it's our magic itbl, this
pushes the returned value onto the stack, which is where the interpreter
expects to find it. It then pushes the BCO (again) and yields. The
scheduler removes the BCO from the top, and enters it, so that the
continuation is interpreted with the stack as shown above.

An interpreted return will create the value to return at the top of the
stack. It then examines the return itbl, which must be immediately
underneath the return value, to see if it is one of the magic
stg\_ctoi\_ret\_REP\_info set. Since this is so, it knows it is
returning to an interpreted contination. It therefore simply enters the
BCO which it assumes it immediately underneath the itbl on the stack.

### Returning to compiled code.

Before the scrutinee is entered, the stack is arranged like this:

The scrutinee value is then entered. The case continuation(s) expect the
stack to look the same, with the returned HNF in a suitable return
register, R1, D1, F1 etc.

A machine code return knows whether it is doing a vectored or direct
return, and, if the former, which vector element it is. So, for a direct
return we jump to \`Sp\[0\]\`, and for a vectored return, jump to
\`((CodePtr\*)(Sp\[0\]))\[ - ITBL\_LENGTH - vector number \]\`. This is
(of course) the scheme that compiled code has been using all along.

An interpreted return will, as described just above, have examined the
itbl immediately beneath the return value it has just pushed, and found
it not to be one of the ret\_REP\_ctoi\_info set, so it knows this must
be a return to machine code. It needs to pop the return value, currently
on the stack, into R1/F1/D1, and jump through the info table.
Unfortunately the first part cannot be accomplished directly since we
are not in Haskellised-C world.

We therefore employ a second family of magic infotables, indexed, like
the first, on the return representation, and therefore with names of the
form stg\_itoc\_ret\_REP\_info. (Note: itoc; the previous bunch were
ctoi). This is pushed onto the stack (note, tagged values have their tag
zapped), giving:

We then return to the scheduler, asking it to enter the itbl at t.o.s.
When entered, stg\_itoc\_ret\_REP\_info removes itself from the stack,
pops the return value into the relevant return register, and returns to
the itbl to which we were trying to return in the first place.

Amazingly enough, this stuff all actually works! Well, mostly ...

Unboxed tuples: a Right Royal Spanner In The Works
--------------------------------------------------

The above scheme depends crucially on having magic infotables
stg\_{itoc,ctoi}\_ret\_REP\_info for each return representation REP. It
unfortunately fails miserably in the face of unboxed tuple returns,
because the set of required tables would be infinite; this despite the
fact that for any given unboxed tuple return type, the scheme could be
made to work fine.

This is a serious problem, because it prevents interpreted code from
doing IO-typed returns, since IO t is implemented as \`(\# t,
RealWorld\# \#)\` or thereabouts. This restriction in turn rules out FFI
stuff in the interpreter. Not good.

Although we have no way to make general unboxed tuples work, we can at
least make IO-types work using the following ultra-kludgey observation:
\`RealWorld\#\` doesn't really exist and so has zero size, in compiled
code. In turn this means that a type of the form \`(\# t, RealWorld\#
\#)\` has the same representation as plain t does. So the bytecode
generator, whilst rejecting code with general unboxed tuple returns,
recognises and accepts this special case. Which means that IO-typed
stuff works in the interpreter. Just.

If anyone asks, I will claim I was out of radio contact, on a 6-month
walking holiday to the south pole, at the time this was ... er ...
dreamt up.

Porting GHC using LLVM backend
==============================

This document is kind of short porting roadmap which serves as a
high-level overview for porters of GHC who decided to use LLVM instead
of implementing new NCG for their target platform. Please have
\[wiki:Commentary/Compiler/Backends/LLVM/Design Design &
Implementation\] at hand since this contains more in-depth information.
The list of steps needed for new GHC/LLVM port is:

**(1)** Make sure GHC unregisterised build is working on your target
platform (using the C backend). This guide isn't intended for porting
GHC to a completely unsupported platform. If the platform in question
doesn't have a GHC unregisterised build then follow the
\[wiki:Building/Porting GHC Porting Guide\] first.

**(2)** Now try to compile some very simple programs such as 'hello
world' or simpler using the GHC you just built. Try with the C backend
First to make sure everything is working. Then try with the LLVM
backend. If the llvm backend built programs are failing find out why.
This is done using a combination of things such as the error message you
get when the program fails, \[wiki:Debugging/CompiledCode tracing the
execution with GDB\] and also just comparing the assembly code produced
by the C backend to what LLVM produces. This last method is often the
easiest and you can occasionally use techniques like doing doing a
'binary search' for the bug by merging the assembly produced by the C
backend and LLVM backend.

**(3)** When the programs you throw at the LLVM backend are running, try
running the GHC testsuite. First run it against the C backend to get a
baseline, then run it against the LLVM backend. Fix any failures that
are LLVM backend specific.

**(4)** If the testsuite is passing, now try to build GHC itself using
the LLVM backend. This is a very tough test. When working though its a
good proof that the LLVM backend is working well on your platform.

**(5)** Now you have LLVM working in unregistered mode, so the next
thing is to implement the GHC calling convention in LLVM that is used by
GHC's LLVM backend. This should then allow you to get the LLVM backend
working in registered mode but with (TABLES\_NEXT\_TO\_CODE = NO in your
build.mk). Majority of this step involves hacking inside the LLVM code.
Usually lib/Target/<your target platform name> is the best way to start.
Also you might study what David Terei did for [x86
support](http://lists.cs.uiuc.edu/pipermail/llvmdev/2010-March/030031.html)
and his [patch
itself](http://lists.cs.uiuc.edu/pipermail/llvmdev/attachments/20100307/714e5c37/attachment-0001.obj)
to get an idea what's really needed.

**(6)** Once **(5)** is working you have it all running except
TABLES\_NEXT\_TO\_CODE. So change that to Yes in your build.mk and get
that working. This will probably involve changing the mangler used by
LLVM to work on the platform you are targeting.

Registerised Mode
-----------------

Here is an expanded version of what needs to be done in step 5 and 6 to
get a registerised port of LLVM working:

1\. GHC in registerised mode stores some of its virtual registers in real
hardware registers for performance. You will need to decide on a mapping
of GHC's virtual registers to hardware registers. So how many registers
you want to map and which virtual registers to store and where. GHC's
design for this on X86 is basically to use as many hardware registers as
it can and to store the more frequently cessed virtual registers like
the stack pointer in callee saved registers rather than caller saved
registers. You can find the mappings that GHC currently uses for
supported architectures in 'includes/stg/MachRegs.h'.

2\. You will need to implement a custom calling convention for LLVM for
your platform that supports passing arguments using the register map you
decided on. You can see the calling convention I have created for X86 in
the llvm source file 'lib/Target/X86/X86CallingConvention.td'.

3\. Get GHC's build system running on your platform in registerised mode.

4\. Add new inline assembly code for your platform to ghc's RTS. See
files like 'rts/StgCRun.c' that include assembly code for the
architectures GHC supports. This is the main place as its where the
boundary between the RTS and haskell code is but I'm sure there are
definitely other places that will need to be changed. Just grep the
source code to find existing assembly and add code for your platform
appropriately.

5\. Will need to change a few things in LLVM code gen.

5.1 'compiler/llvmGen/LlvmCodeGen/Ppr.hs' defines a platform specific
string that is included in all generated llvm code. Add one for your
platform. This string specifies the datalayout parameters for the
platform (e.g pointer size, word size..). If you don't include one llvm
should still work but wont optimise as aggressively.

5.2 'compiler/llvmGen/LlvmCodeGen/CodeGen.hs' has some platform specific
code on how write barriers should be handled.

6\. Probably some stuff elsewhere in ghc that needs to be changed (most
likely in the main/ subfolder which is where most the compiler driver
lives or in codegen/ which is the Cmm code generator).

7\. This is just what I know needs to be done, I'm sure there is many
small pieces missing although they should all fall into one of the above
categories. In the end just trial and error your way to success.

[PageOutline](PageOutline "wikilink")

Packages in GHC
===============

This page summarises our current proposal for packages in GHC. (See also
\[wiki:Commentary/Packages/PackageNamespacesProposal an extended
proposal\] to make namespaces first-class. The two proposals are
mutually exclusive.)

The problem
-----------

A vexed question in the current design of Haskell is the issue of
whether a single program can contain two modules with the same name. In
Haskell 98 that is absolutely ruled out. As a result, packages are
fundamentally non-modular: to avoid collisions *every* module in *every*
package written by *anyone* must have different module names. That's
like saying that every function must have different local variables, and
is a serious loss of modularity.

GHC 6.6 makes a significant step forward by lifting this restriction.
However it leaves an open question, which is what this page is about.

Assumptions
-----------

Before we start, note that we take for granted the following

`*`**`Each` `package` `has` `a` `globally-unique`
`name`**`,organisedbysomesocialprocess.ThisassumptionisdeeplybuiltintoCabal,andlotsofthingswouldneedtochangeifitwasn'tmet.`

`*`**`Module` `names` `describe` *`purpose`* `(what` `it's` `for,`
`e.g.` `),` `whereas` `package` `names` `describe` *`provenance`*
`(where` `it` `comes` `from,` `e.g.`
`)`**`.Weshouldnotmixthesetwoup,andthatisagoodreasonfornotcombiningpackageandmodulenamesintoasinglegrandname.Onequitefrequentlywantstogloballychangeprovenancebutnotpurpose(e.g.compilemyprogramwithanewversionofpackage"foo"),withoutrunningthroughallthesourcefilestochangetheimportstatements.`

`*`**`New:` `a` `module` `name` `must` `be` `unique` `within` `its`
`package`
`(only)`**`.Thatis,asingleprogramcanusetwomoduleswiththesamemodulename,providedtheycomefromdifferentpackages.ThisisnewinGHC6.6.`

For all this to work, GHC must incorporate the package name (and
version) into the names of entities the package defines. That means that
when compiling a module M you must say what package it is part of: Then
C.o will contain symbols like "" etc. In effect, the "original name" of
a function in module of package is .

The open question
-----------------

The remaining question is this: **When you say , from what package does
A.B.C come?**. Three alternatives are under consideration:

`*PlanA(GHC'scurrentstory)`\
`*PlanB:grafting.AnenhancementofplanA;see[wiki:Commentary/Packages/PackageMountingProposalFrederikEaton'sproposal]`\
`*PlanC:optionallyspecifythepackageintheimport.Analternativeto(B),describedina[wiki:Commentary/Packages/PackageImportsProposalseparatepage].`

------------------------------------------------------------------------

Plan A: GHC's current story
---------------------------

GHC already has a fairly elaborate scheme (perhaps too elaborate;
[documentation
here](http://www.haskell.org/ghc/dist/current/docs/users_guide/packages.html))
for deciding what package you mean when you say "import A.B.C":

`*Forastart,itonlylooksin`*`installed`*`packages.`\
`*Evenforinstalledpackages,thepackagemayormaynotbe`*`exposed`*`bydefault(reasoning:youmaywantoldversionsofpackageXtobeinstalled,butnotinscopebydefault).`\
`*Then,youcanusethe``flagtohideanotherwise-exposedpackage,andthe``flagtoexposeanotherwise-hiddenpackage.`

So, you can expose package P1 when compiling module M (say), and expose
P2 when compiling module N by manipulating these flags. Then M and N
could both import module A.B.C, which would come from P1 and P2
respectively. But:

`*WhatifyouwantedtoimportA.B.CfromP1andA.B.CfromP2intothe`*`same`*`module?`\
`*Whatifyouwanttoonlyreplace`*`parts`*`ofP1(e.g.,youwanttouseanupdatedversionofamodulein``)?`\
`*Compilingdifferentmoduleswithdifferentflagsinawaythataffectsthe`*`semantics`*`(ratherthan,say,theoptimisationlevel)seemsundesirable.`\
`*Tosupport``inthissituationwe'dneedtoallow``flagsintheper-module``pragmas,whichisn'tcurrentlysupported.(``alreadygathersthoseoptionstogetherforthelinkstep.)`*`This`
`is` `not` `yet` `implemented,` `but` `it` `is` `close` `to` `being`
`implemented.`*

If we did implement the "\`-package\` in \`OPTIONS\` pragma" fix, then
is is not clear how pressing the need is for anything more. It's still
impossible to import M from P1, and M from P2, into the same module. But
how often will that happen?

------------------------------------------------------------------------

Plan B: package mounting
------------------------

This proposal is described by a
\[wiki:Commentary/Packages/PackageMountingProposal separate page\].

------------------------------------------------------------------------

Plan C: mention the package in the import
-----------------------------------------

This proposal is described by a
\[wiki:Commentary/Packages/PackageImportsProposal separate page\].

This wiki discusses how bringing [Nix](https://nixos.org/nix/)-style
package management facilities to cabal can solve various cabal problems
and help in effective mitigation of cabal hell. It also contains the
goals and implementation plan for the GSoC project. It is based on a
[blog post by Duncan
Coutts](http://www.well-typed.com/blog/2015/01/how-we-might-abolish-cabal-hell-part-2/).

Problems
========

Breaking re-installations
-------------------------

[Image(http://www.well-typed.com/blog/aux/images/cabal-hell/install-example1.png)](Image(http://www.well-typed.com/blog/aux/images/cabal-hell/install-example1.png) "wikilink")

There are situations where Cabal's chosen solution would involve
reinstalling an existing version of a package but built with different
dependencies. In this example, after installing app-1.1, app-1.0 and
other-0.1 will be broken. The root of the problem is having to delete or
mutate package instances when installing new packages. This is due to
the limitation of only being able to have one instance of a package
version installed at once.

Type errors when using packages together
----------------------------------------

[Image(http://www.well-typed.com/blog/aux/images/cabal-hell/install-example2.png)](Image(http://www.well-typed.com/blog/aux/images/cabal-hell/install-example2.png) "wikilink")

The second, orthogonal, problem is that it is possible to install two
packages and then load them both in GHCi and find that you get type
errors when composing things defined in the two different packages.
Effectively you cannot use these two particular installed packages
together. The fundamental problem is that developers expect to be able
to use combinations of their installed packages together, but the
package tools do not enforce consistency of the developer's environment.

Goals
=====

-   Fix breaking re-installs (Persistent package store)
-   Implement garbage collection to free unreachable packages
-   Enable sharing of packages between sandbox
-   Enforce development environment consistency (Giving error earlier
    and better)
-   Implement package manager tools in cabal-install(cabal upgrade and
    cabal remove)

Implementation Plan
===================

Persistent package store
------------------------

A
[patch](https://github.com/ghc/ghc/commit/dd3a7245d4d557b9e19bfa53b0fb2733c6fd4f88)
has been pushed for ghc-7.11 that allows multiple instance of the same
package to be installed. So the remaining work is in cabal tool for
never modifying installed packages. I have written a [patch to make
cabal
(almost)non-destructive](https://github.com/fugyk/cabal/commit/45ec5edbaada1fd063c67d6109e69efa0e732e6a).
This patch makes all the changes to Package database non-destructive if
Installed Package ID is different. To make it fully non-mutable, Thomas
Tuegel suggested to

-   change installed package IDs to be computed by hash of the
    sdist tarball.
-   Enforce that a package is never overwritten by taking out a lock
    when updating the database.(before building the package)

It will have additional benefit that package will not be built again if
same source has already been built earlier, thus saving time.

Views
-----

Views will the subset of packages of package store whose modules can be
imported. Views will be present as various \*.view file in
<Package DB location>/views like default.view. The view file contains
list of installed package IDs. There will exist a default view which
contains packages installed by cabal install outside sandbox. If a
package name is installed two times, default view will contain the
instance of package which is installed at last. Views' packages will
also act like GC roots.

To facilitate views, ghc-pkg will need some new commands:

-   Create a view / Symlink a view
-   Delete a view
-   List all views
-   Modify a view
-   Add a package
-   Remove a package

Sandbox will be a view. Cabal needs to set view when using sandbox. It
also needs the ability to make a view and also add a package to the
view. Packages can be shared between views. View path will be passed to
ghc using -package-env. The view file that sandbox creates lies in the
same directory and is symlinked from the package database view file for
allowing GC. It will have a benefit that when we just delete the sandbox
directory without deleting the view, GC can free that sandbox package.

It looks similar to nix development environment but has some
differences. nix environments are like everything that is visible. It is
kind of like imported packages with dependency closure. nix needs to
make directories visible, while here we already have one more layer.
Here we only need exposed package and ghc can make complete environment
already. Views are just exposed packages of the environment. So,
dependency of the package need not be in the view that the package is
in. The problem that we are trying to solve with views is consistent way
of managing packages and sandboxes, allowing packages to be shared
between sandboxes and its packages being used as GC root.

Summary of design details

When installing a package outside sandbox

-   Package is added to default view / modified in default view

Making a sandbox

-   A view is made in the directory
-   The new view is symlinked from the database
-   Packages that are installed are added to that view. Sandboxes cannot
    affect any other things outside sandbox.

Within sandbox

-   All the cabal commands pass view name to ghc and ghc will use
    relevant package

Consistent developer environment
--------------------------------

It will require additional constraint to check that there is no other
instance of the same package or its dependencies is in the
environment(packages from which we have imported the modules with their
dependency closure) when we are importing the module from a package. It
also needs to be checked when cabal is configuring the package, that a
package do not directly or indirectly depends on two version of same
package. If it is violated it needs to give out an error.

Garbage collection
------------------

This will firstly involve determining the root packages and package
list. Root packages are the packages which are in some view. Then we
find list of all packages in the database. As there will be single
database after implementing views, we don't need to call it for every
sandbox database. Then we need to do mark-sweep and find which package
are not in the reachable package list and select it for garbage
collection. Then the selected packages will be deleted from the package
store and also unregistered from database with ghc-pkg.

cabal remove
------------

With everything implemented above, it is just removing a package from
default view. If package is unreachable it can be freed from disk by GC.
It is guaranteed to not break any package except the package that is
removed.

cabal upgrade
-------------

cabal upgrade is just installing every package that is present in
default view that has update available.

Current Status
--------------

It is possible to install multiple instances of the same package version
with my forks of cabal and ghc. Quite a few problems remain.

See also \[wiki:Commentary/Packages/MultiInstances\]

### Unique Install Location

When specifying the install location there is a new variable \$unique
available. It is resolved to a random number by cabal-install during
configuring. The default libsubdir for cabal-install should be
"\$packageid-\$unique" for example "mtl-2.1.2-1222131559". Cabal the
library does not understand \$unique so multiple instances of the same
package version installed via "runhaskell Setup.hs install" are still
problematic.

### ghc-pkg

ghc-pkg never removes registered packages when registering a new one.
Even if a new package with the same \`InstalledPackageId\` as an
existing package is registered. Or if a new package that points to the
same install directory is registered. \`ghc-pkg\` should probably check
this and issue a warning.

### Adhoc dependency resolution

A new field \`timestamp\` was added to \`InstalledPackageInfo\`. It is
set by Cabal the library when registering a package. It is used by Cabal
the library, GHC and cabal-install to choose between different instances
of the same package version.

### Detect whether an overwrite happens and warn about it

Currently cabal-install still warns about dangerous reinstalls and
requires \`--force-reinstalls\` when it is sure a reinstall would
happen. The correct behaviour here would be to detect if a reinstall
causes overwriting (because of a version of ghc-pkg that does this) and
warn only in this case. In this implementation reinstalls are not
dangerous anymore.

### Communicate the \`InstalledPackageId\` back to cabal-install

An \`InstallPlan\` contains installed packages as well as packages to be
installed and dependencies between those. We want to specify all of
these dependencies with an \`InstalledPackageId\`. Unfortunately the
\`InstalledPackageId\` is determined after installation and therefore
not available for not yet installed packages. After installation it
would have to be somehow communicated back to cabal-install. The current
workaround is to only specify those packages that were previously
installed with an \`InstalledPackageId\` and trust on Cabal picking the
instance that was most recently (during execution of this install plan)
installed for the other ones.

### Garbage Collection

A garbage collection should offer the removal of a certain package
specified by \`InstalledPackageId\`, the removal of broken packages and
the removal of probably unnecessary packages. A package is unnecessary
if all packages that depend on it are unnecessary (this includes the
case that no package depends on it) and it is not the most recently
installed instance for its version. All of this should be accompanied by
a lot of "are you sure" questioning.

### About Shadowing

GHC has the concept of shadowing. It was introduced as far as i
understand (correct me please) because when combining the global and the
user package databases you could end up with two instances of the same
package version. The instance in the user database was supposed to
shadow the one in the global database. Now that there are multiple
instances of the same package version even in one package database this
concepts needs to be rethought. This is non-trivial because flags asking
for a package version as well as flags requiring a certain instance need
to be taken into account.

### About Unique Identifier

Currently a big random number is created by cabal-install during
configuration and passed to Cabal to be appended to the
\`InstalledPackageId\` before registering. The reason is that the
\`InstalledPackageId\` still contains the ABI hash which is only known
after compilation. I personally would like the \`InstalledPackageId\` to
be the name of the package, the version and a big random number. This
could be determined before compilation, used as the \`libsubdir\` and
baked into \`package\_Paths.hs\`. Since it would be determined by
cabal-install it would also make communicating the InstalledPackageId
back to cabal-install after an installation unnecessary. The problem is
that the \`InstalledPackageId\` would not be deterministic anymore.

Original Plan
-------------

Cabal and GHC do not support multiple instances of the same package
version installed at the same time. If a second instance of a package
version is installed it is overwritten on the file system as well as in
the \`PackageDB\`. This causes packages that depended upon the
overwritten instance to break. The idea is to never overwrite an
installed package. As already discussed in
[4](http://hackage.haskell.org/trac/ghc/wiki/Commentary/Packages/MultiInstances)
the following changes need to be made:

`*Cabalshouldinstallpackagestoalocationthatdoesnotjustdependonnameandversion,`\
`` *`ghc-pkg`shouldalwaysaddinstancestothe`PackageDB`andneveroverwritethem, ``\
`` *`ghc--make`,`ghci`,andtheconfigurephaseofCabalshouldselectsuitableinstancesaccordingtosomeruleofthumb(similartothecurrentresolutiontechnique), ``\
`*wewanttobeabletomakemorefine-graineddistinctionsbetweenpackageinstancesthancurrentlypossible,forexamplebydistinguishingdifferentbuildflavoursor"ways"(profiling,etc.)`\
`` *`cabal-install`shouldstillfindan`InstallPlan`,andstillavoidunnecessarilyrebuildingpackageswheneveritmakessense ``\
`*someformofgarbagecollectionshouldbeofferedtohaveachancetoreducetheamountofinstalledpackages`

Hashes and identifiers
----------------------

There are three identifiers:

`` *`XXXX`:theidentifierappendedtotheinstallationdirectorysothatinstalledpackagesdonotclashwitheachother ``\
`` *`YYYY`:the`InstalledPackageId`,whichisanidentifierusedtouniquelyidentifyapackageinthepackagedatabase. ``\
`` *`ZZZZ`:theABIhashderivedbyGHCaftercompilingthepackage ``

The current situation:

`` *`XXXX`:isempty,whichisbad(twoinstancesofapackageinstallinthesameplace) ``\
`` *`YYYY`:iscurrentlyequalto`ZZZZ`,whichisbadbecauseweneedtomakemoredistinctions: ``\
`*weneedtodistinguishbetweentwopackagesthathaveidenticalABIsbutdifferentbehaviour(e.g.abugwasfixed)`\
`*weneedtodistinguishbetweentwoinstancesofapackagethatarecompiledagainstdifferentdependencies,orwithdifferentoptions,orcompiledinadifferentway(profiling,dynamic)`

Some notes:

`` *`XXXX`mustbedecided ``*`before`*`` webegincompiling,becausewehavetogeneratethe`Paths_P.hs`filethatiscompiledalongwiththepackage,whereas`ZZZZ`isonlyavailable ``*`after`*`wehavecompiledthepackage.`\
`` *`ZZZZ`isnotuniquelydeterminedbythecompilationinputs(see#4012),althoughinthefuturewehopeitwillbe ``\
`` *Itisdesirablethatwhentwopackageshaveidentical`YYYY`values,thentheyarecompatible,eveniftheywerebuiltonseparatesystems.Notethatthisisnotguaranteedevenif`YYYY`isadeterministicfunctionofthecompilationinputs,because`ZZZZ`isnon-deterministic(previouspoint).Hence`YYYY`mustbedependenton`ZZZZ`. ``\
`` *Itisdesirablethat`YYYY`beasdeterministicaspossible,i.e.wewouldrathernotuseaGUID,but`YYYY`shouldbedeterminedbythecompilationinputsand`ZZZZ`.Weknowthat`ZZZZ`iscurrentlynotdeterministic,butinthefutureitwillbe,andatthatpoint`YYYY`willbecomedeterministictoo,inthemeantime`YYYY`shouldbenolessdeterministicthan`ZZZZ`. ``

Our proposal:

`*Wedefineanew`*`Cabal`
`Hash`*`` thathashesthecompilationinputs(the`LocalBuildInfo`andthecontentsofthesourcefiles) ``\
`` *`XXXX`isaGUID. ``\
`*Whynotusethe`*`Cabal`
`Hash`*`?Wecould,butthentherecouldconceivablybeaclash.(Andres-pleaseexpandthispoint,Ihaveforgottenthefullrationale).`\
`` *`YYYY`isthecombinationofthe ``*`Cabal`
`Hash`*`` and`ZZZZ`(concatenated) ``\
`` *`ZZZZ`isrecordedinthepackagedatabaseasanewfield`abi-hash`. ``\
`` *Whentwopackageshaveidentical`ZZZZ`sthentheyareinterface-compatible,andtheusermightinthefuturewanttochangeaparticulardependencytouseadifferentpackagebutthethesame`ZZZZ`.Wedonotwanttomakethischangeautomatically,becauseevenwhentwopackageshaveidentical`ZZZZ`s,theymayhavedifferentbehaviour(e.g.bugfixes). ``

Install location of installed Cabal packages
--------------------------------------------

Currently the library part of packages is installed to
\`\$prefix/lib/\$pkgid/\$compiler\`. For example the \`GLUT\` package of
version 2.3.0.0 when compiled with GHC 7.4.1 when installed globally
lands in \`/usr/local/lib/GLUT-2.3.0.0/ghc-7.4.1/\`. This is the default
path. It is completely customizable by the user. In order to allow
multiple instances of this package to coexist we need to change the
install location to a path that is unique for each instance. Several
ways to accomplish this have been discussed:

### Hash

Use a hash to uniquely identify package instances and make the hash part
of both the InstalledPackageId and the installation path.

The ABI hash currently being used by GHC is not suitable for unique
identification of a package, because it is nondeterministic and not
necessarily unique. In contrast, the proposed Cabal hash should be based
on all the information needed to build a package.

This approach requires that we know the hash prior to building the
package, because there is a data directory (per default under
\$prefix/share/\$pkgid/) that is baked into Paths\_foo.hs in preparation
of the build process.

### Unique number

Use a unique number as part of the installation path.

A unique number could be the number of packages installed, or the number
of instances of this package version already installed, or a random
number. It is important that the numbers are guaranteed to be unique
system-wide, so the counter-based approaches are somewhat tricky.

The advantage over using a hash is that this approach should be very
simple to implement. On the other hand, identifying installed packages
(see below) could possibly become more difficult, and migrating packages
to other systems is only possible if the chance of collisions is
reasonably low (for example, if random numbers are being used).

`1.Theuniquenumberisalsopartoftheinstalledpackageid.`

`2.Wecanuseanotheruniqueidentifier(forexample,aCabalhash)toidentifyinstalledpackages.Inthiscase,thatidentifierwouldbeallowedtodependontheoutputofapackagebuild.`

\`ghc-pkg\`
-----------

\`ghc-pkg\` currently identifies each package by means of an
\`InstalledPackageId\`. At the moment, this id has to be unique per
package DB and is thereby limiting the amount of package instances that
can be installed in a single package DB at one point in time.

In the future, we want the \`InstalledPackageId\` to still uniquely
identify installed packages, but in addition to be unique among all
package instances that could possibly be installed on a system. There's
still the option that one InstalledPackageId occurs in several package
DBs at the same time, but in this case, the associated packages should
really be completely interchangeable. \[If we want to be strict about
this, we'd have to include the ABI hash in the \`InstalledPackageId\`.\]

Even though, as discussed above, the ABI hash is not suitable for use as
the \`InstalledPackageId\` given these changed requirements, we will
need to keep the ABI hash as an essential piece of information for ghc
itself.

\`ghc-pkg\` is responsible for storing all information we have about
installed packages. Depending on design decisions about the solver and
the Cabal hash, further information may be required in \`ghc-pkg\`'s
description format (see below).

The following fields will be added to the description format:

A field *Way* of type \`\[String\]\`. It tracks the way in which the
package was compiled. It is a subset of \`{v,d,p}\`. "v" means vanilla,
"d" means dynamic linking and "p" means profiling. Other ways may be
added later.

A \`timestamp\` of the time when the package was installed (or built?).
It is used by GHC and Cabal to put a preference on the latest package of
a certain version.

A currently empty but extensible set of fields starting with
"x-cabal-...". \`ghc-pkg\` ignores them when parsing. During the
resolution phase \`cabal-install\` might use them to decide
compatibility between packages.

A field abi-hash that contains the ABI hash because it is no longer
stored implicitly as part of the \`InstalledPackageId\`.

Simplistic dependency resolution
--------------------------------

The best tool for determining suitable package instances to use as build
inputs is \`cabal-install\`. However, in practice there will be many
situations where users will probably not have the full \`cabal-install\`
functionality available:

`1.invokingGHCifromthecommandline,`\
`2.invokingGHCdirectlyfromthecommandline,`\
`` 3.invokingtheconfigurephaseofCabal(withoutusing`cabal-install`). ``

In these cases, we have to come up with a suitable selection of package
instances, and the only info we have available are the package DBs plus
potential command line flags. Cabal will additionally take into account
the local constraints of the package it is being invoked for, whereas
GHC will only consider command-line flags, but not modules it has been
invoked with.

Currently if GHC is invoked by the user it does some adhoc form of
dependency resolution. The most common case of this is using ghci. If
there are multiple instances of the same package in the
\`PackageDBStack\` the policy used to select a single one prefers DBs
higher in the stack. It then prefers packages with a higher version.
Once we allow package instances with the same version within a single
package DB, we need to refine the algorithm. Options are:

`*pickarandom/unspecifiedinstances`\
`*usethetimeofinstallation`\
`*user-specifiedpriorities`\
`` *usetheorderinthe`PackageDB` ``\
`*lookatthetransitiveclosureofdependenciesandtheirversions`\
`*buildacomplexsolverintoGHC`

Picking a random version is a last resort. A combination of installation
time and priorities seems rather feasible. It makes conflicts unlikely,
and allows to persistently change the priorities of installed packages.
Using the order in the package DB is difficult if directories are being
used as DBs. Looking at the transitive closure of dependencies makes it
hard to define a total ordering of package instances. Adding a complex
solver is unattractive unless we find a way to reuse \`cabal-install\`'s
functionality within GHC, but probably we do not want to tie the two
projects together in this way.

Build flavours
--------------

Once we distinguish several package instances with the same version, we
have a design decision how precise we want that distinction to be.

The minimal approach would be to just take the transitive dependencies
into account. However, we might also want to include additional
information about builds such as Cabal flag settings, compiler options,
profiling, documentation, build tool versions, external (OS)
dependencies, and more.

These differences have to be tracked. The two options we discuss are to
store information in the \`ghc-pkg\` format, or to incorporate them in a
Cabal hash (which is then stored). Both options can be combined.

### The Cabal hash

\[A few notes about where to find suitable information in the source
code:\]

A build configuration consists of the following:

The Cabal hashes of all the package instances that are actually used for
compilation. This is the environment. It is available in the
\`installedPkgs\` field of \`LocalBuildInfo\` which is available in
every step after configuration. It can also be extracted from an
\`InstallPlan\` after dependency resolution.

The compiler, its version and its arguments and the tools and their
version and their arguments. Available from LocalBuildInfo also. More
specifically: \`compiler\`, \`withPrograms\`, \`withVanillaLib\`,
\`withProfLib\`, \`withSharedLib\`, \`withDynExe\`, \`withProfExe\`,
\`withOptimization\`, \`withGHCiLib\`, \`splitObjs\`, \`stripExes\`. And
a lot more. \[Like what?\]

The source code. This is necessary because if the source code changes
the result of compilation changes. For released packages I would assume
that the version number uniquely identifies the source code. A hash of
the source code should be available from hackage to avoid downloading
the source code. For an unreleased package we need to find all the
source files that are needed for building it. Including non-haskell
source files. One way is to ask a source tarball to be built as if the
package was released and then hash all the sources included in that.

OS dependencies are not taken into account because i think it would be
very hard.

### Released and Unreleased packages

If we cabal install a package that is released on hackage we call this a
**clean install**. If we cabal install an unreleased package we call
this a **dirty install**. Clean installs are mainly used to bring a
package into scope for ghci and to install applications. While they can
be used to satisfy dependencies this is discouraged. For released
packages the set of source files needed for compilation is known. For
unreleased packages this is currently not the case.

Dependency resolution in cabal-install
--------------------------------------

There are two general options for communicating knowledge about build
flavors to the solver:

`1.`**`the` `direct`
`way`**`:i.e.,allinfoisavailabletoghc-pkgandcanbecommunicatedbacktoCabalandthereforethesolvercanfigureoutifaparticularpackageissuitabletouseornot,inadvance;`

`2.`**`the` `agnostic`
`way`**`:thisisbasedontheideathatthesolveratfirstdoesn'tconsiderinstalledpackagesatall.It'lljustdoresolutiononthesourcepackagesavailable.Then,takingallbuildparametersintoaccount,Cabalhasheswillbecomputed,whichcanthenbecomparedtohashesofinstalledpackages.`

Reusing installed packages instead of rebuilding them is then an
optimization of the install plan.

The agnostic way does not require \`ghc-pkg\` to be directly aware of
all the build parameters, as long as the hash computation is robust

The options are to support either both by putting all info into
\`InstalledPackageInfo\` or to support only the second option by just
putting a hash into \`InstalledPackageInfo\`. The disadvantage of
supporting both is that \`InstalledPackageInfo\` would have to change
more often. This could be fixed by explicitly making the
\`InstalledPackageInfo\` format extensible in a backwards-compatible
way.

The advantages of having all info available, independently of the solver
algorihm, are that the info might be useful for other tools and user
feedback.

Possible disadvantages of the agnostic approach could be that is is a
rather significant change and can probably not be supported in a similar
way for other Haskell implementation. Also, in the direct approach, we
could in principle allow more complex compatibility rules, such as
allowing non-profiling libraries to depend on profiling libraries.

Also, even if we go for the agnostic approach, we still have to be able
to handle packages such as base or ghc-prim which are in general not
even available in source form.

On the other hand, the agnostic approach might lead to more predictable
and reproducible solver results across many different systems.

Garbage Collection
------------------

The proposed changes will likely lead to a dramatic increase of the
number of installed package instances on most systems. This is
particularly relevant for package developers who will conduct lots of
dirty builds that lead to new instances being installed all the time.

It should therefore be possible to have a garbage collection to remove
unneeded packages. However, it is not possible for Cabal to see all
potential reverse dependencies of a package, so automatic garbage
collection would be extremely unsafe.

Options are to either offer an interactive process where packages that
look unused are suggested for removal, or to integrate with a sandbox
mechanism. If, for example, dirty builds are usually installed into a
separate package DB, that package DB could just be removed completely by
a user from time to time.

The garbage collection functionality is part of cabal-install not of
ghc-pkg. As a first approximation gc does not remove files only
unregisters packages from the \`PackageDB\`.

Currently open design decisions
-------------------------------

### \`InstalledPackageId\` and install path

Options for uniquely identifying \`InstalledPackageId\`:

`*Cabalhashonly`\
`*Cabal+ABIhash(trulyunique)`\
`*randomnumber`

Options for identifying install path:

`*Cabalhash`\
`*randomnumber`

ABI hash cannot be in install path because it's only available after
build.

### Handling of dirty builds

How should hash computation work for dirty builds?

`*Usearandomnumberevenifweotherwiseusehashes`\
`*Hashthecompletebuilddirectory`\
`*Attempttomakeaclean(sdist-like)copyorlinkedcopyofthesourcesandhashandbuildfromthat.`\
`*UsetheCabalfiletodeterminethefilesthatwouldendupinansdistandhashthosedirectlywithoutcopying.`

The third option has the advantage(?) that the build is more guaranteed
to use only files actually mentioned in the Cabal file.

### Build flavours

To what degree should we distinguish package instances?

`*Onlypackageversionstransitively`\
`*WaysandCabalflags`\
`*EverythingHaskell-specificinfothatwecanquery`\
`*Evennon-Haskell-specificinputssuchasOSdependencies`

### \`InstalledPackageInfo\` and solver algorithm

Options for \`InstalledPackageInfo\`:

`*OnlyaddCabalhash.`\
`*Add(nearly)allinformation,butinanextensibleformat.`\
`` *Addallinformationinawaythat`ghc-pkg`itselfcanuseit. ``

\[These aren't necessarily mutually exclusive.\]

Options for the solver:

`` *Direct(seeabove):requiresacertainamountofinfointhe`InstalledPackageInfo`. ``

`` *Agnostic(exceptforbuiltinpackages):couldbedonewithonlytheCabalhashin`InstalledPackageInfo`. ``

### Simplistic dependency resolution

Options (in order of preference):

`*usethetimeofinstallation`\
`*user-specifiedpriorities`\
`*pickarandom/unspecifiedinstances`\
`*(buildacomplexsolverintoGHC)`

A combination of the first two seems possible and useful.

Related topics
--------------

In the following, we discuss some other issues which are related to the
multi-instance problem, but not necessarily directly relevant in order
to produce an implementation.

### Separating storage and selection of packages

Currently the two concepts of storing package instances (cabal store)
and selecting package instances for building (environment) are conflated
into a \`PackageDB\`. Sandboxes are used as a workaround to create
multiple different environments. But they also create multiple places to
store installed packages. The disadvantages of this are disk usage,
compilation time and one might lose the overview. Also if the
multi-instance restriction is not lifted sandboxes will eventually
suffer from the same unintended breakage of packages as non-sandboxed
\`PackageDB\`s. There should be a separation between the set of all
installed packages called the cabal store and a subset of these called
an environment. While the cabal store can contain multiple instances of
the same package version an environment needs to be consistent. An
environment is consistent if for every package version it contains only
one instance of that package version.

### First class environments

It would be nice if we had some explicit notion of an environment.

Questions to remember
---------------------

Should the cabal version be part of the hash?

Does the hash contain characters conflicting under windows?

What about builtin packages like ghc-prim, base, rts and so on?

Inplace Registration?

Who has assumptions about the directory layout of installed packages?

Executables?

Haddock?

Installation Planner?

Custom Builds and BuildHooks?

Other Compilers, backwards compatibility?

What is ComponentLocalBuildInfo for?

The Haskell Execution Model
===========================

The \[wiki:Commentary/Compiler/StgSynType STG language\] has a clear
*operational* model, as well as having a declarative lambda-calculus
reading. The business of the \[wiki:Commentary/Compiler/CodeGen code
generator\] is to translate the STG program into \`C--\`, and thence to
machine code, but that is mere detail. From the STG program you should
be able to understand:

`*Whatfunctionsareinthecompiledprogram,andwhattheirentryandreturnconventionsare`\
`*Whatheapobjectsareallocated,when,andwhattheirlayoutis`

GHC uses an eval/apply execution model, described in the paper [How to
make a fast curry: push/enter vs
eval/apply](http://research.microsoft.com/%7Esimonpj/papers/eval-apply).
This paper is well worth reading if you are interested in this section.

Contents:

`*[wiki:Commentary/Rts/HaskellExecution/RegistersRegisters]`\
`*[wiki:Commentary/Rts/HaskellExecution/FunctionCallsFunctionCalls]`\
`*[wiki:Commentary/Rts/HaskellExecution/CallingConventionCallandReturnConventions]`\
`*[wiki:Commentary/Rts/HaskellExecution/HeapChecksHeapandStackchecks]`\
`*[wiki:Commentary/Rts/HaskellExecution/UpdatesUpdates]`\
`*[wiki:Commentary/Rts/HaskellExecution/PointerTaggingPointerTagging]`

HEAP\_ALLOCED
=============

This page is about the \`HEAP\_ALLOCED()\` macro/function in the runtime
system. See \#8199 which is about getting rid of \`HEAP\_ALLOCED\`.

It is defined in \`rts/sm/MBlock.h\`. The purpose of \`HEAP\_ALLOCED()\`
is to return true if the given address is part of the
dynamically-allocated heap, and false otherwise. Its primary use is in
the Garbage Collector: when examining a pointer, we need to get to the
block descriptor for that object. Static objects don't have block
descriptors, because they live in static code space, so we need to
establish whether the pointer is into the dynamic heap first, hence
\`HEAP\_ALLOCED()\`.

On a 32-bit machine, \`HEAP\_ALLOCED()\` is implemented with a
4096-entry byte-map, one byte per megabyte of the address space (the
dynamic heap is allocated in units of aligned megabytes).

On a 64-bit machine, it's a bit more difficult. The current method (GHC
6.10.1 and earlier) uses a cache, with a 4096-entry map and a 32-bit
tag. If the upper 32 bits of the pointer match the tag, we look up in
the map, otherwise we back off to a slow method that searches a list of
mappings (bug \#2934 is about the lack of thread-safety in the slow path
here). This arrangement works fine for small heaps, but is pessimal for
large (multi-GB) heaps, or heaps that are scattered around the address
space.

Speeding up \`HEAP\_ALLOCED()\`
-------------------------------

We should consider how to speed up \`HEAP\_ALLOCED()\` for large heaps
on 64-bit machines. This involves some kind of cache arrangement - the
memory map is like a page table, and we want a cache that gives us quick
access to commonly accessed parts of that map.

[5](attachment:faster-heap-alloced.patch.gz) implements one such scheme.
Measurements show that it slows down GC by about 20% for small heaps
(hence it wasn't committed), though it would probably speed up GC on
large heaps.

Eliminating \`HEAP\_ALLOCED\` completely
----------------------------------------

Can we eliminate \`HEAP\_ALLOCED\` altogether? We must arrange that all
closure pointers have a valid block descriptor.

### Method 1: put static closures in an aligned section

ELF sections can be arbitrarily aligned. So we could put all our static
closures in a special section, align the section to 1MB, and arrange
that there is space at the beginning of the section for the block
descriptors.

This almost works (see [6](attachment:eliminate-heap-alloced.patch.gz)),
but sadly fails for shared libraries: the system dynamic linker doesn't
honour section-alignment requests larger than a page, it seems. Here is
a simple test program which shows the problem on Linux:

Compare static linking and dynamic linking:

### Method 2: copy static closures into a special area at startup

We could arrange that we access all static closures via indirections,
and then at startup time we copy all the static closures into a special
area with block descriptors.

Disadvantages:

`*referencestostaticobjectsgothroughanotherindirection.(ThisincludesalloftheRTScode!)`\
`*whendoingdynamiclinking,referencestostaticobjectsinanotherpackage`\
`alreadygothroughanindirectionandwecouldarrangethatonlyoneindirectionisrequired.`\
`*Referencestostaticclosuresfromthethefieldsofastaticconstructorwouldnotincurtheextraindirection,`\
`onlydirectreferencestostaticclosuresfromcode.`\
`*wecurrentlyreferencethestaticclosureofafunctionfromtheheap-check-failcode,butinfact`\
`weonlyreallyneedtopasstheinfopointer.`

Advantages

`*wegettofixupallthetagbitsinstaticclosurepointers`\
`*wegettoeliminateHEAP_ALLOCED,speedingupGCandremovingcomplexity`\
`*CAFsmightgetabitsimpler,sincetheyarealreadyindirectionsintotheheap`

Heap and Stack checks
=====================

Source files:
[GhcFile(rts/HeapStackCheck.cmm)](GhcFile(rts/HeapStackCheck.cmm) "wikilink")

When allocating a heap object, we bump \`Hp\` and compare to \`HpLim\`.
If the test fails we branch to ???. Usually this code tests an interrupt
flag (to see if execution should be brought tidily to a halt); grabs the
next block of allocation space; makes \`Hp\` point to it and \`HpLim\`
to its end; and returns. If there are no more allocation-space blocks,
garbage collection is triggered.

------------------------------------------------------------------------

CategoryStub

[PageOutline](PageOutline "wikilink")

GHC Commentary: The Layout of Heap Objects
==========================================

Terminology
-----------

`*A`*`lifted`*`typeisonethatcontainsbottom(_|_),converselyan`*`unlifted`*`typedoesnotcontain_|_.`\
`Forexample,``islifted,but``isunlifted.`

`*A`*`boxed`*`typeisrepresentedbyapointertoanobjectintheheap,an`*`unboxed`*`objectisrepresentedbyavalue.`\
`Forexample,``isboxed,but``isunboxed.`

The representation of \_|\_ must be a pointer: it is an object that when
evaluated throws an exception or enters an infinite loop. Therefore,
only boxed types may be lifted.

There are boxed unlifted types: eg. . If you have a value of type , it
definitely points to a heap object with type (see below), rather than an
unevaluated thunk.

Unboxed tuples are both unlifted and unboxed. They are represented by
multiple values passed in registers or on the stack, according to the
\[wiki:Commentary/Rts/HaskellExecution return convention\].

Unlifted types cannot currently be used to represent terminating
functions: an unlifted type on the right of an arrow is implicitly
lifted to include \`\_|\_\`.

------------------------------------------------------------------------

Heap Objects
------------

All heap objects have the same basic layout, embodied by the type in
\[source:includes/rts/storage/Closures.h Closures.h\]. The diagram below
shows the layout of a heap object:

[Image(heap-object.png)](Image(heap-object.png) "wikilink")

A heap object always begins with a *header*, defined by in
\[source:includes/rts/storage/Closures.h Closures.h\]:

The most important part of the header is the *info pointer*, which
points to the info table for the closure. In the default build, this is
all the header contains, so a header is normally just one word. In other
builds, the header may contain extra information: eg. in a profiling
build it also contains information about who built the closure.

Most of the runtime is insensitive to the size of ; that is, we are
careful not to hardcode the offset to the payload anywhere, instead we
use C struct indexing or . This makes it easy to extend with new fields
if we need to.

The compiler also needs to know the layout of heap objects, and the way
this information is plumbed into the compiler from the C headers in the
runtime is described here:
\[wiki:Commentary/Compiler/CodeGen\#Storagemanagerrepresentations\].

------------------------------------------------------------------------

Info Tables
-----------

The info table contains all the information that the runtime needs to
know about the closure. The layout of info tables is defined by in
\[source:includes/rts/storage/InfoTables.h InfoTables.h\]. The basic
info table layout looks like this:

[Image(basic-itbl.png)](Image(basic-itbl.png) "wikilink")

Where:

`*The`*`closure`
`type`*`isaconstantdescribingthekindofclosurethisis(function,thunk,constructoretc.).All`\
`theclosuretypesaredefinedin[source:includes/rts/storage/ClosureTypes.hClosureTypes.h],andmanyofthemhavecorrespondingCstruct`\
`definitionsin[source:includes/rts/storage/Closures.hClosures.h].`

`*The`*`SRT`
`bitmap`*`fieldisusedtosupport[wiki:Commentary/Rts/Storage/GC/CAFsgarbagecollectionofCAFs].`

`*The`*`layout`*`fielddescribesthelayoutofthepayloadforthegarbagecollector,andisdescribedinmore`\
`detailin`[`ref(Types` `of` `Payload`
`Layout)`](ref(Types_of_Payload_Layout) "wikilink")`below.`

`*The`*`entry`
`code`*`fortheclosureisusuallythecodethatwill`*`evaluate`*`theclosure.Thereisoneexception:`\
`forfunctions,theentrycodewillapplythefunctiontotheargumentsgiveninregistersoronthestack,according`\
`tothecallingconvention.Theentrycodeassumesalltheargumentsarepresent-toapplyafunctiontofewerarguments`\
`ortoapplyanunknownfunction,the[wiki:Commentary/Rts/HaskellExecution/FunctionCalls#Genericapplygenericapplyfunctions]must`\
`beused.`

Some types of object add more fields to the end of the info table,
notably functions, return addresses, and thunks.

Space in info tables is a premium: adding a word to the standard info
table structure increases binary sizes by 5-10%.

### 

Note that the info table is followed immediately by the entry code,
rather than the code being at the end of an indirect pointer. This both
reduces the size of the info table and eliminates one indirection when
jumping to the entry code.

GHC can generate code that uses the indirect pointer instead; the turns
on the optimised layout. Generally is turned off when compiling
unregisterised.

When is off, info tables get another field, , which points to the entry
code. In a generated object file, each symbol representing an info table
will have an associated symbol pointing to the entry code (in , the
entry symbol is omitted to keep the size of symbol tables down).

------------------------------------------------------------------------

Types of Payload Layout
-----------------------

The GC needs to know two things about the payload of a heap object: how
many words it contains, and which of those words are pointers. There are
two basic kinds of layout for the payload: *pointers-first* and
*bitmap*. Which of these kinds of layout is being used is a property of
the *closure type*, so the GC first checks the closure type to determine
how to interpret the layout field of the info table.

### Pointers-first layout

The payload consists of zero or more pointers followed by zero or more
non-pointers. This is the most common layout: constructors, functions
and thunks use this layout. The layout field contains two
half-word-sized fields:

`*Numberofpointers`\
`*Numberofnon-pointers`

### Bitmap layout

The payload consists of a mixture of pointers and non-pointers,
described by a bitmap. There are two kinds of bitmap:

**Small bitmaps.** A small bitmap fits into a single word (the layout
word of the info table), and looks like this:

|| Size (bits 0-4) || Bitmap (bits 5-31) ||

(for a 64-bit word size, the size is given 6 bits instead of 5).

The size field gives the size of the payload, and each bit of the bitmap
is 1 if the corresponding word of payload contains a pointer to a live
object.

The macros , , and in \[source:includes/rts/storage/InfoTables.h
InfoTables.h\] provide ways to conveniently operate on small bitmaps.

**Large bitmaps.** If the size of the stack frame is larger than the 27
words that a small bitmap can describe, then the fallback mechanism is
the large bitmap. A large bitmap is a separate structure, containing a
single word size and a multi-word bitmap: see in
\[source:includes/rts/storage/InfoTables.h InfoTables.h\].

------------------------------------------------------------------------

Dynamic vs. Static objects
--------------------------

Objects fall into two categories:

`*`*`dynamic`*`objectsresideintheheap,andmaybemovedbythegarbagecollector.`

`*`*`static`*`objectsresideinthecompiledobjectcode.Theyarenevermoved,becausepointerstosuchobjectsare`\
`scatteredthroughtheobjectcode,andonlythelinkerknowswhere.`

To find out whether a particular object is dynamic or static, use the
\[wiki:Commentary/Rts/Storage/HeapAlloced HEAP\_ALLOCED()\] macro, from
\[source:rts/sm/HeapAlloc.h\]. This macro works by consulting a bitmap
(or structured bitmap) that tells for each
\[wiki:Commentary/Rts/Storage\#Structureofblocks megablock\] of memory
whether it is part of the dynamic heap or not.

### Dynamic objects

Dynamic objects have a minimum size, because every object must be big
enough to be overwritten by a forwarding pointer ([ref(Forwarding
Pointers)](ref(Forwarding_Pointers) "wikilink")) during GC. The minimum
size of the payload is given by in \[source:includes/rts/Constants.h\].

### Static objects

All static objects have closure types ending in , eg. for static data
constructors.

Static objects have an additional field, called the *static link field*.
The static link field is used by the GC to link all the static objects
in a list, and so that it can tell whether it has visited a particular
static object or not - the GC needs to traverse all the static objects
in order to \[wiki:Commentary/Rts/CAFs garbage collect CAFs\].

The static link field resides after the normal payload, so that the
static variant of an object has compatible layout with the dynamic
variant. To access the static link field of a closure, use the macro
from \[source:includes/rts/storage/ClosureMacros.h\].

------------------------------------------------------------------------

Types of object
---------------

### Data Constructors

All data constructors have pointers-first layout:

|| Header || Pointers... || Non-pointers... ||

Data constructor closure types:

`*``:avanilla,dynamicallyallocatedconstructor`\
`*``:aconstructorwhoselayoutisencodedintheclosuretype(eg.``hasonepointer`\
`andzeronon-pointers.HavingtheseclosuretypesspeedsupGCalittleforcommonlayouts.`\
`*``:astaticallyallocatedconstructor.`\
`*``:TODO:Needsdocumentation`

The entry code for a constructor returns immediately to the topmost
stack frame, because the data constructor is already in WHNF. The return
convention may be vectored or non-vectored, depending on the type (see
\[wiki:Commentary/Rts/HaskellExecution/CallingConvention\]).

Symbols related to a data constructor X:

`*X_``:infotableforadynamicinstanceofX`\
`*X_``:infotableforastaticinstanceofX`\
`*X_``:the`*`wrapper`*`forX(afunction,equivalenttothe`\
`curriedfunction``inHaskell,see`\
`[wiki:Commentary/Compiler/EntityTypes]).`\
`*X_``:staticclosureforX'swrapper`

### Function Closures

A function closure represents a Haskell function. For example: Here,
would be represented by a static function closure (see below), and a
dynamic function closure. Every function in the Haskell program
generates a new info table and entry code, and top-level functions
additionally generate a static closure.

All function closures have pointers-first layout:

|| Header || Pointers... || Non-pointers... ||

The payload of the function closure contains the free variables of the
function: in the example above, a closure for would have a payload
containing a pointer to .

Function closure types:

`*``:avanilla,dynamicallyallocatedfunction`\
`*``:same,specialisedforlayout(seeconstructorsabove)`\
`*``:astatic(top-level)functionclosure`

Symbols related to a function :

`*``:f'sinfotableandcode`\
`*``:f'sstaticclosure,iffisatop-levelfunction.`\
`Thestaticclosurehasnopayload,becausetherearenofree`\
`variablesofatop-levelfunction.Itdoeshaveastaticlink`\
`field,though.`

### Thunks

A thunk represents an expression that is not obviously in head normal
form. For example, consider the following top-level definitions: Here
the right-hand sides of and are both thunks; the former is static while
the latter is dynamic.

Thunks have pointers-first layout:

|| Header || (empty) || Pointers... || Non-pointers... ||

As for function closures, the payload contains the free variables of the
expression. A thunk differs from a function closure in that it can be
\[wiki:Commentary/Rts/HaskellExecution\#Updates updated\].

There are several forms of thunk:

`*``,``:vanilla,dynamicallyallocated`\
`thunks.Dynamicthunksareoverwrittenwithnormalindirections`\
```whenevaluated.`

`*``:astaticthunkisalsoknownasa''constant`\
`applicativeform'',or`*`CAF`*`.Staticthunksareoverwrittenwith`\
`staticindirections(``).`

The only label associated with a thunk is its info table:

`*``isf'sinfotable.`

The empty padding is to allow thunk update code to overwrite the target
of an indirection without clobbering any of the saved free variables.
This means we can do thunk update without synchronization, which is a
big deal.

### Selector thunks

 is a (dynamically allocated) thunk whose entry code performs a simple
selection operation from a data constructor drawn from a
single-constructor type. For example, the thunk is a selector thunk. A
selector thunk is laid out like this:

|| Header || Selectee pointer ||

The \`layout\` word contains the byte offset of the desired word in the
selectee. Note that this is different from all other thunks.

The garbage collector "peeks" at the selectee's tag (in its info table).
If it is evaluated, then it goes ahead and does the selection, and then
behaves just as if the selector thunk was an indirection to the selected
field. If it is not evaluated, it treats the selector thunk like any
other thunk of that shape.

This technique comes from the Phil Wadler paper [Fixing some space leaks
with a garbage
collector](http://homepages.inf.ed.ac.uk/wadler/topics/garbage-collection.html),
and later Christina von Dorrien who called it "Stingy Evaluation".

There is a fixed set of pre-compiled selector thunks built into the RTS,
representing offsets from 0 to , see \[source:rts/StgStdThunks.cmm\].
The info tables are labelled where is the offset. Non-updating versions
are also built in, with info tables labelled .

These thunks exist in order to prevent a space leak. For example, if y
is a thunk that has been evaluated, and y is unreachable, but x is
reachable, the risk is that x keeps both the a and b components of y
live. By making the selector thunk a special case, we make it possible
to reclaim the memory associated with b. (The situation is further
complicated when selector thunks point to other selector thunks; the
garbage collector sees all, knows all.)

### Partial applications

Partial applications are tricky beasts.

A partial application, closure type , represents a function applied to
too few arguments. Partial applications are only built by the
\[wiki:Commentary/Rts/HaskellExecution/FunctionCalls\#Genericapply
generic apply functions\] in \[source:rts/Apply.cmm\].

|| Header || Arity || No. of words || Function closure || Payload... ||

Where:

`*`*`Arity`*`isthearityofthePAP.Forexample,afunctionwith`\
`arity3appliedto1argumentwouldleaveaPAPwitharity2.`

`*`*`No.` `of` `words`*`referstothesizeofthepayloadinwords.`

`*`*`Function` `closure`*`isthefunctiontowhichtheargumentsare`\
`applied.Notethatthisisalwaysapointertooneofthe`\
```family,nevera``.Ifa``isapplied`\
`tomoreargumentstogiveanew``,theargumentsfrom`\
`theoriginal``arecopiedtothenewone.`

`*Thepayloadisthesequenceofargumentsalreadyappliedto`\
`thisfunction.Thepointerhoodofthesewordsaredescribed`\
`bythefunction'sbitmap(see``in`\
`[source:rts/sm/Scav.c]foranexampleoftraversingaPAP).`

There is just one standard form of PAP. There is just one info table
too, called . A PAP should never be entered, so its entry code causes a
failure. PAPs are applied by the generic apply functions in .

### Generic application

An object is very similar to a , and has identical layout:

|| Header || Arity || No. of words || Function closure || Payload... ||

The difference is that an is not necessarily in WHNF. It is a thunk that
represents the application of the specified function to the given
arguments.

The arity field is always zero (it wouldn't help to omit this field,
because it is only half a word anyway).

 closures are used mostly by the byte-code interpreter, so that it only
needs a single form of thunk object. Interpreted thunks are always
represented by the application of a to its free variables.

### Stack application

An is a special kind of object:

|| Header || Size || Closure || Payload... ||

It represents computation of a thunk that was suspended midway through
evaluation. In order to continue the computation, copy the payload onto
the stack (the payload was originally the stack of the suspended
computation), and enter the closure.

Since the payload is a chunk of stack, the GC can use its normal
stack-walking code to traverse it.

 closures are built by in \[source:rts/RaiseAsync.c\] when an
\[wiki:Commentary/Rts/AsyncExceptions asynchronous exception\] is
raised. It's fairly typical for the end of an AP\_STACK's payload to
have another AP\_STACK: you'll get one per update frame.

### Indirections

Indirection closures just point to other closures. They are introduced
when a thunk is updated to point to its value. The entry code for all
indirections simply enters the closure it points to.

The basic layout of an indirection is simply

|| Header || Target closure ||

There are several variants of indirection:

`*``:isthevanilla,dynamically-allocatedindirection.`\
`Itisremovedbythegarbagecollector.An``onlyexistsintheyoungestgeneration.`\
`Theupdatecode(``andfriends)checkswhethertheupdateeisintheyoungest`\
`generationbeforedecidingwhichkindofindirectiontouse.`\
`*``:astaticindirection,ariseswhenweupdatea``.Anew`\
`isplacedonthemutablelistwhenitiscreated(see``in[source:rts/sm/Storage.c]).`

### Byte-code objects

### Black holes

,

Black holes represent thunks which are under evaluation by another
thread (that thread is said to have claimed the thunk). Attempting to
evaluate a black hole causes a thread to block until the thread who
claimed the thunk either finishes evaluating the thunk or dies. You can
read more about black holes in the paper 'Haskell on a Shared-Memory
Multiprocessor'. Black holes have the same layout as indirections.

|| Header || Target closure ||

Sometimes black holes are just ordinary indirection. Check
\`stg\_BLACKHOLE\_info\` for the final word: if the indirectee has no
tag, then we assume that it is the TSO that has claimed the thunk; if
the indirectee is tagged, then it is just a normal indirection. (EZY: I
think this optimization is to avoid having to do two memory writes on
thunk update; we don't bother updating the header, only the target.)

When eager blackholing is enabled, the black hole that is written is not
a true black hole, but an eager black hole. True black holes are
synchronized, and guarantee that only one black hole is claimed (this
property is used to implement non-dupable unsafePerformIO). Eager black
holes are not synchronized; eager black hole are converted into true
black holes in ThreadPaused.c. Incidentally, this facility is also used
to convert update frames to black holes; this is important for
eliminating a space leak caused by the thunk under evaluation retaining
too much data (overwriting it with a black hole frees up variable.)

### Arrays

, , , ,

Non-pointer arrays are straightforward:

||| Header ||| Bytes ||| Array payload |||

Arrays with pointers are a little more complicated, they include a card
table, which is used by the GC to know what segments of the array to
traverse as roots (the card table is modified by the GC write barrier):

||| Header ||| Ptrs ||| Size ||| Array payload + card table |||

You can access the card table by using \`mutArrPtrsCard(array, element
index)\`, which gives you the address of the card for that index.

### MVars

MVars have a queue of the TSOs blocking on them along with their value:

|| Header || Head of queue || Tail of queue || Value ||

An MVar can be in several states. It can be empty (in which case the
value is actually just a \`stg\_END\_TSO\_QUEUE\_closure\`) or it can be
full. When it is full, the queue of TSOs are those waiting to put; when
it is empty, the queue of TSOs are those waiting to read and take (with
readers first). Like many mutable objects, MVars have CLEAN and DIRTY
headers to avoid reapplying a write barrier when an MVar is already
dirty.

### Weak pointers

### Stable Names

### Thread State Objects

Closure type is a Thread State Object. It represents the complete state
of a thread, including its stack.

TSOs are ordinary objects that live in the heap, so we can use the
existing allocation and garbage collection machinery to manage them.
This gives us one important benefit: the garbage collector can detect
when a blocked thread is unreachable, and hence can never become
runnable again. When this happens, we can notify the thread by sending
it the exception.

GHC keeps divides stacks into stack chunks, with logic to handle stack
underflow and overflow:
<http://hackage.haskell.org/trac/ghc/blog/stack-chunks>

The TSO structure contains several fields. For full details see
\[source:includes/rts/storage/TSO.h\]. Some of the more important fields
are:

`*`*`link`*`:fieldforlinkingTSOstogetherinalist.Forexample,thethreadsblockedonan``arekeptin`\
`aqueuethreadedthroughthelinkfieldofeachTSO.`\
`*`*`global_link`*`:linksallTSOstogether;theheadofthislistis``in[source:rts/Schedule.c].`\
`*`*`what_next`*`:howtoresumeexecutionofthisthread.Thevalidvaluesare:`\
`*``:continuebyreturningtothetopstackframe.`\
`*``:continuebyinterpretingtheBCOontopofthestack.`\
`*``:thisthreadhasreceivedanexceptionwhichwasnotcaught.`\
`*``:thisthreadranoutofstackandhasbeenrelocatedtoalargerTSO;thelinkfieldpoints`\
`toitsnewlocation.`\
`*``:thisthreadhasfinishedandcanbegarbagecollectedwhenitisunreachable.`\
`*`*`why_blocked`*`:forablockedthread,indicateswhythethreadisblocked.See[source:includes/rts/Constants.h]for`\
`thelistofpossiblevalues.`\
`*`*`block_info`*`:forablockedthread,givesmoreinformationaboutthereasonforblockage,eg.whenblockedonan`\
`MVar,block_infowillpointtotheMVar.`\
`*`*`bound`*`:pointertoa[wiki:Commentary/Rts/Scheduler#TaskTask]ifthisthreadisbound`\
`*`*`cap`*`:the[wiki:Commentary/Rts/Scheduler#CapabilitiesCapability]onwhichthisthreadresides.`

### STM objects

These object types are used by \[wiki:Commentary/Rts/STM STM\]: , , , .

### Forwarding Pointers

Forwarding pointers appear temporarily during
\[wiki:Commentary/Rts/Storage/GC garbage collection\]. A forwarding
pointer points to the new location for an object that has been moved by
the garbage collector. It is represented by replacing the info pointer
for the closure with a pointer to the new location, with the least
significant bit set to 1 to distinguish a forwarding pointer from an
info pointer.

How to add new heap objects
---------------------------

There are two This page is a stub.

Change History
--------------

`*`*`History` `of` `when` `Hoopl` `was` `integrated` `into` `a` `GHC`
`back` `end`*

`*AfterthepublicationoftheHooplpaper,acontributor(sorryIhaveforgottenwho)didquiteabittointegratethesupplyof``sintoHoopl.(Time?Person?)`

`*`*`Note` `that` `the` `new` `code` `generator` `appears` `about`
`10x` `slower` `than` `the` `old.` `Slowdown` `attributed` `to` `Hoopl`
`dataflow.`*`See`[`Google` `Plus` `post` `by` `Simon`
`Marlow`](https://plus.google.com/107890464054636586545/posts/dBbewpRfw6R)`.`

`*Fixed-pointalgorithmrewrittentoreduceduplicatecomputation.(SimonMarlowinlate2011.AlsoEdwardYanginspring2011.)Isthereanymore?Isuggestlookingattracesinthesimplecases.`

`*Changeinrepresentationofblocks,SimonMarlow,late2011.(Details?)Performancedifferencealmosttoosmalltobemeasurable,butSimonMlikesthenewrepanyway.`

Speculation and Commentary
--------------------------

`*SimonPJhadquestionsabout"optimizationfuel"fromthebeginning.Normanmaintainsthatoptimizationfuelisaninvaluabledebuggingaid,butthatinaproductioncompiler,onewouldlikeittobeturnedoff.Atsomepointwehadabstractedoverthe``sothatwecouldmakea"zero"fuelmonadthatdidnothingandcostnothing.AsofJanuary2012,Normandoesn'tknowwhatthestateofthatplanisorwhetherGHC'soptimisercanactuallyeliminatetheoverheads.`

`*UnlikeFuel,asupplyof``swasbelievedtobeanabsolutenecessity:anoptimisermustbeabletorewriteblocks,andinthegeneralcase,itmustbeabletointroducenewblocks.ItwasbelievedthattheonlywaytodothisconsistentwithGHCwastoplumbinaUniqsupply.`*`Query`*`:wasthisintegratedwithFuelsomehow?`

`*ThepublishedversionofHooplpassesanexplicitdictionarythatcontainsallthedataflowfactsforallthelabels.EarlierversionsofHooplkeptthisinformationinamonad.It'snotknownwhetherthechangehasimplicationsforperformance,butitisprobablyeasiertomanagethespeculativerewritingwithoutamonad.`

`*Normanhasalwaysbeenuneasyaboutthedictionariespassedtothe``function.Heconjecturesthatmostblockshaveasmallnumberofoutedges,andprobablynotthatmanyinedgeseither(caseexpressionsandtheAdamsoptimisationnotwithstanding).Hewondersifinsteadofsomekindoftriestructurewithworst-caselogarithmicperformance,wemightnotbebetteroffwithasimpleassociationlist---especiallybecauseitiscommontosimplyjoin`*`all`*`factsflowingintoablock.`**`Query:`
`Is` `there` `a` `way` `to` `measure` `the` `costs` `of` `using`
`dictionaries` `in` `this` `fashion?` `Cost` `centers,` `perhaps?`**

`*TherewasaGooglePlusthreadinwhichCPSwascriticized(byJanMaessen,Ithink).Theoriginalauthorshadmanybigfights,andoneofthemwasaboutCPS.AtsomepointNormandraftedadataflowanalyserthatwasveryaggressivelyCPS.SimonPJfoundtheextensiveCPSdifficulttoread.Normandoesn'tremembertheeventualoutcome.IsitpossiblethattheCPSiscausingtheallocationoftoomanyfunctionclosures?CouldtheCPSberewritten,perhapsbyadifferentwayofnestingfunctions,toeliminatetheneedtoallocateclosuresintheinnerloop?JohanTibelltriedoptimizingpostorder_dfs,butwasputoffbytheCPSstyleofcode.(Wespeculatethatcachingtheresultoftoposortmayhelp.)`

`*AnotherimportantthingtokeepinmindisthatsomeoftheexistingpassesusedbyGHCmaybeimplementedinefficiently(ofnofaultofHooplitself.)Forexample,therewriteassignmentspasstakesaround15%oftheentirecompilationtime;webelievethisisbecauseithastorewritetheentiregraphintoanewrepresentationbeforedoinganytransformations,andthenrewriteitbacktotheoriginal.Optimizationshere(forexample,storingtheinformationinanexternalmapasopposedtotheASTitself)wouldprobablywouldhelpalot.`

Record of performance improvements made to the Hoopl library starting January 2012
----------------------------------------------------------------------------------

Haskell Program Coverage
========================

This page describes the Haskell Program Coverage implementation inside
GHC. Background information can be found in the paper [Haskell Program
Coverage](http://www.ittc.ku.edu/~andygill/papers/Hpc07.pdf) by Andy
Gill and Colin Runciman, and the Haskell wiki page [Haskell program
coverage](https://wiki.haskell.org/Haskell_program_coverage).

The basic idea is this

`*Foreach(sub)expressionintheHaskellSyntax,writethe(sub)expressionina`\
`` `HsTick` ``\
`` *Each`HsTick`hasamodulelocalindexnumber. ``\
`*Thereisatable(TheMixdatastructure)thatmapsthisindexnumbertooriginalsourcelocation.`\
`` *Each`HsTick`ismappedintheDesugarpasswith: ``

`` *Thistickisaspecialtypeof`Id`,a`TickOpId`whichtakesnocore-levelargument,buthastwopre-appliedarguments;themodulenameandthemodule-localticknumber. ``\
`*WestorebothmodulenameandticknumbertoallowthisIdtobepassed(inlined)insideothermodules.`\
`` *This`Id`hastype ``**`State#` `World#`**\
`*Thecoresimplifiermustnotremovethiscase,butitcanmoveit.`\
`*Thedo-not-removeisenforcedviathe...functionin....`\
`` *Thesemanticsaretickif-and-when-and-asyouenterthe`DEFAULT`case.Butachainofconsecutivetickscanbeexecutedinanyorder. ``\
`` *The!CoreToStgPasstranslatestheticksinto`StgTick` ``

`` *The`Cmm`codegeneratortranslates`StgTick`toa64bitincrement. ``

Other details

`*Aexecutablestartuptime,weperformadepthfirsttraversalsomemodule`\
`specificcode,gatheringalistofallHpcregisteredmodules,andthe`\
`modulespecificticktable.`\
`*Thereisonetablepermodule,sowecanlinktheincrementstatically,`\
`withoutneedingtoknowtheglobalticknumber.`\
`*ThemoduleHpc.cintheRTShandlesallthereadingofthesetable.`\
`*Atstartup,ifa.tixfileisfound,Hpc.cchecksthatthisisthesame`\
`binaryasgeneratedthe.tixfile,andifso,pre-loadsallthetickcounts`\
`inthemodulespecificlocations.`\
`*(Iamlookingforagoodwayofcheckingthebinariesforsameness)`\
`*Atshutdown,wewritebackoutthe.tixfiles,fromthemodule-localtables.`

### Binary Tick Boxes

There is also the concept of a binary tick box. This is a syntactical
boolean, like a guard or conditional for an if. We use tick boxes to
record the result of the boolean, to check for coverage over True and
False.

`` *Each`HsBinaryTick`ismappedintheDesugarpasswith: ``

-   After desugaring, there is no longer any special code for binary
    tick box.

Machine Generated Haskell
-------------------------

Sometimes, Haskell is the target language - for example, Happy and Alex.
In this case, you want to be able to check for coverage of your
**original** program. So we have a new pragma.

This means that the expression was obtained from the given file and
locations. This might be code included verbatim (for example the actions
in Happy), or be generated from a specification from this location.

Compiling one module: !HscMain
==============================

Here we are going to look at the compilation of a single module. There
is a picture that goes with this description, which appears at the
bottom of this page, but you'll probably find it easier to open
\[wiki:Commentary/Compiler/HscPipe this link\] in another window, so you
can see it at the same time as reading the text.

You can also watch a **video** of Simon Peyton-Jones explaining the
compilation pipeline here: [Compiler Pipeline
II](http://www.youtube.com/watch?v=Upm_kYMgI_c&list=PLBkRCigjPwyeCSD_DFxpd246YIF7_RDDI)
(10'16")

Look at the picture first. The yellow boxes are compiler passes, while
the blue stuff on the left gives the data type that moves from one phase
to the next. The entire pipeline for a single module is run by a module
called !HscMain
([GhcFile(compiler/main/HscMain.hs)](GhcFile(compiler/main/HscMain.hs) "wikilink")).
Each data type's representation can be dumped for further inspection
using a \`-ddump-\*\` flag. (Consider also using \`-ddump-to-file\`:
some of the dump outputs can be large!) Here are the steps it goes
through:

`*The`**`Front`
`End`**`processestheprograminthe[wiki:Commentary/Compiler/HsSynTypebigHsSyntype].``isparameterisedoverthetypesofthetermvariablesitcontains.Thefirstthreepasses(thefrontend)ofthecompilerworklikethis:`[`BR`](BR "wikilink")[`BR`](BR "wikilink")\
`*The`**`[wiki:Commentary/Compiler/Parser`
`Parser]`**`produces``parameterisedby`**`[wiki:Commentary/Compiler/RdrNameType`
`RdrName]`**`.Toafirstapproximation,a``` isjustastring.(`-ddump-parsed`) ``[`BR`](BR "wikilink")[`BR`](BR "wikilink")\
`*The`**`[wiki:Commentary/Compiler/Renamer`
`Renamer]`**`transformsthisto``parameterisedby`**`[wiki:Commentary/Compiler/NameType`
`Name]`**`.Toafirstappoximation,a``isastringplusa``(number)thatuniquelyidentifiesit.Inparticular,therenamerassociateseachidentifierwithitsbindinginstanceandensuresthatalloccurrenceswhichassociatetothesamebindinginstanceshareasingle``` .(`-ddump-rn`) ``[`BR`](BR "wikilink")[`BR`](BR "wikilink")\
`*The`**`[wiki:Commentary/Compiler/TypeChecker`
`Typechecker]`**`transformsthisfurther,to``parameterisedby`**`[wiki:Commentary/Compiler/EntityTypes`
`Id]`**`.Toafirstapproximation,an``isa``plusatype.Inaddition,thetype-checkerconvertsclassdeclarationsto``es,andtypedeclarationsto``sand``s.Andofcourse,thetype-checkerdealsin``sand``s.The[wiki:Commentary/Compiler/EntityTypesdatatypesfortheseentities](``,``,``,``,``` )arepervasivethroughouttherestofthecompiler.(`-ddump-tc`) ``

`Thesethreepassescanalldiscoverprogrammererrors,whicharesortedandreportedtotheuser.`\
\
`*The`**`Desugarer`**`(`[`GhcFile(compiler/deSugar/Desugar.hs)`](GhcFile(compiler/deSugar/Desugar.hs) "wikilink")`)convertsfromthemassive``` typeto[wiki:Commentary/Compiler/CoreSynTypeGHC'sintermediatelanguage,CoreSyn].ThisCore-languagedatatypeisunusuallytiny:justeightconstructors.)(`-ddump-ds`) ``[`BR`](BR "wikilink")[`BR`](BR "wikilink")\
`Generallyspeaking,thedesugarerproducesfewusererrorsorwarnings.Butitdoesproduce`*`some`*`` .Inparticular,(a)pattern-matchoverlapwarningsareproducedhere;and(b)whendesugaringTemplateHaskellcodequotations,thedesugarermayfindthat`THSyntax`isnotexpressiveenough.Inthatcase,wemustproduceanerror( ``[`GhcFile(compiler/deSugar/DsMeta.hs)`](GhcFile(compiler/deSugar/DsMeta.hs) "wikilink")`).`[`BR`](BR "wikilink")[`BR`](BR "wikilink")\
`Thislatedesugaringissomewhatunusual.Itismuchmorecommontodesugartheprogrambeforetypechecking,orrenaming,becausethatpresentstherenamerandtypecheckerwithamuchsmallerlanguagetodealwith.However,GHC'sorganisationmeansthat`\
`*errormessagescandisplaypreciselythesyntaxthattheuserwrote;and`\
`*desugaringisnotrequiredtopreservetype-inferenceproperties.`[`BR`](BR "wikilink")[`BR`](BR "wikilink")

`*The`**`!SimplCore`**`pass(`[`GhcFile(compiler/simplCore/SimplCore.hs)`](GhcFile(compiler/simplCore/SimplCore.hs) "wikilink")`)isabunchofCore-to-Corepassesthatoptimisetheprogram;see`[`A`
`transformation-based` `optimiser` `for` `Haskell`
`(SCP'98)`](http://research.microsoft.com/%7Esimonpj/Papers/comp-by-trans-scp.ps.gz)`foramore-or-lessaccurateoverview.See[wiki:Commentary/Compiler/Core2CorePipeline]foranoverviewoftheCore-to-Coreoptimisationpipeline.Themainpassesare:`[`BR`](BR "wikilink")[`BR`](BR "wikilink")\
`*The`**`Simplifier`**`,whichapplieslotsofsmall,localoptimisationstotheprogram.Thesimplifierisbigandcomplicated,becauseitimplementsa`*`lot`*`oftransformations;andtriestomakethemcascadenicely.Thetransformation-basedoptimiserpapergiveslotsofdetails,buttwootherpapersareparticularlyrelevant:`[`Secrets`
`of` `the` `Glasgow` `Haskell` `Compiler` `inliner`
`(JFP'02)`](http://research.microsoft.com/%7Esimonpj/Papers/inlining/index.htm)`and`[`Playing`
`by` `the` `rules:` `rewriting` `as` `a` `practical` `optimisation`
`technique` `in` `GHC` `(Haskell` `workshop`
`2001)`](http://research.microsoft.com/%7Esimonpj/Papers/rules.htm)`` .(`-ddump-simpl`) ``[`BR`](BR "wikilink")[`BR`](BR "wikilink")\
`*The`**`float-out`**`and`**`float-in`**`transformations,whichmovelet-bindingsoutwardsandinwardsrespectively.See`[`Let-floating:`
`moving` `bindings` `to` `give` `faster` `programs` `(ICFP`
`'96)`](http://research.microsoft.com/%7Esimonpj/papers/float.ps.gz)`.`[`BR`](BR "wikilink")[`BR`](BR "wikilink")\
`*The`**`strictness`
`analyser`**`.Thisactuallycomprisestwopasses:the`**`analyser`**`itselfandthe`**`worker/wrapper`**`transformationthatusestheresultsoftheanalysistotransformtheprogram.(Furtherdescribedin[wiki:Commentary/Compiler/DemandDemandanalysis].)Thesameanalyseralsodoes`[`Constructed`
`Product` `Result`
`analysis`](http://research.microsoft.com/%7Esimonpj/Papers/cpr/index.htm)`and`[`Cardinality`
`analysis`](http://research.microsoft.com/en-us/um/people/simonpj/papers/usage-types/cardinality-extended.pdf)`` .(`-ddump-stranal`) ``[`BR`](BR "wikilink")[`BR`](BR "wikilink")\
`*The`**`liberate-case`**`transformation.`[`BR`](BR "wikilink")[`BR`](BR "wikilink")\
`*The`**`constructor-specialialisation`**`transformation.`[`BR`](BR "wikilink")[`BR`](BR "wikilink")\
`*The`**`common` `sub-expression`
`eliminiation`**`` (CSE)transformation.(`-ddump-cse`) ``

`*Thenthe`**`!CoreTidy`
`pass`**`getsthecodeintoaforminwhichitcanbeimportedintosubsequentmodules(whenusing``)and/orputintoaninterfacefile.`\
\
`` Itmakesadifferencewhetherornotyouareusing`-O`atthisstage.With`-O`(orrather,with`-fomit-interface-pragmas`whichisaconsequenceof`-O`),thetidiedprogram(producedby`tidyProgram`)hasunfoldingsforIds,andRULES.Without`-O`theunfoldingsandRULESareomittedfromthetidiedprogram.Andthat,inturn,affectstheinterfacefilegeneratedsubsequently. ``

`Therearegoodnotesatthetopofthefile`[`GhcFile(compiler/main/TidyPgm.hs)`](GhcFile(compiler/main/TidyPgm.hs) "wikilink")`;themainfunctionis``,documentedas"PlanB"("PlanA"isasimplifiedtidypassthatisrunwhenwehaveonlytypechecked,buthaven'trunthedesugarerorsimplifier).`

`*Atthispoint,thedataflowforks.First,thetidiedprogramisdumpedintoaninterfacefile.Thisparthappensintwostages:`\
`*Itis`**`converted` `to`
**`(definedin`[`GhcFile(compiler/iface/IfaceSyn.hs)`](GhcFile(compiler/iface/IfaceSyn.hs) "wikilink")`and`[`GhcFile(compiler/iface/IfaceType.hs)`](GhcFile(compiler/iface/IfaceType.hs) "wikilink")`).`\
`*The``is`**`serialised` `into` `a` `binary` `output`
`file`**`(`[`GhcFile(compiler/iface/BinIface.hs)`](GhcFile(compiler/iface/BinIface.hs) "wikilink")`).`\
`` Theserialisationdoes(prettymuch)nothingexceptserialise.Alltheintelligenceisinthe`Core`-to-`IfaceSyn`conversion;or,rather,inthereverseofthatstep. ``

`*Thesame,tidiedCoreprogramisnowfedtotheBackEnd.Firstthereisatwo-stageconversionfrom``to[wiki:Commentary/Compiler/StgSynTypeGHC'sintermediatelanguage,StgSyn].`\
`*Thefirststepiscalled`**`!CorePrep`**`` ,aCore-to-CorepassthatputstheprogramintoA-normalform(ANF).InANF,theargumentofeveryapplicationisavariableorliteral;morecomplicatedargumentsarelet-bound.Actually`CorePrep`doesquiteabitmore:thereisadetailedlistatthetopofthefile ``[`GhcFile(compiler/coreSyn/CorePrep.hs)`](GhcFile(compiler/coreSyn/CorePrep.hs) "wikilink")`.`\
`*Thesecondstep,`**`!CoreToStg`**`,movestothe``datatype(`[`GhcFile(compiler/stgSyn/CoreToStg.hs)`](GhcFile(compiler/stgSyn/CoreToStg.hs) "wikilink")`).Theoutputof!CorePrepiscarefullyarrangedtoexactlymatchwhat``allows(notablyANF),sothereisverylittleworktodo.However,``isdecoratedwithlotsofredundantinformation(freevariables,let-no-escapeindicators),whichisgeneratedon-the-flyby``.`

`*Next,the`**`[wiki:Commentary/Compiler/CodeGen` `Code`
`Generator]`**`convertstheSTGprogramtoa``program.ThecodegeneratorisaBigMother,andlivesindirectory`[`GhcFile(compiler/codeGen)`](GhcFile(compiler/codeGen) "wikilink")``

`*Nowthepathforksagain:`\
`*IfwearegeneratingGHC'sstylisedCcode,wecanjustpretty-printthe``codeasstylisedC(`[`GhcFile(compiler/cmm/PprC.hs)`](GhcFile(compiler/cmm/PprC.hs) "wikilink")`)`\
`*Ifwearegeneratingnativecode,weinvokethenativecodegenerator.ThisisanotherBigMother(`[`GhcFile(compiler/nativeGen)`](GhcFile(compiler/nativeGen) "wikilink")`).`\
`*IfwearegeneratingLLVMcode,weinvoketheLLVMcodegenerator.Thisisareasonablysimplecodegenerator(`[`GhcFile(compiler/llvmGen)`](GhcFile(compiler/llvmGen) "wikilink")`).`

The Diagram
===========

This diagram is also located \[wiki:Commentary/Compiler/HscPipe here\],
so that you can open it in a separate window.

[Image(Commentary/Compiler/HscPipe:HscPipe2.png)](Image(Commentary/Compiler/HscPipe:HscPipe2.png) "wikilink")

Picture of the main compiler pipeline
=====================================

See \[wiki:Commentary/Compiler compiling one module\] for the commentary
on this diagram.

[Image(HscPipe2.png)](Image(HscPipe2.png) "wikilink")

[PageOutline](PageOutline "wikilink")

Video: [Abstract Syntax
Types](http://www.youtube.com/watch?v=lw7kbUvAmK4&list=PLBkRCigjPwyeCSD_DFxpd246YIF7_RDDI)
(1hr03')

The  types
=========

The program is initially parsed into "****", a collection of data types
that describe the full abstract syntax of Haskell. is a pretty big
collection of types: there are 52 data types at last count. Many are
pretty trivial, but a few have a lot of constructors ( has 40).
represents Haskell in its full glory, complete with all syntactic sugar.

The modules live in the
[GhcFile(compiler/hsSyn)](GhcFile(compiler/hsSyn) "wikilink") directory.
Each module declares a related group of declarations, *and* gives their
pretty-printer.

`*`[`GhcFile(compiler/hsSyn/HsSyn.hs)`](GhcFile(compiler/hsSyn/HsSyn.hs) "wikilink")`:therootmodule.Itexportseverythingyouneed,andit'sgenerallywhatyoushouldimport.`\
`*`[`GhcFile(compiler/hsSyn/HsBinds.hs)`](GhcFile(compiler/hsSyn/HsBinds.hs) "wikilink")`:bindings.`\
`*`[`GhcFile(compiler/hsSyn/HsImpExp.hs)`](GhcFile(compiler/hsSyn/HsImpExp.hs) "wikilink")`:importsandexports.`\
`*`[`GhcFile(compiler/hsSyn/HsDecls.hs)`](GhcFile(compiler/hsSyn/HsDecls.hs) "wikilink")`:top-leveldeclarations.`\
`*`[`GhcFile(compiler/hsSyn/HsExpr.hs)`](GhcFile(compiler/hsSyn/HsExpr.hs) "wikilink")`:expressions,matchexpressions,comprehensions.`\
`*`[`GhcFile(compiler/hsSyn/HsLit.hs)`](GhcFile(compiler/hsSyn/HsLit.hs) "wikilink")`:literals.`\
`*`[`GhcFile(compiler/hsSyn/HsPat.hs)`](GhcFile(compiler/hsSyn/HsPat.hs) "wikilink")`:patterns.`\
`*`[`GhcFile(compiler/hsSyn/HsTypes.hs)`](GhcFile(compiler/hsSyn/HsTypes.hs) "wikilink")`:types.`\
`*`[`GhcFile(compiler/hsSyn/HsUtils.hs)`](GhcFile(compiler/hsSyn/HsUtils.hs) "wikilink")`:utilityfunctions(nodatatypes).`

There is significant mutual recursion between modules, and hence a
couple of files. Look at \[wiki:ModuleDependencies\] to see the
dependencies.

Decorating \`HsSyn\` with type information
------------------------------------------

The type checker adds type information to the syntax tree, otherwise
leaving it as undisturbed as possible. This is done in two ways:

`*Someconstructorshaveafieldoftype``,whichisjustasynonymfor``.Forexample:`

`An``representstheexplicitlistconstructinHaskell(e.g."``").Theparserfillsthe``fieldwithanerrorthunk``;andtherenamerdoesnottouchit.Thetypecheckerfiguresoutthetype,andfillsinthevalue.Sountilthetypechecker,wecannotexamineorprintthe``fields.`

`` Theerrorthunksmeanthatwecan'tconvenientlypretty-printthe`PostTcType`fields,becausethepretty-printerwouldpoketheerrorthunkswhenrunonpre-typcheckedcode.Wecouldhavedefined`PostTcType`tobe`MaybeType`,butthatwouldhavemeantunwrappinglotsof`Just`constructors,whichismessy.Itwouldbenicertoparameterise`HsSyn`overthe`PostTcType`fields.Thus: ``

`ThiswouldbeaGoodThingtodo.`

`*Inafewcases,thetypecheckermovesfromoneconstructortoanother.Example:`

`Theparserandrenameruse``;thetypecheckergeneratesa``.Thisnamingconventionisusedconsistently.`

`*Thereareafewconstructorsaddedbytypechecker(ratherthanreplacinganinputconstructor),particularly:`\
`*``,inthe``type.`\
`*``,inthe``type.`\
`Theseareinvariablytodowithtypeabstractionandapplication,sinceHaskellsourceisimplicitlygeneralizedandinstantiated,whereasGHC'sintermediateformisexplicitlygeneralizedandinstantiated.`

Source Locations
----------------

\`HsSyn\` makes heavy use of the \`Located\` type
([GhcFile(compiler/basicTypes/SrcLoc.hs)](GhcFile(compiler/basicTypes/SrcLoc.hs) "wikilink")):
A \`Located t\` is just a pair of a \`SrcSpan\` (which describes the
source location of \`t\`) and a syntax tree \`t\`. The module \`SrcLoc\`
defines two other types:

`` *`SrcLoc`specifiesaparticularsourcelocation:(filename,linenumber,characterposition) ``\
`` *`SrcSpan`specifesarangeofsourcelocations:(filename,startlinenumberandcharacterposition,endlinenumberandcharacterposition) ``

More details in
[GhcFile(compiler/basicTypes/SrcLoc.hs)](GhcFile(compiler/basicTypes/SrcLoc.hs) "wikilink").

Naming convention within the code: "\`LHs\`" means located Haskell, e.g.

Interface files
===============

An **interface file** supports separate compilation by recording the
information gained by compiling in its interface file . Morally
speaking, the interface file is part of the object file ; it's like a
super symbol-table for .

Interface files are kept in binary, GHC-specific format. The format of
these files changes with each GHC release, but not with patch-level
releases. The contents of the interface file is, however, completely
independent of the back end you are using (\`-fviaC\`, \`-fasm\`,
\`-fcmm\` etc).

Although interface files are kept in binary format, you can print them
in human-readable form using the command: This textual format is not
particularly designed for machine parsing. Doing so might be possible,
but if you want to read GHC interface files you are almost certainly
better off using the \[wiki:Commentary/Compiler/API GHC API\] to do so.
If you are wondering how some particular language feature is represented
in the interface file, this command is really useful! Cross-reference
its output with the \`Outputable\` instance defined in
[GhcFile(compiler/iface/LoadIface.hs)](GhcFile(compiler/iface/LoadIface.hs) "wikilink")

Here are some of the things stored in an interface file

`*TheversionofGHCusedtocompilethemodule,aswellasthecompilationwayandotherknick-knacks`\
`*Alistofwhat``exports.`\
`*Thetypesofexportedfunctions,definitionofexportedtypes,andsoon.`\
`*Versioninformation,usedtodrivethe[wiki:Commentary/Compiler/RecompilationAvoidancerecompilationchecker].`\
`*Thestrictness,arity,andunfoldingofexportedfunctions.Thisiscrucialforcross-moduleoptimisation;butitisonlyincludedwhenyoucompilewith``.`

The contents of an interface file is the result of serialising the ****
family of data types. The data types are in
[GhcFile(compiler/iface/IfaceSyn.lhs)](GhcFile(compiler/iface/IfaceSyn.lhs) "wikilink")
and
[GhcFile(compiler/iface/IfaceType.lhs)](GhcFile(compiler/iface/IfaceType.lhs) "wikilink");
the binary serialisation code is in
[GhcFile(compiler/iface/BinIface.hs)](GhcFile(compiler/iface/BinIface.hs) "wikilink").
The definition of a module interface is the **** data type in
[GhcFile(compiler/main/HscTypes.lhs)](GhcFile(compiler/main/HscTypes.lhs) "wikilink").

Details of some of the types involved in GHC's representation of Modules
and Interface files can be found \[wiki:Commentary/Compiler/ModuleTypes
here\].

When is an interface file loaded?
---------------------------------

The act of loading an interface file can cause various parts of the
compiler to behave differently; for instance, a type class instance will
only be used if the interface file which defines it was loaded.
Additionally, GHC tries to avoid loading interface files if it can avoid
it, since every loaded interface file requires going to the file system
and parsing the result.

The big situations when we load an interface file:

-   When you import it (either explicitly using an \`import\`, or
    implicitly, e.g. through \`-fimplicit-import-qualified\` in
    GHCi; \`loadSrcInterface\`)
-   When we need to get the type for an identifier (\`loadInterface\`
    in \`importDecl\`)
-   When it is listed as an orphan of an imported module
    (\`loadModuleInterfaces "Loading orphan modules"\`)

We also load interface files in some more obscure situations:

-   When it is used as the backing implementation of a signature
    (\`loadSysInterface\` in \`tcRnSignature\`)
-   When we look up its family instances (\`loadSysInterface\`
    in \`getFamInsts\`)
-   When its information or safety (\`getModuleInterface\`
    in \`hscGetSafe\`)
-   When we an identifier is explicitly used (including a use from
    Template Haskell), we load the interface to check if the identifier
    is deprecated (\`loadInterfaceForName\` in
    \`warnIfDeprecated\`/\`loadInterfaceforName\` in \`rn\_bracket\`)
-   Recompilation checking (\`needInterface\` in \`checkModUsage\`)
-   When we need the fixity for an identifier (\`loadInterfaceForName\`
    in \`lookupFixityRn\`)
-   When we reify a module for Template Haskell
    (\`loadInterfaceForModule\` in \`reifyModule\`)
-   When we use a wired-in type constructor, since otherwise the
    interface file would not be loaded because the compiler already has
    the type for the identifier. (\`Loading instances for
    wired-in things\`)
-   When \`-XParallelArrays\` or \`-fvectorise\` are specified for DPH
    (\`loadModule\` in \`initDs\`)
-   When we load a plugin (\`DynamicLoading\`)
-   To check consistency against the \`hi-boot\` of a module
-   To check the old interface file for recompilation avoidance

Immix Garbage Collector
=======================

In a [Google Summer of Code
project](http://socghop.appspot.com/gsoc/student_project/show/google/gsoc2010/haskell/t127230760695),
[marcot](http://wiki.debian.org/MarcoSilva) started an implementation of
the Immix Garbage Collector in GHC. It's not in a state where it can be
included in GHC yet, but it's functional, don't have known bugs and gets
better results than the default GC in the
[nofib](http://www.dcs.gla.ac.uk/fp/software/ghc/nofib.html) suite. On
the other hand, it gets worse results than the default GC for the
nofib/gc suite. The implementation was reported on these blog posts:
[1](http://marcotmarcot.wordpress.com/2010/05/17/google-summer-of-code-weekly-report-1/)
[3](http://marcotmarcot.wordpress.com/2010/05/31/summer-of-code-weekly-report-3/)
[4](http://marcotmarcot.wordpress.com/2010/06/04/summer-of-code-weekly-report-4/)
[5](http://marcotmarcot.wordpress.com/2010/06/15/summer-of-code-weekly-report-5/)
[6](http://marcotmarcot.wordpress.com/2010/06/18/immix-on-ghc-summer-of-code-weekly-report-6/)
[7](http://marcotmarcot.wordpress.com/2010/06/29/immix-on-ghc-summer-of-code-weekly-report-7/)
[8](http://marcotmarcot.wordpress.com/2010/07/05/immix-on-ghc-summer-of-code-weekly-report-8/)
[9](http://marcotmarcot.wordpress.com/2010/07/07/immix-on-ghc-summer-of-code-weekly-report-9/)
[10](http://marcotmarcot.wordpress.com/2010/07/21/immix-on-ghc-summer-of-code-weekly-report-10/)
[11](http://marcotmarcot.wordpress.com/2010/08/10/immix-on-ghc-summer-of-code-report-11/)
[12](http://marcotmarcot.wordpress.com/2010/08/13/immix-on-ghc-summer-of-code-report-12-debconf-debian-day-bh/)

The patches
===========

There are [some patches
available](http://people.debian.org/~marcot/immix/).

The main patch
--------------

`*`[`Generated` `with` `darcs` `diff`
`-u`](http://people.debian.org/~marcot/immix/immix.patch)\
`*`[`Darcs`
`bundle`](http://people.debian.org/~marcot/immix/immix.dpatch)

This patch includes the basic implementation of Immix. It's tested, and
has no known bugs. In [the
measurements](http://people.debian.org/~marcot/immix/log.tar.gz), it has
shown these results:

|| || **Runtime** || **Memory used** || || **nofib** || -2.9% || -1.7%
|| || **nofib/gc** || +4.3% || +1.2% ||

Currently, it overwrites the \[wiki:Commentary/Rts/Storage/GC/Sweeping
mark/sweep algorithm\]. It uses the same mark bits as
\[wiki:Commentary/Rts/Storage/GC/Marking mark/compact and mark/sweep\],
but consider these bits in groups of 32 or 64, depending on the
architecture used, which are called lines. It creates a list of free
lines for each
[generation](http://hackage.haskell.org/trac/ghc/wiki/Commentary/Rts/Storage/GC/Aging),
and allocates on them when possible.

As only the first part of each object in memory is marked in the
\[wiki:Commentary/Rts/Storage/GC/Marking bitmap\], it skips the first
free line for each group of subsequent lines, because it's possible that
an object that starts in the previous line is using part of it. Also, it
doesn't deal with \[wiki:Commentary/Rts/Storage/BlockAlloc blocks\] that
objects bigger than the size of a line, called medium sized objects,
marked with \`BF\_MEDIUM\`.

The mark stack is used to ensure that the objects allocated on lines get
scavenged.

Line before inscreasing block size
----------------------------------

`*`[`Generated` `with` `darcs` `diff`
`-u`](http://people.debian.org/~marcot/immix/order.patch)\
`*`[`Darcs`
`bundle`](http://people.debian.org/~marcot/immix/order.dpatch)

Before the implementation of Immix, the code in todo\_block\_full did
the following:

`1.Trytoincreasetheblocksize.`\
`2.Ifitcouldnotbeincreased,getanewblock.`

With Immix, it turned to:

`1.Ifwewereallocatinginablock,trytoincreasetheblocksize.`\
`2.Ifitcouldnotbeincreased,searchforaline.`\
`3.Ifthere'renofreelines,getanewblock.`

Another possibility for it is:

`1.Searchforaline.`\
`2.Iftherearenofreelines`**`and`**`wewereallocatinginablock,trytoincreasetheblock.`\
`3.Ifitcouldnotbeincreased,getanewblock.`

Basically, this swaps 1 and 2, making it prefer allocating on lines than
increasing the block size. In the measurements done so far, it has not
shown significative improvements over the way the code is now, so I'll
keep it here to benchmark again when another thing changes, like:

Allocate in lines in minor GCs
------------------------------

`*`[`Generated` `with` `darcs` `diff`
`-u`](http://people.debian.org/~marcot/immix/minor.patch)\
`*`[`Darcs`
`bundle`](http://people.debian.org/~marcot/immix/minor.dpatch)

This small patch makes it possible to allocate on lines during minor
GCs, removing the check about being in a major GC for the search for
lines and for the creating of the mark stack. Maybe it shouldn't be so
small, because it's not working. The code is being debugged, and
possibly there will be a fix soon.

Remove partial list
-------------------

With the allocation on lines, it's possible not to allocate on partially
full blocks. By making all blocks full (with possibly free lines),
there'll be no need to use the list of partial blocks. This is not done
yet.

To do
=====

`*MakeitfasteranduselessmemorythanthedefaultGCforallbenchmarks`\
`*Correct"AllocateinlinesinminorGCs"`\
`*Implementandbechmark"Removepartiallists"`

[PageOutline](PageOutline "wikilink")

GHC Source Tree Roadmap: includes/
==================================

This directory contains C header files that are included in a GHC
distribution. The headers fall into several categories.

External APIs
-------------

These are header files that define an external API to the RTS that can
be used by client code. These interfaces are intended to be relatively
stable:

`[source:includes/HsFFI.hHsFFI.h]::`\
`TheexternalFFIapi,asrequiredbytheFFIspec`

`[source:includes/RtsAPI.hRtsAPI.h]::`\
`TheAPIforcallingintotheRTS.Usedbytheimplementation`\
`` of`foreignexport`calls,butmayalsobeusedbyexternal ``\
`clients.`

`[source:includes/Rts.hRts.h]::`\
`Thisheaderfiledefineseverythingthatisvisible`\
`` externallytotheRTS.Itincludes`Stg.h`andeverything ``\
`` inthe`rts`subdirectory. ``

Derived Constants
-----------------

The canonical definition of certain structures are in C header files.
For example, the layout of closures and info tables are defined in the
headers \[source:includes/rts/storage/Closures.h Closures.h\] and
\[source:includes/rts/storage/InfoTables.h InfoTables.h\] respectivesly.
How do we get the information about the layout of these structures to
the parts of the system that are not written in C, such as the compiler
itself, or the C-- code in the RTS?

Our solution is the Haskell program in
\[source:utils/deriveConstants/DeriveConstants.hs\]. It determines the
sizes and fields offsets from the C header files by invoking the C
compiler for the target platform, and then looking at the resulting
object file (we can't *run* the code generated by the target C compiler,
because this is the host platform).

The !DeriveConstants program generates a few header files, notably
\`includes/dist-derivedconstants/header/DerivedConstants.h\`, which
contains C \`\#define\`s for each of the derived constants; this file is
used by C-- code in the RTS. It also generates a few files of Haskell
code which are included into GHC itself, in the \`DynFlags\` module.

Used when compiling via C
-------------------------

These header files are \`\#included\` into the \`.hc\` file generated by
GHC when it compiles Haskell code to C. They are also \`\#included\` by
\`Rts.h\`, so the definitions from these files are shared by the RTS
code.

These days the amount of stuff included this way is kept to a minimum.
In particular, there are no function prototypes: all calls to C
functions from \`.hc\` files are given types at the call site.

`[source:includes/Stg.hStg.h]::`\
`` Thetopofthehierarchyis`Stg.h`,whichincludeseverything ``\
`` requiredby`.hc`code.Mostfiles`#included`by`Stg.h`areinthe ``\
`` `stg`subdirectory. ``

`[source:includes/ghcconfig.hghcconfig.h]::`\
`` Configurationinfoderivedbythe`configure`script. ``\
`[source:includes/MachDeps.hMachDeps.h]::`\
`` Sizesofvariousbasictypes(shouldbeinthe`stg`subdirectory, ``\
`butlefthereforbackwards-compatibilityreasons).`\
`[source:includes/stg/DLL.hstg/DLL.h]::`\
`StuffrelatedtoWindowsDLLs.`\
`[source:includes/stg/MachRegs.hstg/MachRegs.h]::`\
`Globalregisterassignmentsforthisprocessor.`\
`[source:includes/stg/MiscClosures.hstg/MiscClosures.h]::`\
`Declarationsforclosures&infotablesbuilt-intotheRTS`\
`[source:includes/stg/Regs.hstg/Regs.h]::`\
`"registers"inthevirtualmachine.`\
`[source:includes/stg/SMP.hstg/SMP.h]::`\
`AtomicmemoryoperationsforSMPsupport`\
`[source:includes/stg/Ticky.hstg/Ticky.h]::`\
`Declarationsforticky-tickycounters`\
`[source:includes/stg/Types.hstg/Types.h]::`\
`` Basictypesspecifictothevirtualmachine(eg.`StgWord`). ``

The RTS external APIs
---------------------

The header \[source:includes/Rts.h Rts.h\] includes all the headers
below the \`rts\` subdirectory, which together define the RTS external
API. Virtually all RTS code \`\#includes\` \`Rts.h\`.

The rts header files are divided into a few directories:

`*[source:includes/rtsincludes/rts]:Mostof`\
`theexternalRTSAPIs,inseparateheaderfilesper-subsystem`

`*[source:includes/rts/storageincludes/rts/storage]:Definitionsofthelayoutofheapandstack`\
`objects,infotables,structuresthatdefinememoryareasmanaged`\
`bytheGC,andmemorymanagementAPIs.`

`*[source:includes/rts/profincludes/rts/prof]:`\
`Interfacesanddefinitionsforprofiling.`

Included into C-- (\`.cmm\`) code
---------------------------------

`[source:includes/Cmm.hCmm.h]::`\
`` includedinto`.cmm`sourceonly;providesusefulmacrosforwriting ``\
`low-levelC--codeforGHC.`

[PageOutline](PageOutline "wikilink")

Installing & Using the LLVM Back-end
====================================

Installing
----------

The LLVM backend is now included in GHC HEAD. Just grab the git HEAD
version of GHC and build it. The backend now also supports all modes
that GHC can be built in so you shouldn't need to change your build.mk
file either.

For instructions on building GHC go
[here](http://hackage.haskell.org/trac/ghc/wiki/Building)

LLVM Support
------------

The LLVM backend only supports LLVM version **2.7** or later. Problems
with LLVM &gt;= 2.9 and GHC 7.0.3 currently exist (see bug \#5103). GHC
7.2 and later works fine with LLVM &gt;= 2.9.

Simply install GHC and make sure the various llvm tools (opt, llc) are
available on your path.

Using
-----

Once built you can check that you have the LLVM backend GHC will support
these extra options:

`*`*`-fllvm`*`-Compilecodeusingthellvmbackend`\
`*`*`-pgmlo`*`-Theprogramtouseasthellvmoptimiser`\
`*`*`-pgmlc`*`-Theprogramtouseasthellvmcompiler`\
`*`*`-optlo`*`-Extraoptionstopasstothellvmoptimiser`\
`*`*`-optlc`*`-Extraoptionstopasstothellvmcompiler`\
`*`*`-ddump-llvm`*`-DumpsthellvmIRwhilecompiling`\
`*`*`-keep-llvm-files`*`-Keepacopyofthellvmintermediatefilearound`

Supported Platforms & Correctness
---------------------------------

`*Linuxx86-32/x86-64:Currentlywellsupported.Theback-endcanpassthetestsuiteandbuildaworkingversionofGHC(bootstraptest).`\
`*Windowsx86-32:Currentlywellsupported.Theback-endcanpassthetestsuiteandbuildaworkingversionofGHC(bootstraptest).`\
`*MacOSX10.5/10.6(x86-32/x86-64):Currentlywellsupported.Theback-endcanpassthetestsuiteandbootstrapGHC.OSXhascausedalotmoreproblemsthenLinuxorWindowsanddoesafewthingsslightlydifferentlythenthem.Itisquitestablethesedaysthough.`\
`*ARM:WorkiscurrentlyprogressingtofullysupportGHCusingtheLLVMbackendonARM.Youcanseeablogwithinfoaboutthis`[`here`](http://ghcarm.wordpress.com/)`.`\
`*Otherplatformshaven'tbeentestedatall.`

Shared Libraries
----------------

Shared libraries are supported on Linux x64 and Mac OSX x64. Other
platforms aren't supported.

Performance
-----------

(All done on linux/x86-32)

A quick summary of the results are that for the 'nofib' benchmark suite,
the LLVM code generator was 3.8% slower than the NCG (the C code
generator was 6.9% slower than the NCG). The DPH project includes a
benchmark suite which I (David Terei) also ran and for this type of code
using the LLVM back-end shortened the runtime by an average of 25%
compared to the NCG. Also, while not included in my thesis paper as I
ran out of time, I did do some benchmarking with the 'nobench' benchmark
suite. It gave performance ratios for the back-ends of around:

||NCG || 1.11|| ||C || 1.05|| ||LLVM || 1.14||

A nice demonstration of the improvements the LLVM back-end can bring to
some code though can be see at
<http://donsbot.wordpress.com/2010/02/21/smoking-fast-haskell-code-using-ghcs-new-llvm-codegen/>

[PageOutline](PageOutline "wikilink")

GHC Commentary: Libraries/Integer
=================================

GHC is set up to allow different implementations of the \`Integer\` type
to be chosen at build time.

Selecting an Integer implementation
-----------------------------------

You can select which implementation of Integer is used by defining
\`INTEGER\_LIBRARY\` in \`mk/build.mk\`. This tells the build system to
build the library in \`libraries/\$(INTEGER\_LIBRARY)\`, and the
\`cIntegerLibrary\` and \`cIntegerLibraryType\` values in \`Config.hs\`
are defined accordingly.

The default value is \`integer-gmp\`, which uses the [GNU Multiple
Precision Arithmetic Library (GMP)](http://gmplib.org/) to define the
Integer type and its operations.

The other implementation currently available is \`integer-simple\`,
which uses a simple (but slow, for larger Integers) pure Haskell
implementation.

The Integer interface
---------------------

All Integer implementations should export the same set of types and
functions from \`GHC.Integer\` (within whatever \`integer\` package you
are using). These exports are used by the \`base\` package However, all
of these types and functions must actually be defined in
\`GHC.Integer.Type\`, so that GHC knows where to find them.
Specifically, the interface is this:

How Integer is handled inside GHC
---------------------------------

`*`**`Front`
`end`**`` .Integersarerepresentedusingthe`HsInteger`constructorof`HsLit`fortheearlyphasesofcompilation(e.g.typechecking) ``

`*`**`Core`**`` .In`Core`representation,anintegerliteralisrepresentedbythe`LitInteger`constructorofthe`Literal`type. ``

`` While`Integer`saren't"machineliterals"liketheother`Core``Literal`constructors,itismoreconvenientwhenwritingconstantfoldingRULEStopretendthattheyareliteralsratherthanhavingtounderstandtheirconcreterepresentation.(Especiallyastheconcreterepresentationvariesfrompackagetopackage.)Wealsocarryarounda`Type`,representingthe`Integer`type,intheconstructor,asweneedaccesstoitinafewfunctions(e.g.`literalType`). ``

`*`**`Constant`
`folding`**`` .Therearemanyconstant-foldingoptimisationsfor`Integer`expressedasbuilt-inrulesin ``[`GhcFile(compiler/prelude/PrelRules.lhs)`](GhcFile(compiler/prelude/PrelRules.lhs) "wikilink")`` ;lookat`builtinIntegerRules`.Allofthetypesandfunctionsinthe`Integer`interfacehavebuilt-innames,e.g.`plusIntegerName`,definedin ``[`GhcFile(compiler/prelude/PrelNames.lhs)`](GhcFile(compiler/prelude/PrelNames.lhs) "wikilink")`` andincludedin`basicKnownKeyNames`.Thisallowsustomatchonallofthefunctionsin`builtinIntegerRules`in ``[`GhcFile(compiler/prelude/PrelRules.lhs)`](GhcFile(compiler/prelude/PrelRules.lhs) "wikilink")`` ,sowecanconstant-foldIntegerexpressions.AnimportantthingaboutconstantfoldingofIntegerdivisionsisthattheydependoninlining.Here'safragmentof`IntegralInteger`instancedefinitionfrom`libraries/base/GHC/Real.lhs`: ``

`` Constantfoldingrulesfordivisionsaredefinedfor`quotInteger`andotherdivisionfunctionsfrom`integer-gmp`library.If`quot`wasnotinlinedconstantfoldingruleswouldnotfire.Theruleswouldalsonotfireifcallto`quotInteger`wasinlined,butthisdoesnothappenbecauseitismarkedwithNOINLINEpragma-seebelow. ``

`*`**`Converting` `between` `Int` `and`
`Integer`**`` .It'squitecommonlythecasethat,aftersomeinlining,wegetsomethinglike`integerToInt(intToIntegeri)`,whichconvertsan`Int`toan`Integer`andback.This ``*`must`*`` optimiseaway(see#5767).Wedothisbyrequiringthatthe`integer`packageexposes ``

`` Nowwecandefine`intToInteger`(or,moreprecisely,the`toInteger`methodofthe`IntegralInt`instancein`GHC.Real`)thus ``

`` AndwehaveaRULEfor`integerToInt(smallIntegeri)`. ``

`*`**`Representing`
`integers`**`` .Westicktothe`LitInteger`representation(whichhidestheconcreterepresentation)aslateaspossibleinthecompiler.Inparticular,it'simportantthatthe`LitInteger`representationisusedinunfoldingsininterfacefiles,sothatconstantfoldingcanhappenonexpressionsthatgetinlined. ``

`` Wefinallyconvert`LitInteger`toapropercorerepresentationofIntegerin ``[`GhcFile(compiler/coreSyn/CorePrep.lhs)`](GhcFile(compiler/coreSyn/CorePrep.lhs) "wikilink")`` ,whichlooksuptheIdfor`mkInteger`andusesittobuildanexpressionlike`mkIntegerTrue[123,456]`(wherethe`Bool`representsthesign,andthelistof`Int`sare31bitchunksoftheabsolutevaluefromlowesttohighest). ``

`` However,thereisaspecialcasefor`Integer`sthatarewithintherangeof`Int`whenthe`integer-gmp`implementationisbeingused;inthatcase,weusethe`S#`constructor(via`integerGmpSDataCon`in ``[`GhcFile(compiler/prelude/TysWiredIn.lhs)`](GhcFile(compiler/prelude/TysWiredIn.lhs) "wikilink")`)tobreaktheabstractionanddirectlycreatethedatastructure.`

`*`**`Don't` `inline` `integer`
`functions`**`` .MostofthefunctionsintheIntegerimplementationinthe`integer`packagearemarked`NOINLINE`.Forexamplein`integer-gmp`wehave ``

`` Notonlyisthisabigfunctiontoinline,butinliningittypicallydoesnogoodbecausetherepresentationofliteralsisabstact,sonopattern-matchingcancellationhappens.Andevenifyouhave`(a+b+c)`,theconditionalsmeanthatnocancellationhappens,oryougetanexponentialcodeexplosion! ``

An Integrated Code Generator for GHC
====================================

We propose reworking GHC's back end into an **Integrated Code
Generator**, which will widen the interface between machine-independent
and machine-dependent parts of the back end. We wish to **dissolve the
barrier** between the current machine-independent transformations (CPS
conversion, stack layout, etc) and the native-code generators
(instruction selection, calling conventions, register allocation --
including spilling to the C stack, etc). The goal is instead to have a
code generator that **integrates both machine-independent and
machine-dependent components**, which will interact through wide but
well-specified interfaces. From this refactoring we expect the following
benefits:

`*`**`The` `back` `end` `will` `be` `simpler`
`overall`**`,primarilybecausethe`\
`refactoringwillreduceoreliminateduplicationofcode`

`*`**`Complexity` `will` `be`
`isolated`**`intwomoduleswithwell-defined`\
`interfaces:adataflowengineandaregisterallocator`

`*`**`GHC` `will` `generate` `better` `machine`
`code`**`,primarilybecause`\
`importantdecisionsaboutregisterusagewillbemadeatalater`\
`stageoftranslationandwillexploitknowledgeoftheactual`\
`targetmachine.`

Design elements
---------------

The important elements of our design are as follows:

`0.Buildtwobighammers,andhitasmanynailsaspossible.(Thebighammersarethe`**`dataflow`
`optimization` `engine`**`anda`**`coalescing` `register`
`allocator.`**`Formoreontheiruses,seeour[wiki:Commentary/Compiler/IntegratedCodeGen#Designphilosophydesignphilosophy].)Thehammeritselfmaybebigandcomplicated,but`**`using`
`a` `big` `hammer` `should` `be`
`easy`**`andshouldgiveeasilypredictableresults.`\
`0.Loadallbackendsintoeveryinstanceofthecompiler,and`**`treat`
`every` `compilation` `as` `a`
`cross-compilation.`**`Despitehavingbeenusedinproductioncompilersforatleasttwentyyears,thistechniqueisstillseenassomewhatunorthodox,butitremovesmany``sandsavessignificantcomplexityatcompiler-configurationtime.Removing``salsomitigatesproblemswithvalidatingthecompilerunderdifferentbuildconfigurations.`

Design philosophy
-----------------

State-of-the art dataflow optimization and register allocation both
require complex implementations. We live with this complexity because
**creating new clients is easy.**

`*`**`Dataflow` `optimization:`**`Wecandefineanew`\
`optimizationsimplybydefiningalatticeofdataflowfacts(akin`\
`toaspecializedlogic)andthenwritingthedataflow-transfer`\
`functionsfoundincompilertextbooks.Handingthesefunctionsto`\
`thedataflowengineproducesanewoptimizationthatisnotonly`\
`usefulonitsown,butthatcaneasilybecomposedwithother`\
`optimizationstocreateanintegrated"superoptimization"thatis`\
`strictlymorepowerfulthananysequenceofindividualoptimizations,`\
`nomatterhowmanytimestheyarere-run.`\
`Thedataflowengineisbasedon`\
``[`(Lerner,` `Grove,` `and` `Chambers`
`2002)`](http://citeseer.ist.psu.edu/old/lerner01composing.html)`;`\
`youcanfindafunctionalimplementationofthedataflowenginepresentedin`\
``[`(Ramsey` `and` `Dias`
`2005)`](http://www.cs.tufts.edu/~nr/pubs/zipcfg-abstract.html)`.`

`*`**`Coalescing` `register`
`allocator:`**`Thebackendcanusefreshtemporariesandregister-registermoves`\
`withabandon,knowingthatastate-of-the-artregisterallocator`\
`willeliminatealmostallmoveinstructions.`

`*`**`Back`
`ends:`**`Ourultimategoalistomakeaddinganewbackendeasyaswell.`\
`Inthelongrun,wewishtoapplyJohnDias'sdissertationworktoGHC.`\
`Intheshortrun,however,we`\
`thinkitmoresensibletorepresenteachtarget-machineinstruction`\
`setwithanalgebraicdatatype.Weproposetousetypeclassesto`\
`definecommonfunctionssuchasidentifyingtheregistersreadand`\
`writtenbyeachinstruction.`

Proposed compilation pipeline
-----------------------------

`0.Convertfrom``toancontrolflowgraph``:`\
`0.Instructionselection:`\
`0.Optimise:`\
`0.Proc-pointanalysis,andtransformation`\
`0.Registerallocation`\
`0.Stacklayout`\
`0.Tidyup`

### Convert from STG to control flow graph

Convert from to an control flow graph
([GhcFile(compiler/cmm/ZipCfg.hs)](GhcFile(compiler/cmm/ZipCfg.hs) "wikilink"),
[GhcFile(compiler/cmm/ZipCfgCmmRep.hs)](GhcFile(compiler/cmm/ZipCfgCmmRep.hs) "wikilink")).
This step is Simon PJ's "new code generator" from September 2007. This
conversion may introduce new variables, stack slots, and compile-time
constants.

`*Implementscallingconventionsforcall,jump,andreturninstructions:allparameterpassingisturnedintodata-movementinstructions(register-to-registermove,load,orstore),andstack-pointeradjustmentsareinserted.Afterthispoint,calls,returns,andjumpsarejustcontrol-transferinstructions--theparameterpassinghasbeencompiledaway.`\
`*Howdowerefertolocationsonthestackwhenwehaven'tlaiditoutyet?Thecompilernamesastackslotusingtheideaofa"latecompile-timeconstant,"whichisjustasymbolicconstantthatwillbereplacedwithanactualstackoffsetwhenthestacklayoutischosen.Onedeparturefromtheoldcodegeneratoristhat`**`we`
`do` `not` `build` `a` `abstract-syntax`
`tree;`**`insteadwegostraighttoacontrol-flowgraph.`

In practice, we first generate an "abstract control flow graph",
\`CmmAGraph\`, which makes the business of generating fresh \`BlockId\`s
more convenient, and convert that to a \`CmmGraph\`. The former is
convenient for *construction* but cannot be analysed; the latter is
concrete, and can be analyzed, transformed, and optimized.

### Instruction selection

Instruction selection: each and node in the control-flow graph is
replaced with a new graph in which the nodes are machine instructions.
The type represents computational machine instructions; the type
represents control-transfer instructions. The choice of representation
is up to the author of the back end, but for continuity with the
existing native code generators, we expect to begin by using algebraic
data types inspired by the existing definitions in
[GhcFile(compiler/nativeGen/MachInstrs.hs)](GhcFile(compiler/nativeGen/MachInstrs.hs) "wikilink").

Note that the graph still contains:

`*`**`Variables`**`(ielocalregisterthatarenotyetmappedtoparticularmachineregisters)`\
`*`**`Stack-slot` `addressing`
`modes`**`,whichincludelate-boundcompile-timeconstants,suchastheoffsetintheframeoftheavariablespilllocation,or!BlockIdstack-top-on-entry.`

The invariant is that each node could be done by one machine
instruction, provided each \`LocalReg\` maps to a (suitable) physical
register; and an instruction involving a stack-slot can cope with
(Sp+n).

An **extremely important distinction** from the existing code is that we
plan to eliminate and instead provide multiple datatypes, e.g., in , , ,
and so on.

Similarly, we expect a an instruction selector for *each* back end, so
for example, we might have a transformation that maps (with variables,
stack slots, and compile-time constants) (with variables, stack slots,
and compile-time constants).

We expect to **abstract away from the details of these representations**
by borrowing some abstractions from [Machine
SUIF](http://www.eecs.harvard.edu/hube/software/nci/overview.html). In
the longer term we would like to support RTL-based representations such
as are used in gcc, vpo and Quick C--. What this means is that
\`I386.Middle\` (etc) is an abstract type, an instance of type class
that supports the functions that the rest of the pipeline needs. For
example: This allows us to **make code improvements
machine-independent**, by using machine-dependent functions to capture
the semantics of instructions. Figuring out precisely what the interface
should be is a key step. For example, to support copy propagation we
might want an operation Similarly, to support peephole optimsation (eg
transform \`x = y+2; p = bits32\[x\]\` to \`p = bits32\[y+2\]\`) we
might want something like The \`substExprs\` operation returns a
\`Just\` iff a substitution took place.

Interfaces like these would require the machine-specific abstract type
\`i\` to contain enough information to reconstruct a \`LocalReg\` or
\`CmmExpr\`. Later one, we'll need to construct SRTs too, so we must
continue to track pointer-hood.

One possible implementation for \`I386\` or \`Sparc\` would be to use a
generic RTL representation, together with a recogniser to maintain the
machine invariant. Our initial idea, though, is that is an
implementation choice. It's still possible that a machine-independent
optimisation could take advantage of the representation being an RTL.
For example, we could provide a function in the \`Instr\` class which is
particularly cheap for architectures that do use \`RTL\` as the
representation type.

### Optimisation

Optimise the code. (with variables, stack slots, and compile-time
constants) (with variables, stack slots, and compile-time constants),
such as

`*Branchchainelimination.`\
`*Removeunreachableblocks(deadcode).`\
`*Constantpropagation.`\
`*Copypropagation.`\
`*Lazycodemotion(hoisting,sinking,partialredundancyelimination).`\
`*Blockconcatenation.branchtoK;andthisistheonlyuseofK.`\
`*CommonBlockElimination(likeCSE).ThisessentiallyimplementstheAdamsoptimisation,webelieve.`\
`*Consider(sometime):blockduplication.branchtoK;andKisashortblock.Branchchaineliminationisjustaspecialcaseofthis.`\
`*Peepholeoptimisation.Thedifficultyofimplementingagoodpeepholeoptimizervariesgreatlywiththerepresentationofinstructions.WeproposetopostponeseriousworkonpeepholeoptimizationuntilwehaveabackendcapableofrepresentingmachineinstructionsasRTLs,whichmakespeepholeoptimizationtrivial.`

### Proc-point analysis

 Both input and output still have variables and stack-slot addressing
modes.

`*Procpointsarefound,andtheappropriatecontrol-transferinstructionsareinserted.`\
`*Whysoearly(beforeregisterallocation,stacklayout)?Dependingonthebackend(thinkofCastheworstcase),theproc-pointanalysismighthavetosatisfysomehorriblecallingconvention.Wewanttomaketheserequirementsexplicitbeforewegettotheregisterallocator.Wealsowantto`**`exploit`
`the` `register`
`allocator`**`tomakethebestpossibledecisionsabout`*`which`
`live` `variables` `(if` `any)` `should` `be` `in` `registers` `at` `a`
`proc` `point`*`.`

### Register allocation

Register allocation replaces variable references with machine register
and stack slots. This may introduce spills and reloads (to account for
register shortage), which which is why we may get new stack-slot
references.

That is, register allocation takes (with variables, stack slots) (with
stack slots only). No more variables!

We no longer need to spill to the C stack, because we have fully
allocated everything to machine registers.

### Stack layout

Stack Layout: (with stack slots, and compile-time constants)

`*Chooseastacklayout.`\
`*Replacereferencestostackslotswithaddressesonthestack.`\
`*Replacecompile-timeconstantswithoffsetsintothestack.`

No more stack-slot references.

### Tidy up

`0.Proc-pointsplitting:```\
`*Eachprocpointgetsitsownprocedure.`\
`0.Codelayout:`\
`*Areversepostorderdepth-firsttraversalsimultaneouslyconvertsthegraphtosequentialcodeandconvertseachinstructionintoanassembly-codestring:`**`Assembly`
`code` `ahoy`**`!`

Machine-dependence
------------------

A key property of the design is that the scopes of machine-dependent
code and machine-dependent static types are limited as much as possible:

`0.Therepresentationofmachineinstructionsmaybemachine-dependent(algebraicdatatype),orwemayuseamachine-independentrepresentationthatsatisfiesamachine-dependentdynamicinvariant(RTLs).Thebackendshouldbedesignedinsuchawaythatmostpassesdon'tknowthedifference;weintendtoborrowheavilyfromMachineSUIF.Todefinetheinterfaceusedtoconcealthedifference,MachineSUIFusesC++classes;wewilluseHaskell'stypeclasses.`\
`0.Instructionselectionisnecessarilymachine-dependent,andmoreover,itmustknowtherepresentationofmachineinstructions`\
`0.Mostoftheoptimizerneednotknowtherepresentationofmachineinstructions.`\
`0.Otherpasses,includingregisterallocation,stacklayout,andsoon,shouldbecompletelymachine-independent.`\
`0.RTLsarenotanewrepresentation;theyareatrivialextensionofexisting``representations.`

GHC Commentary: The byte-code interpreter and dynamic linker
============================================================

Linker
------

The linker lives in \`rts/Linker.c\` and is responsible for handling
runtime loading of code into a Haskell process. This is something of a
big blob of unpleasant code, and see DynamicGhcPrograms for information
about efforts to reduce our dependence on this linker.

Nevertheless, GHC's linker certainly adds functionality, and this has
been enough to earn its keep (for now). In particular, the linker knows
how to \*\*relocate static libraries\*\* (e.g. \`.o\` and \`.a\`
libraries). This is a pretty rare feature to find: ordinarily, libraries
that are to be loaded at runtime are compiled as position independent
code (-fPIC), which allows the same physical code pages to be shared
between processes, reducing physical memory usage. At runtime, GHC
rewrites the relocations, meaning that the resulting page cannot be
shared across processes, but that the result is just as efficient as if
the code had been statically linked to begin with.

Implementation of the linker cuts three axes: object file format (ELF,
Mach-O, PEi386), operating system (Linux, MingW, Darwin, etc), and
architecture (i386, x86\_64, powerpc, arm), and there are corresponding
sets of macros for fiddling with each (\`OBJFORMAT\_\*\`,
\`\*\_HOST\_OS\` and \`\*\_HOST\_ARCH\`). Are large part of the
unpleasantness of the current linker is the fact that all of these
different concerns are jumbled in one file; refactoring these out to
separate files would be a very nice service.

(write more here)

Bytecode Interpreter
--------------------

------------------------------------------------------------------------

CategoryStub

The I/O Manager
===============

This page describes the internals of the I/O manager, the latest version
of which can be found in
[GHC.Event](http://hackage.haskell.org/packages/archive/base/latest/doc/html/GHC-Event.html).
The I/O manager's job is to to provide a blocking I/O API to the user
without forcing the RTS to create one operating system thread per
Haskell thread. We here focus on the *threaded* RTS on non-Windows
platforms.

ezyang: **WARNING: some of this information may be out of date**

The RTS keeps a global list of pending events, unsuprising called
\`pendingEvents\`, containing a elements of the following data type:

When a thread wants to read from a file descriptor \`fd\` it calls
\`threadWaitRead\` which in turn calls \`waitForReadEvent\`.

\`waitForReadEvent\` creates a new \`MVar\`, adds it to
\`pendingEvents\` and finally blocks on it. \`pendingEvents\` gets read
by the I/O manager thread which runs the event loop, in GHC called
\`service\_loop\`. It roughly performs these steps:

`` 1.PickupnewI/Orequestsfrom`pendingRequests`andsetthevariabletotheemptylist. ``\
`` 2.Createdatastructuresappropriateforcalling`select`. ``\
`` 3.Foreach`Read`requestin`pendingEvents`checkifthefiledescriptorisinthereadysetreturnedby`select`.Ifsoperforma`putMVar`onthe`MVar`associatedwiththatrequesttowakeuptheblockedthread. ``\
`4.Repeatfromstep1.`

Key data types
==============

The key to understanding GHC is to understand its key data types. There
are pages describing many of them here (please add new pages!). The
diagram below shows their inter-dependencies.

`*[wiki:Commentary/Compiler/HsSynTypeThesourcelanguage:HsSyn]`\
`*[wiki:Commentary/Compiler/RdrNameTypeRdrNames,Modules,andOccNames]`\
`*[wiki:Commentary/Compiler/ModuleTypesModIface,ModDetails,ModGuts]`\
`*[wiki:Commentary/Compiler/UniqueUniques]:Notdrawninthediagram,becausenearlyeverythingdependsonUniques.`\
`*[wiki:Commentary/Compiler/NameTypeNames]`\
`*[wiki:Commentary/Compiler/EntityTypesEntities]:variables,typeconstructors,dataconstructors,andclasses.`\
`*Types:[wiki:Commentary/Compiler/TypeTypeTypeandKind],[wiki:Commentary/Compiler/FCequalitytypesandcoercions]`\
`*[wiki:Commentary/Compiler/CoreSynTypeThecorelanguage]`\
`*[wiki:Commentary/Compiler/StgSynTypeTheSTGlanguage]`\
`*[wiki:Commentary/Compiler/CmmTypeTheCmmlanguage]`\
`*[wiki:Commentary/Compiler/BackEndTypesBackendtypes]`

[Image(types.png)](Image(types.png) "wikilink")

Kinds
=====

Kinds classify types. So for example: The base kinds are these:

`` *"`*`"isthekindofboxedvalues.Thingslike`Int`and`MaybeFloat`havekind`*`. ``\
`` *"`#`"isthekindofunboxedvalues.Thingslike`Int#`havekind`#`. ``\
`*Withtheadventof[wiki:GhcKindsdatatypepromotionandkindpolymorphism]wecanhavealotmorekinds.`

(Unboxed tuples used to have a distinct kind, but in 2012 we combined
unboxed tuples with other unboxed values in a single kind "\`\#\`".)

Representing kinds
------------------

Kinds are represented by the data type \`Type\` (see
\[wiki:Commentary/Compiler/TypeType\]): Basic kinds are represented
using type constructors, e.g. the kind \`\*\` is represented as where
\`liftedTypeKindTyCon\` is a built-in \`PrimTyCon\`. The arrow type
constructor is used as the arrow kind constructor, e.g. the kind \`\*
-&gt; \*\` is represented internally as It's easy to extract the kind of
a type, or the sort of a kind: The "sort" of a kind is always one of the
sorts: \`TY\` (for kinds that classify normal types) or \`CO\` (for
kinds that classify coercion evidence). The coercion kind, \`T1 :=:
T2\`, is represented by \`PredTy (EqPred T1 T2)\`.

Kind subtyping
--------------

There is a small amount of sub-typing in kinds. Suppose you see \`(t1
-&gt; t2)\`. What kind must \`t1\` and \`t2\` have? It could be \`\*\`
or \`\#\`. So we have a single kind \`OpenKind\`, which is a super-kind
of both, with this simple lattice:

[Image(https://docs.google.com/drawings/pub?id=1M5yBP8iAWTgqdI3oG1UNnYihVlipnvvk2vLInAFxtNM&w=359&h=229)](Image(https://docs.google.com/drawings/pub?id=1M5yBP8iAWTgqdI3oG1UNnYihVlipnvvk2vLInAFxtNM&w=359&h=229) "wikilink")

(You can edit this picture
[here](https://docs.google.com/drawings/d/1M5yBP8iAWTgqdI3oG1UNnYihVlipnvvk2vLInAFxtNM/edit?hl=en_GB).)

Linearity
=========

The solution is to distinguish call demands from product demands.
Consider again: The demands placed on by the first and second call get
bothed together to yield . But this is incorrect. Consider: Here, the
demands placed on by the body of and by the call to in the -body get
bothed together: . Note that this is the same as the demand placed on
above, yet we want to distinguish between the two situations, because in
the first example, the inner lambda in 's rhs is only called once.

The solution is to treat call demands and product demands differently,
and to define the function for call demands to have the same behavior as
. Then in the first example, has demand placed on it, and in the second,
. This is what we want; now, if has demand placed on it, that implies is
always called with two arguments.

Why does this make sense? Consider what it means if we see an example
like: (where is lazy in , and is strict in and ). is used both with
demand (in the call to and with demand (in the call to ). This means
it's perfectly same to strictly evaluate , so when we both together the
two demands, we should get . On the other hand, if a function is
*called* once with one argument and once with two, we don't want to
treat it as a function that's always called with two arguments; we're
only interested in functions that are *always* called with *n* arguments
for a given *n*. Hence, both should behave the same way as lub for call
demands.

Ticky
=====

(NB out-of-date, but maybe historically useful; cf
\[wiki:Debugging/TickyTicky\])

The following code inserts extra fields into closures when ticky is
enabled (and so had to be commented out): in
[GhcFile(compiler/codeGen/CgTicky.hs)](GhcFile(compiler/codeGen/CgTicky.hs) "wikilink").

Other relevant functions: in
[GhcFile(compiler/codeGen/CgTicky.hs)](GhcFile(compiler/codeGen/CgTicky.hs) "wikilink")
(called by in
[GhcFile(compiler/codeGen/CgClosure.lhs)](GhcFile(compiler/codeGen/CgClosure.lhs) "wikilink")).

Argh! I spent days tracking down this bug: in
[GhcFile(compiler/cmm/CLabel.hs)](GhcFile(compiler/cmm/CLabel.hs) "wikilink")
needs to return for labels of type (i.e., labels for ticky counters.) By
default, it was returning , which caused the ticky counter labels to get
declared with the wrong type in the generated C, which caused C compiler
errors.

Declarations for ticky counters
-------------------------------

 spits out C declarations that look like this: Here, is actually an
(this type is declared in
[GhcFile(includes/StgTicky.h)](GhcFile(includes/StgTicky.h) "wikilink")).
The counters get used by in
[GhcFile(rts/Ticky.c)](GhcFile(rts/Ticky.c) "wikilink"), which prints
out the ticky reports. The counter fields are accessed using offsets
defined in
[GhcFile(includes/GHCConstants.h)](GhcFile(includes/GHCConstants.h) "wikilink")
(), which in turn get generated from
[GhcFile(includes/mkDerivedConstants.c)](GhcFile(includes/mkDerivedConstants.c) "wikilink")
(change it and then run in .

<s>Note that the first 3 fields of the counters are 16-bit ints and so
the generated ticky-counter registration code has to reflect that (I
fixed a bug where the first field was getting treated as a 32-bit
int.)</s> I modified the type so that all fields are s, because it seems
that the code generator can't cope with anything else anyway (i.e., in
the declaration above, is an array of s, even though the C type
declaration implies that some fields are halfwords.)

In in
[GhcFile(compiler/codeGen/CgClosure.lhs)](GhcFile(compiler/codeGen/CgClosure.lhs) "wikilink"),
"eager blackholing" was getting employed in the case where ticky was
turned on; this was causing programs to when they wouldn't with ticky
disabled, so I turned that off.

Strictness and let-floating
===========================

We run into the following problem in the nofib benchmark: suppose we
have: where doesn't depend on . Demand analysis says that has a strict
demand placed on it. Later, gets floated to the top level because it
doesn't depend on (in reality it's more complicated because in this case
probably would have gotten floated out before demand analysis, but bear
with me). still has a strict demand signature, which a top-level binding
isn't allowed to have. Currently this manifests itself as an assertion
failure in
[GhcFile(compiler/simplCore/SimplEnv.lhs)](GhcFile(compiler/simplCore/SimplEnv.lhs) "wikilink").

There are two possible easy solutions: don't float out bindings for
strict things, or "both" the demand for a binder with Lazy when its
binding gets floated out. The question is, is it better to do the
let-floating and lose the strictness into or to evaluate something
strictly but lose sharing?

Coercions
=========

When we run into an expression like that we're placing demand on, we
analyze to get , then check whether the depth of is equal to the depth
of or not. This is necessary because we might be casting a function to a
non-function type. So, if and have equal depth, we return as is; if 's
arity is less, we drop the appropriate number of args from ; if 's arity
is less, we add the appropriate number of dummy argument demands to it.

WARN: arity /
=============

dmdTypeDepth rhs\_dmd\_ty && not (exprIsTrivial rhs) =

This warning was happening for (at least) two reasons: - lambdas with a
strict non-call demand placed on them were being handled wrong (see the
first two examples in
\[wiki:Commentary/Compiler/StrictnessAnalysis/Examples\]) - coercions
were being handled wrong, resulting in a demand type with depth 0 being
assigned to an rhs consisting of a cast from/to a function type

Explaining demand transformers
==============================

For those who, like me, are a little slow, this example might go in
section 5.1 of the paper:

(a): (b):

In both (a) and (b), 's rhs places a strict demand on . So if we see:
with a strict demand placed on it, it wouldn't be sound to look at 's
demand signature and say that places a strict demand on under -- because
we don't know whether is like (a) or like (b). This is why when we see a
partial application of , we discard all of the argument information in
's demand type.

Nofib stuff
===========

I've had weird problems with the and commands under MSYS but I think
it's just when running nofib. At some point I wrote down:

TIME needs to be not

and

MSYS does not work, use cygwin

but of \*course\* I no longer remember what I meant.
[PageOutline](PageOutline "wikilink")

GHC Commentary: Libraries
=========================

All GHC build trees contain a set of libraries, called the **Boot
Packages**. These are the libraries that GHC's source code imports.
Obviously you need the boot packages to build GHC at all. The boot
packages are those packages in the file \[source:packages\] that have a
\`-\` in the "tag" column.

The repository structure of a GHC source tree is described in
\[wiki:Repositories\].

You can see exactly which versions of what packages GHC depends on by
looking in \[source:compiler/ghc.cabal.in\]. The versions of the boot
packages (including the \`base\` library) associated with each GHC
release are tabulated in [GHC Boot Library Version
History](wiki:Commentary/Libraries/VersionHistory "wikilink").

Building packages that GHC doesn't depend on
============================================

You can make the build system build extra packages, on which GHC doesn't
strictly depend, by adding them to the \`\$(TOP)/packages\` file, with
an \`extra\` tag. Then set \`BUILD\_EXTRA\_PKGS=YES\` in your
\`mk/build.mk\` file.

It should be exceptional, but you can make the build system provide
per-package compiler flags, by adding some definitions in
\`\$(TOP)/ghc.mk\`, just below the comment

------------------------------------------------------------------------

Classifying boot packages
=========================

A **boot package** is, by definition, a package that can be built by
GHC's build system.

Boot packages can be classified in four different ways:

`*Requiredvsoptional`\
`*Wired-invsindependent`\
`*Zero-bootvsnotzero-boot`\
`*Installedvsnotinstalled`

These distinctions are described in the following sub-sections.

Required or optional
--------------------

Most boot packages **required** to build \`ghc-stage2\`, or one of the
supporting utilities such as \`ghc-pkg\`, \`hsc2hs\`, etc.

However a few are **optional**, and are built only

`` *Toensurethattheydoindeedbuildcleanly;theyarestresstestsofGHC.E.g.`dph` ``\
`*Becausetheyareusedinregressiontests`

Coupling to GHC
---------------

An important classification of the boot packages is as follows:

`*`**`Wired` `in`
`packages`**`` aretotallyspecifictoGHC.Seethelistin`compiler/main/Packages.lhs`function`findWiredInPackages`,andc.f.[wiki:Commentary/Compiler/Packages].Atthemomenttheseare: ``\
`` *`ghc-prim` ``\
`` *`integer-gmp`,`integer-simple` ``\
`` *`base` ``\
`` *`template-haskell` ``\
`` *`dph` ``

`*`**`Independent`**`` packagesarelooselycoupledtoGHC,andoftenmaintainedbyothers.Mostbootpackagesareindependent;e.g.`containers`,`binary`,`haskeline`andsoon. ``

Independent libraries may have a master repository somewhere separate
from the GHC repositories. Whenever we release GHC, we ensure that the
installed boot libraries (i.e. that come with GHC) that are also
independent are precisely sync'd with a particular released version of
that library.

Zero-boot packages
------------------

Since GHC's source code imports the boot packages, *even the bootstrap
compiler must have the boot packages available*. (Or, more precisely,
all the types and values that are imported must be available from some
package in the bootstrap compiler; the exact set of packages does not
need to be identical.)

For the most part we simply assume that the bootstrap compiler already
has the boot packages installed. The **Zero-boot Packages** are a set of
packages for which this assumption does not hold. Two reasons dominate:

`` *Forcertainfast-movingbootpackages(notably`Cabal`),wedon'twanttorelyontheuserhavinginstalledabang-up-to-dateversionofthepackage. ``\
`` *Theonlypackagesthatwecan"assumethatthebootstrapcompileralreadyhas"arethosepackagesthatcomewithGHCitself;i.e.theinstalledbootpackages.Sonon-installedbootpackagesarealsozero-bootpackages.Example:`bin-package-db`or`hoopl`. ``

So we begin the entire build process by installing the zero-boot
packages in the bootstrap compiler. (This installation is purely local
to the build tree.) This is done in \`ghc.mk\` by setting
\`PACKAGES\_STAGE0\` to the list of zero-boot packages; indeed this is
the only way in which zero-boot packages are identified in the build
system.

As time goes on, a Zero-boot package may become an ordinary boot
package, because the bootstrap compiler is expected to have (a
sufficiently up to date) version of the package already. Remember that
we support bootstrapping with two previous versions of GHC.

To find out which packages are currently zero-boot packages, do the
following in a GHC build:

Some Zero-boot packages are **maintained by other people**. In order to
avoid GHC being exposed to day-by-day changes in these packages, we
maintain a "lagging" Git repository for each that we occasionally sync
with the master repository. We never push patches to lagging repository;
rather we push to the master (in discussion with the package
maintainer), and pull the patches into the lagging repo. The current
Zero-boot packages of this kind are:

`` *`Cabal`:wefrequentlyupdateCabalandGHCinsync ``\
`` *`binary`(renamedto`ghc-binary`inthe6.12branch):requiredby`bin-package-db`. ``

Other Zero-boot packages are **maintained by us**. There is just one Git
repo for each, the master. When we make a GHC release, we simultaneously
tag and release each of these packages. They are:

`` *`hpc` ``\
`` *`extensible-exceptions`:thisisashimthatprovidesanAPItoolderversionsofGHCthatiscompatiblewithwhatthecurrent`base`packagenowexports.So,unusually,`extensible-exceptions`isazero-bootpackage,butnotabootpackage. ``\
`` *`bin-package-db`:aGHC-specificpackagethatprovidesbinaryserialisationofthepackagedatabase,useby`ghc-pkg`andGHCitself. ``

Installation
------------

When we build a distribution of GHC, it includes at least some
libraries, otherwise it would be utterly useless. Since GHC is part of
the Haskell Platform, any library that is installed with GHC is
necessarily part of the Haskell Platform, so we have to be a bit careful
what we include.

Alas, since the \`ghc\` package (implementing the GHC API) is certainly
an installed package, all the packages on which it depends must also be
installed, and hence willy-nilly become part of the Haskell Platform. In
practice that means that almost all the Boot Packages are installed. In
some cases that is unfortunate. For example, we currently have a special
version of the \`binary\` library, which we don't really expect Haskell
users to use; in this case, we call it \`ghc-binary\`, and informally
discourage its use.

Currently the Boot Packages that are not installed are \`haskeline\`,
\`mtl\`, and \`terminfo\`; these are needed to build the GHC front-end,
but not to build the \`ghc\` *package*.

**QUESTION**: where in the build system is the list of installed
packages defined?

------------------------------------------------------------------------

Boot packages dependencies
==========================

`*Attherootofthehierarchywehave`**`` `ghc-prim` ``**`` .Asthenameimplies,thispackagecontainsthemostprimitivetypesandfunctions.Itonlycontainsahandfulofmodules,including`GHC.Prim`(whichcontains`Int#`,`+#`,etc)and`GHC.Bool`,containingthe`Bool`datatype.See"WARNING:patternmatching"below. ``

`` *Above`ghc-prim`arethepackages ``\
`` *`integer-gmp` ``\
`` *`integer-simple` ``\
`` Thetwohavethesameinterface,andonlyoneofthetwoisused.(Whenwewanttobevagueaboutwhichone,wecallit`integer-impl`.)Theyprovideadefinitionofthe`Integer`type(ontopoftheC`gmp`library,orinplainHaskell,respectively).Whichfunctionalityisprovidedin`ghc-prim`ismostlydrivenbywhatfunctionalitythe`integer-impl`packagesneed.Bydefault`integer-gmp`isused;touse`integer-simple`define`INTEGER_LIBRARY=integer-simple`in`mk/build.mk`. ``

`See"WARNING:patternmatching"below.`

`*Nextisthe`**`` `base` ``**`` package.Thiscontainsalargenumberofmodules,manyofwhichareinonebigcyclicimportknot,mostlyduetothe`Exception`type. ``

`*Ontopofbaseareanumberofother,morespecialisedpackages,whosepurposeisgenerallyclearfromtheirname.Ifnot,youcangetmoredetailfromthedescriptionsintheirCabalfiles.Theup-to-datelistofpackagescanbefoundinthefile[source:packages].`

The \`haskell98\`, \`old-time\`, \`old-locale\` and \`random\` packages
are mostly only needed for Haskell 98 support, although \`dph\`
currently uses \`random\` too.

WARNING: Pattern matching in \`ghc-prim\`, \`integer-simple\`, and \`integer-gmp\`
----------------------------------------------------------------------------------

Note that \`ghc-prim\` and \`integer-impl\` are below the dependency
chain from Exception (in \`base\`), which means they must not raise
generate code to raise an exception (it's not enough that this code will
never run). One particularly subtle case of GHC exception-raising code
is in the case of (complete!) pattern matches. Consider the unboxed form
of Integers, which has the constructor S\# or J\#.

GHC will incorrectly generate core that pattern matches against the
second argument twice, the second match being a partial one with (dead)
exception raising code. When compiled with optimizations, the dead code
is eliminated. However, this breaks with -O0, thus: The fix is to
explicitly spell out the constructor in the second and third line, so
that GHC does not generate calls to \`patError\`:

Repositories
============

The list of repository locations has moved to \[wiki:Repositories\].

The LLVM backend
================

David Terei wrote a new code generator for GHC which targets the LLVM
compiler infrastructure. Most of the work was done as part of an honours
thesis at the University of New South Wales under the supervision of
Manuel Chakravarty. It was merged into GHC Head around May of 2010 and
has been included in GHC since the 7.0 release.

Documentation:

`*[wiki:Commentary/Compiler/Backends/LLVM/InstallingInstalling&Using]`\
`*[wiki:Commentary/Compiler/Backends/LLVM/DesignDesign&Implementation]`\
`*[wiki:Commentary/Compiler/Backends/LLVM/ManglerLLVMMangler]`\
`*[wiki:Commentary/Compiler/Backends/LLVM/DevelopmentNotesBugs&OtherProblems]`\
`*[wiki:Commentary/Compiler/Backends/LLVM/GHC_LLVMPortingPortingGHC/LLVMtoanotherplatform]`

Work in Progress:

`*[wiki:SIMDSIMDinstructionsandLLVM]`\
`*[wiki:Commentary/Compiler/Backends/LLVM/AliasImprovingAliasAnalysis]`

Future Ideas:

`*[wiki:Commentary/Compiler/Backends/LLVM/WIPToDoListofSorts]`\
`*[wiki:Commentary/Compiler/Backends/LLVM/ReplacingNCGReplacingtheNativeCodeGenerator]`\
`*`[`David` `Terei` `blog` `post` `of` `LLVM-related`
`projects`](http://dterei.blogspot.com/2011/09/ghc-project-for-all.html)

Other information:

`*The`[`thesis`
`paper`](http://www.cse.unsw.edu.au/~pls/thesis/davidt-thesis.pdf)`whichoffersadetailedperformanceevaluation,aswellasthemotivationanddesignoftheback-end.`\
`*`[`Blog`
`post`](http://blog.llvm.org/2010/05/glasgow-haskell-compiler-and-llvm.html)`ontheLLVMblogaboutthebackend.`\
`*Amorerecent`[`paper`](http://www.cse.unsw.edu.au/~chak/papers/TC10.html)`submittedtotheHaskellSymposium'10,givesupdateddesignoverviewandperformancenumbers.`

Loopification
=============

Loopification is a C-- optimisation pass that turns tail recursion into
proper loops.

Here is a summary of relevant links and tickets

-   [Krzysztof Wos's
    project](http://research.microsoft.com/en-us/um/people/simonpj/tmp/wos-diss-draft.pdf)
    in which he reports great performance improvements by turning tail
    recursion into loops in C--.

<!-- -->

-   Tickets:

`*#8285`\
`*#8793,#11372;seecomment15of#8793)etc,whereitseemsthatwearemissingloopificationforasimpleIOfunction`\
`*#8585concernedgettingthelooptostart`*`after`*`thestackcheck`

LLVM Mangler
============

The LLVM backend sadly includes a 'mangler'. This is a Haskell written
program (well pass of GHC) that runs on the assembly code generated by
the LLVM compiler. We do this as there are a few issues with
communicating to LLVM exactly what we want generated as object code and
so, for now, it is easiest to post-process the assembly.

Long term we ideally would submit patches to LLVM and get rid of the
mangler. The work required to do that may be quite high and the patches
needed potentially fairly specific to GHC. So no one has done that yet.

Below are the issues that the LLVM Mangler addresses in the assembly
code.

TABLES\_NEXT\_TO\_CODE (TNTC)
-----------------------------

TODO

Stack Alignment
---------------

LLVM requires that the C stack be properly aligned for spills. One Win32
the stack is 4-byte aligned, which is not enough for SSE spills, and
even on x64 platforms the stack is only 16-byte aligned, which is not
enough for AVX spills. When the stack is not properly aligned for
spills, LLVM generates prologue/epilogue code that fiddles with the base
pointer, which GHC uses as its stack pointer, and disables tail call
optimization. Both are very bad. Therefore we currently tell LLVM to
always assume the stack is properly aligned and then rewrite all aligned
SSE/AVX move instructions to their unaligned counterparts inside the
mangler.

SIMD / AVX
----------

Migrating Old Commentary
========================

Below you will find a table with a line for each section of the [old
commentary](http://darcs.haskell.org/ghc/docs/comm/). Please replace
*unknown* with **done** if you believe that the wiki commentary
completely captures *all* of the information in that section of the old
commentary, and that there is no longer any reason for people to read
that section of the commentary.

Before the Show Begins
----------------------

||Feedback||**done**|| ||Other Sources of Wisdom||**done**||

Genesis
-------

||Outline of the Genesis||**done**|| ||Mindboggling
Makefiles||**done**|| ||GHC's Marvellous Module Structure||**done**||

The Beast Dissected
-------------------

||Coding style used in the compiler||**done**|| ||The Glorious
Driver||Sections 1 & 2 **done**, *Other sections mostly outdated*||
||Primitives and the Prelude||*unknown*|| ||Just Syntax||*unknown*||
||The Basics||*unknown*|| ||Modules, !ModuleNames and
Packages||*unknown*|| ||The truth about names: Names and
!OccNames||*unknown*|| ||The Real Story about Variables, Ids, !TyVars,
and the like||*unknown*|| ||Data types and constructors||*unknown*||
||The Glorious Renamer||*unknown*|| ||Hybrid Types||*unknown*||
||Checking Types||*unknown*|| ||Sugar Free: From Haskell To
Core||*unknown*|| ||The Mighty Simplifier||*unknown*|| ||The Evil
Mangler||**done**|| ||Alien Functions||*unknown*|| ||You Got Control:
The STG-language||*unknown*|| ||The Native Code Generator||*unknown*||
||GHCi||*unknown*|| ||Implementation of foreign export||*unknown*||
||Compiling and running the Main module||*unknown*||

RTS & Libraries
---------------

||Coding Style Guidelines||**done**|| ||Spineless Tagless C||*unknown*||
||Primitives||*unknown*|| ||Prelude Foundations||*unknown*|| ||Cunning
Prelude Code||*unknown*|| ||On why we have !ForeignPtr||*unknown*||
||Non-blocking I/O for Win32||*unknown*|| ||Supporting multi-threaded
interoperation||*unknown*||

Extensions, or Making a Complicated System More Complicated
-----------------------------------------------------------

||Template Haskell||*unknown*|| ||Parallel Arrays||*unknown*||

The Marvellous Module Structure of GHC
======================================

`*`**`See` `also:` `[ModuleDependencies/Hierarchical` `Proposal` `for`
`hierarchical` `module` `structure]`**

`*`**`NOTE:`**`Possiblyoutdated.`

GHC is built out of about 245 Haskell modules. It can be quite tricky to
figure out what the module dependency graph looks like. It can be
important, too, because loops in the module dependency graph need to be
broken carefully using .hi-boot interface files.

This section of the commentary documents the subtlest part of the module
dependency graph, namely the part near the bottom.

`*Thelistisgivenincompilationorder:thatis,modulenearthetoparemoreprimitive,andarecompiledearlier.`\
`*Eachmoduleislistedtogetherwithitsmostcriticaldependenciesinparentheses;thatis,thedependenciesthatpreventitbeingcompiledearlier.`\
`*Modulesinthesamebulletdon'tdependoneachother.`\
`*Loopsaredocumentedbyadependencysuchas"loopType.Type".ThismeansthathemoduleimportsType.Type,butmoduleTypehasnotyetbeencompiled,sotheimportcomesfromType.hi-boot.`

Compilation order is as follows:
--------------------------------

`*Firstcomesalayerofmodulesthathavefewinterdependencies,andwhichimplementverybasicdatatypes:`\
`*Util`\
`*!OccName`\
`*Pretty`\
`*Outputable`\
`*!StringBuffer`\
`*!ListSetOps`\
`*Maybes`\
`*etc`

`*Nowcomesthemainsubtlelayer,involvingtypes,classes,typeconstructorsidentifiers,expressions,rules,andtheiroperations.`\
`*Name,!PrimRep`\
`*!PrelNames`\
`*Var(Name,loop!IdInfo.!IdInfo,loopType.Type,loopType.Kind)`\
`*!VarEnv,!VarSet,!ThinAir`\
`*Class(loop!TyCon.!TyCon,loopType.Type)`\
`*!TyCon(loopType.Type,loop!DataCon.!DataCon,loopGenerics.!GenInfo)`\
`*!TypeRep(loop!DataCon.!DataCon,loopSubst.substTyWith)`\
`*Type(loop!PprType.pprType,loopSubst.substTyWith)`\
`*!FieldLabel(Type),!TysPrim(Type)`\
`*Literal(!TysPrim,!PprType),!DataCon(loop!PprType,loopSubst.substTyWith,!FieldLabel.!FieldLabel)`\
`*!TysWiredIn(loop!MkId.mkDataConIds)`\
`*!TcType(lotsof!TysWiredInstuff)`\
`*!PprType(lotsof!TcTypestuff)`\
`*!PrimOp(!PprType,!TysWiredIn)`\
`*!CoreSyn[doesnotimportId]`\
`*!IdInfo(!CoreSyn.Unfolding,!CoreSyn.!CoreRules)`\
`*Id(lotsfrom!IdInfo)`\
`*CoreFVs,!PprCore`\
`*!CoreUtils(!PprCore.pprCoreExpr,CoreFVs.exprFreeVars,!CoreSyn.isEvaldUnfolding!CoreSyn.maybeUnfoldingTemplate)`\
`*!CoreLint(!CoreUtils),!OccurAnal(!CoreUtils.exprIsTrivial),!CoreTidy(!CoreUtils.exprArity)`\
`*!CoreUnfold(!OccurAnal.occurAnalyseGlobalExpr)`\
`*Subst(!CoreUnfold.Unfolding,CoreFVs),Generics(!CoreUnfold.mkTopUnfolding),Rules(!CoreUnfold.Unfolding,!PprCore.pprTidyIdRules)`\
`*!MkId(!CoreUnfold.mkUnfolding,Subst,Rules.addRule)`\
`*!PrelInfo(!MkId),!HscTypes(Rules.!RuleBase)`

`*Thatistheendoftheinfrastructure.Nowwegetthemainlayerofmodulesthatperformusefulwork.`\
`*!CoreTidy(!HscTypes.!PersistentCompilerState)`

Typechecker stuff
-----------------

`*!TcType`\
`*!TcEvidence(!TcType)`\
`*TcMType(!TcEvidence)`\
`*!TcUnify(TcMType)`\
`*TcSMonad(TcMType)`\
`*!TcSimplify(TcSMonad)`\
`*!TcValidity(!TcSimplify.simplifyTop,!TcUnify.tcSubType)`\
`*!TcHsType(!TcValidity.checkValidType,!TcValidity.checkValidInstance)`

!HsSyn stuff
------------

`*!HsPat.hs-boot`\
`*!HsExpr.hs-boot(loop!HsPat.LPat)`\
`*!HsTypes(loop!HsExpr.!HsSplice)`\
`*!HsBinds(!HsTypes.LHsType,loop!HsPat.LPat,!HsExpr.pprFunBindandothers)!HsLit(!HsTypes.!SyntaxName)`\
`*!HsPat(!HsBinds,!HsLit)!HsDecls(!HsBinds)`\
`*!HsExpr(!HsDecls,!HsPat)`

Library stuff: base package
---------------------------

`*GHC.Base`\
`*Data.Tuple(GHC.Base),GHC.Ptr(GHC.Base)`\
`*GHC.Enum(Data.Tuple)`\
`*GHC.Show(GHC.Enum)`\
`*GHC.Num(GHC.Show)`\
`*GHC.ST(GHC.Num),GHC.Real(GHC.Num)`\
`*GHC.Arr(GHC.ST)GHC.STRef(GHC.ST)`\
`*GHC.!IOBase(GHC.Arr)`\
`*Data.Bits(GHC.Real)`\
`*Data.!HashTable(Data.Bits,Control.Monad)`\
`*Data.Typeable(GHC.IOBase,Data.!HashTable)`\
`*GHC.Weak(Data.Typeable,GHC.IOBase)`

High-level Dependency Graph
---------------------------

Dark red edges indicate that only one module in one group depends on a
module in the other group. Dark green means 11 or more dependencies.
Arrows point from the importing module to the imported module.

[Image(dep5.png)](Image(dep5.png) "wikilink")

Module Types
============

Here we attempt to describe some of the main data structures involved in
GHC's representation and handling of Haksell modules. GHC uses a number
of different data types to represent modules, for efficiency (some types
load less information) and categorising how other modules relate to the
one being compiled. Most these types are defined in
[GhcFile(compiler/main/HscTypes.lhs)](GhcFile(compiler/main/HscTypes.lhs) "wikilink").

Module
------

Location:
[GhcFile(compiler/basicTypes/Module.lhs)](GhcFile(compiler/basicTypes/Module.lhs) "wikilink")

The **Module** data type is simply an identifier of a module; its fully
qualified name.

!ModIface
---------

Location:
[GhcFile(compiler/main/HscTypes.lhs)](GhcFile(compiler/main/HscTypes.lhs) "wikilink")

The **!ModIface** data type is one of the fullest representations of a
module. It is a complete representation of a modules interface file
(**.hi**). It is this data structure that is serialised to produce a
modules **.hi** file.

!ModDetails
-----------

Location:
[GhcFile(compiler/main/HscTypes.lhs)](GhcFile(compiler/main/HscTypes.lhs) "wikilink")

**!ModDetails** is essentially a cache for information in the
**!ModIface** for home modules only. It stores information about a
module after linking has taken place. **!ModIface** stores information
about a module before linking. Information stored in a **!ModDetails**
is created from a **!ModIface**, typically during type checking.

### !ModGuts

Location:
[GhcFile(compiler/main/HscTypes.lhs)](GhcFile(compiler/main/HscTypes.lhs) "wikilink")

A **!ModGuts** is carried through the compiler, accumulating stuff as it
goes. There is only one **!ModGuts** at any time, the one for the module
being compiled right now. Once it is compiled, a **!ModIface** and
**!ModDetails** are extracted and the **!ModGuts** is discarded.

!ModSummary
-----------

Location:
[GhcFile(compiler/main/HscTypes.lhs)](GhcFile(compiler/main/HscTypes.lhs) "wikilink")

A **!ModSummary** stores a summary of a module that is suitable for
recompilation checking. A **!ModSummary** is a node in the compilation
manager's dependency graph.

!HomeModInfo
------------

Location:
[GhcFile(compiler/main/HscTypes.lhs)](GhcFile(compiler/main/HscTypes.lhs) "wikilink")

A **!HomeModInfo** stores information about a module in the package
being compiled. It simply stores for the **!ModIface**, **!ModDetails**
and linkage information about a single module.

!HomePackageTable
-----------------

Location:
[GhcFile(compiler/main/HscTypes.lhs)](GhcFile(compiler/main/HscTypes.lhs) "wikilink")

The home package table describes already-compiled home-package modules,
/excluding/ the module we are compiling right now.

!ExternalPackageState
---------------------

Location:
[GhcFile(compiler/main/HscTypes.lhs)](GhcFile(compiler/main/HscTypes.lhs) "wikilink")

Stores information about other packages that we have pulled in while
compiling the current module.

Multi-instance packages
=======================

This page is about how to change the package system to allow multiple
instances of a package to be installed at the same time. There are two
reasons we want to be able to do this:

`` *Tobeabletotrackthedifferent"ways"inwhichapackageisavailable:e.g.profiling,dynamic.Atthemoment,thepackagedatabasedoesn'ttrackthisinformation,withtheresultthattheuserhastoreinstallpackageswith`--enable-profiling`onatrial-and-errorbasisinordertogetprofilingsupportforpackagestheyhavealreadyinstalled. ``\
`Thesameholds,inprinciple,fordifferentflagsettingsorotherconfigurationvariationsofapackage.`

`*Tomakeinstallingnewpackagesmorerobust.Wheninstallinganewpackage,wesometimesneedtoupgradepackagesthatarealreadyinstalledtonewversions,whichmayrequirerecompilingotherpackagesagainstthenewversion.Forexample,ifwehaveP1installed,Q1dependsonP(anyversion),andweneedtoinstallRthatdependsonbothP2andQ1.WeneedtobuildP2,rebuildQ1againstP2,andfinallybuildRagainstP2andthenewQ1.WewouldliketodothiswithoutremovingP1ortheoldQ1fromthepackagedatabase,becauseotherpackagesmaybedependingontheoldQ1,andwedon'twanttobreakthosepackages(whichiswhatcurrentlyhappenswithGHC7.0).`

See also

`*[wiki:Commentary/PackagesCommentarypagesaboutpackages]`\
`*PhilippSchuster'sGSoCproject`[`proposal`
`(DEAD)`](http://www.google-melange.com/gsoc/proposal/review/google/gsoc2012/phischu/1)`,`[`GSoC`
`project` `page`
`(DEAD)`](http://www.google-melange.com/gsoc/project/google/gsoc2012/phischu/19001)`,[wiki:Commentary/GSoCMultipleInstancesTracwikipage],`[`git`
`repo`](https://github.com/phischu/cabal)`,and`[`video`](https://www.youtube.com/watch?v=h4QmkyN28Qs)`.`\
`*`[`Mikhail's`
`post`](http://coldwa.st/e/blog/2013-08-20-Cabal-sandbox.html)`aboutCabalsandboxes.`\
`*Mailingliststuff`[`here`](http://comments.gmane.org/gmane.comp.lang.haskell.ghc.devel/443)`and`[`here`](http://markmail.org/message/4qvegvx32lhlo66g#query:+page:1+mid:bwdgykv4g2hzqg5t+state:results)`.`

!ToDo list
----------

`*ghc-pkg:donotoverwritepreviousinstancesinthepackageDB`\
`*butweneedtothinkaboutthecasewhereweoverwriteanexistingpackageonthefilesystemandre-register.Thiswillhappenwithlocal(orin-place)packageregistrationthatoccurswhenbuildingabunchofrelatedcomponents.Inthiscasethetoolshouldknowit'sdoingthatandunregistertheoldinstancefirst(thoughreliablytrackingthatstatemaybetricky,sinceuserscanmakecleanetc).Weshouldcheckmakeitacheckederrortore-registerinthesamefilesystemlocationwithnewpackageid,withoutunregisteringtheoldonefirst.Perhapswecanidentifysomekeyfile.`

`*GHC:discardconflictinginstancesduringitsshadowingphase`\
`*SDM:GHCwillcurrentlydo*something*here,butitmightendupwitharesultthattheuserdidn'twant/expect.Onewaytoimprovethingsistoprioritisepackagesthatwereinstalledmorerecently.`\
`` *AndressuggeststhatGHCshouldbemuchcleverer,andlookattheactualdependenciesofthemodulesbeingcompiledbeforedecidingwhichpackagestoenable.Thiswouldalmostcertainlyresultinmorethingsworkingandpossiblylesssurprisingbehavioursometimes,butSimonthinksthat(a)itistoohard,(b)ifusersneedthis,theyshoulduseCabalanditsdependencyresolver,whichwilldoagoodjob,(c)youcanoftenresolveproblemsbyadding`-packageX`,and(d)eventuallywewillwantasystemwhereusersmanageseparatesessions,sotheycansetupanenvironmentinwhichthepackagestheywantareavailable.Thishasalotincommonwith`cabal-dev`andsandboxes,sothemechanisms(andconcepts)shouldbeshared.(kosmikus:perhapsanalternativeistoforcetheusertomakeanactivedecisionincaseofconflicts,i.e.,tocreateasandboxthatexposesaconsistentpackageset). ``

`*GHC:allowspecifyingapackageinstanceinthe-packageflags`\
`*SDM:alreadydone(-package-idflag)`\
`*DC:alreadyusedbyCabal`

`*Cabal:allowspecifyingapackageinstancewhendoingSetup.hsconfigure`\
`*DC:currentlyonly==versionconstraintscanbeused,notinstalledpackageid.Shouldn'tbetoohardtoaddhowever.`\
`*JT:DoneaccordingtoDC.`

`*instancesofpackagesmustinstallinadifferentlocation`\
`*installdirectoryincludeshash?`\
`` *SDM:notdoneyet.Oneproblemisthatwedon'tknowthehashuntilthepackageisbuilt,butweneedtoknowtheinstalllocationsearlierbecausewebaketheminto`Paths_foo.hs`. ``\
`*SimonandAndresdiscussedthatoneoptionistoletCabalcomputeitsownhash.However,thenwe'dhavetwohashestodealwith.OnlyusingtheCabal-computedhashisn'tanoptioneitheraccordingtoSimon,becauseapparentlyGHC'sABIhashcomputationisnon-deterministic,sowemightendupwithsituationswhereCabal'shashisstable,butGHCcomputesanABI-incompatibleversion.Thisissomewhatworrying...`\
`*DuncanthinksthatweshouldstorebothapackageidentityandapackageABIhash.Currentlyweformthepackageidfromthename,versionandABIhash.WeshouldstoretheABIhashseparatelyanywaybecauseeventuallywewillwanttoknowit,toknowwhichpackagesareABIcompatible.SoCabalcancomputeapackageIdinadvance,howeverissensible,andtheABIhashiscalculatedasnow,afterthebuild.TheinstallationdirectoryfollowsthepackageId.`

`*Cabal:willthedependencysolverworkcorrectlyinthepresenceofmultiplepackageinstances?`\
`*Andresclaimsitwillusingthenewsolver.(Thereisnownopointinupdatingtheoldsolver,thoughit'dbetechnicallypossible.)Alittlebitmoredetail:themodularsolverhasnoconceptofshadowing,onlyofpreference.SoifseveralinstancesareprovidedbyoneormorepackageDBs,they'llallbevalidchoices.`

`*ghc-pkgcleanup:removeold/unusedinstancesofpackages`\
`*howcanwetellwhensomethingisunnecessary?ThisisactuallyratherhardbecauseunlikeNixwedonottrackeveryrandomexecutablethattheusercompiles.`

Next step: dealing with ways
----------------------------

`*Addthe"way"toInstalledPackageInfo,includethewayinthehash`

`*GHC:slicethepackageDBduringstartupaccordingtothecorrectway`

`*Cabal:fixupthedepresolver(kosmikus:anythingstillneededthere?)`

`*Cabal:ways?(thiswouldbereallyeasy,ifwecouldgetmoreinformationaboutinstalledpackagesbackfromghc-pkg)`

`` *Tohandleflagsandotherconfig,addtwonewfieldstoInstalledPackageInfo:`install-agent:{agent-id}`whichidentifiescabal/rpm/etcandthen`configuration:{freetext}`.Theinterpretationoftheconfigurationstringdependsontheinstallationagent,andneedbeknownonlytothatagent.Thisway,agentscanseeifitwasthemthatinstalledapackage,andsotheyshouldknowhowtointerprettheconfigstring.Forcabalthiswouldincludeconfigflagsetc.Itshouldmakeitpossibletoreproduceapackage,e.g.ifwehavetorebuildforsomereason,ortogettheprofilingequivofanormalinstance. ``

The  type
========

Every entity (type constructor, class, identifier, type variable) has a
. The Name type is pervasive in GHC, and is defined in
[GhcFile(compiler/basicTypes/Name.hs)](GhcFile(compiler/basicTypes/Name.hs) "wikilink").
Here is what a looks like, though it is private to the Name module:

`*The``fieldsayswhatsortofnamethisis:see`**`[wiki:Commentary/Compiler/NameType#TheNameSortofaName`
`NameSort]`**`below.`\
`*The``fieldgivesthe"occurrencename",or`**`[wiki:Commentary/Compiler/RdrNameType#TheOccNametype`
`OccName]`**`,oftheName.`\
`*The``fieldallowsfasttestsforequalityofNames.`\
`*The``fieldgivessomeindicationofwherethenamewasbound.`

The  of a Name
-------------

There are four flavours of Name:

`,``::`\
`An````hasonlyanoccurrencename.Distinct````mayhavethesameoccurrencename;the``distinguishesthem.`

`Thereisonlyatinydifferencebetween``and``;theformersimplyremembersthatthenamewasoriginallywrittenbytheprogrammer,whichhelpswhengeneratingerrormessages.`

`::`\
`An````hasaglobally-unique(module,occurrencename)pair,namelytheoriginalnameoftheentity,thatdescribeswherethethingwasoriginallydefined.Soforexample,ifwehave`

`theninmodule``,thefunction``hasanExternalName``.`

`DuringanyinvocationofGHC,each(module,occurrence-name)getsone,andonlyone,``,storedinthe``fieldofthe``.ThisassociationremainsfixedevenwhenGHCfinishesonemoduleandstartstocompileanother.Thisassociationbetween(module,occurrence-name)pairsandthecorresponding``(withits``field)ismaintainedbytheNameCache.`

```::`\
`A````isaspecialsortof````,onethatiscompletelyknowntothecompiler(e.g.the``typeconstructor).See[wiki:Commentary/Compiler/WiredIn].`

`The``fieldisjustabooleanyes/noflagthatidentifiesentitiesthataredenotedbybuilt-insyntax,suchas``fortheemptylist.These``aren't"inscope"assuch,andweoccasionallyneedtoknowthat.`

Entities and 
-------------

Here are the sorts of Name an entity can have:

`*Class:alwayshasan``Name.`

`*!TyCon:alwayshasan``or``Name.`

`*!TyVar:canhave``,or``Names;theformerareonesarisefrominstantiatingprogrammer-writtentypesignatures.`

`*Ids:canhave``,``,or``Names.`\
`*Before!CoreTidy,theIdsthatweredefinedattoplevelintheoriginalsourceprogramget``Names,whereasextratop-levelbindingsgenerated(say)bythetypecheckerget``Names.Thisdistinctionisoccasionallyusefulforfilteringdiagnosticoutput;e.g.for``.`\
`*After!CoreTidy:AnIdwithan``Namewillgeneratesymbolsthatappearasexternalsymbolsintheobjectfile.AnIdwithan``Namecannotbereferencedfromoutsidethemodule,andsogeneratesalocalsymbolintheobjectfile.The!CoreTidypassmakesthedecisionaboutwhichnamesshouldbeExternalandwhichInternal.`

Native Code Generator (NCG)
===========================

For other information related to this page, see:

`*[wiki:BackEndNotes]foroptimisationideasregardingthecurrentNCG`\
`*[wiki:Commentary/Compiler/CmmTypeTheCmmlanguage](theNCGcodeworksfromHaskell'simplementationofC--andmanyoptimisationsintheNCGrelatetoCmm)`\
`*[wiki:Commentary/Compiler/Backends/NCG/RegisterAllocatorTheregisterallocator].`

On some platforms (currently x86 and x86\_64, with possibly bitrotted
support for PowerPC and Sparc), GHC can generate assembly code directly.
The NCG is enabled by default on supported platforms.

The NCG has always been something of a second-class citizen inside GHC,
an unloved child, rather. This means that its integration into the
compiler as a whole is rather clumsy, which brings some problems
described below. That apart, the NCG proper is fairly cleanly designed,
as target-independent as it reasonably can be, and so should not be
difficult to retarget.

NOTE! The native code generator was largely rewritten as part of the C--
backend changes, around May 2004. Unfortunately the rest of this
document still refers to the old version, and was written with relation
to the CVS head as of end-Jan 2002. Some of it is relevant, some of it
isn't.

### Files, Parts

After GHC has produced \[wiki:Commentary/Compiler/CmmType Cmm\] (use
-ddump-cmm or -ddump-opt-cmm to view), the Native Code Generator (NCG)
transforms Cmm into architecture-specific assembly code. The NCG is
located in
[GhcFile(compiler/nativeGen)](GhcFile(compiler/nativeGen) "wikilink")
and is separated into eight modules:

`*`[`GhcFile(compiler/nativeGen/AsmCodeGen.lhs)`](GhcFile(compiler/nativeGen/AsmCodeGen.lhs) "wikilink")[`BR`](BR "wikilink")\
`top-levelmodulefortheNCG,importedby`[`GhcFile(compiler/main/CodeOutput.lhs)`](GhcFile(compiler/main/CodeOutput.lhs) "wikilink")`;alsodefinestheMonadforoptimisinggenericCmmcode,`[`BR`](BR "wikilink")[`BR`](BR "wikilink")\
`*`[`GhcFile(compiler/nativeGen/MachCodeGen.hs)`](GhcFile(compiler/nativeGen/MachCodeGen.hs) "wikilink")[`BR`](BR "wikilink")\
`generatesarchitecture-specificinstructions(aHaskell-representationofassembler)fromCmmcode`[`BR`](BR "wikilink")[`BR`](BR "wikilink")\
`*`[`GhcFile(compiler/nativeGen/MachInstrs.hs)`](GhcFile(compiler/nativeGen/MachInstrs.hs) "wikilink")[`BR`](BR "wikilink")\
`containsdatadefinitionsandsomefunctions(comparison,size,simpleconversions)formachineinstructions,mostlycarriedoutthroughthe``datatype,definedhere`[`BR`](BR "wikilink")[`BR`](BR "wikilink")\
`*`[`GhcFile(compiler/nativeGen/NCGMonad.hs)`](GhcFile(compiler/nativeGen/NCGMonad.hs) "wikilink")[`BR`](BR "wikilink")\
`definesthethemainmonadintheNCG:theNativecodeMachineinstructionMonad,``,andrelatedfunctions.`*`Note:`
`the` `NCG` `switches` `between` `two` `monads` `at` `times,`
`especially` `in` `:` `and` `the` `Monad` `used` `throughout` `the`
`compiler.`*[`BR`](BR "wikilink")[`BR`](BR "wikilink")\
`*`[`GhcFile(compiler/nativeGen/PIC.hs)`](GhcFile(compiler/nativeGen/PIC.hs) "wikilink")[`BR`](BR "wikilink")\
`handlesgenerationofpositionindependentcodeandissuesrelatedtodynamiclinkingintheNCG;relatedtomanyothermodulesoutsidetheNCGthathandlesymbolimport,exportandreferences,including``,``,``andtheRTS,andtheMangler`[`BR`](BR "wikilink")[`BR`](BR "wikilink")\
`*`[`GhcFile(compiler/nativeGen/PprMach.hs)`](GhcFile(compiler/nativeGen/PprMach.hs) "wikilink")[`BR`](BR "wikilink")\
`Prettyprintsmachineinstructions(``)toassemblercode(currentlyreadablebyGNU's``),withsomesmallmodifications,especiallyforcomparingandaddingfloatingpointnumbersonx86architectures`[`BR`](BR "wikilink")[`BR`](BR "wikilink")\
`*`[`GhcFile(compiler/nativeGen/RegAllocInfo.hs)`](GhcFile(compiler/nativeGen/RegAllocInfo.hs) "wikilink")[`BR`](BR "wikilink")\
`definesthemainregisterinformationfunction,``,whichtakesasetofrealandvirtualregistersandreturnstheactualregistersusedbyaparticular``;registerallocationisinAT&Tsyntaxorder(source,destination),inaninternalfunction,``;definesthe``datatype`[`BR`](BR "wikilink")[`BR`](BR "wikilink")\
`*`[`GhcFile(compiler/nativeGen/RegisterAlloc.hs)`](GhcFile(compiler/nativeGen/RegisterAlloc.hs) "wikilink")[`BR`](BR "wikilink")\
`oneofthemostcomplicatedmodulesintheNCG,``managestheallocationofregistersforeach`*`basic`
`block`*`ofHaskell-abstractedassemblercode:managementinvolves`*`liveness`*`analysis,allocationordeletionoftemporaryregisters,`*`spilling`*`temporaryvaluestothe`*`spill`
`stack`*`(memory)andmanyoptimisations.''See[wiki:Commentary/Compiler/CmmTypeTheCmmlanguage]forthedefinitionofa`*`basic`
`block`*`(inHaskell,`**`).''`

and one header file:

`*`[`GhcFile(compiler/nativeGen/NCG.h)`](GhcFile(compiler/nativeGen/NCG.h) "wikilink")[`BR`](BR "wikilink")\
`definesmacrosusedtoseparatearchitecture-specificcodeintheHaskellNCGfiles;sinceGHCcurrentlyonlygeneratesmachinecodeforthearchitectureonwhichitwascompiled(GHCisnotcurrentlyacross-compiler),theHaskellNCGfilesbecomeconsiderablysmallerafterpreprocessing;ideallyallarchitecture-specificcodewouldresideinseparatefilesandGHCwouldhavethemavailabletosupportcross-compilercapabilities.`

The NCG has **machine-independent** and **machine-dependent** parts.

The **machine-independent** parts relate to generic operations,
especially optimisations, on Cmm code. The main machine-independent
parts begin with *Cmm blocks.* (A *Cmm block* is a compilation unit of
Cmm code, a file. See \[wiki:Commentary/Compiler/CmmType The Cmm
language\] for a discussion of what a *Cmm block* is but note that *Cmm*
is a type synonym for .) A machine-specific (assembler) instruction is
represented as a . The machine-independent NCG parts:

`1.optimiseeachCmmblockbyreorderingitsbasicblocksfromtheoriginalorder(the``orderfromthe``)tominimisethenumberofbranchesbetweenbasicblocks,inotherwords,bymaximisingfallthroughofexecutionfromonebasicblocktothenext.`[`BR`](BR "wikilink")[`BR`](BR "wikilink")\
`1.lazilyconverteachCmmblocktoabstractmachineinstructions(``)operatingonaninfinitenumberofregisters--sincetheNCGHaskellfilesonlycontaininstructionsforthehostcomputeronwhichGHCwascompiled,these``aremachine-specific;and,`[`BR`](BR "wikilink")[`BR`](BR "wikilink")\
`1.lazilyallocaterealregistersforeachbasicblock,basedonthenumberofavailableregistersonthetarget(currently,onlythehost)machine;forexample,32integerand32floating-pointregistersonthePowerPCarchitecture.TheNCGdoesnotcurrentlyhavesupportforSIMDregisterssuchasthevectorregistersforAltivecoranyvariationofSSE.`[`BR`](BR "wikilink")*`Note`*`:ifabasicblocksimultaneouslyrequiresmoreregistersthanareavailableonthetargetmachineandthetemporaryvariableneedstobeused(wouldsillbe`*`live`*`)afterthecurrentinstruction,itwillbemoved(`*`spilled`*`)intomemory.`

The **machine-dependent** parts:

`1.definetheabstract(Haskell)assembler``forthetarget(host)machineandconverteveryCmmblockintoit;`\
`1.define,manageandallocatetherealregistersavailableonthetargetsystem;`\
`1.pretty-printtheHaskell-assemblertoGNUAS(GAS)assemblercode`

Overview
--------

The top-level code generator function is The returned \`SDoc\` is for
debugging, so is empty unless you specify \`-ddump-stix\`. The
\`Pretty.Doc\` bit is the final assembly code. Translation involves
three main phases, the first and third of which are target-independent.

#### Translation into the Stix representation

Stix is a simple tree-like RTL-style language, in which you can mention:

`*Aninfinitenumberoftemporary,virtualregisters.`\
`` *TheSTG"magic"registers(`MagicId`),suchastheheapandstackpointers. ``\
`` *Literalsandlow-levelmachineops(`MachOp`). ``\
`*Simpleaddresscomputations.`\
`*Readsandwritesof:memory,virtualregs,andvariousSTGregs.`\
`` *Labelsand`if...goto...`stylecontrol-flow. ``

Stix has two main associated types:

`` *`StixStmt`--treesexecutedfortheirsideeffects:assignments,controltransfers,andauxiliaryjunksuchassegmentchangesandliteraldata. ``\
`` *`StixExpr`--treeswhichdenoteavalue. ``

Translation into Stix is almost completely target-independent. Needed
dependencies are knowledge of word size and endianness, used when
generating code to do deal with half-word fields in info tables. This
could be abstracted out easily enough. Also, the Stix translation needs
to know which \`MagicId\`s map to registers on the given target, and
which are stored in offsets from \`BaseReg\`.

After initial Stix generation, the trees are cleaned up with
constant-folding and a little copy-propagation ("Stix inlining", as the
code misleadingly calls it). We take the opportunity to translate
\`MagicId\`s which are stored in memory on the given target, into
suitable memory references. Those which are stored in registers are left
alone. There is also a half-hearted attempt to lift literal strings to
the top level in cases where nested strings have been observed to give
incorrect code in the past.

Primitive machine-level operations will already be phrased in terms of
\`MachOp\`s in the presented Abstract C, and these are passed through
unchanged. We comment only that the \`MachOp\`s have been chosen so as
to be easy to implement on all targets, and their meaning is intended to
be unambiguous, and the same on all targets, regardless of word size or
endianness.

**A note on \`MagicId\`s**. Those which are assigned to registers on the
current target are left unmodified. Those which are not are stored in
memory as offsets from \`BaseReg\` (which is assumed to permanently have
the value (\`&MainCapability.r\`)), so the constant folder calculates
the offsets and inserts suitable loads/stores. One complication is that
not all archs have \`BaseReg\` itself in a register, so for those
(sparc), we instead generate the address as an offset from the static
symbol \`MainCapability\`, since the register table lives in there.

Finally, \`BaseReg\` does occasionally itself get mentioned in Stix
expression trees, and in this case what is denoted is precisely
(\`&MainCapability.r\`), not, as in all other cases, the value of memory
at some offset from the start of the register table. Since what it
denotes is an r-value and not an l-value, assigning \`BaseReg\` is
meaningless, so the machinery checks to ensure this never happens. All
these details are taken into account by the constant folder.

#### Instruction selection

This is the only majorly target-specific phase. It turns Stix statements
and expressions into sequences of \`Instr\`, a data type which is
different for each architecture. Instr, unsurprisingly, has various
supporting types, such as \`Reg\`, \`Operand\`, \`Imm\`, etc. The
generated instructions may refer to specific machine registers, or to
arbitrary virtual registers, either those created within the instruction
selector, or those mentioned in the Stix passed to it.

The instruction selectors live in \`MachCode.lhs\`. The core functions,
for each target, are:

The insn selectors use the "maximal munch" algorithm. The
bizarrely-misnamed \`getRegister\` translates expressions. A simplified
version of its type is: That is: it (monadically) turns a StixExpr into
a sequence of instructions, and a register, with the meaning that after
executing the (possibly empty) sequence of instructions, the (possibly
virtual) register will hold the resulting value. The real situation is
complicated by the presence of fixed registers, and is detailed below.

Maximal munch is a greedy algorithm and is known not to give globally
optimal code sequences, but it is good enough, and fast and simple.
Early incarnations of the NCG used something more sophisticated, but
that is long gone now.

Similarly, \`getAmode\` translates a value, intended to denote an
address, into a sequence of insns leading up to a (processor-specific)
addressing mode. This stuff could be done using the general
\`getRegister\` selector, but would necessarily generate poorer code,
because the calculated address would be forced into a register, which
might be unnecessary if it could partially or wholly be calculated using
an addressing mode.

Finally, \`assignMem\_IntCode\` and \`assignReg\_IntCode\` create
instruction sequences to calculate a value and store it in the given
register, or at the given address. Because these guys translate a
statement, not a value, they just return a sequence of insns and no
associated register. Floating-point and 64-bit integer assignments have
analogous selectors.

Apart from the complexities of fixed vs floating registers, discussed
below, the instruction selector is as simple as it can be. It looks long
and scary but detailed examination reveals it to be fairly
straightforward.

#### Register allocation

The register allocator, \`AsmRegAlloc.lhs\` takes sequences of Instrs
which mention a mixture of real and virtual registers, and returns a
modified sequence referring only to real ones. It is gloriously and
entirely target-independent. Well, not exactly true. Instead it regards
\`Instr\` (instructions) and \`Reg\` (virtual and real registers) as
abstract types, to which it has the following interface: \`insnFuture\`
is used to (re)construct the graph of all possible control transfers
between the insns to be allocated. \`regUsage\` returns the sets of
registers read and written by an instruction. And \`patchRegs\` is used
to apply the allocator's final decision on virtual-to-real reg mapping
to an instruction.

Clearly these 3 fns have to be written anew for each architecture. They
are defined in \`RegAllocInfo.lhs\`. Think twice, no, thrice, before
modifying them: making false claims about insn behaviour will lead to
hard-to-find register allocation errors.

\`AsmRegAlloc.lhs\` contains detailed comments about how the allocator
works. Here is a summary. The head honcho takes a list of instructions
and a list of real registers available for allocation, and maps as many
of the virtual regs in the input into real ones as it can. The returned
\`Bool\` indicates whether or not it was successful. If so, that's the
end of it. If not, the caller of \`allocUsingTheseRegs\` will attempt
spilling. More of that later. What \`allocUsingTheseRegs\` does is:

`*Implicitlynumbereachinstructionbyitspositionintheinputlist.`\
`` *Using`insnFuture`,createthesetofallflowedges--possiblecontroltransfers--withinthissetofinsns. ``\
`` *Using`regUsage`anditeratingaroundtheflowgraphfromthepreviousstep,calculate,foreachvirtualregister,thesetofflowedgesonwhichitislive. ``\
`*Makeareal-registercommittmentmap,whichgivesthesetofedgesforwhicheachrealregisteriscommitted(inuse).Thesesetsareinitiallyempty.Foreachvirtualregister,attempttofindarealregisterwhosecurrentcommittmentdoesnotintersectthatofthevirtualregister--ie,isuncommittedonalledgesthatthevirtualregislive.Ifsuccessful,thismeansthevregcanbeassignedtotherealreg,soaddthevreg'ssettotherealreg'scommittment.`\
`` *Ifallthevregswereassignedtoarealreg,use`patchInstr`toapplythemappingtotheinsnsthemselves. ``

### Spilling

If \`allocUsingTheseRegs\` fails, a baroque mechanism comes into play.
We now know that much simpler schemes are available to do the same thing
and give better results. Anyways:

The logic above \`allocUsingTheseRegs\`, in \`doGeneralAlloc\` and
\`runRegAllocate\`, observe that allocation has failed with some set R
of real registers. So they apply \`runRegAllocate\` a second time to the
code, but remove (typically) two registers from R before doing so. This
naturally fails too, but returns a partially-allocated sequence.
\`doGeneralAlloc\` then inserts spill code into the sequence, and
finally re-runs \`allocUsingTheseRegs\`, but supplying the original,
unadulterated R. This is guaranteed to succeed since the two registers
previously removed from R are sufficient to allocate all the
spill/restore instructions added.

Because x86 is very short of registers, and in the worst case needs
three removed from R, a softly-softly approach is used.
\`doGeneralAlloc\` first tries with zero regs removed from R, then if
that fails one, then two, etc. This means \`allocUsingTheseRegs\` may
get run several times before a successful arrangement is arrived at.
\`findReservedRegs\` cooks up the sets of spill registers to try with.

The resulting machinery is complicated and the generated spill code is
appalling. The saving grace is that spills are very rare so it doesn't
matter much. I did not invent this -- I inherited it.

### Dealing with common cases fast

The entire reg-alloc mechanism described so far is general and correct,
but expensive overkill for many simple code blocks. So to begin with we
use \`doSimpleAlloc\`, which attempts to do something simple. It
exploits the observation that if the total number of virtual registers
does not exceed the number of real ones available, we can simply dole
out a new realreg each time we see mention of a new vreg, with no regard
for control flow. \`doSimpleAlloc\` therefore attempts this in a single
pass over the code. It gives up if it runs out of real regs or sees any
condition which renders the above observation invalid (fixed reg uses,
for example).

This clever hack handles the majority of code blocks quickly. It was
copied from the previous reg-allocator (the Mattson/Partain/Marlow/Gill
one).

Complications, observations, and possible improvements
------------------------------------------------------

### Real vs virtual registers in the instruction selectors

The instruction selectors for expression trees, namely \`getRegister\`,
are complicated by the fact that some expressions can only be computed
into a specific register, whereas the majority can be computed into any
register. We take x86 as an example, but the problem applies to all
archs.

Terminology: \`rreg\` means real register, a real machine register.
\`vreg\` means one of an infinite set of virtual registers. The type
\`Reg\` is the sum of \`rreg\` and \`vreg\`. The instruction selector
generates sequences with unconstrained use of vregs, leaving the
register allocator to map them all into rregs.

Now, where was I ? Oh yes. We return to the type of \`getRegister\`,
which despite its name, selects instructions to compute the value of an
expression tree.

At first this looks eminently reasonable (apart from the stupid name).
\`getRegister\`, and nobody else, knows whether or not a given
expression has to be computed into a fixed rreg or can be computed into
any rreg or vreg. In the first case, it returns \`Fixed\` and indicates
which rreg the result is in. In the second case it defers committing to
any specific target register by returning a function from \`Reg\` to
\`InstrBlock\`, and the caller can specify the target reg as it sees
fit.

Unfortunately, that forces \`getRegister\`'s callers (usually itself) to
use a clumsy and confusing idiom in the common case where they do not
care what register the result winds up in. The reason is that although a
value might be computed into a fixed rreg, we are forbidden (on pain of
segmentation fault :) from subsequently modifying the fixed reg. This
and other rules are record in "Rules of the game" inside
\`MachCode.lhs\`.

Why can't fixed registers be modified post-hoc? Consider a simple
expression like \`Hp+1\`. Since the heap pointer \`Hp\` is definitely in
a fixed register, call it R, \`getRegister\` on subterm \`Hp\` will
simply return Fixed with an empty sequence and R. But we can't just emit
an increment instruction for R, because that trashes \`Hp\`; instead we
first have to copy it into a fresh vreg and increment that.

With all that in mind, consider now writing a \`getRegister\` clause for
terms of the form \`(1 + E)\`. Contrived, yes, but illustrates the
matter. First we do \`getRegister\` on \`E\`. Now we are forced to
examine what comes back. This seems unreasonably cumbersome, yet the
instruction selector is full of such idioms. A good example of the
complexities induced by this scheme is shown by \`trivialCode\` for x86
in \`MachCode.lhs\`. This deals with general integer dyadic operations
on x86 and has numerous cases. It was difficult to get right.

An alternative suggestion is to simplify the type of \`getRegister\` to
this: and then we could safely write which is about as straightforward
as you could hope for. Unfortunately, it requires \`getRegister\` to
insert moves of values which naturally compute into an rreg, into a
vreg. Consider: On x86 the ccall result is returned in rreg \`%eax\`.
The resulting sequence, prior to register allocation, would be: If, as
is likely, \`%eax\` is not held live beyond this point for any other
purpose, the move into a fresh register is pointless; we'd have been
better off leaving the value in \`%eax\` as long as possible.

The simplified \`getRegister\` story is attractive. It would clean up
the instruction selectors significantly and make it simpler to write new
ones. The only drawback is that it generates redundant register moves. I
suggest that eliminating these should be the job of the register
allocator. Indeed:

`*Therehasbeensomeworkonthisalready("Iteratedregistercoalescing"?),sothisisn'tanewidea.`

`*Youcouldarguethattheexistingschemeinappropriatelyblurstheboundarybetweentheinstructionselectorandtheregisterallocator.Theinstructionselectorshould..well..justselectinstructions,withouthavingtofutzaroundworryingaboutwhatkindofregisterssubtreesgetgeneratedinto.Registerallocationshouldbe`*`entirely`*`thedomainoftheregisterallocator,withtheprovisothatitshouldendeavourtoallocateregisterssoastominimisethenumberofnon-redundantreg-regmovesinthefinaloutput.`

Selecting insns for 64-bit values/loads/stores on 32-bit platforms
------------------------------------------------------------------

Note that this stuff doesn't apply on 64-bit archs, since the
\`getRegister\` mechanism applies there. The relevant functions are:

\`iselExpr64\` is the 64-bit, plausibly-named analogue of
\`getRegister\`, and \`ChildCode64\` is the analogue of \`Register\`.
The aim here was to generate working 64 bit code as simply as possible.
To this end, I used the simplified \`getRegister\` scheme described
above, in which iselExpr64generates its results into two vregs which can
always safely be modified afterwards.

Virtual registers are, unsurprisingly, distinguished by their
\`Unique\`s. There is a small difficulty in how to know what the vreg
for the upper 32 bits of a value is, given the vreg for the lower 32
bits. The simple solution adopted is to say that any low-32 vreg may
also have a hi-32 counterpart which shares the same unique, but is
otherwise regarded as a separate entity. \`getHiVRegFromLo\` gets one
from the other. Apart from that, 64-bit code generation is really
simple. The sparc and x86 versions are almost copy-n-pastes of each
other, with minor adjustments for endianness. The generated code isn't
wonderful but is certainly acceptable, and it works.

Shortcomings and inefficiencies in the register allocator
---------------------------------------------------------

### Redundant reconstruction of the control flow graph

The allocator goes to considerable computational expense to construct
all the flow edges in the group of instructions it's allocating for, by
using the \`insnFuture\` function in the \`Instr\` pseudo-abstract type.

This is really silly, because all that information is present at the
abstract C stage, but is thrown away in the translation to Stix. So a
good thing to do is to modify that translation to produce a directed
graph of Stix straight-line code blocks, and to preserve that structure
through the insn selector, so the allocator can see it.

This would eliminate the fragile, hacky, arch-specific \`insnFuture\`
mechanism, and probably make the whole compiler run measurably faster.
Register allocation is a fair chunk of the time of non-optimising
compilation (10% or more), and reconstructing the flow graph is an
expensive part of reg-alloc. It would probably accelerate the vreg
liveness computation too.

### Really ridiculous method for doing spilling

This is a more ambitious suggestion, but ... reg-alloc should be
reimplemented, using the scheme described in "Quality and speed in
linear-scan register allocation." (Traub?) For straight-line code
blocks, this gives an elegant one-pass algorithm for assigning registers
and creating the minimal necessary spill code, without the need for
reserving spill registers ahead of time.

I tried it in Rigr, replacing the previous spiller which used the
current GHC scheme described above, and it cut the number of spill loads
and stores by a factor of eight. Not to mention being simpler, easier to
understand and very fast.

The Traub paper also describes how to extend their method to multiple
basic blocks, which will be needed for GHC. It comes down to reconciling
multiple vreg-to-rreg mappings at points where control flow merges.

### Redundant-move support for revised instruction selector suggestion

As mentioned above, simplifying the instruction selector will require
the register allocator to try and allocate source and destination vregs
to the same rreg in reg-reg moves, so as to make as many as possible go
away. Without that, the revised insn selector would generate worse code
than at present. I know this stuff has been done but know nothing about
it. The Linear-scan reg-alloc paper mentioned above does indeed mention
a bit about it in the context of single basic blocks, but I don't know
if that's sufficient.

x86 arcana that you should know about
-------------------------------------

The main difficulty with x86 is that many instructions have fixed
register constraints, which can occasionally make reg-alloc fail
completely. And the FPU doesn't have the flat register model which the
reg-alloc abstraction (implicitly) assumes.

Our strategy is: do a good job for the common small subset, that is
integer loads, stores, address calculations, basic ALU ops (+, -, and,
or, xor), and jumps. That covers the vast majority of executed insns.
And indeed we do do a good job, with a loss of less than 2% compared
with gcc.

Initially we tried to handle integer instructions with awkward register
constraints (mul, div, shifts by non-constant amounts) via various
jigglings of the spiller et al. This never worked robustly, and putting
platform-specific tweaks in the generic infrastructure is a big No-No.
(Not quite true; shifts by a non-constant amount are still done by a
giant kludge, and should be moved into this new framework.)

Fortunately, all such insns are rare. So the current scheme is to
pretend that they don't have any such constraints. This fiction is
carried all the way through the register allocator. When the insn
finally comes to be printed, we emit a sequence which copies the
operands through memory (\`%esp\`-relative), satisfying the constraints
of the real instruction. This localises the gruesomeness to just one
place. Here, for example, is the code generated for integer divison of
\`%esi\` by \`%ecx\`: This is not quite as appalling as it seems, if you
consider that the division itself typically takes 16+ cycles, whereas
the rest of the insns probably go through in about 1 cycle each.

This trick is taken to extremes for FP operations.

All notions of the x86 FP stack and its insns have been removed.
Instead, we pretend, to the instruction selector and register allocator,
that x86 has six floating point registers, \`%fake0\` .. \`%fake5\`,
which can be used in the usual flat manner. We further claim that x86
has floating point instructions very similar to SPARC and Alpha, that
is, a simple 3-operand register-register arrangement. Code generation
and register allocation proceed on this basis.

When we come to print out the final assembly, our convenient fiction is
converted to dismal reality. Each fake instruction is independently
converted to a series of real x86 instructions. \`%fake0\` .. \`%fake5\`
are mapped to \`%st(0)\` .. \`%st(5)\`. To do reg-reg arithmetic
operations, the two operands are pushed onto the top of the FP stack,
the operation done, and the result copied back into the relevant
register. When one of the operands is also the destination, we emit a
slightly less scummy translation. There are only six \`%fake\` registers
because 2 are needed for the translation, and x86 has 8 in total.

The translation is inefficient but is simple and it works. A cleverer
translation would handle a sequence of insns, simulating the FP stack
contents, would not impose a fixed mapping from \`%fake\` to \`%st\`
regs, and hopefully could avoid most of the redundant reg-reg moves of
the current translation.

There are, however, two unforeseen bad side effects:

`*Thisdoesn'tworkproperly,becauseitdoesn'tobservethenormalconventionsforx86FPcodegeneration.Itturnsoutthateachofthe8elementsinthex86FPregisterstackhasatagbitwhichindicateswhetherornotthatregisterisnotionallyinuseornot.IfyoudoaFPUoperationwhichhappenstoreadatagged-as-emptyregister,yougetanx87FPU(stackinvalid)exception,whichisnormallyhandledbytheFPUwithoutpassingittotheOS:theprogramkeepsgoing,buttheresultingFPvaluesaregarbage.TheOScanaskfortheFPUtopassitFPstack-invalidexceptions,butitusuallydoesn't.`

`Anyways:insideNCGcreatedx86FPcodethisallworksfine.However,theNCG'sfictionofaflatregistersetdoesnotoperatethex87registerstackintherequiredstack-likeway.Whencontrolreturnstoagcc-generatedworld,thestacktagbitssooncausestackexceptions,andthusgarbageresults.`

`` TheonlyfixIcouldthinkof--anditishorrible--istoclearallthetagbitsjustbeforethenextSTG-levelentry,inchunksofcodewhichuseFPinsns.`i386_insert_ffrees`insertstherelevant`ffree`insnsintosuchcodeblocks.Itdependscriticallyon`is_G_instr`todetectsuchblocks. ``

`*It'sverydifficulttoreadthegeneratedassemblyandreasonaboutitwhendebugging,becausethere'ssomuchclutter.Weprintthefakeinsnsascommentsintheoutput,andthathelpsabit.`

Generating code for ccalls
--------------------------

For reasons I don't really understand, the instruction selectors for
generating calls to C (genCCall) have proven surprisingly difficult to
get right, and soaked up a lot of debugging time. As a result, I have
once again opted for schemes which are simple and not too difficult to
argue as correct, even if they don't generate excellent code.

The sparc ccall generator in particular forces all arguments into
temporary virtual registers before moving them to the final
out-registers (\`%o0\` .. \`%o5\`). This creates some unnecessary
reg-reg moves. The reason is explained in a comment in the code.

Duplicate implementation for many STG macros
--------------------------------------------

This has been discussed at length already. It has caused a couple of
nasty bugs due to subtle untracked divergence in the macro translations.
The macro-expander really should be pushed up into the Abstract C phase,
so the problem can't happen.

Doing so would have the added benefit that the NCG could be used to
compile more "ways" -- well, at least the 'p' profiling way.

How to debug the NCG without losing your sanity/hair/cool
---------------------------------------------------------

Last, but definitely not least ...

The usual syndrome is that some program, when compiled via C, works, but
not when compiled via the NCG. Usually the problem is fairly simple to
fix, once you find the specific code block which has been mistranslated.
But the latter can be nearly impossible, since most modules generate at
least hundreds and often thousands of them.

My solution: cheat.

Because the via-C and native routes diverge only late in the day, it is
not difficult to construct a 1-1 correspondence between basic blocks on
the two routes. So, if the program works via C but not on the NCG, do
the following:

`` *Recompile`AsmCodeGen.lhs`intheafflictedcompilerwith`-DDEBUG_NCG`,sothatitinserts`___ncg_debug_markers`intotheassemblyitemits. ``\
`*Usingabinarysearchonmodules,findthemodulewhichiscausingtheproblem.`\
`` *Compilethatmoduletoassemblycode,withidenticalflags,twice,onceviaCandonceviaNCG.Calltheoutputs`ModuleName.s-gcc`and`ModuleName.s-nat`.Checkthatthelatterdoesindeedhave`___ncg_debug_markers`init;otherwisethenextstepsfail. ``\
`` *Build(withaworkingcompiler)theprogram`utils/debugNCG/diff_gcc_nat`. ``\
`` *Run:`diff_gcc_natModuleName.s`.Thiswillconstructthe1-1correspondence,andemitsonstdoutacppableassemblyoutput.Placethisinafile--Ialwayscallitsynth.S.Note,thecapitalSisimportant;otherwiseitwon'tgetcpp'd.Youcanfeedthisfiledirectlytoghcanditwillautomaticallygetcpp'd;youdon'thavetodosoyourself. ``\
`` *Bymessingwiththe`#define`satthetopof`synth.S`,doabinarysearchtofindtheincorrectblock.Keepacarefulrecordofwhereyouareinthesearch;itiseasytogetconfused.Rememberalsothatmultipleblocksmaybewrong,whichalsoconfusesmatters.Finally,Iusuallystartoffbyre-checkingthatIcanbuildtheexecutablewithallthe`#define`ssetto0andthenallto1.Thisensuresyouwon'tgethalfwaythroughthesearchandthengetstuckduetosomesnafuwithgcc-specificliterals.UsuallyIset`UNMATCHED_GCC`to1allthetime,andthisbitshouldcontainonlyliteraldata.`UNMATCHED_NAT`shouldbeempty. ``

\`diff\_gcc\_nat\` was known to work correctly last time I used it, in
December 01, for both x86 and sparc. If it doesn't work, due to changes
in assembly syntax, or whatever, make it work. The investment is well
worth it. Searching for the incorrect block(s) any other way is a total
time waster.

Historical page
---------------

This page describes state of the new code generator sometime back in
2008. It is completely outdated and is here only for historical reasons.
See \[wiki:Commentary/Compiler/CodeGen Code Generator\] page for a
description of current code generator.

Overview of modules in the new code generator
=============================================

This page gives an overview of the new code generator, including
discussion of:

`*the[wiki:Commentary/Compiler/NewCodeGenModules#ThenewCmmdatatypenewCmmtype]`\
`*the[wiki:Commentary/Compiler/NewCodeGenModules#Modulestructureofthenewcodegeneratormodulestructureofthenewcodegenerator]`

See also \[wiki:Commentary/Compiler/NewCodeGenPipeline the description
of the new code generation pipeline\].

The new Cmm data type
---------------------

There is a new Cmm data type:

`*`[`GhcFile(compiler/cmm/ZipCfg.hs)`](GhcFile(compiler/cmm/ZipCfg.hs) "wikilink")`containsagenericzipper-basedcontrol-flowgraphdatatype.Itisgenericinthesensethatit'spolymorphicinthetypeof`**`middle`
`nodes`**`and`**`last`
`nodes`**`ofablock.(Middlenodesdon'tdocontroltransfers;lastnodesonlydocontroltransfers.)Thereareextensivenotesatthestartofthemodule.`[`BR`](BR "wikilink")[`BR`](BR "wikilink")\
`Thekeytypesitdefinesare:`\
`` *Blockidentifiers:`BlockId`,`BlockEnv`,`BlockSet` ``\
`` *Control-flowblocks:`Block` ``\
`` *Control-flowgraphs:`Graph` ``[`BR`](BR "wikilink")[`BR`](BR "wikilink")\
`*`**`` `ZipDataFlow` ``**`` containsagenericframeworkforsolvingdataflowproblemsover`ZipCfg`.Itallowsyoutodefineanewoptimizationsimplybydefiningalatticeofdataflowfacts(akintoaspecializedlogic)andthenwritingthedataflow-transferfunctionsfoundincompilertextbooks.Handingthesefunctionstothedataflowengineproducesanewoptimizationthatisnotonlyusefulonitsown,butthatcaneasilybecomposedwithotheroptimizationstocreateanintegrated"superoptimization"thatisstrictlymorepowerfulthananysequenceofindividualoptimizations,nomatterhowmanytimestheyarere-run.Thedataflowengineisbasedon ``[`(Lerner,`
`Grove,` `and` `Chambers`
`2002)`](http://citeseer.ist.psu.edu/old/lerner01composing.html)`;youcanfindafunctionalimplementationofthedataflowenginepresentedin`[`(Ramsey`
`and` `Dias`
`2005)`](http://www.cs.tufts.edu/~nr/pubs/zipcfg-abstract.html)`.`[`BR`](BR "wikilink")[`BR`](BR "wikilink")\
`*`**[`GhcFile(compiler/cmm/ZipCfgCmmRep.hs)`](GhcFile(compiler/cmm/ZipCfgCmmRep.hs) "wikilink")**`` instantiates`ZipCfg`forCmm,bydefiningtypes`Middle`and`Last`andusingthesetoinstantiatethepolymorphicfieldsof`ZipCfg`.Italsodefinesabunchofsmartconstructor(`mkJump`,`mkAssign`,`mkCmmIfThenElse`etc)whichmakeiteasytobuild`CmmGraph`. ``[`BR`](BR "wikilink")[`BR`](BR "wikilink")\
`*`**`` `CmmExpr` ``**`containsthedatatypesforCmmexpressions,registers,andthelike.Hereisafullerdescriptionofthesetypesisat[wiki:Commentary/Compiler/BackEndTypes].Itdoesnotdependonthedataflowframeworkatall.`

Module structure of the new code generator
------------------------------------------

The new code generator has a fair number of modules, which can be split
into three groups:

`*basicdatatypesandinfrastructure`\
`*analysesandtransformations`\
`*linkingthepipeline`

All the modules mentioned are in the \`cmm/\` directory, unless
otherwise indicated.

### Basic datatypes and infrastructure

Ubiquitous types:

`` *`CLabel`(`CLabel`):Allsortsofgooformakingandmanipulatinglabels. ``

`` *`BlockId`(`BlockId`,`BlockEnv`,`BlockSet`): ``\
`Thetypeofabasic-blockid,alongwithsetsandfinitemaps.`

`` *`CmmExpr`(`CmmType`,`LocalReg`,`GlobalReg`,`Area`,`CmmExpr`): ``\
`Lotsoftypedefinitions:forCmmtypes(bitwidth,GCptr,float,etc),`\
`registers,stackareas,andCmmexpressions.`

`` *`Cmm`(`GenCmm`,`CmmInfo`,`CmmInfoTable`): ``\
`` Moretypedefinitions:theparameterizedtop-levelCmmtype(`GenCmm`), ``\
`alongwiththetypedefinitionsforinfotables.`

Control-flow graphs:

`` *`ZipCfg`(`Graph`,`LGraph`,`Block`): ``\
`Describesazipper-likerepresentationfortruebasic-block`\
`control-flowgraphs.Ablockhasasingleentrypoint,`\
`whichisaalwaysalabel,followedbyzeroormode'middle`\
`nodes',eachofwhichrepresentsanuninterruptible`\
`single-entry,single-exitcomputation,thenfinallya'last`\
`node',whichmayhavezeroormoresuccessors.`\
`` `ZipCFG`ispolymorphicinthetypeofmiddleandlastnodes. ``\
`` *`ZipCfgCmmRep`(`Middle`,`Last`,`CmmGraph`) ``\
`` Typestoinstantiate`ZipCfg`forC--:middleandlastnodes, ``\
`` andabunchofabbreviationsoftypesin`ZipCfg`and`Cmm`. ``

`` *`MkZipCfg`(`AGraph`,`mkLabel`,`mkMiddle`,`mkBranch`) ``\
`Smartconstructorsforcontrol-flowgraphs(andtheconstructorshave`\
`non-monadictypes).`\
`` Like`ZipCfg`,`MkZipCfg`ispolymorphicinthetypesofmiddleandlastnodes. ``\
`` *`MkZipCfgCmm`(`mkNop`,`mkAssign`,`mkStore`,`mkCall`,...) ``\
`Smartconstructorsforcreatingmiddleandlastnodesin`\
`control-flowgraphs(andtheconstructorshavenon-monadictypes).`

Calling conventions:

`` *`CmmInfo`(`cmmToRawCmm`,`mkBareInfoTable`): ``\
`` ConvertsCmmcodeto"raw"Cmm.Whatthismeansis:converta`CmmInfo`datastructuredescribingtheinfotableforeach`CmmProc`toa`[CmmStatic]`. ``\
`` `mkBareInfoTable`istheworkhorsethatproducesthe`[CmmStatic]`.Itisalsousedtoproducetheinfotablerequiredforsafeforeigncalls(amiddlenode). ``\
`` *`CmmCallConv`(`ArgumentFormat`,`assignArgumentsPos`): ``\
`ImplementsCmmcallingconventions:givenargumentsandacallingconvention,`\
`thismoduledecideswheretoputthearguments.`\
`(JD:Crufty.Lotsofoldcodeinhere,needscleanup.)`

Dataflow analysis:

`` *`CmmTx`(`Tx`,`TxRes`): ``\
`Asimplemonadfortrackingwhenatransformationhas`\
`occurred(somethinghaschanged).`\
`Usedbythedataflowanalysistokeeptrackofwhenthegraphisrewritten.`

`` *`OptimizationFuel`(`OptimizationFuel`,`FuelMonad`,`maybeRewriteWithFuel`) ``\
`Wecanuseameasureof"fuel"tolimitthenumberofrewritesperformed`\
`byatransformation.Thismoduledefinesamonadfortracking(andlimiting)`\
`fueluse.`\
`(JD:Largelyuntested.)`

`` *`DFMonad`(`DataflowLattice`,`DataflowAnalysis`,`runDFM`): ``\
`Definesthetypeofadataflowlatticeandananalysis.`\
`Definesthemonadusedbythedataflowframework.`\
`Themonadkeepstrackofdataflowfacts,alongwithfuel,`\
`anditcanprovideuniqueid's.`\
`Allinsupportofthedataflowmodule.`

`` *`ZipDataflow`(`ForwardTransfers`,`BackwardTransfers`,`ForwardRewrites`,`BackwardRewrites`, ``\
`` `zdfSolveFrom`,`zdfRewriteFrom`,etc) ``\
`ThismoduleimplementstheLerner/Grove/Chambersdataflowanalysisframeword.`\
`Giventhedefinitionsofalatticeanddataflowtransfer/rewritefunctions,`\
`thismoduleprovidesalltheworkofrunningthedataflowanalysisandtransformation.`\
`Anumberofthephasesofthebackendrelyonthiscode,`\
`andhopefullymoreoptimizationswilltargetitinthefuture.`

And a few basic utilities:

`` *`CmmZipUtil`:(JD:Unused,Ibelieve,butprobablyshouldbeusedinafewplaces.) ``\
`Afewutilityfunctionsformanipulatingazipcfg.`\
`` *`PprC`:PrettyprintingtogenerateCcode. ``\
`` *`PprCmm`:PrettyprintingtheC--code. ``\
`` *`PprCmmZ`:(JD:Unused,Ibelieve.) ``\
`` Prettyprintingfunctionsrelatedto`ZipCfg`and`ZipCfgCmm`. ``

### Analyses and transformations

`` *`CmmLint`(`cmmLint`,`cmmLintTop`): ``\
`SomesanitycheckingontheoldCmmgraphs.`\
`Notsurehoweffectivethisis.`\
`` *`CmmLiveZ`(`CmmLive`,`livelattice`,`cmmLivenessZ`): ``\
`Livenessanalysisforregisters(usesdataflowframework).`\
`` *`CmmProcPointZ`(`ProcPointSet`,`callProcPoints`,`minimalProcPointSet`, ``\
`` `procPointAnalysis`,`splitAtProcPoints`) ``\
`Aprocpointisablockinacontrol-flowgraphthatmustbethe`\
`entrypointofanewprocedurewhenwegenerateCcode.`\
`Forexample,successorsofcallsandjoinpointsthatfollowcalls`\
`areprocpoints.`\
`Thismoduleprovidestheanalysestofindprocpoints,aswellas`\
`thetransformationtosplittheprocedureintopieces.`\
`Theprocpointanalysisdoesn'tusethedataflowframework,`\
`butitreallyshould-dominatorsarethewayforward.`\
`` *`CmmSpillReload`(`DualLive`,`dualLiveLattice`,`dualLiveness`, ``\
`` `dualLivenessWithInsertion`,`insertLateReloads`, ``\
`` `removeDeadAssignmentsAndReloads`): ``\
`Insertsspillsandreloadstoestablishtheinvariantthat`\
`atasafecall,therearenolivevariablesinregisters.`\
`` *`CmmCommonBlockElimZ`(`elimCommonBlocks`): ``\
`FindblocksintheCFGthatareidentical;mergethem.`\
`` *`CmmContFlowOpt`(`branchChainElimZ`,`removeUnreachableBlocksZ`, ``\
`` `runCmmOpts`): ``\
`Branch-chaineliminationandeliminationofunreachablecode.`\
`` *`CmmStackLayout`(`SlotEnv`,`liveSlotAnal`,`manifestSP`,`stubSlotsOnDeath`): ``\
`Thelive-slotanalysisdiscoverswhichstackslotsarelive`\
`ateachbasicblock.`\
`Weusetheresultsfortwopurposes:`\
`stacklayout(manifestSP)andinfotables(in!CmmBuildInfoTables).`\
`` Thefunction`stubSlotsOnDeath'isusedasadebuggingpass: ``\
`itstubseachstackslotwhenitdies,hopefullycausingbad`\
`programstofailfaster.`\
`` *`CmmBuildInfoTables`(`CAFEnv`,`cafAnal`,`lowerSafeForeignCalls`, ``\
`` `setInfoTableSRT`,`setInfoTableStackMap`): ``\
`Thismoduleisresponsibleforbuildinginfotables.`\
`Specifically,itbuildsthemapsoflivevariables(stackmaps)`\
`andSRTs.`\
`Italsohascodetolowersafeforeigncallsintoasequence`\
`thatmakesthemsafe(butsuspendingandresumingthreadsverycarefully).`\
`(JD:Thelatterfunctionprobablyshouldn'tbehere.)`

### Linking the pipeline

`` *`CmmCvt`:Convertsbetween`Cmm`and`ZipCfgCmm`representations. ``\
`(JD:TheZip->Cmmpathdefinitelyworks;haven'ttriedthe`\
`otherinalongtime--there'snoreasontouseitwith`\
`thenewStg->Cmmpath).`\
`` *`CmmCPSZ`:Linksthephasesofthebackendinsequence,alongwith ``\
`somepossibledebuggingoutput.`

### Dead code

`` *`CmmCPSGen`,`CmmCPS`(MichaelAdams),`CmmBrokenBlock`,`CmmLive`,`CmmPprCmmZ`,`StackColor`,`StackPlacements` ``

Historical page
---------------

This page stores historical information about Cmm Pipeline in the new
code generator. This description has been updated and is maintained on
the \[wiki:Commentary/Compiler/CodeGen Code Generator\] page. This page
has also historical notes about Adams optimisation. That optimisation is
also described in Note \[sharing continuations\] in
[GhcFile(compiler/codeGen/StgCmmMonad.hs)](GhcFile(compiler/codeGen/StgCmmMonad.hs) "wikilink")
and probably deserves its own wiki page.

Design of the new code generator
================================

This page contains notes about the design of the new code generator. See
also: \[wiki:Commentary/Compiler/NewCodeGenModules overview of the
module structure in the new code generator\].

Overview
--------

Code generation now has three stages:

`1.ConvertSTGtoCmm,withimplicitstackimplicit,andnativeCmmcalls.`\
`2.OptimisetheCmm,andCPS-convertittohaveanexplicitstack,andnonativecalls.`\
`` Thispartofthepipelineisstitchedtogetherin`cmm/CmmPipeline.hs`. ``\
`3.FeedtheCPS-convertedCmmtotheexisting,unmodifiednativecodegenerators.`

Ultimately our plan is to expand the capability of the new pipeline so
that it does native code generation too, and we can ultimately discard
the existing code generators. The design of this stage is here:
\[wiki:Commentary/Compiler/IntegratedCodeGen\]

The Cmm pipeline
----------------

The first two steps are described in more detail here:

`*`**`Code`
`generator`**`` convertsSTGto`CmmGraph`.Implementedin`StgCmm*`modules(indirectory`codeGen`). ``\
`` *`Cmm.CmmGraph`isprettymuchaHooplgraphof`CmmNode.CmmNode`nodes.Controltransferinstructionsarealwaysthelastnodeofabasicblock. ``\
`` *Parameterpassingismadeexplicit;thecallingconventiondependsonthetargetarchitecture.Thekeyfunctionis`CmmCallConv.assignArgumentsPos`. ``\
`*ParametersarepassedinvirtualregistersR1,R2etc.[Thesemap1-1torealregisters.]`\
`*Overflowparametersarepassedonthestackusingexplicitmemorystores,tolocationsdescribedabstractlyusingthe[wiki:Commentary/Compiler/StackAreas`*`Stack`
`Area`*`abstraction.].`\
`` *Makingthecallingconventionexplicitincludesanexplicitstoreinstructionofthereturnaddress,whichisstoredexplicitlyonthestackinthesamewayasoverflowparameters.Thisisdone(obscurely)in`MkGraph.mkCall`. ``

`*`**`Simple` `control` `flow`
`optimisation`**`` ,implementedin`CmmContFlowOpt`.It'scalledbothatthebeginningandendofthepipeline. ``\
`*Branchchainelimination.`\
`*Removeunreachableblocks.`\
`*Blockconcatenation.branchtoK;andthisistheonlyuseofK.`

`*`**`More` `control` `flow` `optimisations`**`.`\
`*CommonBlockElimination(likeCSE).ThisessentiallyimplementstheAdamsoptimisation,webelieve.`\
`*Consider(sometime):blockduplication.branchtoK;andKisashortblock.Branchchaineliminationisjustaspecialcaseofthis.`

`*`**`Proc-point`
`analysis`**`and`**`transformation`**`` ,implementedin`CmmProcPoint`.Thetransformationpartaddsafunctionprologuetothefrontofeachproc-point,followingastandardentryconvention. ``\
`` *Theanalysisproducesasetof`BlockId`thatshouldbecomeproc-points ``\
`*Thetransformationinsertsafunctionprologueatthestartofeachproc-point,andafunctionepiloguejustbeforeeachbranchtoaproc-point.`

`*`**`(OUTDATED` `-` `!CmmSpillReload` `does` `not` `exist` `any`
`more)`**``**`Add`
`spill/reload`**`` ,implementedin`CmmSpillReload`,tospillliveC--variablesbeforeacallandreloadthemafterwards.Thespillandreloadinstructionsaresimplymemorystoresandloadsrespectively,usingsymbolicstackoffsets(see[wiki:Commentary/Compiler/StackAreas#Layingoutthestackstacklayout]).Forexample,aspillofvariable'x'wouldlooklike`Ptr32[SS(x)]=x`. ``\
`` *`dualLivenessWithInsertion`doestwothings: ``\
`*Spillsatthedefinitionofanyvariablethatissubequentlyliveacrossacall(usesabackwardanalysis)`\
`*Addsareloadateachreturn(orproc)point`\
`` Atthispoint,no(`LocalReg`)variablesareliveacrossacall. ``\
`` *TODO:avoid`f();g()`turninginto`spillx;f();reloadx;spillx;g();reloadx`. ``

`*`**`(OUTDATED` `-` `!CmmRewriteAssignments` `is` `not` `used` `any`
`more)`**``**`Rewrite`
`assignments`**`(assignmentstolocalregs,thatis,notstores).`\
`` *Convertgraphtoannotatedgraphwhosenodesare`CmmRewriteAssignments.WithRegUsage`.Specifically,`CmmAssign`isdecoratedwithaflag`RegUsage`sayingwhetheritisusedonceormanytimes. ``\
`*Sinkorinlineassignmentsnearertheirusepoints`\
`*Doconstantmach-opfolding.Thisisdoneinthisphase,becausefoldedmach-opscanbeinlined,andinliningexposesopportunitiesformach-opfolding.`

`*`**`Remove` `dead` `assignments` `and`
`stores`**``` ,implementedin`CmmLive`,removesassignmentstodeadvariablesandthingslike``a=a``or``I32[Hp]=I32[Hp]``.Thelattermaymoreappropriatelybedoneinageneraloptimizationpass,asitdoesn'ttakeadvantageoflivenessinformation. ```

`*`**`Figure` `out` `the` `stack`
`layout`**`` ,implementedin`CmmStackLayout`. ``\
`*Eachvariable'x',andeachproc-pointlabel'K',hasanassociated`*`Area`*`,writtenSS(x)andSS(k)resp,thatnamesacontiguousportionofthestackframe.`\
`*Thestacklayoutpassproducesamappingof:`*`` (`Area` `` `->`
`` `StackOffset`) ``*`.Formoredetail,see[wiki:Commentary/Compiler/StackAreas#Layingoutthestackthedescriptionofstacklayout.]`\
`` *A`StackOffset`isthebyteoffsetofastackslotfromtheoldend(highaddress)oftheframe.Itdoesn'tvaryasthephysicalstackpointermoves. ``

`*`**`Manifest` `the` `stack`
`pointer`**`` ,implementedin`CmmStackLayout`.Oncethestacklayoutmappinghasbeendetermined,asecondpasswalksoverthegraph,makingthestackpointer,`Sp`explicit.Beforethispass,thereisno`Sp`atall.Afterthis,`Sp`iscompletelymanifest. ``\
`` *replacingreferencesto`Areas`withoffsetsfrom`Sp`. ``\
`` *addingadjustmentsto`Sp`. ``\
\
`*`**`Split` `into` `multiple`
`!CmmProcs`**`` ,implementedin`CmmProcPointZ`.Atthispointwebuildaninfo-tableforeachofthe!CmmProcs,includingSRTs.Doneonthebasisofthelivelocalvariables(bynowmappedtostackslots)andliveCAFstatics. ``\
`` *`LastCall`and`LastReturn`nodesarereplacedby`Jump`s. ``

`*`**`Build` `info`
`tables`**`` ,implementedin`CmmBuildInfoTables`.. ``\
`` *Findeachsafe`MidForeignCall`node,"lowers"itintothesuspend/call/resumesequence(see`Note[Foreigncalls]`in`CmmNode.hs`.),andbuildaninfotableforthem. ``\
`` *Convertthe`CmmInfo`foreach`CmmProc`intoa`[CmmStatic]`,usingthelivevariableinformationcomputedjustbefore"Figureoutstacklayout". ``

### Branches to continuations and the "Adams optimisation"

A GC block for a heap check after a call should only take one or two
instructions. However the natural code: The label M is the head of the
call-gc-and-try-again loop. If we do this, we'll generate two info
tables, one for L and one for K.

We can do better like this:

Now the call has the same return signature as and can use the same
continuation, thus: Now we can coalesce the uniquely-used block M into
L, thus: (A call followed by a thus gets optimized down to just the
call.)

Now things are good. Simple common block elimination (CBE) will common
up K and L, so both calls share the same info table.

Runtime system
--------------

`*`**`Garbage` `collector` `entry`
`points`**`` :see`Note[Heapchecks]`in`StgCmmHeapery`. ``

`*`**`PAPs`**\
\
`*`**`Update` `frames`**`and`**`exception`
`handling`**`.AlsoSTMframes.`

`*`**`Primitives`**`canberewritten:`\
`*Useparameters`\
`*Inafewcases,usenativecalls(notablyeval)`

NOTE: Historical page
=====================

This page is here for historical reasons. Most of the issues described
here are now fixed (2 Aug 2012), and the new code generator produces
code approximately as good as the old code generator. Any remaining
issues will be made into tickets as necessary. See
\[wiki:Commentary/Compiler/CodeGen Code Generator\] page for an
up-to-date description of the current code generator.

Stupidity in the New Code Generator
===================================

Presently compiling using the new code generator results in a fairly
sizable performance hit, because the new code generator produces
sub-optimal (and sometimes absolutely terrible code.) There are [a lot
of ideas for how to make things
better](http://darcs.haskell.org/ghc/compiler/cmm/cmm-notes); the idea
for this wiki page is to document all of the stupid things the new code
generator is doing, to later be correlated with specific refactorings
and fixes that will hopefully eliminate classes of these stupid things.
The hope here is to develop a sense for what the most endemic problems
with the newly generated code is.

Cantankerous Comparisons
------------------------

FIXED in newcg branch, 15/2/2012

In \`cgrun065\` we have

Which compiles to the nice STG code

But the comparison is compiled into stupid code:

etc.

We're actually converting to a \`Bool\` and then doing an algebraic
case! This is a StgCmm issue, not a pipeline issue.

Dead stack/heap checks
----------------------

FIXED in newcg branch, but in an ad-hoc way (the stack allocator does
it). We probably want to do this as part of a more general optimisation
pass.

See in \`cgrun065\`

Instruction reordering
----------------------

NEW. We should be able to reorder instructions in order to decrease
register pressure. Here's an example from 3586.hs

R1 and Sp probably don't clobber each other, so we ought to use \_cPY
twice in quick succession. Fortunately stg\_IND\_STATIC\_info is a
constant so in this case the optimization doesn't help to much, but in
other cases it might make sense. TODO Find better example

Stack space overuse
-------------------

FIXED in the newcg branch. (stack layout algorithm redesigned)

CONFIRMED. \`T1969.hs\` demonstrates this:

The call area for the jump in cbG is using an extra word on the stack,
but in fact Sp + 0 at the entry of the function immediately becomes dead
after the assignment, so we ought to be able to save some space in our
layout. Simon Marlow suggests we distinguish between the return address
and the old call area; however, since this can also happen for the
return parameters from call areas, we need a more general scheme.

After I discussed this with SPJ, we've decided that we need to teach the
stack layout how to handle partial conflicts. There is a complication
here, in that if we do this naively, the interference graph will blow up
(since, rather than conflicting call areas, we now have conflicting
words of call areas.) Simon suggested that we bound the amount of
conflicts we track: either up to 3 or conflict with everything (in which
case we just place the area as far down as necessary rather than try to
be clever.) I plan on doing this once I understand the current layout
code...

Double temp-use means no inlinining?
------------------------------------

CONFIRMED. Here's a curious piece of code that fails to get inlined
(from \`cc004\`):

Why is that? Because the temp gets reused later on:

In this case, we want more aggressive inlining because there are too
many temps and they're going to have to get spilled to the stack anyway.
IS THAT TRUE? For comparison's sake, the old codegen doesn't appear to
do any rewriting, because it just reuses the call area.

Stupid spills
-------------

CONFIRMED. If something is already in memory, why do we have to spill it
again?

Well, it's because the spiller isn't clever enough:

Ick! The old codegen was much better...

The trouble is that the spiller doesn't know that the old call area is
also valid game for locations that variables can live in. So, the
solution is to rewrite the spiller to know about existing incoming
memory locations. Make sure that this information gets to the stack
layout engine when we do partial layouts (it should automatically
notice, but double check!)

Noppy proc-points
-----------------

CONFIRMED. Consider

We generate an extra proc-point for \`\`cmM\`\`, where in theory we
ought to be able to stick the subsequent \`\`stg\_ap\_pp\_fast\`\` onto
the stack as another return point.

Lots of temporary variables
---------------------------

WONTFIX. Lots of temporary variables (these can tickle other issues when
the temporaries are long-lived, but otherwise would be optimized away).
You can at least eliminate some of them by looking at the output of
\`-ddump-opt-cmm\`, which utilizes some basic temporary inlining when
used with the native backend \`-fasm\`, but this doesn't currently apply
to the GCC or LLVM backends.

\~\~At least one major culprit for this is \`allocDynClosure\`,
described in Note \`Return a LocalReg\`; this pins down the value of the
\`CmmExpr\` to be something for one particular time, but for a vast
majority of use-cases the expression is used immediately afterwards.
Actually, this is mostly my patches fault, because the extra rewrite
means that the inline pass is broken.\~\~ Fixed in latest version of the
pass; we don't quite manage to inline enough but there's only one extra
temporary.

Another cause of all of these temporary variables is that the new code
generator immediately assigns any variables that were on the stack to
temporaries immediately upon entry to a function. This is on purpose.
The idea is we optimize these temporary variables away.

Double proc points
------------------

FIXED in newcg branch.

Given a simple case expression

we generate \*two\* proc points, not one.

Both \`cbE\` and \`cbW\` are going to become proc points.

To avoid it we should generate code that re-uses \`cbE\` as the
destination for the first \`if\`; that is, we need to load up the
registers as if we were returning from the call. This needs some
refactoring in the code generator.

Rewriting stacks
----------------

FIXED. \`3586.hs\` emits the following code:

We see that these temporary variables are being repeatedly rewritten to
the stack, even when there are no changes.

Since these areas on the stack are all old call areas, one way to fix
this is to inline all of the memory references. However, this has
certain undesirable properties for other code, so we need to be a little
more clever. The key thing to notice is that these accesses are only
used once per control flow path, in which case sinking the loads down
and then inlining them should be OK (it will increase code size but not
execution time.) However, the other difficulty is that the CmmOpt
inliner, as it stands, won't inline things that look like this because
although the variable is only used once in different branches, the same
name is used, so it can't distinguish between the temporaries with
mutually exclusive live ranges. Building a more clever inliner with
Hoopl is also a bit tricky, because inlining is a forward
analysis/transformation, but usage counting is a backwards analysis.

This looks fixed with the patch from April 14.

Spilling Hp/Sp
--------------

FIXED. \`3586.hs\` emits the following code:

We see \`Hp - 4\` being allocated to a temp, and then consequently being
spilled to the stack even though \`newCAF\` definitely will not change
\`Hp\`, so we could have floated the expression down.

This seems to happen whenever there's a \`newCAF\` ccall.

We also seem to reload these values multiple times.

\~\~We need to not spill across certain foreign calls, but for which
calls this is OK for is unclear.\~\~ Variables stay live across all
unsafe foreign calls (foreign calls in the middle), except for the
obvious cases (the return registers), so no spilling should happen at
all. The liveness analysis is too conservative.

This is not fixed in the April 14 version of the patch... we still need
to fix the liveness analysis? I thought I fixed that... that's because
the transform did extra spilling for CmmUnsafeForeignCalls. Removed that
code, and now it's fixed.

Up and Down
-----------

FIXED. A frequent pattern is the stack pointer being bumped up and then
back down again, for no particular reason.

This is mentioned at the very top of \`cmm-notes\`. This was a bug in
the stack layout code that I have fixed.

Sp is generally stupid
----------------------

FIXED. Here is an optimized C-- sample from \`arr016.hs\`.

Compare with the old code:

You can see the up and down behavior here, but that's been fixed, so
ignore it for now. (Update the C--!) The unfixed problem is this (some
of the other problems were already addressed): we do an unnecessary
stack check on entry to this function. We should eliminate the stack
check (and by dead code analysis, the GC call) in such cases.

This pattern essentially happens for every function, since we always
assign incoming parameters to temporary variables before doing anything.

Heap and R1 aliasing
--------------------

FIXED. Values on the heap and values from R1 don't necessarily clobber
each other. allocDynClosure seems like a pretty safe bet they don't. But
is this true in general? ANSWER: Memory writes with Hp are always new
allocations, so they don't clobber anything.

Historical page
---------------

This page stores notes about progress of work on the "new" code
generator. This page is here for historical reasons. See
\[wiki:Commentary/Compiler/CodeGen Code Generator\] page for an
up-to-date description of the current code generator.

GHC's glorious new code generator
=================================

This page summarises work that Norman Ramsey, Simon M, Simon PJ, and
John Dias are doing on re-architecting GHC's back end. Here is the state
of play; see also \[wiki:Commentary/Compiler/Backends/LLVM work on the
LLVM back end\].

`*Buglist(code-genrelatedbugsthatwemaybeabletofix):`\
`*#1498(avoidredundantheapcheckonthefastpath)`\
`*#3552(unreachablecode)`\
`*#3462(afeature)`\
`*#2249`\
`*#2253`\
`*#2289`\
`*#7219(reinstateconstant-prop)`\
`*#7213(massivearray)`

`*(Sept12)Newcodegeneratorislive.Here'sthe[wiki:Commentary/Compiler/NewCodeGen/Cleanuppagelistingclean-uptasks]thatwecannowdo.`

`*SimonMaddeda[blog:newcg-updateBlogPost]aboutthenewcodegeneratorstatus`

`*Linkto`[`Krzysztof` `Wos's`
`project`](http://research.microsoft.com/en-us/um/people/simonpj/tmp/wos-diss-draft.pdf)`,inwhichhereportsgreatperformanceimprovementsbyturningtailrecursionintoloopsinC--.`

`*Normanaddeda[wiki:Commentary/Compiler/HooplPerformanceHooplperformancepage]`

`*EdwardYanghasawikipagethatdescribesshortcomingsofthecodegeneratedbythenewpipeline:[wiki:Commentary/Compiler/NewCodeGenStupidity]`

`` *JohnDhasbuiltacompletenewcodegenpipeline,runningalongsidetheoldone,enabledby`-fuse-new-codegen`.Itisdescribedhere:[wiki:Commentary/Compiler/NewCodeGenPipeline].Itusesanewrepresentationfor`Cmm`,mostlywith"Z"inthename.(Let'scalltheoriginalCmm`OldCmm`andthisnewone`CmmZ`.)IthasanewconversionSTG->CmmZ,andthensequenceofpassesthatoptimiseandcps-converttheCmm.Finally,itisconvertedbacktotheoldCmmsothatitcanflowtotheoldcodegenerators. ``

`*CompilingthroughthenewpipelinepassesalltestsandGHCisbootstrappable.`

`` *Separately,wehavedevelopedyetanother,andstillbetter,Cmmrepresentation,thesubjectofanupcomingICFP2010submission.ItusesphantomtypesandGADTstoaddveryusefulopen/closedinvariants.Thisisn'tinGHCatallyet.I'llcallit`CmmGADT`foreasyreference. ``

Generally we want to keep old and new pipelines working simultaneously,
so that we can switch only when we are sure the new stuff works. Next
steps in this grand plan are:

`*Checktheimpactoncompilationtimeofthenewroute.`

`` *Finalise`CmmGADT`andmakethenewpipelineuseit. ``

`` *MaketheCmmparser(whichparses`.cmm`filesfromtheRTS)produce`CmmGADT`,andpushthatdownthenewpipeline. ``

`*Implementthemanyrefactoringsandimprovementstothenewpipelinedescribedin`[`http://darcs.haskell.org/ghc/compiler/cmm/cmm-notes`](http://darcs.haskell.org/ghc/compiler/cmm/cmm-notes)`.Seealso:[wiki:Commentary/Compiler/NewCodeGenStupidity]`

`` *InsteadofconvertingnewCmmtooldCmm,makethedownstreamcodegeneratorsconsume`CmmGADT`,andconvertoldCmmto`CmmGADT`. ``

Longer term

`*Expandthecapabilityofthenewpipelinesothatitdoesnativecodegenerationtoo,andwecanultimatelydiscardtheexistingcodegenerators.Thedesignofthisstageishere:[wiki:Commentary/Compiler/IntegratedCodeGen]`

Workflow for the new code generator and Hoopl
---------------------------------------------

We have the following repositories:

`` *HEAD:themainGHCgitrepo.`http://darcs.haskell.org/ghc.git` ``

`*!HooplMaster:themasterHooplGitrepository.`\
``[`BR`](BR "wikilink")``**`Location`**`` :`http://ghc.cs.tufts.edu/hoopl/hoopl.git/` ``\
``[`BR`](BR "wikilink")`` (Physicallocation:`linux.cs.tufts.edu:/r/ghc/www/hoopl/hoopl.git`) ``

`*!HooplLag:aGitrepothatisguaranteedtoworkwithGHCHEAD.Itis`\
`notautomaticallyupdatedbypushesto!HooplMaster.Insteadamanual`\
`process(below)updatesit;hence"lag".`\
``[`BR`](BR "wikilink")``**`Location`**`` :`http://darcs.haskell.org/packages/hoopl.git`. ``

Normal GHC developers, who are uninterested in Hoopl, ignore all this.
If they download HEAD including all submodules, they'll get !HooplLag,
which is always guaranteed to work with HEAD.

Developers who work on GHC and also need to modify Hoopl need to ensure
their changes end up in both repositories.

`*Inyourhoopldirectoryinyourdevelopmenttree,add!HooplMasterasaremoteandupdateyourreferencethere.`\
`*Hackawayinthedevelopmenttree.`\
`*RecordHooplcommits.`\
`*Runvalidateinthedevelopmenttree`\
`*Pushthecommitsinhoopltothe!HooplMasterGitrepo`\
`` *Waitforthemirrorstoupdate(theimpatientcanrun`/srv/darcs/do_mirrors`ondarcs.haskell.org) ``\
`*Pushthecommitsinhoopltothe!HooplLagGitrepo(probablytheoriginremote)`

Status report April 2011
------------------------

Term

Old Code Generator (prior to GHC 7.8)
=====================================

Material below describes old code generator that was used up to GHC 7.6
and was retired in 2012. This page is not maintained and is here only
for historical purposes. See \[wiki:Commentary/Compiler/CodeGen Code
generator\] page for an up to date description of the current code
generator.

Storage manager representations
-------------------------------

See \[wiki:Commentary/Rts/Storage The Storage Manager\] for the
\[wiki:Commentary/Rts/Storage/Stack Layout of the stack\].

The code generator needs to know the layout of heap objects, because it
generates code that accesses and constructs those heap objects. The
runtime also needs to know about the layout of heap objects, because it
contains the garbage collector. How can we share the definition of
storage layout such that the code generator and the runtime both have
access to it, and so that we don't have to keep two independent
definitions in sync?

Currently we solve the problem this way:

`*CtypesrepresentingheapobjectsaredefinedintheCheaderfiles,seeforexample`[`GhcFile(includes/rts/storage/Closures.h)`](GhcFile(includes/rts/storage/Closures.h) "wikilink")`.`

`*ACprogram,`[`GhcFile(includes/mkDerivedConstants.c)`](GhcFile(includes/mkDerivedConstants.c) "wikilink")`` ,`#includes`theruntimeheaders. ``\
`` Thisprogramisbuiltandrunwhenyoutype`make`or`makeboot`in`includes/`.Itis ``\
`` runtwice:oncetogenerate`includes/DerivedConstants.h`,andagaintogenerate ``\
`` `includes/GHCConstants.h`. ``

`` *Thefile`DerivedConstants.h`containslotsof`#defines`likethis: ``

`` whichsaysthattheoffsettothewhy_blockedfieldofan`StgTSO`is18bytes.Thisfile ``\
`` is`#included`into ``[`GhcFile(includes/Cmm.h)`](GhcFile(includes/Cmm.h) "wikilink")`,sotheseoffestsareavailabletothe`\
`[wiki:Commentary/Rts/Cmmhand-written.cmmfiles].`

`` *Thefile`GHCConstants.h`containssimilardefinitions: ``

`` ThistimethedefinitionsareinHaskellsyntax,andthisfileis`#included`directlyinto ``\
``[`GhcFile(compiler/main/Constants.lhs)`](GhcFile(compiler/main/Constants.lhs) "wikilink")`.Thisisthewaythattheseoffsetsaremade`\
`availabletoGHC'scodegenerator.`

Generated Cmm Naming Convention
-------------------------------

See
[GhcFile(compiler/cmm/CLabel.hs)](GhcFile(compiler/cmm/CLabel.hs) "wikilink")

Labels generated by the code generator are of the form where is for
external names and for internal names. is one of the following:

`info::Infotable`\
`srt::Staticreferencetable`\
`srtd::Staticreferencetabledescriptor`\
`entry::Entrycode(function,closure)`\
`slow::Slowentrycode(ifany)`\
`ret::Directreturnaddress`\
`vtbl::Vectortable`\
``*`n`*`_alt::Casealternative(tag`*`n`*`)`\
`dflt::Defaultcasealternative`\
`btm::Largebitmapvector`\
`closure::Staticclosure`\
`con_entry::DynamicConstructorentrycode`\
`con_info::DynamicConstructorinfotable`\
`static_entry::StaticConstructorentrycode`\
`static_info::StaticConstructorinfotable`\
`sel_info::Selectorinfotable`\
`sel_entry::Selectorentrycode`\
`cc::Costcentre`\
`ccs::Costcentrestack`

Many of these distinctions are only for documentation reasons. For
example, \_ret is only distinguished from \_entry to make it easy to
tell whether a code fragment is a return point or a closure/function
entry.

Modules
-------

### 

Top level, only exports .

Called from for each module that needs to be converted from Stg to Cmm.

For each such module does three things:

`*``forthe`\
`*``forthe``(Theseareconstructorsnotconstructorcalls).`\
`*``forthemodule`

 generates several boilerplate initialization functions that:

`*regiserthemodule,`\
`*createsanHpctable,`\
`*setupitsprofilinginfo(``,codecoverageinfo``),and`\
`*callstheinitializationfunctionsofthemodulesitimports.`

If neither SCC profiling or HPC are used, then the initialization code
short circuits to return.

If the module has already been initialized, the initialization function
just returns.

The and modules get special treatment.

 is a small wrapper around which in turn disptaches to:

`*``for`\
`(thesearebindingsofconstructorapplicationsnotconstructorsthemselves)and`\
`*``for``.`

 and are located in and which are the primary modules called by .

### 

TODO

### 

TODO

### 

The monad that most of codeGen operates inside

`*Reader`\
`*State`\
`*(couldbeWriter?)`\
`*fork`\
`*flatten`

### 

Called by and .

Since everything in STG is an expression, almost everything branches off
from here.

This module exports only one function , which for the most part just
dispatches to other functions to handle each specific constructor in .

Here are the core functions that each constructor is disptached to
(though some may have little helper functions called in addition to the
core function):

`::Callsto``in`\
`::Callsto``in`\
`::`\
`Callsto``in`\
`and``in`\
`::`\
`Isabitmorecomplicatedseebelow.`\
`::Callsto``in`\
`::Callsto``in`\
`::`\
`Callsto``in``,butwithalittlebitofwrapping`\
`by``and``.`\
`::Callsto``in`\
`::Callsto``in`\
`::`\
`Doesnothaveacasebecauseitisonlyfor``'swork.`

Some of these cases call to functions defined in . This is because they
need a little bit of wrapping and processing before calling out to their
main worker function.

`::`\
`*For``callsoutto``in``.`\
`*For``callsoutto``.`\
`Inturn,``callsoutto``forselectorsandthunks,`\
`andcallsoutto``inthedefaultcase.`\
`Boththesearedefinedin``.`

`::`\
`*Wrapsacallto``with`\
`dependingonwhetheritiscalledonarecursiveoranon-recursivebinding.`\
`Inturn``wraps`\
`definedin``.`

 has a number of sub-cases.

`*`\
`*``ofa!TagToEnumOp`\
`*``thatisprimOpOutOfLine`\
`*``thatreturnsVoid`\
`*``thatreturnsasingleprimitive`\
`*``thatreturnsanunboxedtuple`\
`*``thatreturnsanenumerationtype`

(It appears that non-foreign-call, inline \[wiki:Commentary/PrimOps
PrimOps\] are not allowed to return complex data types (e.g. a |Maybe|),
but this fact needs to be verified.)

Each of these cases centers around one of these three core calls:

`*``in`\
`*``in`\
`*``in`

There is also a little bit of argument and return marshelling with the
following functions

`Argumentmarshelling::`\
```,`\
`Returnmarshelling::`\
```,``,`\
`Performingthereturn::`\
```,``,`\
```,`

In summary the modules that get called in order to handle a specific
expression case are:

#### Also called for top level bindings by 

`::for``andthe``partof`\
`::forthe``partof`

#### Core code generation

`::for``,``,and`\
`::for`\
`::for`\
`::for`

#### Profiling and Code coverage related

`::for`\
`::for`

#### Utility modules that happen to have the functions for code generation

`::for`\
`::for`

Note that the first two are the same modules that are called for top
level bindings by , and the last two are really utility modules, but
they happen to have the functions needed for those code generation
cases.

### Memory and Register Management

`::`\
`Modulefor``whichmapsvariablenames`\
`toallthevolitileorstablelocationswheretheyarestored`\
`(e.g.register,stackslot,computedfromotherexpressions,etc.)`\
`Providesthe``,``and``functions`\
`foradding,modifyingandlookingupbindings.`

`::`\
`Mostlyutilityfunctionsforallocatingandfreeingstackslots.`\
`Butalsohasthingsonsettingupupdateframes.`

`::`\
`Functionsforallocatingobjectsthatappearontheheapsuchasclosuresandconstructors.`\
`Alsoincludescodeforstackandheapchecksand``.`

### Function Calls and Parameter Passing

(Note: these will largely go away once CPS conversion is fully
implemented.)

`,``,``::`\
`Handledifferenttypesofcalls.`\
`::`\
`Usebytheothersinthiscategorytodeterminelivenessand`\
`toselectinwhatregistersandstacklocationsargumentsandreturn`\
`valuesgetstored.`

### Misc utilities

`::`\
`Utilityfunctionsformakingbitmaps(e.g.``withtype``)`\
`::`\
`Storesinfoaboutclosuresandbindings.`\
`Includesinformationaboutmemorylayout,howtocallabinding(``)`\
`andinformationusedtobuildtheinfotable(``).`\
`::`\
`Storagemanagerrepresentationofclosures.`\
`Partof!ClosureInfobutkeptseparateto"keepnhchappy."`\
`::TODO`\
`::TODO`

### Special runtime support

`::Ticky-tickyprofiling`\
`::Cost-centreprofiling`\
`::SupportfortheHaskellProgramCoverage(hpc)toolkit,insideGHC.`\
`::`\
`Codegenerationfor!GranSim(GRAN)andparallel(PAR).`\
`Allthefunctionsaredeadstubsexcept``and``.`

Ordering the Core-to-Core optimisation passes
=============================================

This page has notes about the ordering of optimisation phases. An
overview of the whole Core-to-Core optimisation pipeline can be found
\[wiki:Commentary/Compiler/Core2CorePipeline here\].

**NOTE:** This is old documentation and may not be very relevant any
more!

This ordering obeys all the constraints except (5)
--------------------------------------------------

`*fulllaziness`\
`*simplifywithfoldr/build`\
`*float-in`\
`*simplify`\
`*strictness`\
`*float-in`

\[check FFT2 still gets benefits with this ordering\]

Constraints
-----------

### 1. float-in before strictness

Reason: floating inwards moves definitions inwards to a site at which
the binding might well be strict. The strictness analyser will do a
better job of the latter than the former.

### 2. Don't simplify between float-in and strictness

...unless you disable float-let-out-of-let, otherwise the simiplifier's
local floating might undo some useful floating-in. This is a bad move,
because now y isn't strict. In the pre-float case, the binding for y is
strict. Mind you, this isn't a very common case, and it's easy to
disable float-let-from-let.

### 3. Want full-laziness before foldr/build

Reason: Give priority to sharing rather than deforestation. In the
post-full-laziness case, xs is shared between all applications of the
function. If we did foldr/build first, we'd have got and now we can't
share xs.

### 4. Want strictness after foldr/build

Reason: foldr/build makes new function definitions which can benefit
from strictness analysis. Here we clearly want to get strictness
analysis on g.

### 5. Want full laziness after strictness

Reason: absence may allow something to be floated out which would not
otherwise be. TOO BAD. This doesn't look a common case to me.

### 6. Want float-in after foldr/build

Reason: Desugaring list comprehensions + foldr/build gives rise to new
float-in opportunities. Now v could usefully be floated into the second
branch.

### 7. Want simplify after float-inwards

(Occurred in the prelude, compiling \`ITup2.hs\`, function
\`dfun.Ord.(\*,\*)\`) This is due to the following (that happens with
dictionaries): floating inwards will push the definition of a1 into m1
(supposing it is only used there): if we do strictness analysis now we
will not get a worker-wrapper for m1, because of the "let a1 ..."
(notice that a1 is not strict in its body).

Not having this worker wrapper might be very bad, because it might mean
that we will have to rebox arguments to m1 if they are already unboxed,
generating extra allocations, as occurs with m2 (cc) above.

To solve this problem we have decided to run the simplifier after
float-inwards, so that lets whose body is a HNF are floated out, undoing
the float-inwards transformation in these cases. We are then back to the
original code, which would have a worker-wrapper for m1 after strictness
analysis and would avoid the extra let in m2.

What we lose in this case are the opportunities for case-floating that
could be presented if, for example, a1 would indeed be demanded (strict)
after the floating inwards.

The only way of having the best of both is if we have the worker/wrapper
pass explicitly called, and then we could do with

`*float-in`\
`*strictnessanalysis`\
`*simplify`\
`*strictnessanalysis`\
`*worker-wrappergeneration`

as we would

`*beabletodetectthestrictnessofm1afterthefirstcalltothestrictnessanalyser,andexploititwiththesimplifier(incaseitwasstrict).`\
`*afterthecalltothesimplifier(ifm1wasnotdemanded)itwouldbefloatedoutjustlikewecurrentlydo,beforestricnessanalysisIIandworker/wrapperisation.`

The reason to not do worker/wrapperisation twice is to avoid generating
wrappers for wrappers which could happen.

### 8. If full laziness is ever done after strictness

...remember to switch off demandedness flags on floated bindings! This
isn't done at the moment.

### 9. Ignore-inline-pragmas flag for final simplification

\[Occurred in the prelude, compiling ITup2.hs, function
dfun.Ord.(\*,\*)\] Sometimes (e.g. in dictionary methods) we generate
worker/wrappers for functions but the wrappers are never inlined. In
dictionaries we often have and if we create worker/wrappers for
f1,...,fn the wrappers will not be inlined anywhere, and we will have
ended up with extra closures (one for the worker and one for the
wrapper) and extra function calls, as when we access the dictionary we
will be acessing the wrapper, which will call the worker. The simplifier
never inlines workers into wrappers, as the wrappers themselves have
INLINE pragmas attached to them (so that they are always inlined, and we
do not know in advance how many times they will be inlined).

To solve this problem, in the last call to the simplifier we will ignore
these inline pragmas and handle the workers and the wrappers as normal
definitions. This will allow a worker to be inlined into the wrapper if
it satisfies all the criteria for inlining (e.g. it is the only
occurrence of the worker etc.).

### 10. Run Float Inwards once more after strictness-simplify

\[Occurred in the prelude, compiling \`IInt.hs\`, function
\`const.Int.index.wrk\`\] When workers are generated after strictness
analysis (worker/wrapper), we generate them with "reboxing" lets, that
simply reboxes the unboxed arguments, as it may be the case that the
worker will need the original boxed value: in this case the simplifier
will remove the binding for y as it is not used (we expected this to
happen very often, but we do not know how many "reboxers" are eventually
removed and how many are kept), and will keep the binding for x. But
notice that x is only used in \*one\* of the branches in the case, but
is always being allocated! The floating inwards pass would push its
definition into the True branch. A similar benefit occurs if it is only
used inside a let definition. These are basically the advantages of
floating inwards, but they are only exposed after the
S.A./worker-wrapperisation of the code! As we also have reasons to float
inwards before S.A. we have to run it twice.

Overall organisation of GHC
===========================

Start at the [GHC home page](http://haskell.org/ghc). The most important
links are in the left-hand column:

`*`[`Documentation`](http://haskell.org/haskellwiki/GHC)`.Thisisthe`*`user`*`documentation,aimedatpeoplewhouseGHC,butdon'tcarehowitworks.It'sontheHaskellWiki(poweredbyMediaWiki),andwestronglyencouragepeopletoeditandimproveit.`

`*`[`Developers`](http://hackage.haskell.org/trac/ghc)`.Thislinktakesyoutothehomepagefor`*`developers`*`;thatis,peopleinterestedinhackingonGHCitself(i.e.you).It'saWikitoo,butpoweredbyTrac,andincludesbug-trackingetc.ThereisabigsectioncalledDeveloperDocumentation:`**`please`
`help` `us` `to` `improve` `it`**`.`

`*`[`Download`](http://www.haskell.org/ghc/download.html)`.Atanymoment,GHChasa`**`STABLE`
`branch`**`andthe`**`HEAD`**`,bothofwhichyoucandownloadfromthispage.`[`BR`](BR "wikilink")[`BR`](BR "wikilink")\
`*TheSTABLEbranchisthecurrentreleasedversion.Ithasanevenversionnumber(e.g.6.4,6.6),withanextrasuffixforpatch-levelrelease(e.g.6.4.2).Patch-levelrelesesfixbugs;theydonotchangeanyAPIs.`[`BR`](BR "wikilink")[`BR`](BR "wikilink")\
`*TheHEADissimplythelatest,greatestversionthatweareworkingon;itmaybebrokenonanygivenday,althoughyouareencouragednottobreakitgratuitiously.TheHEADhasanoddversionnumbers(e.g6.5,6.7).EverynightwebuildtheHEAD,anddumptheresultonthedownloadsiteunder"Developmentsnapshots",withaversionnumberthatencodesthedate(e..g6.5.20060831).`

`Averyusefullinkonthedownloadpageisthe`[`documentation`
`for` `the`
`HEAD`](http://www.haskell.org/ghc/dist/current/docs/)`(underDevelopmentsnapshots).UsefulbecausetypesettingthedocumentationusesDocBook,whicheasytoinstalloneveryplatform.`

GHC source code
===============

GHC's source code is several Darcs repositories. The important ones are:

<http://darcs.haskell.org/ghc>:: All of GHC: compiler, run-time system,
support utilities.

<http://darcs.hasekll.org/packages/pkg>:: A library package *pkg*. A
certain number of packages are essential to build GHC. They are listed
in and currently comprise: , , , , , , , , , , , .

<http://darcs.haskell.org/testsuite>:: GHC's test suite.

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
``<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />\
``<meta http-equiv="Content-Style-Type" content="text/css" />\
``<meta name="generator" content="pandoc" />\
``

<title>
</title>
<style type="text/css">
code{white-space: pre;}

</style>
</head>
<body>
<div id="TOC">
-   <a href="#the-ghc-commentary-checking-types">The GHC Commentary:
    Checking Types</a>
    -   <a href="#the-overall-flow-of-things">The Overall Flow of
        Things</a>
        -   <a href="#entry-points-into-the-type-checker">Entry Points
            Into the Type Checker</a>
        -   <a href="#renaming-and-type-checking-a-module">Renaming and
            Type Checking a Module</a>
    -   <a href="#type-checking-a-declaration-group">Type Checking a
        Declaration Group</a>
    -   <a href="#type-checking-type-and-class-declarations">Type
        checking Type and Class Declarations</a>
    -   <a href="#more-details">More Details</a>
        -   <a href="#types-variables-and-zonking">Types Variables and
            Zonking</a>
        -   <a href="#type-representation">Type Representation</a>
        -   <a href="#type-checking-environment">Type Checking
            Environment</a>
        -   <a href="#expressions">Expressions</a>
        -   <a href="#handling-of-dictionaries-and-method-instances">Handling
            of Dictionaries and Method Instances</a>
    -   <a href="#connection-with-ghcs-constraint-solver">Connection
        with GHC's Constraint Solver</a>
    -   <a href="#generating-evidence">Generating Evidence</a>
    -   <a href="#the-solver">The Solver</a>
        -   <a href="#given-constraints">Given Constraints</a>
        -   <a href="#derived-constraints">Derived Constraints</a>
        -   <a href="#wanted-constraints">Wanted Constraints</a>

</div>
<h1 id="the-ghc-commentary-checking-types">
The GHC Commentary: Checking Types

</h1>
Probably the most important phase in the frontend is the type checker,
which is located at
<a href="GhcFile(compiler/typecheck/)" class="uri" title="wikilink">GhcFile(compiler/typecheck/)</a>.
GHC type checks programs in their original Haskell form before the
desugarer converts them into Core code. This complicates the type
checker as it has to handle the much more verbose Haskell AST, but it
improves error messages, as those message are based on the same
structure that the user sees.

GHC defines the abstract syntax of Haskell programs in
<a href="GhcModule(compiler/hsSyn/HsSyn.lhs)" class="uri" title="wikilink">GhcModule(compiler/hsSyn/HsSyn.lhs)</a>
using a structure that abstracts over the concrete representation of
bound occurences of identifiers and patterns. The module
<a href="GhcModule(compiler/typecheck/TcHsSyn.lhs)" class="uri" title="wikilink">GhcModule(compiler/typecheck/TcHsSyn.lhs)</a>
defines a number of helper function required by the type checker. Note
that the type
<a href="GhcModule(compiler/typecheck/TcRnTypes.lhs)" class="uri" title="wikilink">GhcModule(compiler/typecheck/TcRnTypes.lhs)</a>.\`TcId\`
used to represent identifiers in some signatures during type checking
is, in fact, nothing but a synonym for a
\[wiki:Commentary/Compiler/EntityTypes\#Typevariablesandtermvariables
plain Id\].

It is also noteworthy, that the representations of types changes during
type checking from \`HsType\` to \`TypeRep.Type\`. The latter is a
\[wiki:Commentary/Compiler/TypeType hybrid type\] representation that is
used to type Core, but still contains sufficient information to recover
source types. In particular, the type checker maintains and compares
types in their \`Type\` form.

<h2 id="the-overall-flow-of-things">
The Overall Flow of Things

</h2>
<code>\*

Updates
=======

Source files: $$\[GhcFile(rts/Updates.h)$$\\\],
$$\[GhcFile(rts/Updates.cmm)$$\\\]

----CategoryStub

.. contents::

`:depth:3`

..

Unique
------

\`\`Unique\`\`\\ s provide a fast comparison mechanism for more complex
things. Every \`\`RdrName\`\`, \`\`Name\`\`, \`\`Var\`\`, \`\`TyCon\`\`,
\`\`TyVar\`\`, etc. has a \`\`Unique\`\`. When these more complex
structures are collected (in \`\`UniqFM\`\`\\ s or other types of
collection), their \`\`Unique\`\` typically provides the key by which
the collection is indexed.

+----------------------------+ | == Current design == |
+----------------------------+ | A \`\`Unique\`\` consists of | | the
*domain* of the | | thing it identifies and a | | unique integer value |
| 'within' that domain. The | | two are packed into a | | single
\`\`Int\#\`\`, with the | | *domain* being the top 8 | | bits. |
+----------------------------+ | The domain is never | | inspected (SLPJ
believes). | | The sole reason for its | | existence is to provide a | |
number of different ranges | | of \`\`Unique\`\` values that | | are
guaranteed not to | | conflict. | +----------------------------+ | ===
Lifetime | +----------------------------+ | The lifetime of a | |
\`\`Unique\`\` is a single | | invocation of GHC, i.e. | | they must not
'leak' to | | compiler output, the | | reason being that | |
\`\`Unique\`\`\\ s may be | | generated/assigned | |
non-deterministically. | | When compiler output is | |
non-deterministic, it | | becomes significantly | | harder to, for
example, | | \[wiki:Commentary/Compiler/ | | RecompilationAvoidance | |
avoid recompilation\]. | | Uniques do not get | | serialised into .hi
files, | | for example. | +----------------------------+ | Note, that
"one compiler | | invocation" is not the | | same as the compilation of
| | a single \`\`Module\`\`. | | Invocations such as | | \`\`ghc
--make\`\` or | | \`\`ghc --interactive\`\` give | | rise to longer
invocation | | life-times. | +----------------------------+ | This is
also the reasons | | why \`\`OccName\`\`\\ s are | | *not* ordered based
on | | the \`\`Unique\`\`\\ s of their | | underlying | |
\`\`FastString\`\`\\ s, but | | rather | | *lexicographically* (see | |
[ | pes/OccName.lhs)](GhcFile(compiler/basicTy "wikilink") | | for
details). &gt; &gt; | | **SLPJ:** I am far from | | sure that the Ord
instance | | for \`\`OccName\`\` is ever | | used, so this remark is | |
probably misleading. Try | | deleting it and see where | | it is used
(if at all). &gt; | | **PKFH:** At least | | \`\`Name\`\` and
\`\`RdrName\`\` | | (partially) define their | | own \`\`Ord\`\`
instances in | | terms of the instance of | | \`\`OccName\`\`. Maybe
these | | \`\`Ord\`\` instances are also | | redundant, but for now it |
| seems wise to keep them | | in. When everything has | | \`\`Data\`\`
instances (after | | this and many other | | redesigns), I'm sure it | |
will be easier to find | | such dependency relations. |
+----------------------------+ | === Known-key things === |
+----------------------------+ | A hundred or two library | | entities
(types, classes, | | functions) are so-called | | "known-key things".
See | | \[wiki:Commentary/Compiler/ | | WiredIn | | this page\]. A
known-key | | thing has a fixed | | \`\`Unique\`\` that is fixed | |
when the compiler is | | built, and thus lives | | across all
invocations of | | that compiler. These | | known-key \`\`Unique\`\`\\ s
| | *are* written into .hi | | files. But that's ok | | because they are
fully | | deterministic and never | | change. |
+----------------------------+ | &gt; **PKFH** That's fine | | then; we
also know for | | sure these things fit in | | the 30 bits used in the |
| \`\`hi\`\`-files. I'll comment | | appropriately. |
+----------------------------+ | === Interface files === |
+----------------------------+ | Entities in a interface | | file (.hi
file) are, for | | the most part, stored in a | | symbol table, and
referred | | to (from elsewhere in the | | same interface file) by an |
| index into that table. | | Here are the details from | | [ |
inIface.lhs)](GhcFile(compiler/iface/B "wikilink"): | |

------------------------------------------------------------------------

Redesign (2014)
---------------

=== TL;DR The redesign is to accomplish the following: \\\* Allow
derivation of type class instances for \`\`Unique\`\` \\\* Restore
invariants from the original design; hide representation details \\\*
Eliminate violations of invariants and design-violations in other places
of the compiler (e.g. \`\`Unique\`\`\\ s shouldn't be written to
\`\`hi\`\`-files, but are). &gt; &gt; **SLPJ** I don't think this is a
design violation; see above. Do you have any other examples in mind?
&gt; **PKFH** Not really of design-violations (and no other
compiler-output stuff) other than the invariants mentioned above it,
just yet. The key point, though, is that there are a lot of comments in
\`\`Unique\`\` about not exporting things so that we know X, Y and Z,
but then those things *are* exported, so we don't know them to be true.
Case in point is the export of \`\`mkUnique\`\`, but also
\`\`mkUniqueGrimily\`\`. The latter has a comment 'only for
\`\`UniqSupply\`\`' but is also used in other places (like Template
Haskell). One redesign is to put this restriction in the name, so there
still is the facility offered by \`\`mkUniqueGrimily\`\`, but now it's
called \`\`mkUniqueOnlyForUniqSupply\`\` (and
\`\`mkUniqueOnlyForTemplateHaskell\`\`), the ugliness of which should
help, over time, to get rid of them.

=== Longer

In an attempt to give more of GHC's innards well-behaved instances of
\`\`Typeable\`\`, \`\`Data\`\`, \`\`Foldable\`\`, \`\`Traversable\`\`,
etc. the implementation of \`\`Unique\`\`\\ s was a bit of a sore spot.
They were implemented (20+ years earlier) using custom boxing, viz.
making automatic derivation of such type class instances hard. There was
already a comment asking why it wasn't simply a \`\`newtype\`\` around a
normal (boxed) \`\`Int\`\`. Independently, there was some discussion on
the mailinglists about the use of (signed) \`\`Int\`\`\\ s in places
where \`\`Word\`\`\\ s would be more appropriate. Further inspection of
the \`\`Unique\`\` implementation made clear that a lot of invariants
mentioned in comments had been violated by incremental edits. This is
discussed in more detail below, but these things together (the desire
for automatic derivation and the restoration of some important
invariants) motivated a moderate redesign.

=== Status Quo (pre redesign)

A \`\`Unique\`\` has a domain (\`\`TyCon\`\`, \`\`DataCon\`\`,
\`\`PrelName\`\`, \`\`Builtin\`\`, etc.) that was codified by a
character. The remainder of the \`\`Unique\`\` was an integer that
should be unique for said domain. This **was** once guaranteed through
the export list of
[GhcFile(compiler/basicTypes/Unique.lhs)](GhcFile(compiler/basicTypes/Unique.lhs) "wikilink"),
where direct access to the domain-character was hidden, i.e. were not
exported. This should have guaranteed that every domain was assigned its
own unique character, because only in
[GhcFile(compiler/basicTypes/Unique.lhs)](GhcFile(compiler/basicTypes/Unique.lhs) "wikilink")
could those \`\`Char\`\`\\ s be assigned. However, through this
separation of concerns leaked out to
[GhcFile(compiler/basicTypes/UniqSupply.lhs)](GhcFile(compiler/basicTypes/UniqSupply.lhs) "wikilink"),
because its \`\`Int\`\` argument is the *entire* \`\`Unique\`\` and not
just the integer part 'under' the domain character. &gt; &gt; **SLPJ**
OK, but to eliminate \`\`mkUniqueGrimily\`\` you need to examine the
calls, decide how to do it better, and document the new design. &gt;
**PKFH** See above; the solution for now is
\`\`mkUniqueOnlyForUniqSupply\`\`. A separate patch will deal with
trying to refactor/redesign \`\`UniqSupply\`\` if this is necessary.

The function \`\`mkSplitUniqSupply\`\` made the domain-character
accessible to all the other modules, by having a wholly separate
implementation of the functionality of \`\`mkUnique\`\`.

Where the intention was still to have a clean interface, the (would-be)
hidden \`\`mkUnique\`\` is only called by functions defined in the
\`\`Unique\`\` module with the corresponding character, e.g.

=== New plan

In the new design, the domains are explicitly encoded in a sum-type
\`\`UniqueDomain\`\`. At the very least, this should help make the code
a little more self-documenting *and* prevent accidental overlap in the
choice of bits to identify the domain. Since the purpose of
\`\`Unique\`\`\\ s is to provide *fast* comparison for different types
of things, the redesign should remain performance concious. With this in
mind, keeping the \`\`UniqueDomain\`\` and the integer-part explicitly
in the type seems unwise, but by choosing we win the ability to
automatically derive things and should also be able to test how far
optimisation has come in the past 20+ years; does default boxing with
\`\`newtype\`\`-style wrapping have (nearly) the same performance as
manual unboxing? This should follow from the tests.

The encoding is kept the same, i.e. the \`\`Word\`\` is still built up
with the domain encoded in the most significant bits and the
integer-part in the remaining bits. However, instead encoding the domain
as a \`\`Char\`\` in the (internal *and* external interface), we now
create an ADT (sum-type) that encodes the domain. This has two
advantages. First, it prevents people from picking domain-tags ad hoc an
possibly overlapping. Second, encoding in the \`\`Word\`\` does not rely
on the assumption that the domain requires and/or fits in 8 bits. Since
Haskell \`\`Char\`\`\\ s are unicode, the 8-bit assumption is wrong for
the old design. In other words, the above examples are changed to:

Ideal world scenario, the entire external interface would be: and the
instances for \`\`Eq\`\`, \`\`Ord\`\`, \`\`Data\`\`, etc. For now,
though, it will also have

``**`SLPJ`**``` Iagreethata``newtype``arounda``Word``is ```\
``` betterthana``data``typearound``Int#``.Thatisasmall, ```\
`simplechange.ButIthinkyouplantodomorethanthis,and`\
`that"more"isnotdocumentedhere.E.g.whatisthenewAPIto`\
``` ``Unique``? ```**`PKFH`**`Added.Seeabove.`

[PageOutline](PageOutline "wikilink")

The data type  and its friends
=============================

GHC compiles a typed programming language, and GHC's intermediate
language is explicitly typed. So the data type that GHC uses to
represent types is of central importance.

The single data type is used to represent \\\* Types (possibly of higher
kind); e.g. \`\`\[Int\]\`\`, \`\`Maybe\`\` \\\* Kinds (which classify
types and coercions); e.g. \`\`(\* -&gt; \*)\`\`, \`\`T :=: \[Int\]\`\`.
See \[wiki:Commentary/Compiler/Kinds\] \\\* Sorts (which classify
types); e.g. \`\`TY\`\`, \`\`CO\`\`

GHC's use of \[wiki:Commentary/Compiler/FC coercions and equality
constraints\] is important enough to deserve its own page.

The module exposes the representation because a few other modules (, , ,
etc) work directly on its representation. However, you should not
lightly pattern-match on ; it is meant to be an abstract type. Instead,
try to use functions defined by , etc.

Views of types
--------------

Even when considering only types (not kinds, sorts, coercions) you need
to know that GHC uses a *single* data type for types. You can look at
the same type in different ways:

- The "typechecker view" regards the type as a Haskell type, complete

`withimplicitparameters,classconstraints,andthelike.For`\
`example:``Functionsin`\
``` ``TcType``takethisviewoftypes;e.g.``tcSplitSigmaTy``splitsup ```\
`atypeintoitsforall'dtypevariables,itsconstraints,andthe`\
`rest.`

- The "core view" regards the type as a Core-language type, where class

`andimplicitparameterconstraintsaretreatedasfunctionarguments:`\
````` Functionsin``Type``take ```\
`thisview.`

The data type \`\`Type\`\` represents type synonym applications in
un-expanded form. E.g. Here \`\`f\`\`'s type doesn't look like a
function type, but it really is. The function \`\`Type.coreView :: Type
-&gt; Maybe Type\`\` takes a type and, if it's a type synonym
application, it expands the synonym and returns \`\`Just
<expanded-type>\`\`. Otherwise it returns \`\`Nothing\`\`.

Now, other functions use \`\`coreView\`\` to expand where necessary,
thus: Notice the first line, which uses the view, and recurses when the
view 'fires'. Since \`\`coreView\`\` is non-recursive, GHC will inline
it, and the optimiser will ultimately produce something like:

The representation of 
----------------------

Here, then is the representation of types (see
[GhcFile(compiler/types/TypeRep.hs)](GhcFile(compiler/types/TypeRep.hs) "wikilink")
for more details):

Invariant: if the head of a type application is a , GHC *always* uses
the constructor, not . This invariant is maintained internally by 'smart
constructors'. A similar invariant applies to ; is never used with an
arrow type.

Type variables are represented by the \`\`TyVar\`\` constructor of the
\[wiki:Commentary/Compiler/EntityTypes data type Var\].

Overloaded types
----------------

In Haskell we write but in Core the \`\`=&gt;\`\` is represented by an
ordinary \`\`FunTy\`\`. So f's type looks like this: Nevertheless, we
can tell when a function argument is actually a predicate (and hence
should be displayed with \`\`=&gt;\`\`, etc), using The various forms of
predicate can be extracted thus: These functions are defined in module
\`\`Type\`\`.

Classifying types
-----------------

GHC uses the following nomenclature for types:

**Unboxed**:: A type is unboxed iff its representation is other than a
pointer. Unboxed types are also unlifted.

**Lifted**:: A type is lifted iff it has bottom as an element. Closures
always have lifted types: i.e. any let-bound identifier in Core must
have a lifted type. Operationally, a lifted object is one that can be
entered. Only lifted types may be unified with a type variable.

**Data**:: A type declared with ****. Also boxed tuples.

**Algebraic**:: An algebraic data type is a data type with one or more
constructors, whether declared with or . An algebraic type is one that
can be deconstructed with a case expression. "Algebraic" is **NOT** the
same as "lifted", because unboxed (and thus unlifted) tuples count as
"algebraic".

**Primitive**:: a type is primitive iff it is a built-in type that can't
be expressed in Haskell.

Currently, all primitive types are unlifted, but that's not necessarily
the case. (E.g. Int could be primitive.)

Some primitive types are unboxed, such as Int\#, whereas some are boxed
but unlifted (such as \`\`ByteArray\#\`\`). The only primitive types
that we classify as algebraic are the unboxed tuples.

Examples of type classifications:

\\|\\| \\|\\| **Primitive** \\|\\| **Boxed** \\|\\| **Lifted** \\|\\|
**Algebraic** \\|\\| \\|\\| \`\`Int\#\`\` \\|\\| Yes \\|\\| No \\|\\| No
\\|\\| No \\|\\| \\|\\| \`\`ByteArray\#\`\` \\|\\| Yes \\|\\| Yes \\|\\|
No \\|\\| No \\|\\| \\|\\| \`\`(\# a, b \#)\`\` \\|\\| Yes \\|\\| No
\\|\\| No \\|\\| Yes \\|\\| \\|\\| \`\`( a, b )\`\` \\|\\| No \\|\\| Yes
\\|\\| Yes \\|\\| Yes \\|\\| \\|\\| \`\`\[a\]\`\` \\|\\| No \\|\\| Yes
\\|\\| Yes \\|\\| Yes \\|\\|

Package Compatibility
=====================

In GHC 6.8.1 we reorganised some of the contents of the packages we ship
with GHC, see \#710. The idea was to lessen the problem caused by the
base package being essentially static between GHC major releases. By
separating modules out of base and putting them into separate packages,
it is possible to updgrade these modules independently of GHC.

The reorganisations unfortunately exposed some problems with our package
infrastructure, in particular most packages that compiled with 6.6 do
not compile with 6.8.1 because they don't depend on the new packages.
Some instructions for upgrading packages are here: [Upgrading
packages](http://haskell.org/haskellwiki/Upgrading_packages).

We anticipated the problem to some extent, adding "configurations" to
Cabal to make it possible to write conditional package specifications
that work with multiple sets of dependencies. We are still left with the
problem that the \`.cabal\` files for all packages need to be updated
for GHC 6.8.1. This seems like the wrong way around: the change we made
to a few packages has to be propagated everywhere, when there should be
a way to confine it locally, at least for the purposes of continued
compatibility with existing source code. In many cases, the underlying
APIs are still available, just from a different place. (in general this
may not be true - modifications to packages may make changes to APIs
which require real changes to dependent packages).

Some of the problems that contributed to this situation can be
addressed. We wrote the [Package Versioning
Policy](http://haskell.org/haskellwiki/Package_versioning_policy) so
that packages can start using versions that reflect API changes, and so
that dependencies can start being precise about which dependencies they
work with. If we follow these guidelines, then

`*failureswillbemorepredictable`\
`*failureswillbemoreinformative`

because dependencies and API changes are better documented. However, we
have no fewer failures than before, in fact we have more because
packages cannot now "accidentally work" by specifying loose dependency
ranges.

So the big question is, what changes do we need to make in the future to
either prevent this happening, or to reduce the pain when it does
happen? Below are collected various proposals. If the proposals get too
long we can separate them out into new pages.

1. Don't reorganise packages
----------------------------

We could do this, but that just hides the problem and we're still left
with a monolithic base package. We still have to contend with API
changes causing breakage.

2. Provide older version(s) of base with a new GHC release
----------------------------------------------------------

We could fork the base package for each new release, and keep compiling
the old one(s). Unfortunately we would then have to compile every other
package two (or more) times, once against each version of base. And if
we were to give the same treatment to any other library, we end up with
exponential blowup in the number of copies.

The GHC build gets slower, and the testing surface increases for each
release.

Furthermore, the package database cannot currently understand multiple
packages compiled against different versions of dependencies. One
workaround is to have multiple package databases, but that's not too
convenient.

4. Allow packages to re-export modules
--------------------------------------

Packages currently cannot re-export modules from other packages. Well,
that's not strictly true, it is possible to do this but it currently
requires an extra package and two stub modules per module to be
re-exported (see
[7](http://www.haskell.org/pipermail/haskell-cafe/2007-October/033141.html)).

This could be made easier. Suppose you could write this:

to construct a module called \`Data.Maybe\` that re-exports the module
\`Data.Maybe\` from package \`base-2.0\`. This extension to the import
syntax was proposed in PackageImports.

Using this extension, we can construct packages that re-export modules
using only one stub module per re-exported module, and Cabal could
generate the stubs for us given a suitable addition to the \`.cabal\`
file syntax.

Package re-exports are useful for

`*Constructingpackagesthatarebackwards-compatiblewitholdpackagesbyre-exportingpartsofthenewAPI.`\
`*Providingasinglewrapperforchoosingoneofseveralunderlyingproviders`

4.1 Provide backwards-compatible versions of base
-------------------------------------------------

So using re-exports we can construct a backwards-compatible version of
base (\`base-2.0\` that re-exports \`base-3.0\` and the other packages
that were split from it). We can do this for other packages that have
changed, too. This is good because:

`*Codeissharedbetweenthetwoversionsofthepackage`\
`*Multipleversionsofeachpackagecancoexistinthesameprogrameasily(unlikeinproposal2)`

However, this approach runs into problems when types or classes, rather
than just functions, change. Suppose in \`base-3.0\` we changed a type
somewhere; for example, we remove a constructor from the \`Exception\`
type. Now \`base-2.0\` has to provide the old \`Exception\` type. It can
do this, but the \`Exception\` type in \`base-2.0\` is now incompatible
with the \`Exception\` type in \`base-3.0\`, so every function that
refers to \`Exception\` must be copied into \`base-2.0\`. At this point
we start to need to recompile other packages against \`base-2.0\` too,
and before long we're back in the state of proposal (2) above.

This approach therefore doesn't scale to API changes that include types
and classes, but it can cope with changes to functions only.

4.2 Rename base, and provide a compatibility wrapper
----------------------------------------------------

This requires the re-exporting functionality described above. When
splitting base, we would rename the base package, creating several new
packages. e.g. \`base-3.0\` would be replaced by \`newbase-1.0\`,
\`concurrent-1.0\`, \`generics-1.0\`, etc. Additionally, we would
provide a wrapper called \`base-4.0\` that re-exports all of the new
packages.

Advantages:

`*Updatestoexistingpackagesaremucheasier(noconfigurationsrequired)`\
`*Doesn'tfallintothetrapoftryingtomaintainacompletelybackwards-compatibleversionoftheoldAPI,asin4.1`

Disadvantages:

`*AllpackagesstillbreakwhenthebaseAPIchanges(iftheyareusingprecisedependenciesonbase,whichtheyshouldbe)`\
`` *Backwardscompatibilitycruftintheformofthe`base`wrapperwillbehardtogetridof;there'sno ``\
`incentiveforpackagestostopusingit.Perhapsweneedadeprecationmarkeronpackages.`\
`*Eachtimewesplitbasewehavetoinventanewnameforit,andweaccumulateanewcompatibilitywrapper`\
`fortheoldone.`

4.3 Don't rename base
---------------------

This is a slight variation on 4.2, in which instead of renaming \`base\`
to \`newbase\`, we simply provide two versions of \`base\` after the
split. Take the example of splitting \`base-3.0\` into \`base +
concurrent + generics\` again:

`` *`base-4.0`istheremainingcontentsof`base-3.0`afterthesplit ``\
`` *`base-3.1`isacompatibilitywrapper,re-exporting`base-4.0+concurrent-1.0+generics-1.0`. ``

The idea is that all existing packages that worked with \`base-3.0\`
will have or similar. To make these work after the split, all that is
needed is to modify the upper bound: which is better than requiring a
conditional dependency, as was the case with the \`base-3.0\` split. In
due course, these packages can be updated to use the new \`base-4.0\`.

Advantages: the same as 4.2, plus there's no need to rename \`base\` for
each split. Disadvantages: multiple versions of \`base\` could get
confusing. The upgrade path is still not completely smooth (existing
packages all need to be modified manually).

5. Do some kind of provides/requires interface in Cabal
-------------------------------------------------------

Currently, Cabal's idea of API is asymmetric and very coarse: the client
depends on a package by name and version only, the provider implements a
single package name and version by exposing a list of modules. That has
several disadvantages:

`*Cabalcannotensurebuildsafety:mosterrorswillnotshowupbeforebuild-time(contrastthatwithHaskell'susualmodelofstatictypesafety).`\
`*Cabalhasnoideawhatadependencyconsistsofunlessitisinstalled.evenifitisinstalled,itonlyknowsthemodulesexposed.TheactualAPImightbedefinedinHaddockcomments,butisnotformallyspecifiedorverified.`

### 5.1 Make API specifications more symmetric

Just as a provider lists the modules it exposes, clients should list the
modules they import (this field should be inferred by a 'ghc -M'-style
dependency analysis). Advantages:

`*Cabalwouldhaveanideawhichpartsofapackageaclientdependsoninsteadofdefaultingto"everyclientneedseverything"(example:clientsusingonlypartsoftheoldbasenotsplitoffshouldbehappywiththenewbase)`\
`*Cabalwouldhaveanideawhatamissingdependencywasmeanttoprovide(example:clientsusingpartsoftheoldbasethathavebeensplitoffcouldbeofferedthesplit-offpackagesasalternativeprovidersofthemodulesimported)`

### 5.2 Make API specifications explicit

Currently, the name and version of a package are synonymous with its
API. That is like modules depending on concrete data type
representations instead of abstract types. It should not really matter
that the functionality needed by package P was only available in package
Q-2.3.42 at the time P was written. What should matter is which parts of
Q are needed for P, and which packages are able to provide those parts
when P is built.

Section 5.1 above suggests to make this specification at least at the
level of modules, in both providers and clients. But even if one wanted
to stay at the coarser level of API names and versions, one should
distinguish between an API and one of its implementing packages. Each
client should list the APIs it depends on, each provider should list the
APIs it can be called upon to provide.

One can achieve some of this in current Cabal by introducing
intermediate packages that represent named APIs to clients while
re-exporting implementations of those APIs by providers. Apart from
needing re-export functionality, this is more complicated than it should
be.

### 5.3 Make API specifications more specific

If one compares Cabal's ideas of packages and APIs with Standard ML's
module language, with its structures, functors, and interfaces forming
part of a statically typed functional program composition language, one
can see a lot of room for development.

6. Distributions at the Hackage level
-------------------------------------

The idea here is to group packages into "distributions" in Hackage, with
the property that all packages within a distribution are mutually
compatible. Todo... expand.

7. Allow package overlaps
-------------------------

This is not a solution to the problem of splitting a package but helps
in the case that we want to use a new package that provides an updated
version of some modules in an existing package. An example of this is
the bytestring and base package. The base-2.0 package included
Data.ByteString but it was split off into a bytestring package and not
included in base-3.0. At the moment ghc allows local .hs files to
provide modules that can shadow modules from a package but does not
packages to shadow each other.

So an extension that would help this case would be to let packages
shadow each other. The user would need to specify an ordering on
packages so ghc knows which way round the shadowing should go. This
could be specified by the order of the -package flags on the command
line, which is equivalent to the order in which they are listed in the
build-depends field in a .cabal file. This would be a relatively easy
extension to implement.

Note that it only solves the problem of backporting packages to be used
on top of older versions of the package they were split from. It also
provides a way for people to experiment with packages that provide
alternative implementations of standard modules.

There is potential for confusion if this is used too heavily however.
For example two packages built against standard and replacement modules
may not be able to be used together because they will re-export
different types.

The problem of lax version dependencies
---------------------------------------

Supposing that we used solution 2 above and had a base-2.x and a
base-3.x. If we take an old package and build it against base-2.x then
it will work and if we build it against base-3.x then it'll fail because
it uses modules from the split out packages like directory, bytestring
etc. So obviously Cabal should select base-2.x, but how is this decision
actually supposed to be made automatically? From a quick survey of the
packages on hackage we find that 85% specify unversioned dependencies on
the base package and none of them specify upper bounds for the version
of base. So presented with a package that says:

how are we to know if we should use base-2.x or base-3.x. It may be that
this package has been updated to work with base-3.x or that it only ever
used the parts of base-2.x that were not split off. This dependency does
not provide us with enough information to know which to choose. So we
are still left with the situation that every package must be updated to
specify an api version of base.

One possible remedy would be to call version 3 something other than
base. Any dependency on 'base' would then refer to the set of modules
that comprise base-2.x (this is (4.2) above, incedentally).

Note about this page
====================

``*`Apparently,` `this` `page` `is` `out` `of` `date` `and` `the`
`issue` `has` `been` `settled` `in` `favour` `of` `the` `syntax:`*\
``\
`Seealso:`\
``[`8`](http://haskell.org/ghc/docs/latest/html/users_guide/syntax-extns.html#package-imports)

Explicit package imports
========================

This proposal is one possibility for addressing the question of
identifying which package is meant in an import declaration. For the
context, read the \[wiki:Commentary/Packages/GhcPackagesProposal GHC
packages summary page\] first.

The main idea of this proposal is to allow the programmer to specify the
source package in the import line, something like this: That would
presumably get the most recent installed incarnation of the package. If
you want a particular version of the package, we could allow The exact
syntax is unimportant. The important thing is that the programmer can
specify the package in the source text. Note that this fundamentally
conflicts with the second assumption we started with. We were trying to
avoid specifying "provenance" at the same time as "purpose", on the
grounds that we wanted to avoid editing lots of source text when the
provenance changed. (And so it begs the question, if we need to edit the
source anyway, why separate the syntax of packages from modules at all?)

If we adopt the idea that an import statement can specify the source
package, several design choices arise:

Is the 'from <package>' compulsory?
-----------------------------------

If you want to import A.B.C, a module exported by package "foo", can you
say just , or must you say ?

We think of this as rather like the question "If you import f from
module M, can you refer to it as plain "f", or must you refer to it as
"M.f"? The answer in Haskell 98 is that you can refer to it as plain "f"
so long as plain "f" is umambiguous; otherwise you can use a qualified
reference "M.f" to disambiguate.

We propose to adopt the same principle for imports. That is, an import
with no package specified, such as "", means:

`FindallmodulesA.B.Cexportedbyallexposedpackages,orthepackageorprogrambeingcompiled.Ifthereisexactlyonesuchmodule,that'stheonetoimport.Otherwisereport"ambiguousimport".`

If the reference to A.B.C is ambiguous, you can qualify the import by
adding "".

Package versions
----------------

We probably want some special treatment for multiple versions of the
same package. What if you have both "foo-3.9" and "foo-4.0" installed,
both exporting A.B.C? This is jolly useful when you want to install new
packages, but keep old ones around so you can try your program with the
older one. So we propose that this is not regarded as ambiguous:
importing A.B.C gets the latest version, unless some compiler flag
(-hide-package) takes it of the running.

In short, an installed package can be of two kinds:

`*`**`Exposed`**`:thepackage'smodulespopulatetheglobalmodulenamespace,andcanbeimportedwithoutmentioningthepacckagenameexplicitly(``).Explicit"from"importsmaybeusedtoresolveambiguity.`\
`*`**`Available`**`,butnotexposed:thepackagecanbeusedonlybyanexplicit"from"import.Thisisratherlike"``,exceptatthepackagelevel.`

Typically, if multiple versions of the same package are installed, then
all will be available, but only one will be exposed.

GHC's command-line flags (, ) can be used to manipulate which packages
are exposed, but typically an entire package or program will be compiled
with a single set of such flags. GHC does not curretly support in-module
control, thus , and we do not propose to change that.

Simon suggested that an installed package might be hidden (so that it
cannot be used at all) but I'm not sure why we need that.

Importing from the home package
-------------------------------

If A.B.C is in the package being compiled (which we call "the home
package"), and in an exposed package, and you say , do you get an
"ambiguous import" error , or does the current package override. And if
the former, how can you say "import A.B.C from the current package"?

One possibility is to reuqire the code to know its own package name, and
mention that in the import. For exmaple, in a module that is being
compiled as part package "foo", you'd say . What about modules that are
part of the main program (not a package at all). Perhaps you could then
say .

Another way is to have a special package name meaning "the home
package". The special name could be

`*""`\
`*"home"`\
`*"this"`\
`*this(withnoquotes)`

The 'as P' alias
----------------

We propose to maintain the local, within-module "as P" alias mechanism
unchanged. Thus: Here, the qualified name "M.T" refers to the T imported
from A.B.C in package "foo".

Qualified names
---------------

We propose that the default qualified name of an entity within a module
is just the module name plus the entity name. Thus If you want to import
multiple A.B.C's (from different packages) then perhaps they define
different entities, in which case there is no problem: But if they both
export entities with the same name, there is no alternative to using the
'as M' mechanism:

Exporting modules from other packages
-------------------------------------

It is perfectly OK to export entities, or whole modules, imported from
other packages:

Syntax
------

Should package names be in quotes? Probably yes, because they have a
different lexcal syntax to the rest of Haskell. ("foo-2.3" would parse
as three tokens, "foo", "-", and "2.3".

It's been suggested that one might want to import several modules from
one package in one go: What we don't like about that is that it needs a
new keyword "". Perhaps all imports can start with the keyword , and
then we are free to use extra (context-specific) keywords. (Haskell
already has several of these, such as . Something like this:

 Here the layout is explicit, but the braces and semicolons could be
avoided by making use of the layout rule as usual.

Indeed, we could allow this multiple form even for ordinary imports:

It is clear from the above examples that the keyword is redundant - the
presence of a string literal (or special keyword to denote the home
package) after the keyword is sufficient to distinguish per-package
imports from the ordinary shared-namespace imports, so the above could
instead be written as

### Syntax formalised and summarised

A possible syntax which covers everything in this proposal is therefore:

``**`import`**`[`*`package-name`*`]`**`{`**``*`import-specifier`*`[`**`;`**``*`import-specifier`*`]`**`}`**

where *package-name* is a string literal or the keyword , the
*import-specifier* corresponds to everything that is currently allowed
after the keyword , and the braces and semicolons would be added by the
layout rule.

### Proposal for Package Mounting

It may help to refer to \[wiki:Commentary/Packages/GhcPackagesProposal\]
for an introduction to some of the issues mentioned here.

A message by Frederik Eaton to the Haskell mailing list describing the
present proposal is archived:
[9](http://www.haskell.org/pipermail/libraries/2005-June/004009.html).
(Also, see note at the end of this document regarding an earlier
proposal by Simon Marlow)

This document will go over Frederik's proposal again in brief. The
proposal doesn't involve any changes to syntax, only an extra command
line option to , etc., and a small change to Cabal syntax.

In this proposal, during compilation of a module, every package would
have a "mount point" with respect to which its particular module
namespace would be resolved. Each package should have a default "mount
point", but this default would be overridable with an option to , etc.

For example, the library currently has module namespace:

In this proposal, it might instead have default mount point and
(internal) module namespace:

To most users of the X11 package, there would be no change - because of
the mounting, modules in that package would still appear with the same
names in places where the X11 package is imported: , etc. However, if
someone wanted to specify a different the mount point, he could use a
special compiler option, for instance :

(so the imported namespace would appear as , , etc.) Note that the
intention is for each option to refer to the package specified in the
preceding option, so to give package a mount point of we use the syntax

Ideally one would also be able to link to two different versions of the
same package, at different mount points:

(yielding , , ...; , , ...)

However, usually the default mount point would be sufficient, so most
users wouldn't have to learn about .

Additionally, Cabal syntax should be extended to support mounting. I
would suggest that the optional mount point should appear after a
package in the Build-Depends clause of a Cabal file:

And in the package Cabal file, a new clause to specify the default mount
point:

### Evaluation

This proposal has several advantages over the
\[wiki:Commentary/Packages/PackageImportsProposal\] proposal.

`*`*`No` `package` `names` `in`
`code`*`.Inthisproposal,packagenameswouldbedecoupledfromcode.Thisisveryimportant.Itshouldbepossibletorenameapackage(orcreateanewversionofapackagewithanewname),anduseitinaproject,withouteditingeverysinglemoduleoftheprojectand/orpackage.Eveniftheeditscouldbedoneautomatically,theywouldstillcauserevisioncontrolheadaches.AnyproposalwhichputspackagenamesinHaskellsourcecodeshouldbeconsideredunacceptable.`

`*`*`No` `syntax`
`changes`*`.The[wiki:Commentary/Packages/PackageImportsProposal]proposalrequiresnewsyntax,butthisproposaldoesnot.Ofcourse,inthisproposalitwouldbeslightlymoredifficultfortheprogrammertofindoutwhichpackageamoduleiscomingfrom.Hewouldhavetolookatthecommandlinethatcompilesthecodehe'sreading.However,Ithinkthatthatisappropriate.Provenanceshouldnotbespecifiedincode,sinceitchangesallthetime.(AndtherecouldbeasimpledebuggingoptiontoGHCwhichoutputsadescriptionofthenamespaceusedwhencompilingeachfile)`

`*`*`Simpler` `module`
`names`*`.Thisproposalwouldallowlibraryauthorstousesimplermodulenamesintheirpackages,whichwouldinturnmakelibrarycodemorereadable,andmoreportablebetweenprojects.Forinstance,imaginethatIwantedtoimportsomeofthecodefromthe``libraryintomyownproject.Currently,Iwouldhavetodeleteeveryoccurrenceof``inthosemodules.Mergingfuturechangesaftersuchanextensivemodificationwouldbecomedifficult.Thisisarealproblem,whichIhaveencounteredwhileusingJohnMeacham'scurseslibrary.Thereareseveraldifferentversionsofthatlibrarybeingusedbydifferentpeopleindifferentprojects,anditisdifficulttoconsolidatethembecausetheyallhavedifferentmodulenames.Thereasontheyhavedifferentmodulenamesisthatpackagemountinghasn'tbeenimplementedyet.The[wiki:Commentary/Packages/PackageImportsProposal]proposalwouldnotfixtheproblem.`

`*`*`Development` `decoupled` `from`
`naming`*`.(thereisabitofoverlapwithpreviouspointshere)Inthepresentproposal,programmerswouldbeabletostartwritingalibrarybeforedecidingonanameforthelibrary.Forinstance,everymoduleinthe``librarycontainstheprefix``.Thismeansthateithertheauthorofthelibraryhadtochoosethename``attheverybeginning,orhehadtomakeseveralchangestothetextofeachmoduleafterdecidingonthename.Underthepresentproposal,hewouldsimplycallhismodules``,``,``,etc.;the``prefixwouldbespecifiedinthebuildsystem,forinstanceintheCabalfile.`

Frederik's mailing list message discusses some other minor advantages,
but the above points are the important ones. In summary, it is argued
that the above proposal should be preferred to
\[wiki:Commentary/Packages/PackageImportsProposal\] because it is both
easier to implement (using command line options rather than syntax), and
more advantageous for the programmer.

### Note on Package Grafting

A proposal by Simon Marlow for "package grafting" predates this one:
[10](http://www.haskell.org/pipermail/libraries/2003-August/001310.html).
However, the "package grafting" proposal is different in that it
suggests selecting a "mount point" at library installation time, where
in the present proposal, the "mount point" is selected each time a
module using the library in question is compiled. The difference is
important, as one doesn't really want to have to install a new copy of a
library just to use it with a different name. Also, Simon Marlow's
proposal puts package versions in the module namespace and therefore
source code, where we argue for decoupling source code from anything to
do with provenance - be it package names or version numbers.

[PageOutline](PageOutline "wikilink")

Alternative Proposal for Packages (with explicit namespaces)
------------------------------------------------------------

This proposal is an alternative to
\[wiki:Commentary/Packages/GhcPackagesProposal\]. Large parts overlap
with that proposal. To motivate this new proposal, let's consider
another proposed and desirable feature of the import/export language,
which may interact in interesting ways with packages.

A different, but related, problem
---------------------------------

A problem that has been mentioned several times on mailing lists, is
grafting part of a directory hierarchy into an arbitrary location
elsewhere in the hierarchy. (See
[11](http://www.haskell.org/pipermail/libraries/2005-June/004009.html))

Another way of expressing a similar wish is the ability to re-export
imports with a different qualified name, as in the scenario suggested by
the developers of the package gtk2hs:
[12](http://www.haskell.org/pipermail/libraries/2004-December/002800.html)

There are several desires in play here:

`*adesiretominimisetypingoflongqualifiednames`\
`*adesiretoreferto"leaf"nodesofthehierarchyinawaythatmakesiteasytorelocatethosemodulesinthehierarchy,withoutneedingtoediteveryimportdeclarationthatusesthem`\
`*adesiretopartially-qualifynamesfordisambiguation`

Proposal
--------

We introduce the new concept of *namespace* as something that can be
declared in source code. A namespace can contain only module names. (The
specification of what module names are contained in a namespace is
rather like our current concept of a package, i.e. not declared in the
source code, but rather by some external mechanism e.g. grouping of
files in a filesystem hierarchy.)

There are now two separate kinds of .

`*`\
`*`

The new semi-reserved word is introduced, having special meaning only
directly after the keyword. There is a *level* difference in what this
new form of import means. The declaration brings into availability the
subset of the hierarchy of *module* names rooted in the package , at the
position . That is, if the package version contains the modules

`*Data.Foo.Bar`\
`*Data.Foo.Baz`\
`*Data.Bar`

then the namespace import brings into the "importable" namespace only
the modules

`*Data.Foo.Bar`\
`*Data.Foo.Baz`

However, for the program to use those modules, it is still necessary to
go ahead and actually them in the normal way, although the names used to
import them will now be *relative* to the available namespaces, rather
than absolute. So the declaration brings into scope all the entities
defined in . Like any normal import, these can be qualified or hidden.

Thus,

`*``bringsintoscopeabunchofnamesformodules`\
`fromthegivenprovenance.`\
`*``bringsintoscopeabunchofentitiesfromthegiven`\
`module.`

### Naming a namespace

Are namespaces first class? Can we give them a name? Indeed, why not?

`*`\
`*`

Here, we have declared that we want to be able to refer to the namespace
as , and so, a subsequent specifically asks for the from the package ,
just in case there might be a module also available from another
namespace.

### What namespaces are available by default?

If no namespaces are explicitly brought into scope, what modules are
implicitly available?

`*Anythinginthe`*`current`*`package,i.e.theexecutableorlibrary`\
`whosemodulesareallphysicallyrootedatthesamelocationinthe`\
`filesystemasthismodule.`

`*Isthereanimplicit``,justasthereisan`\
`implicit``?`

### Namespace resolution

In essence, namespaces take over the role formerly played by commandline
arguments like and . The search path used by the compiler for finding
modules is now partially declared in the source code itself. (Note
however that that the search path is declared symbolically, involving
package names, not directories. This is a very important separation of
the thing itself from where it is stored.)

Resolution of which module is referred to by an import statement (taking
into account the namespaces) is just like the current process of
resolving which entity is referred to by program text (taking into
account the imported modules). The source text may import multiple
namespaces. If any module import is ambiguous (i.e. the module exists in
more than one namespace), it is a static error. Resolution is lazy, in
the sense that there is no error if namespaces contain the same module
name, only if the program tries to import that module name.

So when you say "import A.B.C", from what package does A.B.C come?

There must be a single namespace in scope containing a module called .
(Sidenote: or in fact a namespace called , containing a module named )

### Syntax

The precise syntax can be debated. New keywords like or could be
substituted for . The key important features however are the inclusion
of:

`*thepackagename(mandatory)`\
`*anoptionalpackageversion,ifseveralareavailable`\
`*anoptionalpathtouseastherootoftheavailablenamespace`\
`*anoptionalrenaming`

### Exports

One might wonder whether it is now either necessary or desirable to
permit *namespaces* to be re-exported in the same way that *modules* can
be? For instance:

The idea is that any module saying would thereby implicitly open the
namespace of package at the root , in addition to having access to
entities defined in itself.

Note that, just as with a current module re-export it is no longer
possible for the importing location to use the original module name as a
qualifier; so with a namespace re-export, there is no way to refer to
the namespace in the importing location either. It is purely a signal to
the compiler telling it where to look for modules when resolving
imports.

I argue that namespace export *is* desirable, because it allows (but
does not require) all package (namespace) dependencies to be gathered
together in a single module for an entire project. With such an
organising principle, when dependencies change, there is only one source
file to update. But without namespace re-exports, it would be impossible
to localise those dependencies to a single file.

Note how this feature addresses several of the initial stated desires,
of reducing the verbosity of imports, and of referring to leaf modules
conveniently. For instance:

### Implicit imports

One could go further. If I write a qualified name in the source text,
must I also write at the top? The qualified entity is unambiguous,
whether or not there is an explicit import for it, because the module
qualification must be unambiguous within the current namespaces. In the
Gtk example above, this would eliminate the need for , and who knows how
many other imports, leaving a single to bring all of the qualified
entities into scope.

### Exposed vs Hidden packages

GHC's scheme of exposed vs hidden packages can now be replaced with full
source-code control of namespace visibility. To setup a default set of
exposed packages, you just write a module to export their namespaces:

and import it in every module of your project. Or if importing it
everywhere sounds too painful, one can even imagine that a compiler
might provide a command-line option (or use a configuration file) to
specify one distinguished module to be implicitly imported everywhere:

### What if you wanted to import A.B.C from P1 and A.B.C from P2 into the *same* module?

[PageOutline](PageOutline "wikilink")

Package Reorg
=============

In this page we collect proposals and design discussion for reorganising
the packages that come with compilers, and the contents of those
packages.

None of the ideas herein are claimed to belong to any particular person,
many of the ideas have been extracted from mailing list discussions, eg.

[`13`](http://www.haskell.org/pipermail/libraries/2006-November/006396.html)

Some of the points are GHC-specific. Please feel free to insert points
specific to other compilers.

Goals
-----

`*Itwouldbegoodtohavesetof'core'packagesthatisinstalledwith`\
`everyHaskellimplementation.MoreonthisatPackageReorg/Rationalepage`\
`*Forwardscompatibility.Userswouldliketheirprogramswrittenagainstthe'core'packagestocontinuetowork,without`\
`modificationtosourcetextorbuildsystem,afterupgradingthe`\
`compiler,oritspackages,orswitchingtoadifferentcompiler.`\
`*Backwardscompatibility.Userswouldliketobeabletotakea`\
`programwrittenagainstsomeversionofthe'core'packages,and`\
`builditwithanoldercompiler,acceptingthattheymayhaveto`\
`installnewerversionsofthe'core'packagesinordertodoso.`

It may not be possible to fully achieve these goals (in particular,
backwards compatibility), but that does not mean we should not aim for
them.

Proposal
--------

Here's a straw-man proposal

`*ThereisasetofpackagesthatcomewitheveryconformingHaskell`\
`implementation.Let'scallthesethe`**`Core` `Packages`**`to`\
`avoidconfusion(Bulatcalledthesethe"basepackages",butthat'san`\
`` over-usedtermgiventhatthereisapackagecalled`base`). ``\
`ThegoodthingabouttheCorePackagesisthat`\
`usersknowthattheywillbethere,andtheyareconsistentwith`\
`eachother.`

`*Anyparticularimplementationmayinstallmorepackagesbydefault;`\
`` forexampleGHCwillinstallthe`template-haskell`and`stm` ``\
`packages.Let'scallthesethe`**`GHC` `Install`
`Packages`**`,'''Hugs`\
`InstallPackages'''etc;theInstallPackagesareasupersetofthe`\
`CorePackages.`

### What is in the Core Packages?

The Core Packages are installed with every conforming Haskell
implementation. What should be in the Core? There is a tension:

`1.`**`As` `much` `as`
`possible`**`;whichmeansinpracticewidely-usedandreasonablystablepackages.Itisconvenientforprogrammerstohaveasmuchaspossibleinaconsistent,bundlethatis(a)knowntoworktogetherbundle,and(b)knowntoworkonallimplementations.`[`BR`](BR "wikilink")[`BR`](BR "wikilink")\
`2.`**`As` `little` `as`
`possible`**`;whichinpracticemeansenoughtorunCabalsothatyoucanruntheSetupfilesthatcomewhendownloadingnewpackages.AsIanputsit:thelessweforcetheimplementationstocomewith,thequickercompilationwillbewhendeveloping,thesmallerDebianpackages(forexample)canbe,thelowerthediskspacerequirementstobuildGHC,thelowerthetimewastedwhenaDebianpackage(forexample)buildfailsandthefewerpackageswearetanglingupwithcompilerreleaseschedules.`

There's a real choice here: Bulat wants (1) and Ian wants (2).

Initial stab at (1):

`` *`base` ``\
`` *`Cabal` ``\
`` *`haskell98` ``\
`` *Some`regex`packages(preciselywhich?) ``\
`` *`unix`or`Win32`.Questionable,partlybecauseitmeanstheCoreinterfacebecomesplatform-dependent;andpartlybecause`Win32`woulddoublethesizeoftheHugsdistribution. ``\
`` *`parsec` ``\
`` *`mtl` ``\
`` *`time` ``\
`` *`network` ``\
`` *`QuickCheck`(questionable) ``\
`` *`HUnit`(questionable) ``

Initial stab at (2):

`` *`base` ``\
`` *`haskell98` ``\
`` *`Cabal` ``\
`` *`filepath`(?) ``

Bulat: i think that all regex packages should be included and of course
libs that helps testing. overall, it should be any general-purpose lib
that porters accept (enlarging this set makes users live easier, and
porters live harder)

about unix/win32 - these libs provide access to OS internals, not some
everywhere-portable API. moreover, other world-interfacing libs (i/o,
networking) should use APIs provided by these libs with a conditional
compilation (CPPery) tricks in order to provide portable APIs! current
situation where such libs use FFI isn't ideal. WinHugs size problem is
rather technical - it includes a lot of DLLs which contains almost the
same code

i agree to start with minimal stub, and then proceed with discussing
inclusion of each library. what we need now is requirements to include
library in this set and lifetime support procedure. so:

### Requirements to libraries to be included in core set

`*BSD-licensed,andevenbelongstoHaskellcommunity?`\
`*portable(issenseofcompilerandOS),maybejustHaskell'compatible?`\
`*alreadywidelyused`\
`*shouldn'tduplicateexistingcorelibsfunctionality(?)`

Exact inclusion, support and exclusion processes?

### The base package

The base package is a bit special

`` *Package`base`isratherbigatthemoment. ``

`*Fromauser'spointofviewitwouldbenicertogiveita`\
`` compiler-independentAPI.(Amodulelike`GHC.Exts`wouldmoveto ``\
`` anewpackage`ghc-base`.) ``

Thinking of GHC alone for a moment, we could have a package \`ghc-base\`
(which is pretty much the current \`base\`) and a thin wrapper package
\`base\` that re-exposes some, but not all, of what \`ghc-base\`
exposes. To support this re-exposing, we need a small fix to both GHC
and Cabal, but one that is independently desirable.

Similarly, Hugs could build \`hugs-base\` from the same souce code, by
using CPP-ery, exactly as now. The thin \`base\` wrapper package would
not change.

To make \`base\` smaller, we could remove stuff, and put it into
separate packages. But be careful: packages cannot be cyclic, so
anything that is moved out can't be used in \`base\`. Some chunks that
would currently be easy to split off are:

`*Data.!ByteString.*(plusfuturepackedCharstrings)`\
`*Control.Applicative(?),Data.Foldable,Data.Monoid(?),Data.Traversable,Data.Graph,Data.!IntMap,Data.!IntSet,Data.Map,Data.Sequence,Data.Set,Data.Tree`\
`*System.Console.!GetOpt`\
`*Text.!PrettyPrint.*`\
`*Text.Printf`

Some other things, such as arrays and concurrency, have nothing else
depending on them, but are so closely coupled with GHC's internals that
extracting them would require exposing these internals in the interface
of \`base\`.

Bulat: my ArrayRef library contains portable implementation of arrays.
there is only thin ghc/hugs-specific layer which should be provided by
ghcbase/hugsbase libs. except for MPTC problem (IArray/MArray classes
has multiple parameters), this library should be easily portable to any
other haskell compiler

See also BaseSplit.

### Other packages

Other non-core packages would probably have their own existence. That
is, they don't come with an implementation; instead you use
\`cabal-get\`, or some other mechanism, such as your OS's package
manager. Some of these currently come with GHC, and would no longer do
so

`` *`GLUT` ``\
`` *`ALUT` ``\
`` *`OpenAL` ``\
`` *`OpenGL` ``\
`` *`HGL` ``\
`` *`HUnit` ``\
`` *`ObjectIO` ``\
`` *`X11` ``\
`` *`arrows` ``\
`` *`cgi` ``\
`` *`fgl` ``\
`` *`html` ``\
`` *`xhtml` ``

Bulat: i propose to unbundle only graphics/sound libs because these
solves particular problems and tends to be large, non-portable (?) and
some are just legacy ones - like ObjectIO. we should keep everything
small & general purpose, including HUnit, arrows, fgl, html and xhtml,
and include even more: ByteString, regex-\*, Edison, Filepath, MissingH,
NewBinary, QuickCheck, monads

Testing
-------

We should separate out package-specifc tests, which should be part of
the repository for each package. Currently they are all squashed
together into the testsuite repository.

Implementation-specific notes
-----------------------------

### Notes about GHC

Currently GHC installs a set of packages by default, the so-called **GHC
Boot Packages**. They are graphed here, with arrows representing
dependencies between them: [Image(packagegraph.png,
800)](Image(packagegraph.png,_800) "wikilink")

These are exactly the libraries required to build GHC. That shouldn't be
the criterion for the core packages.

One reason we do this is because it means that every GHC installation
can build GHC. Less configure-script hacking. (NB: even today if you
upgrade any of these packages, and then build GHC, the build might fail
because the CPP-ery in GHC's sources uses only the version number of
GHC, not the version number of the package.)

Still, for convenience we'd probably arrange that the GHC Install
Packages included all the GHC Boot Packages.

Every GHC installation must include packages: \`base\`, \`ghc-prim\`,
\`integer\` and \`template-haskell\`, else GHC itself will not work. (In
fact \`haskell98\` is also required, but only because it is linked by
default.)

So GHC's Install Packages would be the Core Packages plus

`` *`template-haskell` ``\
`` *`editline` ``\
`` *`integer` ``\
`` *`ghc-prim` ``

You can upgrade any package, including \`base\` after installing GHC.
However, you need to take care. You must not change a number of things
that GHC "knows about". In particular, these things must not change

`*Name`\
`*Definingmodule`

GHC knows even more about some things, where you must not change

`*Typesignature`\
`*Fordatatypes,thenames,types,andorderoftheconstructors`

The latter group are confined to packages base and template-haskell.

(Note: a few other packages are used by tests in GHC's test suite,
currently: \`mtl\`, \`QuickCheck\`. We should probably eliminate the mtl
dependency; but \`QuickCheck\` is used as part of the test
infrastructure itself, so we'll make it a GHC Boot Package.)

### Notes about Hugs

Recent distributions of Hugs come in two sizes, jumbo and minimal.
Minimal distributions include only the packages \`base\`, \`haskell98\`
and \`Cabal\`. (Hugs includes another package \`hugsbase\` containing
interfaces to Hugs primitives.) The requirements for this set are to

`*runHaskell98programs`\
`*allowpackagestobeaddedandupgradedusingCabal`

(Currently \`cpphs\` is a Haskell 98 program, so the latter implies the
former.)

It should be possible to upgrade even the core packages using Cabal.

Commentary: The Package System
==============================

See also: \[wiki:Commentary/Compiler/Packages Packages\], where we
describe how this is implemented in GHC.

Architecture
------------

GHC maintains a package database, that is basically a list of
\`InstalledPackageInfo\`. The \`InstalledPackageInfo\` type is defined
in \`Distribution.InstalledPackageInfo\` in Cabal, and both \`ghc-pkg\`
and GHC itself import it directly from there.

There are four main components of the package system:

`Cabal::`\
`CabalisaHaskelllibrary,whichprovidesbasicdatatypesforthepackagesystem,andsupportforbuilding,`\
`configuring,andinstallingpackages.`

`GHCitself::`\
`` GHCreadsthepackagedatabase(s),understandstheflags`-package`,`-hide-package`,etc.,andusesthepackagedatabase ``\
`` tofind`.hi`filesandlibraryfilesforpackages.GHCimportsmodulesfromCabal. ``

`` `ghc-pkg`:: ``\
`` The`ghc-pkg`toolmanagesthepackagedatabase,includingregistering/unregisteringpackages,queries,and ``\
`` checkingconsistency.`ghc-pkg`alsoimportsmodulesfromCabal. ``

`` `cabal-install`:: ``\
`AtoolbuiltontopofCabal,whichaddssupportfordownloadingpackagesfromHackage,andbuildingandinstalling`\
`multiplepackageswithasinglecommand.`

For the purposes of this commentary, we are mostly concerned with GHC
and \`ghc-pkg\`.

Identifying Packages
--------------------

`` `Cabal.PackageName`("base"):: ``\
`` Astring.Definedin`Distribution.Package`.Doesnotuniquelyidentifyapackage:thepackage ``\
`databasecancontainseveralpackageswiththesamename.`

`` `Cabal.PackageId`("base-4.1.0.0"):: ``\
`` A`PackageName`plusa`Version`.A`PackageId`namesanAPI.Iftwo`PackageId`sare ``\
`thesame,theyareassumedtohavethesameAPI.`\
``[`BR`](BR "wikilink")[`BR`](BR "wikilink")\
`` `InstalledPackageInfo`containsthefield`sourcePackageId::PackageId`. ``\
``[`BR`](BR "wikilink")[`BR`](BR "wikilink")\
`` InGHC6.11,the`PackageId`alsouniquelyidentifiesapackageinstanceinthepackagedatabase,but ``\
`onlybyconvention(wemayliftthisrestrictioninthefuture,andallowthedatabasetocontain`\
`` multiplepackageinstanceswiththesame`PackageId`(anddifferent`InstalledPackageId`s). ``

`` `Cabal.InstalledPackageId`("base-4.1.0.0-1mpgjN"):: ``\
`(introducedinGHC6.12/Cabal1.7.2)Astringthatuniquelyidentifiesapackageinstanceinthedatabase.`\
`` An`InstalledPackageId`identifiesanABI:iftwo`InstalledPackageIds`arethesame,theyhavethe ``\
`sameABI.`\
``[`BR`](BR "wikilink")[`BR`](BR "wikilink")\
`` `InstalledPackageInfo`containsthefield`installedPackageId::InstalledPackageId`. ``\
``[`BR`](BR "wikilink")[`BR`](BR "wikilink")\
`` Dependenciesbetweeninstalledpackagesareidentifiedbythe`InstalledPackageId`.An`InstalledPackageId`is ``\
`` chosenwhenapackageisregistered.Itischosenbycalling`ghc--abi-hash`onthecompiledmodulesandappending ``\
`` thehashasasuffixtothestringrepresentingthe`PackageIdentifier`. ``

`` `GHC.PackageId`(thesecurrentlylooklike"base-4.1.0.0"inGHC6.12):: ``\
`` InsideGHC,weusethetype`PackageId`,whichisa`FastString`.The(Z-encodingof)`PackageId`prefixeseach ``\
`externalsymbolinthegeneratedcode,sothatthemodulesofonepackagedonotclashwiththoseofanotherpackage,`\
`evenwhenthemodulenamesoverlap.`

Design constraints
------------------

`1.Wewant[wiki:Commentary/Compiler/RecompilationAvoidancerecompilationavoidance]towork.Thismeansthatsymbolnamesshouldnotcontainanyinformationthatvariestoooften,suchastheABIhashofthemoduleorpackage.TheABIofanentityshoulddependonlyonitsdefinition,andthedefinitionsofthethingsitdependson.`

`2.WewanttobeabletodetectABIincompatibility.Ifapackageisrecompiledandinstalledoverthetopoftheoldone,andthenewversionisABI-incompatiblewiththeoldone,thenpackagesthatdependedontheoldversionshouldbedetectablybrokenusingthetools.`

`3.ABIcompatibility:`\
`*Wewantrepeatablecompilations.Compilingapackagewiththesameinputsshouldyieldthesameoutputs.`\
`*Furthermore,wewanttobeabletomakecompiledpackagesthatexposeanABIthatiscompatible(e.g.asuperset)`\
`ofanexistingcompiledpackage.`\
`*Modularupgrades:wewanttobeabletoupgradeanexistingpackagewithoutrecompilingeverythingthatdepends`\
`onit,byensuringthatthereplacementisABI-compatible.`\
`*Sharedlibraryupgrades.WewanttobeabletosubstituteanewABI-compatiblesharedlibraryforanoldone,andalltheexistingbinarieslinkedagainsttheoldversioncontinuetowork.`\
`*ABIcompatibilityisdependentonGHCtoo;changestothecompilerandRTScanintroduceABIincompatibilities.We`\
`guaranteetoonlymakeABIincompatiblechangesinamajorreleaseofGHC.Betweenmajorreleases,ABIcompatibility`\
`isensured;soforexampleitshouldbepossibletouseGHC6.12.2withthepackagesthatcamewithGHC6.12.1.`

Right now, we do not have repeatable compilations, so while we cannot do
(3), we keep it in mind.

The Plan
--------

We need to talk about some more package Ids:

`` *`PackageSymbolId`:thesymbolprefixusedincompiledcode. ``\
`` *`PackageLibId`:thepackageIdinthenameofacompiledlibraryfile(staticandshared). ``

### Detecting ABI incompatibility

`` *inthepackagedatabase,dependenciesspecifythe`InstalledPackageId`. ``

`*Thepackagedatabasewillcontainatmostoneinstanceofagivenpackage/versioncombination.Thetools`\
`arenotcurrentlyabletocopewithmultipleinstances(e.g.GHC's-packageflagselectsbyname/version).`

`*If,say,packageP-1.0isrecompiledandre-installed,thenewinstanceofthepackagewillalmost`\
`certainlyhaveanincompatibleABIfromthepreviousversion.Wegivethenewpackageadistinct`\
`` `InstalledPackageId`,sothatpackagesthatdependontheoldP-1.0willnowbedetectablybroken. ``

`` *`PackageSymbolId`:Wedonotusethe`InstalledPackageId`asthesymbolprefixinthecompiledcode,because ``\
`thatinteractsbadlywith[wiki:Commentary/Compiler/RecompilationAvoidancerecompilationavoidance].Everytimewepicka`\
`` newunique`InstalledPackageId`(e.g.whenreconfiguringthepackage),wewouldhavetorecompile ``\
`` theentirepackage.Hence,the`PackageSymbolId`ispickeddeterministicallyforthepackage,e.g. ``\
`` itcanbethe`PackageIdentifier`. ``

`` *`PackageLibId`:wedowanttoputthe`InstalledPackageId`inthenameofalibraryfile,however.Thisallows ``\
`ABIincompatibilitytobedetectedbythelinker.Thisisimportantforsharedlibrariestoo:we`\
`wantanABI-incompatiblesharedlibraryupgradetobedetectedbythedynamiclinker.Hence,`\
`` `PackageLibId`==`InstalledPackageId`. ``

### Allowing ABI compatibilty

`*ThesimplestschemeistohaveanidentifierforeachdistinctABI,e.g.apairofthepackagenameandaninteger`\
`thatisincrementedeachtimeanABIchangeofanykindismadetothepackage.TheABIidentifier`\
`` isdeclaredbythepackage,andisusedasthe`PackageSymbolId`.SincepackageswiththesameABIidentifier ``\
`` areABI-compatible,the`PackageLibId`canbethesameasthe`PackageSymbolId`. ``

`*ThepreviousschemedoesnotallowABI-compatiblechanges(e.g.ABIextension)tobemade.Hence,wecould`\
`generaliseittoamajor/minorversioningscheme.`\
`` *theABImajorversionisasbefore,thepackagename+aninteger.Thisisalsothe`PackageSymbolId`. ``\
`*theABIminorversionisanintegerthatisincrementedeachtimetheABIisextendedinacompatibleway.`\
`*packagedependenciesinthedatabasespecifythemajor+minorABIversiontheyrequire,inadditiontothe`\
`` `InstalledPackageId`.Theymaybesatisfiedbyagreaterminorversion;whenupgradingapackagewithan ``\
`` ABI-compatiblereplacement,ghc-pkgupdatesdependenciestopointtothenew`InstalledPackageId`. ``\
`` *`PackageLibId`isthemajorversion.Inthecaseofsharedlibraries,wemaynamethelibraryusingthe ``\
`major+minorversions,withasymboliclinkfromthemajorversiontomajor+minor.`\
`` *thesharedlibrary`SONAME`isthemajorversion. ``

`*ThepreviousschemeonlyallowsABI-compatiblechangestobemadeinalinearsequence.Ifwewantatree-shaped`\
`compatibilitystructure,thensomethingmorecomplexisneeded(ToDo).`

`*ThepreviousschemesonlyallowcompatibleABIchangestobemade.Ifwewanttoallowincompatiblechangestobe`\
`made,thenweneedsomethinglikeELF'ssymbolversioning.Thisisprobablyoverkill,sincewewillbemaking`\
`incompatibleABIchangesinthecompilerandRTSatregularintervalsanyway,solong-termABIcompatibilityis`\
`impracticalatthisstage.`

[PageOutline](PageOutline "wikilink")

The Parser
==========

\[Very incomplete. Please extend as you learn more.\]

The parser is written using

`*`[`Alex`](http://www.haskell.org/alex/)`,forlexicalanalysis.Sourcefile`[`GhcFile(compiler/parser/Lexer.x)`](GhcFile(compiler/parser/Lexer.x) "wikilink")\
`*`[`Happy`](http://www.haskell.org/happy/)`,fortheparseritself.Sourcefile`[`GhcFile(compiler/parser/Parser.y)`](GhcFile(compiler/parser/Parser.y) "wikilink")`.`\
`` *`RdrHsSyn`,forHaskellsupportfunctions.Sourcefile ``[`GhcFile(compiler/parser/RdrHsSyn.lhs)`](GhcFile(compiler/parser/RdrHsSyn.lhs) "wikilink")

Principles
----------

Making a parser parse *precisely* the right language is hard. So GHC's
parser follows the following principle:

`*`**`We` `often` `parse` `"over-generously",` `and` `filter` `out`
`the` `bad` `cases` `later.`**

Here are some examples:

`` *Patternsareparsedasexpressions,andtransformedfrom`HsExpr.HsExp`into`HsPat.HsPat`in`RdrHsSyn.checkPattern`.Anexpressionlike`[x|x<-xs]`thatdoesn'tlooklikeapatternisrejectedby`checkPattern`. ``

`` *Thecontextofatypeisparsedasatype,andthenconvertedintoacontextby`RdrHsSyn.checkContext`.Forexample,whenparsing ``

`` theparsercanonlydiscoverthat`(Reada,Numa)`isacontext,ratherthanatype,whenitmeetsthe`=>`.Thatrequiresinfinitelookahead.Soinsteadweparse`(Reada,Numa)`asatupletype,andthenconvertittoacontextwhenweseethe`=>`. ``

Sometimes the over-generous parsing is only dealt with by the renamer.
For example:

`*Infixoperatorsareparsedasiftheywereallleft-associative.Therenamerusesthefixitydeclarationstore-associatethesyntaxtree.`

There are plenty more examples. A good feature of this approach is that
the error messages later in compilation tend to produce much more
helpful error messages. Errors generated by the parser itself tend to
say "Parse error on line X" and not much more.

The main point is this. If you are changing the parser, feel free to
make it accept more programs than it does at the moment, provided you
also add a later test that rejects the bad programs. Typically you need
this flexibility if some new thing you want to add makes the pars
ambiguous, and you need more context to disambiguate. Delicate hacking
of the LR grammar is to be discouraged. It's very hard to maintain and
debug.

Avoiding right-recursion
------------------------

Be sure to read [this
section](https://www.haskell.org/happy/doc/html/sec-sequences.html) of
the Happy manual for tips on avoiding right recursion. In GHC, the
preferred method is using a left-recursive \`OrdList\`, as below:

\`OrdList\` operationally works the same way as building a list in
reverse (as in the Happy manual), but it makes it less likely you'll
forget to call \`reverse\` when you need to get the \`final\` list out.

One interesting, non-obvious fact, is that if you \*do\* use a
right-recursive parser, the "extra semi-colons" production should NOT be
pluralized:

Indentation
-----------

Probably the most complicated interaction between the lexer and parser
is with regards to //whitespace-sensitive layout.// The most important
thing to know is that the lexer understands layout, and will output
virtual open/close curlies (productions \`vocurly\` and \`vccurly\`) as
well as semicolons, which can then be used as part of productions in
\`Parser.y\`. So for example, if you are writing a rule that will make
use of indentation, you should accept both virtual and literal curlies:

Notice the use of \`close\` rather than \`vccurly\`: \`close\` is a
production that accepts both \`vccurly\` and a Happy \`error\`; that is,
if we encounter an error in parsing, we try exiting an indentation
context and trying again. This ensures, for example, that the top-level
context can be closed even if no virtual curly was output.

The top-level of a Haskell file does not automatically have a layout
context; when there is no \`module\` keyword, a context is implicitly
pushed using \`missing\_module\_keyword\`.

When writing grammars that accept semicolon-separated sequences, be sure
to include a rule allowing for trailing semicolons (see the previous
section), otherwise, you will reject layout.

Syntax extensions
-----------------

Many syntactic features must be enabled with a \`LANGUAGE\` flag, since
they could cause existing Haskell programs to stop compiling, as turn
some identifiers into keywords. We primarily affect this change of
behavior in the lexer, by turning on/off certain tokens. This is done
using predicates, which let Alex turn token rules on and off depending
on what extensions are enabled:

To add a new syntax extension, add a constructor to \`ExtBits\` and set
the bit appropriately in \`mkPState\`.

Pinned Objects
==============

The GC does not support pinning arbitrary objects. Only objects that
have no pointer fields can be pinned. Nevertheless, this is a useful
case, because we often want to allocate garbage-collectable memory that
can be passed to foreign functions via the FFI, and we want to be able
to run the GC while the foreign function is still executing (for a
\`safe\` foreign call). Hence, the memory we allocated must not move.

Bytestrings are currently allocated as pinned memory, so that the
bytestring contents can be passed to FFI calls if necessary.

The RTS provides an API for allocating pinned memory, in
[GhcFile(includes/rts/storage/GC.h)](GhcFile(includes/rts/storage/GC.h) "wikilink"):

This allocates memory from the given Capability's nursery.

Pinned objects work in the GC as follows:

`*Pinnedobjectsareallocatedintoablockoftheirown,notmixedupwithunpinnedobjects.`\
`*Theblockcontainingpinnedobjectsismarkedasa`*`large`
`block`*`` ,i.e.the`BF_LARGE`bitissetin`bd->flags`. ``\
`` *Whenencounteringaliveobjectina`BF_LARGE`block,theGCnevercopiestheobject,insteaditjustre-linksthewholeblockontothe`large_objects`listofthedestinationgeneration. ``\
`*TheGCdoesn'thavetoscavengethepinnedobject,sinceitdoesnotcontainanypointers.Thisisjustaswell,becausewecannotscanblocksforlivepinnedobjects,dueto[wiki:Commentary/Rts/Storage/Slopslop].Hencetherestrictionthatpinnedobjectsdonotcontainpointers.`

This means that using pinned objects may lead to memory fragmentation,
since a single pinned object keeps alive the whole block in which it
resides. If we were to implement a non-moving collector such as
\[wiki:Commentary/Rts/Storage/GC/Sweeping mark-region\], then we would
be able to reduce the impact of fragmentation due to pinned objects.

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>
Commentary/Rts/Storage/GC/Pinned

Overview
========

GHC is structured into two parts:

`` *The`ghc`package(insubdirectory`compiler`),whichimplementsalmostallGHC'sfunctionality.ItisanordinaryHaskelllibrary,andcanbeimportedintoaHaskellprogrambysaying`importGHC`. ``\
`` *The`ghc`binary(insubdirectory`ghc`)whichimportsthe`ghc`package,andimplementstheI/Oforthe`ghci`interactiveloop. ``

Here's an overview of the module structure of the top levels of GHC
library. (Note: more precisly, this is the plan. Currently the module
\`Make\` below is glommed into the giant module \`GHC\`.)

The driver pipeline
===================

The driver pipeline consist of a couple of phases that call other
programs and generate a series of intermediate files. Code responsible
for managing the order of phases is in
[GhcFile(compiler/main/DriverPhases.hs)](GhcFile(compiler/main/DriverPhases.hs) "wikilink"),
while managing the driver pipeline as a whole is coded in
[GhcFile(compiler/main/DriverPipeline.hs)](GhcFile(compiler/main/DriverPipeline.hs) "wikilink").
Note that driver pipeline is not the same thing as compilation pipeline:
the latter is part of the former.

Let's take a look at the overall structure of the driver pipeline. When
we compile or ("lhs" extension means that Literate Haskell is being
used) the following phases are being called (some of them depending on
additional conditions like file extensions or enabled flags):

`*Runthe`**`unlit`
`pre-processor`**`,``,toremovetheliteratemarkup,generating``.The``processorisaCprogramkeptin`[`GhcFile(utils/unlit)`](GhcFile(utils/unlit) "wikilink")`.`

`*Runthe`**`C`
`preprocessor`**`` ,`cpp`,(if ```isspecified),generating``.`

`*Run`**`the` `compiler`
`itself`**`.Thisdoesnotstartaseparateprocess;it'sjustacalltoaHaskellfunction.Thisstepalwaysgeneratesan[wiki:Commentary/Compiler/IfaceFiles`**`interface`
`file`**`]``,anddependingonwhatflagsyougive,italsogeneratesacompiledfile.AsGHCsupportsthreebackendcodegeneratorscurrently(anativecodegenerator,aCcodegeneratorandanllvmcodegenerator)thepossiblerangeofoutputsdependsonthebackendused.Allthreesupportassemblyoutput:`\
`*Objectcode:noflagsrequired,file``(supportedbyallthreebackends)`\
`*Assemblycode:flag``,file``(supportedbyallthreebackends)`\
`*Ccode:flags``,file``(onlysupportedbyCbackend)`

`*Inthe``case:`\
`*Runthe`**`C`
`compiler`**`` on`Foo.hc`,togenerate`Foo.s`. ``

`` *If`-split-objs`isinforce,runthe ``**`splitter`**`` on`Foo.s`.Thissplits`Foo.s`intolotsofsmallfiles.Theideaisthatthestaticlinkerwilltherebyavoidlinkingdeadcode. ``

`` *Runtheassembleron`Foo.s`,orif`-split-objs`isinforce,oneachindividualassemblyfile. ``

The compiler pipeline
=====================

The **compiler itself**, independent of the external tools, is also
structured as a pipeline. For details (and a diagram), see
\[wiki:Commentary/Compiler/HscMain\]

Video
=====

Video of compilation pipeline explanation from 2006: [Compilation
Pipeline](http://www.youtube.com/watch?v=dzSc8ACz_mw&list=PLBkRCigjPwyeCSD_DFxpd246YIF7_RDDI)
and interface files (17'30")

Platforms
=========

Please read \[wiki:CrossCompilation this wiki page\] on cross
compilation for a better understanding of the situation here. There are
three platforms of interest to GHC when compiling and running:

`*The`**`Build`**`platform.ThisistheplatformonwhichwearebuildingGHC.`\
`*The`**`Host`**`platform.ThisistheplatformonwhichwearegoingtorunthisGHCbinary,andassociatedtools.`\
`*The`**`Target`**`platform.ThisistheplatformforwhichthisGHCbinarywillgeneratecode.`

Limitations
-----------

At the moment, there is limited support for having different values for
build, host, and target. Please refer to the \[wiki:CrossCompilation
cross compilation\] page for more details. In particular:

The build platform is currently always the same as the host platform.
The build process needs to use some of the tools in the source tree, for
example ghc-pkg and hsc2hs.

If the target platform differs from the host platform, then this is
generally for the purpose of building .hc files from Haskell source for
porting GHC to the target platform. Full cross-compilation isn't
supported (yet).

Macros
------

In the compiler's source code, you may make use of the following CPP
symbols:

`*`*`xxx`*`` `_TARGET_ARCH` ``\
`*`*`xxx`*`` `_TARGET_VENDOR` ``\
`*`*`xxx`*`` `_TARGET_OS` ``\
`*`*`xxx`*`` `_HOST_ARCH` ``\
`*`*`xxx`*`` `_HOST_VENDOR` ``\
`*`*`xxx`*`` `_HOST_OS` ``

where *xxx* is the appropriate value: eg. \`i386\_TARGET\_ARCH\`.
However **GHC is moving away from using CPP for this purpose** in many
cases due to the problems it creates with supporting cross compilation.

So instead of it the new plan is to always build GHC as a cross compiler
and select the appropriate values and backend code generator to run and
runtime. For this purpose there is the Platform module
([GhcFile(compiler/utils/Platform.hs)](GhcFile(compiler/utils/Platform.hs) "wikilink")).
That contains various methods for querying the !DynFlags
([GhcFile(compiler/main/DynFlags.hs)](GhcFile(compiler/main/DynFlags.hs) "wikilink"))
value for what platform GHC is currently compiling for. You should use
these when appropriate over the CPP methods.

Pointer Tagging
===============

Paper: [Faster laziness using dynamic pointer
tagging](http://research.microsoft.com/pubs/67969/ptr-tagging.pdf)

In GHC we "tag" pointers to heap objects with information about the
object they point to. The tag goes in the low 2 bits (3 bits on a 64-bit
platform) of the pointer, which would normally be zero since heap
objects are always \[wiki:Commentary/Rts/Word word\]-aligned.

Meaning of the tag bits
-----------------------

The way the tag bits are used depends on the type of object pointed to:

`*Iftheobjectisa`**`constructor`**`,thetagbitscontainthe`*`constructor`
`tag`*`,ifthenumberof`\
`constructorsinthedatatypeislessthan4(lessthan8ona64-bitplatform).Ifthenumberof`\
`constructorsinthedatatypeisequaltoormorethan4(resp8),thenthetagbitshavethevalue1,andtheconstructortag`\
`isextractedfromtheconstructor'sinfotableinstead.`

`*Iftheobjectisa`**`function`**`,thetagbitscontainthe`*`arity`*`ofthefunction,ifthearityfits`\
`inthetagbits.`

`*Forapointertoanyotherobject,thetagbitsarealwayszero.`

Optimisations enabled by tag bits
---------------------------------

The presence of tag bits enables certain optimisations:

`*Inacase-expression,ifthevariablebeingscrutinisedhasnon-zerotagbits,thenweknow`\
`thatitpointsdirectlytoaconstructorandwecanavoid`*`entering`*`ittoevaluateit.`\
`Furthermore,fordatatypeswithonlyafewconstructors,thetagbitswilltellus`*`which`*\
`constructoritis,eliminatingafurthermemoryloadtoextracttheconstructortagfromthe`\
`infotable.`

`*Ina[wiki:Commentary/Rts/HaskellExecution/FunctionCalls#Genericapplygenericapply],ifthefunctionbeingappliedhasatagvaluethatindicatesithasexactlythe`\
`rightarityforthenumberofargumentsbeingapplied,wecanjumpdirectlytothefunction,insteadof`\
`inspectingitsinfotablefirst.`

Pointer-tagging is a fairly significant optimisation: we measured 10-14%
depending on platform. A large proportion of this comes from eliminating
the indirect jumps in a case expression, which are hard to predict by
branch-prediction. The paper has full results and analysis.

Garbage collection with tagged pointers
---------------------------------------

The \[wiki:Commentary/Rts/Storage/GC garbage collector\] maintains tag
bits on the pointers it traverses. This is easier, it turns out, than
*reconstructing* tag bits. Reconstructing tag bits would require that
the GC knows not only the tag of the constructor (which is in the info
table), but also the family size (which is currently not in the info
table), since a constructor from a large family should always have tag
1. To make this practical we would probably need different closure types
for "small family" and "large family" constructors, and we already
subdivide the constructor closures types by their layout.

Additionally, when the GC eliminates an indirection it takes the tag
bits from the pointer inside the indirection. Pointers to indirections
always have zero tag bits.

Invariants
----------

Pointer tagging is *not* optional, contrary to what the paper says. We
originally planned that it would be: if the GC threw away all the tags,
then everything would continue to work albeit more slowly. However, it
turned out that in fact we really want to assume tag bits in some
places:

`*Inthecontinuationofanalgebraiccase,R1isassumedtagged`\
`*Onentrytoanon-top-levelfunction,R1isassumedtagged`

If we don't assume the value of the tag bits in these places, then extra
code is needed to untag the pointer. If we can assume the value of the
tag bits, then we just take this into account when indexing off R1.

This means that everywhere that enters either a case continuation or a
non-top-level function must ensure that R1 is correctly tagged. For a
case continuation, the possibilities are:

`*thescrutineeofthecasejumpsdirectlytothealternativeifR1isalreadytagged.`\
`*theconstructorentrycodereturnstoanalternative.Thiscodeaddsthecorrecttag.`\
`*ifthecasealternativefailsaheaporstackcheck,thentheRTSwillre-enterthealternativeafter`\
`GC.Inthiscase,ourre-entryarrangestoentertheconstructor,sowegetthecorrecttagby`\
`virtueofgoingthroughtheconstructorentrycode.`

For a non-top-level function, the cases are:

`` *unknownfunctionapplicationgoesvia`stg_ap_XXX`(see[wiki:Commentary/Rts/HaskellExecution/FunctionCalls#GenericapplyGenericApply]). ``\
`ThegenericapplyfunctionsmustthereforearrangetocorrectlytagR1beforeenteringthefunction.`\
`*Aknownfunctioncanbeentereddirectly,ifthecallismadewithexactlytherightnumberofarguments.`\
`*Ifafunctionfailsitsheapcheckandreturnstotheruntimetogarbagecollect,onre-entrytheclosure`\
`pointermustbestilltagged.`\
`*thePAPentrycodejumpstothefunction'sentrycode,soitmusthaveataggedpointertothefunction`\
`closureinR1.WethereforeassumethataPAPalwayscontainsataggedpointertothefunctionclosure.`

In the second case, calling a known non-top-level function must pass the
function closure in R1, and this pointer *must* be correctly tagged. The
code generator does not arrange to tag the pointer before calling the
function; it assumes the pointer is already tagged. Since we arrange to
tag the pointer when the closure is created, this assumption is normally
safe. However, if the pointer has to be saved on the stack, say across a
call, then when the pointer is retrieved again we must either retag it,
or be sure that it is still tagged. Currently we do the latter, but this
imposes an invariant on the garbage collector: all tags must be retained
on non-top-level function pointers.

Pointers to top-level functions are not necessarily tagged, because we
don't always know the arity of a function that resides in another
module. When optimisation is on, we do know the arities of external
functions, and this information is indeed used to tag pointers to
imported functions, but when optimisation is off we do not have this
information. For constructors, the interface doesn't contain information
about the constructor tag, except that there may be an unfolding, but
the unfolding is not necessarily reliable (the unfolding may be a
constructor application, but in reality the closure may be a CAF, e.g.
if any of the fields are references outside the current shared library).

Compacting GC
-------------

Compacting GC also uses tag bits, because it needs to distinguish
between a heap pointer and an info pointer quickly. The compacting GC
has a complicated scheme to ensure that pointer tags are retained, see
the comments in
[GhcFile(rts/sm/Compact.c)](GhcFile(rts/sm/Compact.c) "wikilink").

Dealing with tags in the code
-----------------------------

Every time we dereference a pointer to a heap object, we must first zero
the tag bits. In the RTS, this is done with the inline function
(previously: macro) \`UNTAG\_CLOSURE()\`; in \`.cmm\` code this is done
with the \`UNTAG()\` macro. Surprisingly few places needed untagging to
be added.

Position-Independent Code and Dynamic Linking
=============================================

We need to generate position-independent code on most platforms when we
want our code to go into dynamic libraries (also referred to as shared
libraries or DLLs). On some platforms (AIX, powerpc64-linux,
x86\_64-darwin), PIC is required for all code.

To access things defined in a dynamic library, we might need to do
special things, such as look up the address of the imported thing in a
table of pointers, depending on what platform we are on.

How to access symbols
---------------------

A C compiler is in an unfortunate position when generating PIC code, as
it does not have any hints, whether an accessed symbol ends up in the
same dynamic library or if it is truely an external symbol (from the
dynamic library point of view). It can only generate non-PIC access for
symbols generated within the same object file. In Haskell, we can do
better as we assume all package code to end up in a single dynamic
library. Hence, all intra-package symbol accesses can be generated as
code that does direct access. For all inter-package accesses (package
haskell98 accessing symbols in package base, e.g.), we have to generate
PIC code. For the following we establish the following:

`*`*`object-local`
`symbols`*`,symbolswithinthesameobjectfile.Alwaysgeneratedirectaccess.`\
`*`*`package-local`
`symbols`*`,symbolswithinthesameHaskellpackage.TheNCGcangeneratedirectaccesscode,Ccompilerscan't.`\
`*`*`local` `symbols`*`,eitherobject-localorpackage-local.`\
`*`*`global`
`symbols`*`,symbolindifferentlibraries/packages.AlwaysgeneratePIC.`

CLabel.labelDynamic
-------------------

On most platforms, we can access any global symbol as if it was imported
from a dynamic library; this usually means a small performance hit (an
extra pointer dereference), but it is otherwise harmless. On some
platforms, we have to access all global symbols this way. On Windows, we
must know exactly which symbols are DLL-imported and which aren't.

Module \`CLabel\` contains a function \`labelDynamic :: CLabel -&gt;
Bool\` which is supposed to know whether a \`CLabel\` is imported from a
dynamic library. On Windows, this function needs to be exact; everywhere
else, we don't mind the occasional false positive.

Info Tables
-----------

Info tables are in the text segment, which is supposed to be read-only
and position-independent. Therefore, an info table *must not* contain
any absolute address; instead, all addresses in info tables are instead
encoded as relative offsets from the info label.

Note that this is done even when we are generating code that is
otherwise position-dependent, in order to preserve binary compatibility
between PIC and non-PIC.

It is not possible to generate those relative references from C code, so
for the via-C compilation route, we pretty-print these relative
references (\`CmmLabelDiffOff\` in cmm) as absolute references and have
the mangler convert them to relative references again.

Imported labels in SRTs (Windows)
---------------------------------

Windows doesn't support references to imported labels in the data
segment; on other platforms, the dynamic linker will just relocate the
pointers in the SRTs to point to the right symbols. There is a hack in
the code that tries to work around it; it might be bitrotted, and it
might have been made unnecessary by the GNU linker's new auto-import on
Windows.

PIC and dynamic linking support in the NCG
------------------------------------------

The module \`PositionIndependentCode\` lies at the heart of PIC and
dynamic linking support in the native code generator.

The basic idea is to call a function \`cmmMakeDynamicReference\` for all
labels accessed from the code during the cmm-to-cmm transformation
phase. This function will decide on the appropriate way to access the
given label for the current platform and the current combination of
-fPIC and -dynamic flags.

We extend Cmm and the \`CLabel\` module by a few things to allow us to
express all the different things that occur on different platforms:

The \`Cmm.GlobalReg\` datatype has a constructor \`PicBaseReg\`. This
PIC base register is the register relative to which position-independent
references are calculated. This can be a general-purpose register that
is allocated on a per-!CmmProc basis, or it can be a dedicated register,
like the instruction pointer \`%rip\` on x86\_64.

How things are done on different platforms
------------------------------------------

This section is a survey of how PIC and dynamic linking works on
different platforms. There are small snippets of assembly code for
several platforms, platforms that are similar to other platforms are
left out (e.g. powerpc-darwin is left out, because the logic is the same
as for i386-darwin). I hope the reader will not be too confused by
irrelevant differences between the platforms, such as the fact that
Darwin and Windows prefix all symbols with an underscore, and Linux
doesn't.

### Position dependent code

In the absence of PIC and dynamic linking, things are simple; when we
use a label in assembly code, the linker will make sure it points to the
right place.

Now, to access a symbol \`xfoo\` that has been imported from a dynamic
library, we do not want to mention the address of \`xfoo\` in the text
section, because it would need to be modified at load-time.

One solution is to allocate a pointer to the imported symbol in a
writable section and have the dynamic linker fill in this pointer table.
The pointer table itself resides at a statically known address. The
\_\_imp\_\_\* symbols on Windows are automatically generated by the
linker.

On Mac OS X, the same system is used for data imports, but this time we
have to define the symbol pointers ourselves. For references to code,
there is an additional mechanism available; we can jump to a small piece
of stub code that will resolve the symbol the first time it is used, in
order to reduce application load times. Unfortunately, everything on Mac
OS X requires 16-byte stack alignment, even the dynamic linker, so we
cannot use this for a tail call.

In theory, dynamic linking is transparent to position-dependent code on
Linux, i.e. the code for accessing imported labels should look exactly
the same as for non-imported labels. Unfortunately, things just don't
work as they should for strange stuff like info tables.

When the ELF static linker finds a jump or call to an imported symbol,
it automatically redirects the jump or call to a linker generated code
stub (in the so-called procedure linkage table, or PLT). The linker then
considers the label to be a code label and redirects all further
references to the label to the code stub, even if they are data
references. If this ever happens to an info label, our program will
crash, as there is no info table in front of the code stub.

When the ELF static linker finds a data reference to an imported symbol
(that it doesn't consider a code label), it allocates space for that
symbol in the executable's data section and issues an \`R\_COPY\`
relocation, which instructs the dynamic linker to copy the (initial)
contents of the symbol to its new place in the executable's image. All
references to the symbol from the dynamic library are relocated to point
to the symbol's new location, instead.

If \`R\_COPY\` is ever used for an info label, our program will also
crash, because the data we're interested in is \*before\* the info label
and is not copied to the symbol's new home.

Fortunately, if the static linker finds a pointer to an imported symbol
in a writable section, it just instructs the dynamic linker to update
that pointer to the symbols address, without doing anything "funny". We
can therefore work around these problems.

The workaround is inspired by the position-independent code that GCC
generates for powerpc-linux, a platform that is amazingly broken.

Things look pretty much the same on x86\_64-linux, powerpc-linux and
powerpc-darwin; PowerPC has the added handicap that it takes two
instructions to load a 32 bit quantity into a register. On
x86\_64-darwin, powerpc64-linux and all versions of AIX, PIC is
*required*.

### Position independent code

First, let it be said that there is no such thing as
position-independent code on Windows. The dynamic linker will just
patiently relocate all dynamic libraries that are not loaded at their
preferred base address. On all other platforms, PIC is at least strongly
recommended for dynamic libraries.

In an ideal world, there would be assembler instructions for referring
to things via an offset from the current instruction pointer. Jump
instructions are ip-relative on all platforms that GHC runs on, but for
data accesses, only x86\_64 is this ideal world.

On x86\_64, on both Linux and Mac OS X, we can use \`foo(%rip)\` to
encode an instruction pointer relative data reference to \`foo\`, and
\`foo@GOTPCREL(%rip)\` to encode an instruction pointer relative
referece to a linker-generated symbol pointer for symbol \`foo\`. A
linker-generated code stub for imported code can be accessed by
appending \`@PLT\` to the label on Linux, and is used implicitly when
necessary on Mac OS X.

Again, we have to avoid the code stubs for tail-calls and use the symbol
pointer instead, because there is a stack alignment requirement.

Other platforms are not nearly as nice; i386 and powerpc\[64\] do not
have a way of accessing the current instruction pointer or referring to
data relative to it. The \*only\* way to get at the current instruction
pointer is to issue a call instruction. To generate PIC code, we have to
do just that at the beginning of each function.

On Darwin, things are relatively straightforward:

There is one more small additional complication on Darwin. The assembler
doesn't support label difference expressions involving labels not
defined in the same source file, so we have to treat all symbols not
defined in the same source file as dynamically imported.

On Linux, we need to first calculate the address of the Global Offset
Table (GOT) and then use \`bar@GOT\` to refer to symbol pointers and
\`bar@GOTOFF\` to refer to a local symbol relative to the GOT. Also, the
linker-generated code-stubs (\`xfoo@PLT\`) require the address of the
GOT to be in register \`%ebx\` when they are invoked. The NCG currently
doesn't do this, so we avoid code stubs altogether on i386.

**To be done:** powerpc-linux, AIX/powerpc64-linux

Linking on ELF
--------------

To generate a DSO on ELF platform, we use GNU ld. Except for
\`-Bsymbolic\`, ld is invoked regularly with the \`-shared\` option, and
\`-o\` pointing to the output DSO file followed objects that in its sum
compose an entire package. In Haskell, we assume that there is a
one-to-one mapping from packages to DSOs. So, all parts of the base
package will end up in a libHSbase.so. As intra-package references are
not generated as PIC code, we have to supply all objects that make up a
package, so that ld is able to resolve these references before writing a
(.text) relocation free DSO library file. To enable these cross-object
relocations GNU ld needs \`-Bsymbolic\`.

Mangling dynamic library names
------------------------------

As Haskell DSOs might end up in standard library paths, and as they
might not be compatible among compilers and compiler version, we need to
mangle their names to include the compiler and its version.

The scheme is
libHS*<package>*-*<package-version>*-*<compiler><compilerversion>*.so.
E.g. libHSbase-2.1-ghc6.6.so

GHC Commentary: The C code generator
====================================

Source:
[GhcFile(compiler/cmm/PprC.hs)](GhcFile(compiler/cmm/PprC.hs) "wikilink")

This phase takes \[wiki:Commentary/Compiler/CmmType Cmm\] and generates
plain C code. The C code generator is very simple these days, in fact it
can almost be considered pretty-printing. It is only used for
unregisterised compilers.

Header files
------------

GHC was changed (from version 6.10) so that the C backend no longer uses
header files specified by the user in any way. The \`c-includes\` field
of a \`.cabal\` file is ignored, as is the \`-\#include\` flag on the
command-line. There were several reasons for making this change:

This has several advantages:

`*ViaCcompilationisconsistentwiththeotherbackendwithrespecttoFFIdeclarations:`\
`allbindtotheABI,nottheAPI.`\
``\
`*foreigncallscannowbeinlinedfreelyacrossmoduleboundaries,since`\
`aheaderfileisnotrequiredwhencompilingthecall.`\
``\
`*bootstrappingviaCwillbemorereliable,becausethisdifference`\
`inbehaviorbetweenthetwobackendshasbeenremoved.`\
``

There are some disadvantages:

`*wegetnocheckingbytheCcompilerthattheFFIdeclaration`\
`iscorrect.`

`*wecan'tbenefitfrominlinedefinitionsinheaderfiles.`\
``

Prototypes
----------

When a label is referenced by an expression, the compiler needs to know
whether to declare the label first, and if so, at what type.

C only lets us declare an external label at one type in any given source
file, even if the scopes of the declarations don't overlap. So we either
have to scan the whole code to figure out what the type of each label
should be, or we opt for declaring all labels at the same type and then
casting later. Currently we do the latter.

`*alllabelsreferencedasaresultofanFFIdeclaration`\
`` aredeclaredas`externStgWord[]`,includingfunctionlabels. ``\
`Ifthelabeliscalled,itisfirstcasttothecorrect`\
`functiontype.Thisisbecausethesamelabelmightbe`\
`referredtobothasafunctionandanuntypeddatalabelin`\
`thesamemodule(e.g.Foreign.Marsal.Allocrefersto"free"`\
`thisway).`

`*Anexceptionismadetotheaboveforfunctionsdeclaredwith`\
`` the`stdcall`callingconventiononWindows.Thesefunctionsmust ``\
`` bedeclaredwiththe`stdcall`attributeandafunctiontype, ``\
`` otherwisetheCcompilerwon'taddthe`@n`suffixtothesymbol. ``\
`` Wecan'taddthe`@n`suffixourselves,becauseitisillegal ``\
`syntaxinC.However,wealwaysdeclaretheselabelswiththe`\
`` type`void(*)(void)`,toavoidconflictsifthesamefunction ``\
`` iscalledatdifferenttypesinonemodule(see`Graphics.Win32.GDI.HDC.SelectObject`). ``

`` *Anotherexceptionismadeforfunctionsthataremarked`neverreturns`inC--.We ``\
`` havetoputan`__attribute__((noreturn))`onthedeclarationforthesefunctions, ``\
`anditonlyworksifthefunctionisdeclaredwithaproperfunctiontypeand`\
`calledwithoutcastingitto/fromapointer.Soonlythecorrectprototype`\
`willdohere.`

`*allRTSsymbolsalreadyhavedeclarations(mostlywiththecorrect`\
`type)in`[`GhcFile(includes/StgMiscClosures.h)`](GhcFile(includes/StgMiscClosures.h) "wikilink")`,sonodeclarationsaregenerated.`

`*certainlabelsareknowntohavebeendefinedearlierinthesamefile,`\
`soadeclarationcanbeomitted(e.g.SRTlabels)`

`` *certainmathfunctions(`sin()`,`cos()`etc.)arealreadydeclaredbecause ``\
`we#includemath.h,sowedon'temitdeclarationsforthese.Weneed`\
`to#includemath.hbecausesomeofthesefunctionshaveinline`\
`definitions,andwegetterriblecodeotherwise.`

When compiling the RTS cmm code, we have almost no information about
labels referenced in the code. The only information we have is whether
the label is defined in the RTS or in another package: a label that is
declared with an import statement in the .cmm file is assumed to be
defined in another package (this is for dynamic linking, where we need
to emit special code to reference these labels).

For all other labels referenced by RTS .cmm code, we assume they are RTS
labels, and hence already declared in
[GhcFile(includes/StgMiscClosures.h)](GhcFile(includes/StgMiscClosures.h) "wikilink").
This is the only choice here: since we don't know the type of the label
(info, entry etc.), we can't generate a correct declaration.

[PageOutline](PageOutline "wikilink")

Primitive Operations (!PrimOps)
===============================

!PrimOps are functions that cannot be implemented in Haskell, and are
provided natively by GHC. For example, adding two values is provided as
the !PrimOp , and allocating a new mutable array is the !PrimOp .

!PrimOps are made available to Haskell code through the virtual module .
This module has no implementation, and its interface never resides on
disk: if is imported, we use a built-in value - see in
[GhcFile(compiler/iface/LoadIface.hs)](GhcFile(compiler/iface/LoadIface.hs) "wikilink").

It would also be useful to look at the
\[wiki:Commentary/Compiler/WiredIn Wired-in and known-key things\] wiki
page to understand this topic.

The primops.txt.pp file
-----------------------

The file
[GhcFile(compiler/prelude/primops.txt.pp)](GhcFile(compiler/prelude/primops.txt.pp) "wikilink")
includes all the information the compiler needs to know about a !PrimOp,
bar its actual implementation. For each !PrimOp, lists:

`*Itsname,asitappearsinHaskellcode(eg.int2Integer#)`\
`*Itstype`\
`*ThenameofitsconstructorinGHC's``datatype.`\
`*Variousproperties,suchaswhethertheoperationiscommutable,orhassideeffects.`

For example, here's the integer multiplication !PrimOp:

The file is processed first by CPP, and then by the program (see
[GhcFile(utils/genprimopcode)](GhcFile(utils/genprimopcode) "wikilink")).
generates the following bits from :

`*Variousfilesthatare``dinto`[`GhcFile(compiler/prelude/PrimOp.hs)`](GhcFile(compiler/prelude/PrimOp.hs) "wikilink")`,`\
`containingdeclarationsofdatatypesandfunctionsdescribingthe!PrimOps.See`\
``[`GhcFile(compiler/Makefile)`](GhcFile(compiler/Makefile) "wikilink")`.`

`*``,afilethatcontains(curried)wrapper`\
`functionsforeachofthe!PrimOps,sothattheyareaccessiblefrombyte-code,and`\
`sothatthe[wiki:Commentary/Rts/Interpreterbyte-codeinterpreter]doesn'tneedtoimplementany!PrimOpsatall:it`\
`justinvokesthecompiledonesfrom``.`

`*``,asourcefilecontainingdummydeclarationsfor`\
`allthe!PrimOps,solelysothatHaddockcanincludedocumentationfor`\
`initsdocumentationforthe``package.Thefile``isnever`\
`actuallycompiled,onlyprocessedbyHaddock.`

Note that if you want to create a polymorphic primop, you need to return
, not .

Implementation of !PrimOps
--------------------------

!PrimOps are divided into two categories for the purposes of
implementation: inline and out-of-line.

### Inline !PrimOps

Inline !PrimOps are operations that can be compiled into a short
sequence of code that never needs to allocate, block, or return to the
scheduler for any reason. An inline !PrimOp is compiled directly into
\[wiki:Commentary/Rts/Cmm Cmm\] by the
\[wiki:Commentary/Compiler/CodeGen code generator\]. The code for doing
this is in
[GhcFile(compiler/codeGen/StgCmmPrim.hs)](GhcFile(compiler/codeGen/StgCmmPrim.hs) "wikilink").

### Out-of-line !PrimOps

All other !PrimOps are classified as out-of-line, and are implemented by
hand-written C-- code in the file
[GhcFile(rts/PrimOps.cmm)](GhcFile(rts/PrimOps.cmm) "wikilink"). An
out-of-line !PrimOp is like a Haskell function, except that

`*!PrimOpscannotbepartiallyapplied.Callstoall!PrimOpsaremadeatthecorrectarity;thisisensuredby`\
`the[wiki:Commentary/Compiler/HscMainCorePrep]pass.`

`*Out-of-line!PrimOpshaveaspecial,fixed,[wiki:Commentary/Rts/HaskellExecution#CallingConventioncallingconvention]:`\
`allarguments`\
`areinthe[wiki:Commentary/Rts/HaskellExecution#Registersregisters]R1-R8.Thisistomakeiteasytowritethe`\
`C--codeforthese!PrimOps:wedon'thavetowritecodeformultiplecallingconventions.`

It's possible to provide inline versions of out-of-line !PimOps. This is
useful when we have enough static information to generated a short, more
efficient inline version. For example, a call to can be implemented more
efficiently as an inline !PrimOp as the heap check for the array
allocation can be combined with the heap check for the surrounding code.
See \`shouldInlinePrimOp\` in
[GhcFile(compiler/codeGen/StgCmmPrim.hs)](GhcFile(compiler/codeGen/StgCmmPrim.hs) "wikilink").

### Foreign out-of-line !PrimOps and \`foreign import prim\`

A new and somewhat more flexible form of out-of-line !PrimOp is the
foreign out-of-line !PrimOp. These are essentially the same but instead
of their Cmm code being included in the RTS, they can be defined in Cmm
code in any package and instead of knowledge of the !PrimOp being baked
into the compiler, they can be imported using special FFI syntax:

The string (e.g. "int2Integerzh") is the linker name of the Cmm
function. Using this syntax requires the extensions
\`ForeignFunctionInterface\`, \`GHCForeignImportPrim\`, \`MagicHash\`,
\`UnboxedTuples\` and \`UnliftedFFITypes\`. The current type restriction
is that all arguments and results must be unlifted types, with two
additional possibilities: An argument may (since GHC 7.5) be of type
\`Any\` (in which case the called function will receive a pointer to the
heap), and the result type is allowed to be an unboxed tuple. The
calling convention is exactly the same as for ordinary out-of-line
primops. Currently it is not possible to specify any of the !PrimOp
attributes.

The \`integer-gmp\` package now uses this method for all the primops
that deal with GMP big integer values. The advantage of using this
technique is that it is a bit more modular. The RTS does not need to
include all the primops. For example in the integer case the RTS no
longer needs to link against the GMP C library.

The future direction is to extend this syntax to allow !PrimOp
attributes to be specified. The calling convention for primops and
ordinary compiled Haskell functions may be unified in future and at that
time it the restriction on using only unlifted types may be lifted.

It has been suggested that we extend this !PrimOp definition and import
method to cover all !PrimOps, even inline ones. This would replace the
current \`primops.txt.pp\` system of builtin !PrimOps. The inline
!PrimOps would still be defined in the compiler but they would be
imported in any module via \`foreign import prim\` rather than appearing
magically to be exported from the \`GHC.Prim\` module. Hugs has used a
similar system for years (with the syntax \`primitive seq :: a -&gt; b
-&gt; b\`).

Adding a new !PrimOp
--------------------

To add a new primop, you currently need to update the following files:

`*`[`GhcFile(compiler/prelude/primops.txt.pp)`](GhcFile(compiler/prelude/primops.txt.pp) "wikilink")`,whichincludesthe`\
`typeoftheprimop,andvariousotherproperties.Syntaxand`\
`examplesareinthefile.`

`*iftheprimopisinline,then:`\
``[`GhcFile(compiler/codeGen/StgCmmPrim.hs)`](GhcFile(compiler/codeGen/StgCmmPrim.hs) "wikilink")`definesthetranslationof`\
`theprimopinto``.`\
``\
`*foranout-of-lineprimop:`\
`*`[`GhcFile(includes/stg/MiscClosures.h)`](GhcFile(includes/stg/MiscClosures.h) "wikilink")`(justaddthedeclaration),`\
`*`[`GhcFile(rts/PrimOps.cmm)`](GhcFile(rts/PrimOps.cmm) "wikilink")`(implementithere)`\
`*`[`GhcFile(rts/Linker.c)`](GhcFile(rts/Linker.c) "wikilink")`(declarethesymbolforGHCi)`

`*foraforeignout-of-lineprimopYoudonotneedtomodifythertsorcompileratall.`\
`` *`yourpackage/cbits/primops.cmm`:implementyourprimopshere.Youhavetoarrangeforthe.cmmfiletobecompiledandlinkedintothepackage.TheGHCbuildsystemhassupportforthis.Cabaldoesnotyet. ``\
`` *`yourpackage/TheCode.hs`:use`foreignimportprim`toimporttheprimops. ``

In addition, if new primtypes are being added, the following files need
to be updated:

`*`[`GhcFile(utils/genprimopcode/Main.hs)`](GhcFile(utils/genprimopcode/Main.hs) "wikilink")`--extendppType::Type->Stringfunction`\
``\
`*`[`GhcFile(compiler/prelude/PrelNames.hs)`](GhcFile(compiler/prelude/PrelNames.hs) "wikilink")`--addanewuniqueidusingmkPreludeTyConUnique`

`*`[`GhcFile(compiler/prelude/TysPrim.hs)`](GhcFile(compiler/prelude/TysPrim.hs) "wikilink")`--therearearaftofchangeshere;youneedtocreate``,``and``variables.ThemostimportantthingtomakesureyougetrightiswhenyoumakeaPrimTyCon,youpickthecorrect``foryourtype.Forexample,ifyou`

Profiling
=========

GHC includes two types of profiling: cost-centre profiling and
ticky-ticky profiling. Additionally, HPC code coverage is not
"technically" profiling, but it uses a lot of the same mechanisms as
cost-centre profiling (you can read more about it at
\[wiki:Commentary/Hpc\]).

Cost-centre profiling operates at something close to the source level,
and ticky-ticky profiling operates at something much closer to the
machine level. This means that the two types of profiling are useful for
different tasks. Ticky-ticky profiling is mainly meant for compiler
implementors, and cost-centre profiling for mortals. However, because
cost-centre profiling operates at a high level, it can be difficult (if
not impossible) to use it to profile optimized code. Personally, I
(Kirsten) have had a lot of success using cost-centre profiling to find
problems that were due to my own bad algorithms, but less success once I
was fairly sure that I wasn't doing anything obviously stupid and was
trying to figure out why my code didn't get optimized as well as it
could have been.

You can't use cost-centre profiling and ticky-ticky profiling at the
same time; in the past, this was because ticky-ticky profiling relied on
a different closure layout, but now that's no longer the case. You
probably can't use both at the same time as it is unless you wanted to
modify the build system to allow using way=p and way=t at the same time
to build the RTS. I haven't thought about whether it would make sense to
use both at the same time.

Cost-centre profiling
---------------------

Cost-center profiling in GHC, e.g. of SCCs, consists of the following
components:

`*Data-structuresforrepresentingcost-centresin`[`GhcFile(compiler/profiling/CostCentre.lhs)`](GhcFile(compiler/profiling/CostCentre.lhs) "wikilink")`.`\
`*Front-endsupportin`[`GhcFile(compiler/deSugar/DsExpr.lhs)`](GhcFile(compiler/deSugar/DsExpr.lhs) "wikilink")`,forconverting``pragmaintothe``constructorinCore.`\
`*Modificationstooptimizationbehaviorin`[`GhcFile(compiler/coreSyn/CoreUtils.lhs)`](GhcFile(compiler/coreSyn/CoreUtils.lhs) "wikilink")`and`[`GhcFile(compiler/coreSyn/CorePrep.lhs)`](GhcFile(compiler/coreSyn/CorePrep.lhs) "wikilink")`topreventoptimizationswhichwouldresultinmisleadingprofileinformation.MostofthisistohandlethefactthatSCCsalsocountentries(tickishCounts,alsoappliesto[wiki:Commentary/Hpc]);otherwisetheonlyrelevantoptimizationisavoidingfloatingexpressionsoutofSCCs.Notethatthesimplifieralsohas"ticks"(soitcandecidewhentostopoptimizing);thesearenotthesamethingatall.`\
`*The``constructorinSTG,andcodegenerationforit`[`GhcFile(compiler/codeGen/StgCmmProf.hs)`](GhcFile(compiler/codeGen/StgCmmProf.hs) "wikilink")\
`*ApassoverSTGin`[`GhcFile(compiler/profiling/SCCfinal.lhs)`](GhcFile(compiler/profiling/SCCfinal.lhs) "wikilink")`tocollectcostcentressothattheycanbestaticallydeclaredby`[`GhcFile(compiler/profiling/ProfInit.hs)`](GhcFile(compiler/profiling/ProfInit.hs) "wikilink")`,andaddextraSCCsinthecaseof``;seealso`[`GhcFile(compiler/profiling/NOTES)`](GhcFile(compiler/profiling/NOTES) "wikilink")\
`*Code-generationforsettinglabelsfoundin`[`GhcFile(compiler/codeGen/StgCmmProf.hs)`](GhcFile(compiler/codeGen/StgCmmProf.hs) "wikilink")`,inparticularsavingandrestoringCClabelsandwellascountingticks;notethatcost-centresevengettheirownconstructorinC--asCC_Labels(cost-centrelabels).`\
`*Runtimesupportforinitializingandmanipulatingtheactualruntime``structswhichstoreinformation,in`[`GhcFile(rts/Profiling.c)`](GhcFile(rts/Profiling.c) "wikilink")`;headersarelocatedin`[`GhcFile(includes/rts/prof/CCS.h)`](GhcFile(includes/rts/prof/CCS.h) "wikilink")

Ticky-ticky profiling
---------------------

Ticky-ticky profiling is very simple (conceptually): instrument the C
code generated by GHC with a lot of extra code that updates counters
when various (supposedly) interesting things happen, and generate a
report giving the values of the counters when your program terminates.
GHC does this instrumentation for you when you compile your program with
a special flag. Then, you use another flag to tell the RTS to generate
the profiling report.

You might want to use ticky-ticky profiling for one of the following two
reasons:

-   You are an implementor trying to understand the effect of an
    optimization in GHC more precisely.

<!-- -->

-   You are a user trying to observe the behavior of your programs with
    optimization turned on. GHC doesn't do certain transformations in
    the presence of cost centres, so cost-centre profiling can be less
    than accurate if you're trying to understand what really happens
    when you're compiling with .

I won't necessarily try to argue that ticky-ticky is useful at all for
the second group of people, but it's better than nothing, and perhaps
the ticky-ticky data could be used to build a better profiler.

For more info, including HOWTO details, see
\[wiki:Debugging/TickyTicky\]. like "computer is a net", nowadays
language is a library. there is nothing exceptional in C++ and Java
languages except for their huge library codebase that makes them so
widely appreciated

while it's impossible for Haskell to have the same level of libraries
maturity, we can try to do our best. Libraries was considered so
important, that in H98 report libs required more pages than language
itself. But, really, all libraries described there together is
appropriate only for learning and small programs - to do real work, we
need even much, much more

fortunately, now we have large enough set of libs. moreover, this set
grows each year. but these libs don't have official/recommended status.
now we have two languages - H98 as reported with its bare libs, which is
appropriate only for teaching, and real Haskell language with many
extensions and rich set of libs, used to develop real programs

with a language itself, now we go to standardize current practice and
include into language definition all popular extensions. this will close
the gap between standard and practice. Haskell' committee also plan to
define new version of standard Haskell library. but what a library can
be defined in this way? slightly extended version of standard Haskell98
lib? or, if it will be significantly extended - how much time this work
will require and isn't that a duplication of work done at libraries
list?

i propose not to try to define reality, but accept existing one and join
committee's work on new library definition with a current discussion of
core libraries, which should define a set of libs available on any
Haskell compiler on any platform - aren't goals the same?

instead of providing rather small and meaningless standard Haskell
library, now we can just include in Report docs existing and widely used
libs, such as Network, mtl and so on. This will mean that language,
defined in Haskell standard, can be used to write real programs, which
will be guaranteed to run in any Haskell environment.

of course, this mind game can't change anything in one moment. but it
will change \*accents\*

first, Haskell with its libraries will become language for a real work.
such extended language isn't small nor easy to master in full, but it is
normal for any mature programming environment. people learning Haskell
should select in which area they need to specialize - be it gaming or
web service development, and study appropriate subset of libs. people
teaching Haskell now can show how \*standard\* Haskell may be used to
solve real world problems, and this should change treatment of Haskell
as academic language. also, we may expect that books teaching Haskell
will start to teach on using standard libs, while their authors now
don't consider teaching for non-standard libs

second, by declaring these libs as standard ones we create sort of
lingua franca, common language spoken by all Haskell users. for example,
now there are about 10 serialization libs. by declaring one of them as
standard, we will make choice simpler for most of users (who don't need
very specific features) and allow them to speak in common language. in
other words, number of Haskell libs is so large now that we should
define some core subset in order to escape syndrome of Babel tower.
defining core libraries set is just sharing knowledge that some
libraries are more portable, easier to use, faster and so on, so they
become more popular than alternatives in this area

third. now we have Cabal that automates installation of any lib. next
year we will got Hackage that automates downloading and checking
dependencies. but these tools still can't replace a rich set of standard
libs shipped with compiler. there are still many places and social
situations where Internet downloading isn't available. Compiler can be
sold on CD, transferred on USB stick. and separate Haskell libs probably
will be not included here. Standard libraries bundled with compiler will
ensure that at least this set of libs will be available for any haskell
installation. Internet access shouldn't be a precondition for Haskell
usage! :)

fourth. now there is tendency to write ghc-specific libs. by defining
requirements to the standard libs we may facilitate development of more
portable, well documented and quick-checked ones. or may be some good
enough libraries will be passed to society which will "polish" them in
order to include in the set. anyway, i hope that \*extensible\* set of
standard libraries with a published requirements to such libs would
facilitate "polishing" of all Haskell libs just because ;)

and this leads us to other question - whether this set and API of each
library should be fixed in language standard or it can evolve during the
time?...

[PageOutline](PageOutline "wikilink")

, , and 
========

When the parser parses an identifier, it generates a . A is pretty much
just a string, or a pair of strings, for a qualified name, such as .
Here's the data type declaration, from
[GhcFile(compiler/basicTypes/RdrName.hs)](GhcFile(compiler/basicTypes/RdrName.hs) "wikilink"):

User-written code never gets translated into the last two alternatives.
They are used only internally by the compiler. For example, code
generated by might use an to refer to , ignoring whatever might happen
to be in scope (dammit).

The \`Module\` and \`ModuleName\` types
---------------------------------------

In GHC, a *module* is uniquely defined by a pair of the module name and
the package where the module is defined. The details are in
[GhcFile(compiler/basicTypes/Module.hs)](GhcFile(compiler/basicTypes/Module.hs) "wikilink")
and
[GhcFile(compiler/main/PackageConfig.hs)](GhcFile(compiler/main/PackageConfig.hs) "wikilink"),
but here are the key definitions: You'll notice that a \`Qual\`
\`RdrName\` contains a \`ModuleName\`; which module is referred to
depends on the import declarations in that module. In contrast, a
\`Orig\` \`RdrName\` refers to a unique \`Module\`.

The  type
--------

An is more-or-less just a string, like "foo" or "Tree", giving the
(unqualified) name of an entity. Well, not quite just a string, because
in Haskell a name like "C" could mean a type constructor or data
constructor, depending on context. So GHC defines a type \`OccName\`
that is a pair of a and a indicating which name space the name is drawn
from. The data type is defined (abstractly) in
[GhcFile(compiler/basicTypes/OccName.hs)](GhcFile(compiler/basicTypes/OccName.hs) "wikilink"):
The name spaces are:

Attaching the names to their name spaces makes it very convenient to
build mappings from names to things; where such a mapping might contain
two strings that are identical, they can be distinguished by the name
space, so when mapping s, a single map suffices.

[PageOutline](PageOutline "wikilink")

Recompilation Avoidance
=======================

What is recompilation avoidance?
--------------------------------

When GHC is compiling a module, it tries to determine early on whether

`*Theobjectfile(orbyte-codeinthecaseofGHCi)and[wiki:Commentary/Compiler/IfaceFilesinterfacefile]existfromapreviouscompilation`\
`*Recompilationissuretoproduceexactlythesameresults,soit`\
`isnotnecessary.`

If both of these hold, GHC stops compilation early, because the existing
object code and interface are still valid. In GHCi and \`--make\`, we
must generate the \`ModDetails\` from the \`ModIface\`, but this is
easily done by calling \`MkIface.typecheckIface\`.

Example
-------

Let's use a running example to demonstrate the issues. We'll have four
modules with dependencies like this:

\`A.hs\`:

\`B.hs\`:

\`C.hs\`:

\`D.hs\`:

Why do we need recompilation avoidance?
---------------------------------------

### GHCi and \`--make\`

The simple fact is that when you make a small change to a large program,
it is often not necessary to recompile every module that depends
directly or indirectly on something that changed. In GHCi and
\`--make\`, GHC considers every module in the program in dependency
order, and decides whether it needs to be recompiled, or whether the
existing object code and interface will do.

### \`make\`

\`make\` works by checking the timestamps on dependencies and
recompiling things when the dependencies are newer. Dependency lists for
\`make\` look like this (generated by \`ghc -M\`):

Only the \`.hi\` files of the *direct imports* of a module are listed.
For example, \`A.o\` depends on \`C.hi\` and \`B.hi\`, but not \`D.hi\`.
Nevertheless, if D is modified, we might need to recompile A. How does
this happen?

`*first,makewillrecompileDbecauseitssourcefilehaschanged,`\
`` generatinganew`D.o`and`D.hi`. ``

`*IfafterrecompilingD,wenoticethatitsinterfaceisthesame`\
`` asbefore,thereisnoneedtomodifythe`.hi`file.Ifthe`.hi` ``\
`` fileisnotmodifiedbythecompilation,then`make`willnotice ``\
`` andnotrecompile`B`or`C`,orindeed`A`.Thisisanimportant ``\
`optimisation.`

`` *Supposethechangeto`D`didcauseachangeintheinterface ``\
`` (e.g.thetypeof`f`changed).Now,`make`willrecompileboth ``\
`` `B`and`C`.Supposethattheinterfacesto`B`and`C` ``\
`` remainthesame:B'sinterfacesaysonlythatitre-exports`D.f`, ``\
`` sothefactthat`f`hasanewtypedoesnotaffect`B`'s ``\
`interface.`

`` *Now,`A`'sdependenciesareunchanged,so`A`willnotbe ``\
`` recompiled.Butthisiswrong:`A`mightdependonsomethingfrom ``\
`` `D`thatwasre-exportedvia`B`or`C`,andthereforeneed ``\
`recompiling.`

To ensure that \`A\` is recompiled, we therefore have two options:

`1.arrangethatmakeknowsaboutthedependencyofAonD.`

`` 2.arrangetotouch`B.hi`and`C.hi`eveniftheyhaven'tchanged. ``

GHC currently does (2), more about that in a minute.

Why not do (1)? Well, then *every* time \`D.hi\` changed, GHC would be
invoked on \`A\` again. But \`A\` doesn't depend directly on \`D\`: it
imports \`B\`, and it might be therefore be insensitive to changes in
\`D\`. By telling make only about direct dependencies, we gain the
ability to avoid recompiling modules further up the dependency graph, by
not touching interface files when they don't change.

Back to (2). In addition to correctness (recompile when necessary), we
also want to avoid unnecessary recompilation as far as possible. Make
only knows about very coarse-grained dependencies. For example, it
doesn't know that changing the type of \`D.f\` can have no effect on
\`C\`, so \`C\` does not in fact need to be recompiled, because to do so
would generate exactly the same \`.o\` and \`.hi\` files as last time.
GHC does have enough information to figure this out, so when GHC is
asked to recompile a module it invokes the *recompilation checker* to
determine whether recompilation can be avoided in this case.

How does it work?
-----------------

We use
[fingerprints](http://en.wikipedia.org/wiki/Fingerprint_%28computing%29)
to uniquely identify the interface exposed by a module, and to detect
when it changes. In particular, we currently use 128-bit hashes produced
by the MD5 algorithm (see
[GhcFile(compiler/utils/Fingerprint.hsc)](GhcFile(compiler/utils/Fingerprint.hsc) "wikilink")).

An \[wiki:Commentary/Compiler/IfaceFiles interface file\] contains:

`*Variousfingerprints:`\
`*The`*`interface`
`hash`*`,whichdependsontheentirecontentsofthe`\
`interfacefile.Thisisusedtodetectwhetherweshould`\
`updatetheinterfaceondiskafterrecompilingthemodule.Ifthe`\
`interfacedidn'tchangeatall,thenwedon'twanttotouchthe`\
`` on-diskversionbecausethatwouldcause`make`toperformmore ``\
`compilations.`\
`*The`*`ABI` `hash`*`,whichdependsoneverythingthatthemodule`\
`exposesaboutitsimplementation:thinkofthisasahashof`\
``*`export-list` `hash`*`and`*`decls`*`.`\
`*The`*`export-list` `hash`*`,whichdependson`\
`*Theexportlistitself.Theexport-listhashonlydependsonthe`*`names`*`oftheexportsforthemodules.The`*`types`*`oftheseexportsareignoredincalculatingthehash.Onlyachangeofnameorremovaloradditionofanexportwillchangethehash.Notatypechangeofdefinitionchange.`\
`*the`*`orphan`
`hash`*`,whichdependsonalltheorphaninstances/rulesinthe,andtheorphanhashesofallorphanmodulesbelowthismoduleinthedependencytree(see[#OrphansOrphans]).`\
`*thepackagedependencies(see[#PackageversionchangesPackageVersionChanges]).`\
`*`*`exports`*`:whatthemoduleexports`\
`*`*`dependencies`*`:modulesandpackagesthatthismoduledependson`\
`*`*`usages`*`:whatspecificentitiesthemoduledependson`\
`*`*`decls`*`:whatthemoduledefines`\
`*variousotherstuff,buttheabovearetheimportantbits`

To look at the contents of an interface, use \`ghc --show-iface\`. For
example, here's the output of \`ghc --show-iface D.hi\` for the module
\`D\` in our example:

Lines beginning \`import\` are the *usages*, and after the usages are
the decls.

### Deciding whether to recompile

If we already have an object file and interface file for a module, we
might not have to recompile it, if we can be sure the results will be
the same as last time.

`*Ifthesourcefilehaschangedsincetheobjectfilewascreated,`\
`webetterrecompile.`

`*Ifanythingelsehaschangedinawaythatwouldaffecttheresults`\
`ofcompilingthismodule,wemustrecompile.`

In order to determine the second point, we look at the *dependencies*
and *usages* fields of the old interface file. The dependencies
contains:

`*`*`dep_mods`*`:Transitiveclosureofhome-packagemodulesthatare`\
`importedbythismodule.Thatis,allmodulesbelowthecurrent`\
`oneinthedependencygraph.`

`*`*`dep_pkgs`*`:Transitiveclosureofpackagesdependedonbythis`\
`module,orbyanymodulein`*`dep_mods`*`.`

`*otherlessimportantstuff.`

First, the direct imports of the current module are resolved to
\`Module\`s using \`Finder.findModule\` (a \`Module\` contains a module
name and a package identifier). If any of those \`Module\`s are not
listed amongst the dependencies of the old interface file, then either:

`*anexposedpackagehasbeenupgraded`\
`*wearecompilingwithdifferentpackageflags`\
`*ahomemodulethatwasshadowingapackagemodulehasbeenremoved`\
`*anewhomemodulehasbeenaddedthatshadowsapackagemodule`

and we must recompile.

Second, the *usages* of the module are checked. The usages contains two
types of information:

`*foramodulethatwasimported,theexport-listfingerprintofthe`\
`importedmoduleisrecorded.Ifanyofthemodulesweimportednow`\
`hasadifferentexportlistwemustrecompile,sowecheckthe`\
`currentexport-listfingerprintsagainstthoserecordedinthe`\
`usages.`

`*foreveryexternalnamementionedinthesourcecode,the`\
`fingerprintofthatnameisrecordedintheusages.Thisisso`\
`` thatifwementionforexampleanexternalfunction`M.f`,we'll ``\
`` recompileif`M.f`'stypehaschanged,oranythingreferredto ``\
`` by`M.f`'stypehaschanged,or`M.f`'sunfoldinghaschanged ``\
`(when-Oison),andsoon.`

The interface files for everything in the usages are read (they'll
already be in memory if we're doing \`--make\`), and the current
versions for each of these entities checked against the usages from the
old interface file. If any of these versions has changed, the module
must be recompiled.

### Example

There are some tricky cases to consider.

Suppose we change the definition of \`D.f\` in the example, and make it
Now, ultimately we need to recompile \`A\`, because it might be using an
inlined copy of the old \`D.f\`, which it got via \`B\`.

It works like this:

`` *`D`isrecompiled;thefingerprintof`D.f`changes ``\
`` *`B`isconsidered;itrecordedausageontheold`D.f`,so ``\
`` getsrecompiled,andnowitsinterfacerecordsausageonthenew`D.f` ``\
`` *`C`isconsidered;itdoesn'tneedtoberecompiled. ``\
`` *`A`isconsidered(ifwe'reusingmake,thisisbecause`B.hi` ``\
`` changed);itrecordedausageontheold`D.f`,andsogets ``\
`recompiled.`

Now a slightly more tricky case: suppose we add an INLINE pragma to
\`D.f\` (this is a trick to prevent GHC from inlining \`D.h\`, so that
we can demonstrate dependencies between unfoldings). The code for D.hs
is now

Looking at the interface file we can see what happened (snipped
slightly):

Note that the unfolding of \`D.f\` mentions \`D.h\`.

Now, let's modify \`D.h\`, and look at the interface file again:

The fingerprint for \`D.h\` has changed, because we changed its
definition. The fingerprint for \`D.f\` has also changed, because it
depends on \`D.h\`. And consequently, the ABI hash has changed, and so
has the interface hash (although the export hash and orphan hash are
still the same). Note that it is significant that we used '-O' here. If
we hadn't used '-O' then a change of a definition doesn't change any of
the hashes because of the lack of inlining.

Why did the fingerprint for \`D.f\` have to change? This is vital,
because anything that referred to \`D.f\` must be recompiled, because it
may now see the new unfolding for \`D.h\`.

So the fingerprint of an entity represents not just the definition of
the entity itself, but also the definitions of all the entities
reachable from it - its transitive closure. The consequence of this is
that when recording usages we only have to record the fingerprints of
entities that were referred to directly in the source code, because the
transitive nature of the fingerprint means that we'll recompile if
anything reachable from these entities changes.

### How does fingerprinting work?

We calculate fingerprints by serialising the data to be fingerprinted
using the \`Binary\` module, and then running the md5 algorithm over the
serlialised data. When the data contains external \`Name\`s, the
serialiser emits the fingerprint of the \`Name\`; this is the way that
the fingerprint of a declaration can be made to depend on the
fingerprints of the things it mentions.

### Mutually recursive groups of entities

When fingerprinting a recursive group of entities, we fingerprint the
group as a whole. If any of the definitions changes, the fingerprint of
every entity in the group changes.

### Fixities

We include the fixity of an entity when computing its fingerprint.

### Instances

Instances are tricky in Haskell, because they aren't imported or
exported explicitly. Haskell requires that any instance defined in a
module directly or indirectly imported by the current module is visible.
So how do we track instances for recompilation, such that if a relevant
instance is changed, added, or removed anywhere beneath the current
module we will trigger a recompilation?

Here's how it works. For each instance we pick a distinguished entity to
attach the instance to - possibly the class itself, or a type
constructor mentioned in the instance. The entity we pick must be
defined in the current module; if there are none to pick, then the
instance is an orphan (more about those in the section on Orphans,
below).

Having picked the distinguished entity, when fingerprinting that entity
we include the instances. For example, consider an instance for class C
at type T. Any module that could use this instance must depend (directly
or indirectly) on both C and T, so it doesn't matter whether we attach
the instance to C or T - either way it will be included in the
fingerprint of something that the module depends on. In this way we can
be sure that if someone adds a new instance, or removes an existing
instance, if the instance is relevant to a module then it will affect
the fingerprint of something that the module depends on, and hence will
trigger recompilation.

In fact, we don't need to include the instance itself when
fingerprinting C or T, it is enough to include the DFun (dictionary
function) Id, since the type of this Id includes the form of the
instance. Furthermore, we *must* include the DFun anway, because we must
have a dependency on the dictionary and its methods, just in case they
are inlined in a client module. A DFun looks something like this:

Making a type or class depend on its instances can cause a lot of
recompilation when an instance changes. For example:

now the DFun for the instance \`C T\` will be attached to \`T\`, and so
\`T\`'s fingerprint will change when anything about the instance
changes, including \`C\` itself. So there is now have a dependency of
\`T\` on \`C\`, which can cause a lot of recompilation whenever \`C\`
changes. Modules using \`T\` who do not care about \`C\` will still be
recompiled.

This seems like it would cause a lot of unnecessary recompilation.
Indeed, in GHC 7.0.1 and earlier we tried to optimise this case, by
breaking the dependency of \`T\` on \`C\` and tracking usages of DFuns
directly - whenever a DFun was used, the typechecker would record the
fact, and a usage on the DFun would be recorded in the interface file.
Unfortunately, there's a bug in this plan (see \#4469). When we're using
\`make\`, we only recompile a module when any of the interfaces that it
directly imports have changed; but a DFun dependency can refer to any
module, not just the directly imported ones. Instead, we have to ensure
that if an instance related to a particular type or class has changed,
then the fingerprint on either the type or class changes, which is what
the current plan does. It would be nice to optimise this in a safe way,
and maybe in the future we will be able to do that.

### Orphans

What if we have no declaration to attach the instance to? Instances with
no obvious parent are called *orphans*, and GHC must read the interface
for any module that contains orphan instances below the current module,
just in case those instances are relevant when compiling the current
module.

Orphans require special treatment in the recompilation checker.

`*Everymodulehasan`*`orphan`
`hash`*`,whichisafingerprintofall`\
`theorphaninstances(andrules)inthecurrentmodule.`

`*The`*`export` `hash`*`dependsonthe`*`orphan`
`hash`*`ofthecurrent`\
`module,andallmodulesbelowthecurrentmoduleinthedependency`\
`tree.Thismodelsthefactthatallinstancesdefinedinmodules`\
`belowthecurrentmoduleareavailabletoimportersofthismodule.`

So if we add, delete, or modify an orphan instance, the orphan hash of
the current module will change, and so will the export hash of the
current module. This will trigger recompilation of modules that import
the current module, which will cause their export hashes to change, and
so on up the dependency tree.

This means a lot of recompilation, but it is at least safe. The trick is
to avoid orphan instances as far as possible, which is why GHC has the
warning flag \`-fwarn-orphans\`.

### Rules

RULEs are treated very much like instances: they are attached to one
particular parent declaration, and if a suitable parent cannot be found,
they become orphans and are handled in the same way as orphan instances.

### On ordering

When fingerprinting a collection of things, for example the export list,
we must be careful to use a canonical ordering for the collection.
Otherwise, if we recompile the module without making any changes, we
might get a different fingerprint due to accidental reordering of the
elements.

Why would we get accidental reordering? GHC relies heavily on "uniques"
internally (see
[GhcFile(compiler/basicTypes/Unique.lhs)](GhcFile(compiler/basicTypes/Unique.lhs) "wikilink")):
every entity has a unique, and uniques are assigned semi-randomly.
Asking for the contents of a \`UniqSet\` or \`UniqFM\` will return the
elements in order of their uniques, which may vary from run to run of
the compiler.

The solution is to sort the elements using a stable ordering, such as
lexicographic ordering.

### Packages

We need to record usage information about package modules too, so that
we can correctly trigger recompilation if we depend on a package that
has changed. But packages change rarely, so it would be wasteful to
record detailed usage information for every entity that we use from an
external package (imagine recording the fingerprints for \`Bool\`,
\`Int\`, etc.). Instead, we simply record the ABI fingerprint for every
package module that was imported by the current module. That way, if
anything about the ABI of that package module has changed, then we can
trigger a recompilation.

(Correctly triggering recompilation when packages change was one of the
things we fixed when implementing fingerprints, see \#1372).

### Package version changes

If the version of a package is bumped, what forces recompilation of the
things that depend on it?

`1.Ifamodulefromthepackageisimporteddirectly,thenwewillnoticethattheimportedmoduleisnotamongstthedependenciesofthemodulewhenitwascompiledlast,andforcearecompilation(see[#DecidingwhethertorecompileDecidingwhethertorecompile]).`

`` 2.Ifamodulefromtheoldpackageisimportedindirectly,thentheoldpackagewillbeamongstthepackagedependencies(`dep_pkgs.mi_deps`),sowemustrecompileotherwisethesedependencieswillbeinconsistent.Thewaywehandlethiscaseisbyincludingthepackagedependenciesinthe ``*`export`
`hash`*`ofamodule,sothatothermoduleswhichimportthismodulewillautomaticallyberecompiledwhenoneofthepackagedependencieschanges.Therecompiledmodulewillhavenewpackagedependencies,whichwillforcerecompilationofitsimporters,andsoon.Thereforeifapackageversionchanges,thechangewillbepropagatedthroughoutthemoduledependencygraph.`

Interface stability
-------------------

For recompilation avoidance to be really effective, we need to ensure
that fingerprints do not change unnecessarily. That is, if a module is
modified, it should be the case that the only fingerprints that change
are related to the parts of the module that were modified. This may seem
obvious, but it's surprisingly easy to get wrong. Here are some of the
ways we got it wrong in the past, and some ways we still get it wrong.

`` *PriortoGHC6.12,dictionaryfunctionswerenamedsomethinglike`M.$f23`,where`M`isthemoduledefiningtheinstance,andthenumber`23`wasgeneratedbysimplyassigningnumberstothedictionaryfunctionsdefinedby`M`sequentially.Thisisaproblemforrecompilationavoidance,becausenowremovingoraddinganinstancein`M`willchangethenumbering,andforcerecompilationofanythingthatdependsonanyinstancein`M`.Worse,thenumbersareassignednon-deterministically,sosimplyrecompiling`M`withoutchangingitscodecouldchangethefingerprints.InGHC6.12wechangeditsothatdictionaryfunctionsarenamedaftertheclassandtype(s)oftheinstance,e.g.`M.$fOrdInteger`. ``

`*compiler-generatedbindingsusedtobenumberedinthesameway,non-deterministically.Thenon-determinismarisesbecauseUniquesareassignedbythecompilernon-deterministically.Well,theyaredeterministicbutnotinawaythatyoucansensiblycontrol,becauseitdependsontheorderinwhichinterfacebindingsareread,etc.InternalmappingsuseUniquesasthekey,soaskingfortheelementsofamappinggivesanon-deterministicordering.Thelistofbindingsemittedbythesimplifier,althoughindependencyorder,canvarynon-deterministicallywithintheconstraintsofthedependencies.Soifwenumberthecompiler-generatedbindingssequentially,theresultwillbeanon-deterministicABI.`\
``[`BR`](BR "wikilink")[`BR`](BR "wikilink")\
`` InGHC6.12wechangedthissothatcompiler-generatedbindingsaregivennamesoftheform`f_x`,where`f`isthenameoftheexportedIdthatreferstothebinding.Iftherearemultiple`f_x`s,thentheyaredisambiguatedwithanintegersuffix,butthenumbersareassigneddeterministically,bytraversingthedefinitionof`f`indepth-firstleft-to-rightordertofindreferences.See`TidyPgm.chooseExternalIds`. ``

`*Therearestillsomecaseswhereaninterfacecanchangewithoutchangingthesourcecode.Theonesweknowaboutarelistedin#4012`

The Register Allocator
======================

Overview
--------

The register allocator is responsible for assigning real/hardware regs
(hregs) to each of the virtual regs (vregs) present in the code emitted
by the native code generator. It also inserts spill/reload instructions
to save vregs to the stack in situations where not enough hregs are
available.

GHC currently provides three register allocation algorithms, one which
does simple linear scan and two version of graph coloring. Support for
linear scan is likely to be removed in a subequent version.

`*`**`Linear` `scan`**[`BR`](BR "wikilink")\
`Thelinearallocatoristurnedonbydefault.Thisiswhatyougetwhenyoucompilewith``.Thelinearallocatordoesasinglepassthroughthecode,allocatingregistersonafirst-come-first-servedbasis.Itisquick,anddoesareasonablejobforcodewithlittleregisterpressure.`

`Thisalgorithmhasnolook-ahead.Ifsay,aparticularhregwillbeclobberedbyafunctioncall,itdoesnotknowtoavoidallocatingtoitinthecodebeforethecall,andsubsequentlyinsertsmorespill/reloadinstructionsthanstrictlyneeded.`

`*`**`Graph` `coloring`**`(enabledwith``)`[`BR`](BR "wikilink")\
`Thegraphcoloringalgorithmoperatesonthecodeforawholefunctionatatime.Fromeachfunctionitextractsaregisterconflictgraphwhichhasanodeforeveryvregandanedgebetweentwovregsiftheyareinuseatthesametimeandthuscannotsharethesamehreg.Thealgorithmtriestoassignhregs(imaginedascolors)tothenodessothatnotwoadjacentnodessharethesamecolor,ifitcan'tthenitinsertsspillcode,rebuildsthegraphandtriesagain.`

`Graphcoloringtendstodobetterthanthelinearallocatorbecausetheconflictgraphhelpsitavoidthelook-aheadproblem.Thecoloringallocatoralsotrieshardertoallocatethesourceanddestinationofreg-to-regmoveinstructionstothesamehreg.Thisisdonebycoalescing(merging)move-relatednodes.Ifthissucceedsthentheassociatedmovescanbeerased.`

`*`**`Graph` `coloring` `with` `iterative`
`coalescing`**`(enabledwith``)`[`BR`](BR "wikilink")\
`Iterativecoalescingisanimprovementoverregulargraphcoloringwherebycoalescingpassesareinterleavedwithcoloringpasses.Iterativecoalescingdoesabetterjobthanregulargraphcoloring,butisslowerbecauseitmustalternatebetweenthecoloringandcoalescingofnodes.`

Code map
--------

For an outline of the code see
\[wiki:Commentary/Compiler/Backends/NCG/RegisterAllocator/Code\]

References
----------

If you decide to do some hacking on the register allocator then take a
look at (at least) these papers first:

**Iterated Register Coalescing**[BR](BR "wikilink") *George, Appel,
1996*[BR](BR "wikilink") Decribes the core graph coloring algorithm
used.

**A Generalised Algorithm for Graph-Coloring Register
Allocation**[BR](BR "wikilink") *Smith, Ramsey, Holloway,
2004*[BR](BR "wikilink") For a decription of how to deal with
overlapping register sets, which aren't fully implemented. Explains what
the , and functions are for.

**Design and Implementation of a Graph Coloring Register Allocator for
GCC**[BR](BR "wikilink") *Matz, 2003*[BR](BR "wikilink") For an overview
of techniques for inserting spill code.

Register pressure in Haskell code
---------------------------------

Present GHC compiled code places very little pressure on the register
set. Even on x86 with only 3 allocable registers, most modules do not
need spill/reloads. This is a mixed blessing - on one hand the conflict
graphs are small so we can avoid performance problems related to how the
graph is represented, on the other hand it can be hard to find code to
test against. Register pressure is expected to increase as the
Stg-&gt;Cmm transform improves.

In the meantime, here are some good sources for test code:

`*`**`Nofib`**[`BR`](BR "wikilink")\
`Onlyafewnofibbenchmarkscreatespillswith``,twoare``and``.`

`*`**`Turn` `on` `profiling`**[`BR`](BR "wikilink")\
`Registerpressureincreasessignificantlywhenthemoduleiscompiledwithprofiling.`[`14`](attachment:checkSpills.report)`givestuplesof``presentinoutputcodegeneratedbythethreealgorithmswhencompiledwith``.Lefttorightarethestatsforthelinear,graphcoloringanditerativecoalescingalgorithms.Notethatmostmodulescompilewithnospill/reloadsinserted,butafew(notably``)needseveralhundred.`

`I'vefounditusefultomaintainthreedarcsreposwhenworkingontheallocator.``compiledwith``forfastcompilationduringhacking,``fortestingwithprofilingturnedon,and``forrunningthevalidatescript.Patchesarecreatedin``,pushedinto``where``isusedtocompilethenofibbenchmarkswiththemostregisterpressure.Oncewe'rehappythattheperformanceisok,thepatchisthenpushedinto``forvalidationbeforepushingtothemainrepoon`

`*`**`SHA` `from` `darcs`**[`BR`](BR "wikilink")\
`The``modulefromthedarcssource,compiledwith``createsthemostregisterpressureoutofanyHaskellcodethatI'mawareof.WhencompilingSHA1,GHCinlinesseveralworkerfunctionsandthenativecodeblockthatcomputesthehashendsupbeingaround1700instructionslong.vregsthatliveinthemiddleoftheblockhaveintheorderof30conflictneighbors.(evidently,theconflictgraphistoolargeformostofthegraphvizlayoutalgorithmstocopewith)`

`Forthesereasons,``canbetreatedasagoodworst-caseinputtotheallocator.Infact,thecurrentlinearallocatorcannotcompileitwith``onx86asitrunsoutofstackslots,whichareallocatedfromastaticpool.Makesuretotestanychangestotheallocatoragainstthismodule.`

Hacking/Debugging
-----------------

`*`**`Turn` `on` **[`BR`](BR "wikilink")\
`Breakingtheallocatorcanresultincompiledprogramscrashingrandomly(ifyou'relucky)orproducingthewrongoutput.Makesuretoalwaysturnon``.Doingthismakestheallocatorcall``aftereveryspill/colorstage.``checksthatalltheedgespointtovalidnodes,thatnoconflictingnodeshavethesamecolor,andifthegraphissupposedtobecoloredthenallnodesarereallycolored.`

`*`**`Some` `useful` `dump` `flags`**

``[`BR`](BR "wikilink")\
`Showsthecodeandconflictgraphaftereverspill/colorstage.Alsoshowsspillcosts,andwhatregisterswerecoalesced.`

``[`BR`](BR "wikilink")\
`Givesstatisticsabouthowmanyspills/reloads/reg-reg-movesareintheoutputprogram.`

``[`BR`](BR "wikilink")\
`Givesthefinaloutputcode.`

``[`BR`](BR "wikilink")\
`Divertsdumpoutputtofiles.Thiscanbeusedtogetdumpsfromeachmoduleinanofibbenchmark.`\
\
``

`*`**`Visualisation` `of` `conflict` `graphs`**[`BR`](BR "wikilink")\
`Graphviz,availablefrom`[`15`](http://www.graphviz.org)`canbeusedtomakenicevisualisationsoftheregisterconflictgraphs.Use``,andcopyoneofthegraphdescriptionsintoanewfile`

``\
`Here'stwofrom``compiledwith``:`

``[`16`](attachment:graph.dot)`->`[`17`](attachment:graph.png)

``[`18`](attachment:graph-colored.dot)`->`[`19`](attachment:graph-colored.png)

`*`**`checkSpills`**[`BR`](BR "wikilink")\
``[`20`](attachment:checkSpills.hs)`isanasty,throwawayscriptwhichcanbeusedtoautomatethecomparisonofallocationalgorithms.Copyitandalistoftestlike`[`21`](attachment:checkSpills.tests)`tothetoplevelnofibdirectory,compileandrun.Itwillbuildthenofibbenchmarksinthelist6timeseach,onceeachwitheachoftheallocatorstoextractspillcounts,andthenonceagaintogetcompiletimingswhichareunperterbedbythespaceleaksintroducedbycompilingwithdebuggingturnedon.It'sonlyneededifyou'rehackingontheallocator,parsesthenofibmakeoutputdirectly,andislikelytorot-whichiswhyitisn'tincludedinthemainsourcetree.`

Runtime performance
-------------------

Runtime performance of the graph coloring allocator is proportional to
the size of the conflict graph and the number of build/spill cycles
needed to obtain a coloring. Most functions have graphs &lt; 100 nodes
and generate no spills, so register allocation is a small fraction of
overall compile time.

Possible Improvements
---------------------

These are some ideas for improving the current allocator, most
potentially useful first.

`*`**`Work` `lists` `for` `iterative`
`coalescing.`**[`BR`](BR "wikilink")\
`Theiterativecoalescingalternatesbetweenscanningthegraphfortriviallycolorable(triv)nodesandperforingcoalescing.Whentwonodesarecoalesced,othernodesthatarenotadjacenttothecoalescednodesdonotchangeanddonotneedtoberescannedstraightaway.Runtimeperformanceoftheiterativecoalescercouldprobablybeimprovedbykeepingawork-listof"nodesthatmighthavebecometriviallycolorable",tohelpfindnodesthatwon'thavechanged.`

`*`**`Improve` `spill` `code`
`generator/cleaner.`**[`BR`](BR "wikilink")\
`Whenspillingaparticularvreg,thecurrentspillcodegeneratorsimplyinsertsaspillaftereachdefandareloadbeforeeachuse.Thisquicklyreducesthedensityofconflictsinthegraph,butproducesinefficientcodebecausemorespill/reloadsareinsertedthanstrictlynessesary.Goodcodeisrecoveredbythespillcleanerwhichrunsafterallocationandremovesspill/reloadinstructionsthataren'tnessesary.Somethingstotry:`\
`*`**`Spill` `coalescing`**[`BR`](BR "wikilink")``\
`Noattemptiscurrentlymadetosharespillslotsbetweendifferentvregs.EachnamedvregisspilledtoitsownstaticspillslotontheCstack.Theamountofstackspaceneededcouldbereducedbysharingspillslotsbetweenvregssolongastheirliverangesdonotoverlap.`\
`*`**`Try` `to` `split` `live` `ranges` `before`
`spilling`**[`BR`](BR "wikilink")\
`Ifaliverangehasseveraluse/defsthenwecouldinsertfreshreg-regmovestobreakitupintoseveralsmallerliveranges.Wethenmightgetawaywithspillingjustonesectioninsteadofthewholerange.Notsureifthiswouldbeawinoverthecurrentsituation.Wewouldneedspill-coalescingtobeimplementedbeforethissothatwedon'trequireanextraslotforeachnewliverange.`\
`*`**`Rematerialization`**[`BR`](BR "wikilink")\
`Asthespillcleanerwalksthroughthecodeitbuildsamappingofwhichslotsandregistersholdthesamevalue.Oneachreloadinstruction,iftheslotandregareknowntoalreadyhavethesamevaluethenthereloadcanbeerased.Thismappingcouldbeextendedwithconstants,sothatifavregholdingaconstantvaluecannotbeallocatedahreg,theconstantvaluecanberematerializedinsteadofbeingspilled/reloadedtoastackslot.`

`*`**`Revisit` `choosing` `of` `spill`
`candidates`**[`BR`](BR "wikilink")\
`Ifthegraphcannotbecoloredthenanode/vregmustbechosentobepotentiallyspilled.Chaitin'sforumulasaystocalculatethespillcostbyaddingupthenumberofusesanddefsofthatvreganddividebythedegreeofthenode.InthecodethatI'vetestedagainst,it'sbeenbettertojustchoosetheliverangethatlivesthelongest.Perhapsthisisbecausethe'real'spillcostwoulddependonthespills/reloadsactuallyinserted,notasimplecountofuse/defs.PerhapschoosingthelongestliverangeisjustbetterfortheparticularkindofcodethatGHCgenerates.`

`*`**`Revisit` `trivColorable` `/` `aliasing` `of` `register`
`sets`**[`BR`](BR "wikilink")\
`Forthearchitecturescurrentlysupported,x86,x86_64andppc,thenativecodegeneratorcurrentlyemitscodeusingonlytworegisterclasses``and``.Astheseclassesaredisjoint(ie,noneoftheregsfromoneclassaliaswithwithregsfromanother),checkingwhetheranodeofacertainclassistriviallycolorablereducestocountingupthenumberofneighboursofthatclass.`

`IftheNCGstartstousealiasingregisterclasseseg:both32bit``sand64bit``sonsparc;combinationsof8,16,and32bitintegersonx86/x86_x6orusageofsse/altivecregsindifferentmodes,thenthiscanbesupportedviathemethoddescribedin[Smithetal].Theallocatorwasdesignedwiththisinmind-ie,bypassingafunctiontotestifanodeistriviallycolorableasaparametertothecoloringfunction-andthereisalreadyadescriptionoftheregistersetforx86in`[`GhcFile(compiler/nativeGen/RegArchX86.hs)`](GhcFile(compiler/nativeGen/RegArchX86.hs) "wikilink")`,butthenativecodegeneratordoesn'tcurrentlyemitcodetotestitagainst.`

Haskell Excecution: Registers
=============================

Source files:
[GhcFile(includes/stg/Regs.h)](GhcFile(includes/stg/Regs.h) "wikilink"),
[GhcFile(includes/stg/MachRegs.h)](GhcFile(includes/stg/MachRegs.h) "wikilink")

During execution of Haskell code the following (virtual) registers are
always valid:

`` *`Hp`pointstothebytebeforethefirstfreebyteinthe(contiguous)allocationspace. ``

`` *`HpLim`pointstothelastavailablebyteinthecurrentchunkofallocationspace. ``

`` *`Sp`pointstotheyoungestallocatedbyteofstack.Thestackgrowsdownwards.Why?Becausethatmeansareturnaddressisataloweraddressthanthestackframeit"knowsabout",andthatinturnmeansthatwecantreatastackframeverylikeaheapobject,withaninfopointer(returnaddress)asitsfirstword. ``

`` *`SpLim`pointstothelast(youngest)availablebyteinthecurrentstack. ``

There are bunch of other virtual registers, used for temporary argument
passing, for words, floats and doubles: \`R1\` .. \`R10\`, \`F1\` ..
\`F4\`, \`D1\` .. \`D4\`, \`L1\` .. \`L2\`.

In a register-rich machine, many of these virtual registers will be
mapped to real registers. In a register-poor machine, they are instead
allocated in a static memory record, pointed to by a real register,
\`BaseReg\`.

The code generator knows how many real registers there are, and tries to
avoid using virtual registers that are not mapped to real registers. So,
for example, it does not use \`R5\` if the latter is memory-mapped;
instead, it passes arguments on the stack.

Relevant GHC parts for Demand Analysis results
==============================================

`` *`compiler/basicTypes/Demand.lhs`--containsallinformationaboutdemandsandoperationsonthem,aswellasaboutserialization/deserializationofdemandsignatures.Thismoduleissupposedtobechangedwheneverthedemandnatureshouldbeenhanced; ``

`` *`compiler/stranal/DmdAnal.lhs`--thedemandanalysisitself.Checkmultiplecommentstofigureoutmainprinciplesofthealgorithm. ``

`` *`compiler/stranal/WorkWrap.lhs`--aworker-wrappertransform,mainclientofthedemandanalysis.Thefunctionsplitisperformedin`worthSplittingFun`basingondemandannotationsofafunction'sparameters. ``

`` *`compiler/stranal/WwLib.lhs`--ahelpermodulefortheworker-wrappermachinery.The"deep"splittingofaproducttypeargumentmakesuseofthestrictnessinfoandisimplementedbythefunction`mkWWstr_one`.Thefunction`mkWWcpr`makesuseoftheCPRinfo. ``

`` *`compiler/basicTypes/Id.lhs`--implementationofidentifierscontainsanumberofutilityfunctionstocheck/setdemandannotationsofbinders.Allofthemarejustdelegatingtoappropriatefunctions/fieldsofthe`IdInfo`record; ``

`` *`compiler/basicTypes/IdInfo.lhs`--`IdInfo`recordcontainsallinformationaboutdemandandstrictnessannotationsofanidentifier.`strictnessInfo`containsarepresentationofanabstracttwo-pointdemandtransformerofabinder,consideredasareferencetoavalue.`demandInfo`indicates,whichdemandisputtotheidentifier,whichisafunctionparameter,ifthefunctioniscalledinastrict/usedcontext.`seq*`-functionsareinvokedtoavoidmemoryleakscausedbytransformingnewASTsbyeachofthecompilerpasses(i.e.,nothunkspointingtothepartsoftheprocessedtreesareleft). ``

`` *`compiler/basicTypes/MkId.lhs`--Amachinery,responsibleforgenerationofworker-wrappersmakesuseofdemands.Forinstance,whenasignatureforaworkerisgenerated,thefollowingstrictnesssignatureiscreated: ``

`` Inwords,anon-bottomingdemandtypewith`N`lazy/usedarguments(`top`)iscreatedforaworker,where`N`isjustaworker'spre-computedarity.Also,particulardemandsareusedwhencreatingsignaturesfordictionaryselectors(see`mkDictSelId`). ``

`` *`compiler/prelude/primops.txt.pp`--thisfiledefinesdemandsignaturesforprimitiveoperations,whichareinsertedby`cpp`passonthemodule`compiler/basicTypes/MkId.lhs`; ``

`` *`compiler/coreSyn/CoreArity.lhs`--demandsignaturesareusedinordertocomputetheunfoldinginfoofafunction:bottomingfunctionsshouldnobeunfolded.See`exprBotStrictness_maybe`and`arityType`. ``

`` *`compiler/coreSyn/CoreLint.lhs`--thechecksareperformed(in`lintSingleBinding`): ``\
`*whetherarityanddemandtypeareconsistent(onlyifdemandanalysisalreadyhappened);`\
`*ifthebinderistop-levelorrecursive,it'snotdemanded(i.e.,itsdemandisnotstrict).`

`` *`compiler/coreSyn/CorePrep.lhs`--strictnesssignaturesareexaminingbeforeconvertingexpressiontoA-normalform. ``

`` *`compiler/coreSyn/MkCore.lhs`--abottomingstrictnesssignaturecreatedfor`error`-likefunctions(see`pc_bottoming_Id`). ``

`` *`compiler/coreSyn/PprCore.lhs`--standardpretty-printingmachinery,shouldbemodifiedtochangePPofdemands. ``

`` *`compiler/iface/IfaceSyn.lhs`--serialization,grepfor`HsStrictness`constructors. ``

`` *`compiler/iface/MkIface.lhs`--aclientof`IfaceSyn`,seeusagesof`HsStrictness`. ``

`` *`compiler/iface/TcIface.lhs`--thefunction`tcUnfolding`checksifanidentifierbindsabottomingfunctioninordertodecideifitshouldbeunfoldedornot ``

`` *`compiler/main/TidyPgm.lhs`--Multiplechecksofanidentifiertobindabottomingexpression,runningacheap-an-cheerfulbottomanalyser.See`addExternal`andoccurrencesof`exprBotStrictness_maybe`. ``

`` *`compiler/simplCore/SetLevels.lhs`--Itisimportanttozapdemandinformation,whenanidentifierismovedtoatop-level(duetolet-floating),hencelookforoccurrencesof`zapDemandIdInfo`. ``

`` *`compiler/simplCore/SimplCore.lhs`--thismoduleisresponsibleforrunningthedemandanalyserandthesubsequentworker-wrappersplitpasses. ``

`` *`compiler/simplCore/SimplUtils.lhs`--isanewarityislessthanthearityofthedemandtype,awarningisemitted;check`tryEtaExpand`. ``

`` *`compiler/specialise/SpecConstr.lhs`--strictnessinfoisusedwhencreatingaspecializedcopyofafunction,see`spec_one`and`calcSpecStrictness`. ``

[PageOutline](PageOutline "wikilink")

Remembered Sets
===============

Since in generational GC we may need to find all the live objects in a
young generation without traversing the older generation(s), we need a
record of the pointers from those old generations into the young
generations. This is termed the "remembered set".

In GHC each \`generation\` structure contains a field \`mut\_list\`,
which points to a chain of blocks. Each block in the chain contains a
list of pointers to objects in that generation which contain pointers to
objects in younger generations. There are alternative schemes, e.g.

`*Keepingtrackofeach`*`pointer`*`,ratherthan`*`object`*`thatpointstoayoungergeneration.Therememberedsetwould`\
`belarger(possiblyverymuchlarger,inthecaseofarrays),butitwouldbemoreaccurate,andtraversingthe`\
`rememberedsetatGCtimewouldbefaster.`

`*SomeGCsuse"card-marking"schemeswherebytheheapisdividedinto"cards"ofafixedsize,andeachcardhasabitto`\
`indicatewhetherthatcardcontainspointerstoayoungergeneration.Thisismuchlessaccuratethanarememberedset,`\
`butitisfasteratruntimeifalotofmutationistakingplace,andittakeslessspacethanarememberedset.InGHC`\
`wetypicallydonothavemuchmutationtoworryabout,socardmarkingwouldbeapoorcompromiseinourcase.`

The remembered set may contain duplicates, or it may contain pointers to
objects that don't really point to young generations.

Remembered set maintenance during mutation
------------------------------------------

While the mutator is running, we have to add any old-to-new generation
pointers that are created. Old-to-new pointers are created by mutating
(writing to) an object in the old generation, and catching these writes
is called a "write barrier".

A pointer can be added to a remembered set using

This adds the pointer \`p\` to the remembered set for generation
\`gen\`, using Capability \`cap\`. Each Capability has its own
remembered set for each generation, so that when running in parallel we
can update remembered sets without taking a lock, and also so that we
can take advantage of locality in the GC, by traversing a remembered set
on the same CPU that created it.

Here are the cases where we need a write barrier in GHC:

### Thunk Updates

Updating a thunk in an old generation. This is taken care of by the
update code, see
[GhcFile(rts/Updates.h)](GhcFile(rts/Updates.h) "wikilink").

### Mutable objects: MUT\_VAR, MVAR

For \`MUT\_VAR\`, the writer must call \`dirty\_MUT\_VAR\`:

(in [GhcFile(rts/sm/Storage.c)](GhcFile(rts/sm/Storage.c) "wikilink")).
The code generator inserts calls to \`dirty\_MUT\_VAR\` when it compiles
a call to the primitive \`writeMutVar\#\`.

\`dirty\_MUT\_VAR\` does the following: if the object's header is
\`MUT\_VAR\_CLEAN\`, then the header is set to \`MUT\_VAR\_DIRTY\`, and
the object is added to the remembered set if it resides in an old
generation. If the header was already \`MUT\_VAR\_DIRTY\`, no action is
taken.

\`MVAR\` is handled in the same way, with

### Arrays: MUT\_ARR\_PTRS

Unlike mutable variables and MVARs, mutable arrays are kept in the
remembered set permanently. This reflects the fact that mutable arrays
are likely to be written to more often, and there are likely to be fewer
of them. However, we still mark arrays according to whether the array is
dirty or not, using \`MUT\_ARR\_PTRS\_DIRTY\` and
\`MUT\_ARR\_PTRS\_CLEAN\`.

There are also \`MUT\_ARR\_PTRS\_FROZEN\` and
\`MUT\_ARR\_PTRS\_FROZEN0\`, which are used to indicate arrays that have
been frozen using \`unsafeFreezeArray\#\`. A frozen array is different
from a mutable array in the sense that while it may have old-to-new
pointers, it is not going to be mutated any further, and so we probably
want to use \[wiki:Commentary/Rts/Storage/GC/EagerPromotion eager
promotion\] on it.

### Threads: TSO

Threads (TSOs) have stacks, which are by definition mutable. Running a
thread is therefore an act of mutation, and if the thread resides in an
old generation, it must be placed in the remembered set. Threads have
two dirty bits: \`tso-&gt;dirty\` is set to non-zero if the thread's
stack or any part of the TSO structure may be dirty, and also there is a
bit \`TSO\_LINK\_DIRTY\` in \`tso-&gt;flags\` which is set if the TSO's
link field may be dirty. If the thread is executed, then
\`dirty\_TSO()\` must be called in order to set the \`tso-&gt;dirty\`
bit and add the TSO to the appropriate remembered set.

To set the TSO's link field, use \`setTSOLink()\` (from
[GhcFile(rts/sm/Storage.c)](GhcFile(rts/sm/Storage.c) "wikilink")) which
arranges to add the TSO to the remembered set if necessary.

there are a few exceptions where \`setTSOLink()\` does not need to be
called; see
[GhcFile(rts/sm/Storage.c)](GhcFile(rts/sm/Storage.c) "wikilink") for
details.

Remembered set maintenance during GC
------------------------------------

During GC, the principle of write barriers is quite similar: whenever we
create an old-to-new pointer, we have to record it in the remembered
set. The GC achieves this as follows:

`` *TheGCthreadstructurehasafield`gct->evac_gen`whichspecifiesthedesireddestinationgeneration. ``\
`` *thereisaflag`gct->failed_to_evac`,whichissettotrueby`evacuate`ifitdidnotmanagetoevacuate ``\
`theobjectintothedesiredgeneration.`\
`` *afterscavenginganobject,`scavenge_block`checksthe`failed_to_evac`flag,andifitisset,addstheobjecttotherememberedset,using`recordMutableGen_GC()`(theequivalentof`recordMutableCap`forcallingwithintheGC). ``

The renamer
===========

The renamer's Number One task is to replace
\[wiki:Commentary/Compiler/RdrNameType RdrNames\] with
\[wiki:Commentary/Compiler/NameType Names\]. For example, consider
(where all the variables are s). The result of renaming module M is:
where all these names are now s.

`*Thetop-levelunqualifed``"``"hasbecomethe``````.`\
`*Theoccurrences"``"and"``"arebothboundtothis``.`\
`*Thequalified``"``"becomesthe````,becausethefunctionisdefinedinmoduleK.`\
`*Thelambda-bound"``"becomesan``name,herewritten``.(Allthe``nameshaveuniquestoo,butweoftendonotprintthem.)`

In addition, the renamer does the following things:

`*Sortoutfixities.Theparserparsesallinfixapplicationsas`**`left-associative`**`,regardlessoffixity.Forexample"``"isparsedas"``".Therenamerre-associatessuchnestedoperatorapplications,usingthefixitiesdeclaredinthemodule.`

`*Dependencyanalysisformutually-recursivegroupsofdeclarations.Thisdividesthedeclarationsintostrongly-connectedcomponents.`

`*Lotsoflexicalerrorchecking:variablesoutofscope,unusedbindings,unusedimports,patternsthatusethesamebindermanytimes,etc.`

The renamer sits between the parser and the typechecker. However, its
operation is quite tightly interwoven with the typechecker. This is
mainly due to support for Template Haskell, where spliced code has to be
renamed and type checked. In particular, top-level splices lead to
multiple rounds of renaming and type checking. It uses the
\[wiki:Commentary/Compiler/TcRnMonad same monad as the typechecker\].

The global renamer environment, 
--------------------------------

A big part of the renamer's task is to build the **global rdr-env** for
the module, of type . This environment allows us to take a qualified or
un-qualified and figure out which it means. The global rdr-env is built
by looking at all the imports, and the top-level declarations of the
module.

You might think that the global rdr-env would be a mapping from to , but
it isn't. Here is what it looks like, after at least three iterations
(all in
[GhcFile(compiler/basicTypes/RdrName.hs)](GhcFile(compiler/basicTypes/RdrName.hs) "wikilink")):
Here is how to understand these types:

`` *Theenvironment(`GlobalRdrEnv`)mapsan ```toalistofallentitieswiththatoccurrencenamethatareinscope(inanyway).`

`*Eachoftheseisrepresentedbya``,whichgivestheentity's``plusaspecificationofhowitisinscope,its``.`

`*The``hasoneoftwoforms.Eitheritisinscopebecauseitisdefinedinthismodule(``),orbecauseitisimported.Inthelattercase,the``describesalltheimportstatementsthatbringitintoscope.`

`*An``hastwocomponents:`\
`*An``thatdescribestheentireimportdeclaration.Thisissharedbetweenallentitiesbroughtintoscopebyaparticularimportdeclaration.`\
`*An``thatdescribestheimportitemthatbroughttheentityintoscope.`\
`Forexample,given`

`the``woulddescribethe``and``part,whilethe``describesthe``part.Youcanlookin``toseewhatan``and``arelike!`\
`*The``ofanentityisthe``` underwhichitisgroupedwhentheforms`T(..)`or`T(C,D)`areusedinanexportorimportlist.Inthe`T(..)`form,allthethingswhose ```` is`T`arechosen.Inthe`T(C,D)`form,itisrequiredthat`C`and`D`have`T`asparents. ``\
`Forexample,`\
`` *The`Parent`ofadataconstructorisitsdatatype ``\
`` *The`Parent`ofarecordfieldselectorisitsdatatype ``\
`` *The`Parent`ofaclassoperationisitsclass ``

With all that information, we can give good error messages, especially
in the case where an occurrence "f" is ambiguous (i.e. different
entities, both called "f", were imported by different import
statements).

The global rdr-env is created by
[GhcFile(compiler/rename/RnNames.hs)](GhcFile(compiler/rename/RnNames.hs) "wikilink").

It is important to note that the global rdr-env is created *before* the
renamer actually descends into the top-level bindings of a module. In
other words, before performs the renaming of a module by way of , it
uses to set up the global rdr-env environment, which contains for all
imported and all locally defined toplevel binders. Hence, when the
helpers of come across the defining occurences of a toplevel , they
don't rename it by generating a new name, but they simply look up its
name in the global rdr-env.

Unused imports
--------------

See \[wiki:Commentary/Compiler/UnusedImports how the renamer reports
unused imports\]

Name Space Management
---------------------

(too much detail?)

As anticipated by the variants and of , some names should not change
during renaming, whereas others need to be turned into unique names. In
this context, the two functions and are important: The two functions
introduces new toplevel and new local names, respectively, where the
first two arguments to newTopSrcBinder determine the currently compiled
module and the parent construct of the newly defined name. Both
functions create new names only for
\[wiki:Commentary/Compiler/RdrNameType RdrNames\] that are neither exact
nor original.

Rebindable syntax
-----------------

(!ToDo: Not fully proof-read.)

In Haskell when one writes "3" one gets "fromInteger 3", where
"fromInteger" comes from the Prelude (regardless of whether the Prelude
is in scope). If you want to completely redefine numbers, that becomes
inconvenient. So GHC lets you say "-fno-implicit-prelude"; in that case,
the "fromInteger" comes from whatever is in scope. (This is documented
in the User Guide.)

This feature is implemented as follows (I always forget).

`*NamesthatareimplicitlyboundbythePrelude,aremarkedbythetype``.Moreover,theassociationlist``issetupbytherenamertomaprebindablenamestothevaluetheyareboundto.`\
`*Currently,fiveconstructsrelatedtonumerals(``,``,``,``,and``)andtwoconstructsrelatedtodo-expressions(``and``)haverebindablesyntax.`\
`*Whentheparserbuildstheseconstructs,itputsinthebuilt-inPreludeName(e.g.``).`\
`*Whentherenamerencounterstheseconstructs,itcalls``.Thischecksfor``;ifnot,itjustreturnsthesameName;otherwiseittakestheoccurrencenameoftheName,turnsitintoanunqualified``,andlooksitupintheenvironment.Thereturnednameispluggedbackintotheconstruct.`\
`*Thetypecheckerusesthe``togeneratetheappropriatetypingconstraints.`

Replacing the Native Code Generator
===================================

The existence of LLVM is definitely an argument not to put any more
effort into backend optimisation in GHC, at least for those
optimisations that LLVM can already do. There's also the question of
whether it's worth extending the NCG to support SIMD primops. At the
moment only the LLVM backend supports these, but current processor
architectures will rely more and more on wide vector SIMD instructions
for performance. Given that the LLVM project is now stable and widely
used, it may be better to drop the NCG entirely (and delete the code).

However, there are a few ways that the LLVM backend needs to be improved
before it can be considered to be a complete replacement for the
existing NCG:

1\. Compilation speed. LLVM approximately doubles compilation time.
Avoiding going via the textual intermediate syntax would probably help
here.

2\. Shared library support (\#4210, \#5786). It works (or worked?) on a
couple of platforms. But even on those platforms it generated worse code
than the NCG due to using dynamic references for \*all\* symbols,
whereas the NCG knows which symbols live in a separate package and need
to use dynamic references.

3\. Some low-level optimisation problems (\#4308, \#5567). The LLVM
backend generates bad code for certain critical bits of the runtime,
perhaps due to lack of good aliasing information. This hasn't been
revisited in the light of the new codegen, so perhaps it's better now.

Someone should benchmark the LLVM backend against the NCG with new
codegen in GHC 7.8. It's possible that the new codegen is getting a
slight boost because it doesn't have to split up proc points, so it can
do better code generation for let-no-escapes. It's also possible that
LLVM is being penalised a bit for the same reason.

Other considerations:

1\. The GHC distribution would need to start shipping with its own copy
of LLVM. The LLVM code that GHC produces typically lags the current
version of LLVM, so we'd need to ensure there was a usable version.

2\. If we did ship our own version of LLVM, we could add custom plugins
to improve the GHC generated code. At one stage Max Bolingbroke wrote an
LLVM alias analysis plugin, but making it work against an arbitrary
existing LLVM version would be infeasible.

note (carter): If we're very thoughtful about the changes / extensions
to llvm needed for GHC, I'm somewhat confident that we could get any
such patches upstreamed to llvm proper. The down side of this is that
any such features would be subject to the llvm release cycle, plus we'd
want to make sure that we're not just completely changing what we'd like
upstreamed every ghc release cycle. The upside is that we'd get a lot
more scrutiny / feedback / checking by llvm devs than we'd get with our
own patched variant

Resource Limits
===============

This page describes a proposed resource limits capabilities for GHC. The
idea is to give users the ability to create and utilize resource
containers inside programs, and then provide in-program access to heap
census and other information. The semantics of resource containers are
quite similar to cost centers used in profiling, except that they do not
have "stack" semantics (more on this later). The end result is the
ability to impose resource limits on space usage.

Code generation changes
-----------------------

Resource limits is a new way (similar to profiled and dynamic). Here are
the relevant changes:

### Dynamic closure allocation

[GhcFile(compiler/codeGen/StgCmmHeap.hs)](GhcFile(compiler/codeGen/StgCmmHeap.hs) "wikilink"):allocDynClosureCmm
(via StgCmmCon, also handles StgCmmBind:mkRhsClosure/cgRhsStdThunk.
link\_caf needs special treatment.)

Changes to:

I.e. no change from un-profiled.

### CAF Allocation

[GhcFile(compiler/codeGen/StgCmmBind.hs)](GhcFile(compiler/codeGen/StgCmmBind.hs) "wikilink"):thunkCode

Here is an interesting bugger:

Notice the heap check serves for the later branch too. On the other
hand, the CCCS coincides with the later change. This seems to be the
general pattern. So we might be able to handle this CAF by
special-casing CAFs.

We also hit the slow function application path.

### Thunk code

[GhcFile(compiler/codeGen/StgCmmBind.hs)](GhcFile(compiler/codeGen/StgCmmBind.hs) "wikilink"):thunkCode

Changes to:

### Foreign calls

Changes to:

No change from unprofiled

Case split
----------

Do a nursery swap.

-   -   Warning:\*\* The rest of this document describes an old
        iteration of the system, which directly used

Front-end changes
-----------------

The basic idea behind this patch is that data collected during
\*\*profiling\*\* can also be used at runtime to enforce limits. So most
of the API involves (1) dynamically setting cost-centres, which GHC uses
to do profiling, and (2) querying and receiving callbacks when certain
events happen during profiling. Costs can be collected anywhere you
could have placed an annotation statically.

The general usage of this API goes like:

Another use-case is more fine-grained SCCs based on runtime properties,
not source-level features.

I am planning on providing semantics, based on GHC

Garbage Collection Roots
========================

The "roots" are the set of pointers that the GC starts traversing from,
i.e. the roots of the live object graph.

Most roots belong to a particular Capability. Traversing the roots of a
capbility is done by \`markSomeCapabilities()\` in
[GhcFile(rts/Capability.c)](GhcFile(rts/Capability.c) "wikilink"). The
roots of a Capability are:

`*Therunqueue(headandtail)`\
`*Thewakeupqueue(headandtail)`\
`` *ForeachTaskonthe`suspended_ccalling_tasks`list,theTSOforthatTask ``\
`*TheSparkPool`\
`*Onlyforthenon-threadedRTS:Theblockedqueue(headandtail),andthesleepingqueue`

In addition, each Capability has a
\[wiki:Commentary/Rts/Storage/GC/RememberedSets remembered set\] for
each generation. A remembered set is a source of roots if that
generation is *not* being collected during this cycle; otherwise the
remembered set is discarded. During GC, all remembered sets are
discarded and new ones will be constructed for each generation and
Capability; see \`scavenge\_capability\_mut\_lists()\` in
[GhcFile(rts/sm/Scav.c)](GhcFile(rts/sm/Scav.c) "wikilink").

There are also roots from other parts of the system:

`` *Signalhandlers(onlyinthenon-threadedRTS;inthethreadedRTSsignalhandlersaremaintainedbytheIOmanagerin`GHC.Conc`ratherthantheRTS). ``\
`*[wiki:Commentary/Rts/Storage/GC/WeakWeakpointers]`\
`*[wiki:Commentary/Rts/StableStablepointers]`

[PageOutline](PageOutline "wikilink")

GHC Source Tree Roadmap: rts/
=============================

This directory contains the source code for the runtime system.

There are three types of files:

`::`\
`Headerfilesthatare`*`private` `to` `the`
`RTS`*`.Thatis,headerfilesinthisdirectoryare`\
`notshippedwithGHC,andAPIstheydefinearethereforeintendedtobeprivateandnot`\
`usablebyclientcode(inpractice,wedonotandprobablycannotenforcethis).Header`\
`filesthatwe`*`do`*`shipwithGHCareinthe[wiki:Commentary/SourceTree/Includesincludes]`\
`directory.`

`::`\
`Csourcecodefortheruntimesystem.Conventionsusedinthiscodearedescribedin`\
`[wiki:Commentary/Rts/Conventions].`

`::`\
`C--codeforpartsoftheruntimethatarepartoftheHaskellexecutionenvironment:for`\
`example,theimplementationofprimitives,exceptions,andsoon.A``fileis`\
`pseudoC--:moreorlessC--syntaxwithsomeomissionsandsomeadditionalmacro-like`\
`extensionsimplementedbyGHC.The``filesarecompiledusingGHCitself:see`\
`[wiki:Commentary/Rts/Cmm].`

### Subdirectories of rts/

`::`\
`::`\
`POSIXandWin32-specificpartsoftheruntimerespectively.Wetrytoputplatform-specificstuffinthesedirectories,`\
`howevernotalloftheRTSfollowsthisconventionrightnow.`

`::`\
`HooksforchangingtheRTSbehaviourfromclientcode,eg.changingthedefaultheapsize.`\
`(see`[`User's` `Guide` `for` `more` `about`
`hooks`](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/runtime-control.html#rts-hooks)`).`

`::`\
`The[wiki:Commentary/Rts/StorageStorageManager].`

### Haskell Execution

All this code runs on the Haskell side of the Haskell/C divide; is the
interface between the two layers.

[`Apply.cmm`](http://darcs.haskell.org/ghc/rts/Apply.cmm)`,`[`AutoApply.h`](http://darcs.haskell.org/ghc/rts/AutoApply.h)`,``,`[`Apply.h`](http://darcs.haskell.org/ghc/rts/Apply.h)`::`\
`Theeval/applymachinery.Note:``isthefamily`\
`offunctionsforperforminggenericapplicationofunknown`\
`functions,thiscodedependsonthenumberofregistersavailable`\
`forargumentpassing,soitisgeneratedautomaticallybytheprogram`\
```in``.`

[`Exception.cmm`](http://darcs.haskell.org/ghc/rts/Exception.cmm)`::`\
`Supportforexecptions.`

[`HeapStackCheck.cmm`](http://darcs.haskell.org/ghc/rts/HeapStackCheck.cmm)`::`\
`CodeforpreparingthestackwhenthecurrentHaskellthreadneeds`\
`toreturntotheRTS,becauseweeitherranoutofheaporstack,or`\
`needtoblock(eg.``),oryield.`

[`PrimOps.cmm`](http://darcs.haskell.org/ghc/rts/PrimOps.cmm)`::`\
`Implementationofout-of-lineprimitives(see[wiki:Commentary/PrimOps]).`

[`StgMiscClosures.cmm`](http://darcs.haskell.org/ghc/rts/StgMiscClosures.cmm)`::`\
`Somebuilt-inclosures,suchasthefamilyofsmall``sand`\
```,andsomebuilt-ininfotablessuchas`\
`and``.`

[`StgStartup.cmm`](http://darcs.haskell.org/ghc/rts/StgStartup.cmm)`::`\
`CodethatexecuteswhenaHaskellthreadbeginsandends.`

[`StgStdThunks.cmm`](http://darcs.haskell.org/ghc/rts/StgStdThunks.cmm)`::`\
`Somebuilt-inthunks:[wiki:Commentary/Rts/Storage/HeapObjects#Selectorthunksselectorthunks]and"apply"thunks.`

[`Updates.cmm`](http://darcs.haskell.org/ghc/rts/Updates.cmm)`,`[`Updates.h`](http://darcs.haskell.org/ghc/rts/Updates.h)`::`\
`[wiki:CommentaryUpdates].`

[`HCIncludes.h`](http://darcs.haskell.org/ghc/rts/HCIncludes.h)`::`\
`Headerfileincludedwhencompiling``filesviaC.`

[`StgCRun.c`](http://darcs.haskell.org/ghc/rts/StgCRun.c)`,`[`StgRun.h`](http://darcs.haskell.org/ghc/rts/StgRun.h)`::`\
`TheinterfacebetweentheCexecutionlayerandtheHaskell`\
`executionlayer.`

[`StgPrimFloat.c`](http://darcs.haskell.org/ghc/rts/StgPrimFloat.c)`::`\
`Floating-pointstuff.`

[`STM.c`](http://darcs.haskell.org/ghc/rts/STM.c)`::`\
`ImplementationofSoftwareTransactionalMemory.`

### The \[wiki:Commentary/Rts/Storage Storage Manager\]

[`sm/Storage.c`](http://darcs.haskell.org/ghc/rts/sm/Storage.c)`::`\
`Top-levelofthestoragemanager.`

[`sm/MBlock.c`](http://darcs.haskell.org/ghc/rts/sm/MBlock.c)`,`[`sm/MBlock.h`](http://darcs.haskell.org/ghc/rts/sm/MBlock.h)`,`[`sm/OSMem.h`](http://darcs.haskell.org/ghc/rts/sm/OSMem.h)`::`\
`The"megablock"allocator;thisisthethinlayerbetweentheRTSand`\
`theoperatingsystemforallocatingmemory.`

[`sm/BlockAlloc.c`](http://darcs.haskell.org/ghc/rts/sm/BlockAlloc.c)`,`[`sm/BlockAlloc.h`](http://darcs.haskell.org/ghc/rts/sm/BlockAlloc.h)`::`\
`Thelow-levelblockallocator,requiresonly``.`

[`sm/GC.c`](http://darcs.haskell.org/ghc/rts/sm/GC.c)`,`[`sm/Scav.c`](http://darcs.haskell.org/ghc/rts/sm/Scav.c)`,`[`sm/Evac.c`](http://darcs.haskell.org/ghc/rts/sm/Evac.c)`,`[`sm/GCUtils.c`](http://darcs.haskell.org/ghc/rts/sm/GCUtils.c)`,`[`sm/MarkWeak.c`](http://darcs.haskell.org/ghc/rts/sm/MarkWeak.c)`::`\
`Thegenerationalcopyinggarbagecollector.`

[`sm/Compact.c`](http://darcs.haskell.org/ghc/rts/sm/Compact.c)`,`[`sm/Compact.h`](http://darcs.haskell.org/ghc/rts/sm/Compact.h)`::`\
`Thecompactinggarbagecollector.`

[`ClosureFlags.c`](http://darcs.haskell.org/ghc/rts/ClosureFlags.c)`::`\
`Determiningpropertiesofvarioustypesofclosures.`

[`Sanity.c`](http://darcs.haskell.org/ghc/rts/Sanity.c)`,`[`Sanity.h`](http://darcs.haskell.org/ghc/rts/Sanity.h)`::`\
`Asanity-checkerfortheheapandrelateddatastructures.`

[`Stats.c`](http://darcs.haskell.org/ghc/rts/Stats.c)`,`[`Stats.h`](http://darcs.haskell.org/ghc/rts/Stats.h)`::`\
`Statisticsforthegarbagecollectorandstoragemanager.`

[`Stable.c`](http://darcs.haskell.org/ghc/rts/Stable.c)`::`\
`Stablenamesandstablepointers.`

[`Weak.c`](http://darcs.haskell.org/ghc/rts/Weak.c)`,`[`Weak.h`](http://darcs.haskell.org/ghc/rts/Weak.h)`::`\
`Weakpointers.`

### Data Structures

Data structure abstractions for use in the RTS:

[`Arena.c`](http://darcs.haskell.org/ghc/rts/Arena.c)`,`[`Arena.h`](http://darcs.haskell.org/ghc/rts/Arena.h)`::`\
`Anarenaallocator`

[`Hash.c`](http://darcs.haskell.org/ghc/rts/Hash.c)`,`[`Hash.h`](http://darcs.haskell.org/ghc/rts/Hash.h)`::`\
`Agenerichashtableimplementation.`

### The \[wiki:Commentary/Rts/Scheduler Scheduler\]

[`Capability.c`](http://darcs.haskell.org/ghc/rts/Capability.c)`,`[`Capability.h`](http://darcs.haskell.org/ghc/rts/Capability.h)`::`\
`Capabilities:virtualCPUsforexecutingHaskellcode.`

[`RaiseAsync.c`](http://darcs.haskell.org/ghc/rts/RaiseAsync.c)`,`[`RaiseAsync.h`](http://darcs.haskell.org/ghc/rts/RaiseAsync.h)`::`\
`Asynchronousexceptions.`

[`Schedule.c`](http://darcs.haskell.org/ghc/rts/Schedule.c)`,`[`Schedule.h`](http://darcs.haskell.org/ghc/rts/Schedule.h)`::`\
`Thescheduleritself.`

[`Sparks.c`](http://darcs.haskell.org/ghc/rts/Sparks.c)`,`[`Sparks.h`](http://darcs.haskell.org/ghc/rts/Sparks.h)`::`\
`Sparks:theimplementationof``.`

[`ThreadLabels.c`](http://darcs.haskell.org/ghc/rts/ThreadLabels.c)`,`[`ThreadLabels.h`](http://darcs.haskell.org/ghc/rts/ThreadLabels.h)`::`\
`Labellingthreads.`

[`Threads.c`](http://darcs.haskell.org/ghc/rts/Threads.c)`,`[`Threads.h`](http://darcs.haskell.org/ghc/rts/Threads.h)`::`\
`Variousthread-relatedfunctionality.`

[`ThreadPaused.c`](http://darcs.haskell.org/ghc/rts/ThreadPaused.c)`::`\
`SuspendingathreadbeforeitreturnstotheRTS.`

[`Task.c`](http://darcs.haskell.org/ghc/rts/Task.c)`,`[`Task.h`](http://darcs.haskell.org/ghc/rts/Task.h)`::`\
`Task:anOS-threadabstraction.`

[`AwaitEvent.h`](http://darcs.haskell.org/ghc/rts/AwaitEvent.h)`::`\
`Waitingforevents(non-threadedRTSonly).`

[`Timer.c`](http://darcs.haskell.org/ghc/rts/Timer.c)`,`[`Timer.h`](http://darcs.haskell.org/ghc/rts/Timer.h)`,`[`Ticker.h`](http://darcs.haskell.org/ghc/rts/Ticker.h)`::`\
`Theruntime'sintervaltimer,usedforcontextswitchingandprofiling.`

### C files: the \[wiki:Commentary/Rts/FFI FFI\]

[`Adjustor.c`](http://darcs.haskell.org/ghc/rts/Adjustor.c)`::`\
`Veryhairysupportfor``.`

[`HsFFI.c`](http://darcs.haskell.org/ghc/rts/HsFFI.c)`,`[`RtsAPI.c`](http://darcs.haskell.org/ghc/rts/RtsAPI.c)`::`\
`ImplementationoftheHaskellFFICinterface:``,`\
```,etc.`\
``

### The \[wiki:Commentary/Rts/Interpreter Byte-code Interpreter\]

[`Disassembler.c`](http://darcs.haskell.org/ghc/rts/Disassembler.c)`,`[`Disassembler.h`](http://darcs.haskell.org/ghc/rts/Disassembler.h)`::`\
[`Interpreter.c`](http://darcs.haskell.org/ghc/rts/Interpreter.c)`,`[`Interpreter.h`](http://darcs.haskell.org/ghc/rts/Interpreter.h)`::`\
`The[wiki:Commentary/Rts/Interpreterbyte-codeinterpreter]anddisassembler.`

[`Linker.c`](http://darcs.haskell.org/ghc/rts/Linker.c)`::`\
[`LinkerInternals.h`](http://darcs.haskell.org/ghc/rts/LinkerInternals.h)\
`The[wiki:Commentary/Rts/Linkerdynamicobject-codelinker].`

### \[wiki:Commentary/Profiling Profiling\]

[`LdvProfile.c`](http://darcs.haskell.org/ghc/rts/LdvProfile.c)`,`[`LdvProfile.h`](http://darcs.haskell.org/ghc/rts/LdvProfile.h)`::`\
`Lag-drag-voidprofiling(alsoknownasBiographicalProfiling).`

[`ProfHeap.c`](http://darcs.haskell.org/ghc/rts/ProfHeap.c)`,`[`ProfHeap.h`](http://darcs.haskell.org/ghc/rts/ProfHeap.h)`::`\
`Genericheap-profilngsupport.`

[`Profiling.c`](http://darcs.haskell.org/ghc/rts/Profiling.c)`,`[`Profiling.h`](http://darcs.haskell.org/ghc/rts/Profiling.h)`::`\
`Genericprofilngsupport.`

[`Proftimer.c`](http://darcs.haskell.org/ghc/rts/Proftimer.c)`,`[`Proftimer.h`](http://darcs.haskell.org/ghc/rts/Proftimer.h)`::`\
`Theprofilingtimer.`

[`RetainerProfile.c`](http://darcs.haskell.org/ghc/rts/RetainerProfile.c)`,`[`RetainerProfile.h`](http://darcs.haskell.org/ghc/rts/RetainerProfile.h)`::`\
[`RetainerSet.c`](http://darcs.haskell.org/ghc/rts/RetainerSet.c)`,`[`RetainerSet.h`](http://darcs.haskell.org/ghc/rts/RetainerSet.h)`::`\
`Retainerprofiling.`

[`Ticky.c`](http://darcs.haskell.org/ghc/rts/Ticky.c)`,`[`Ticky.h`](http://darcs.haskell.org/ghc/rts/Ticky.h)`::`\
`Ticky-tickyprofiling(currentlydefunct;needsreviving).`

### RTS Debugging

[`Printer.c`](http://darcs.haskell.org/ghc/rts/Printer.c)`,`[`Printer.h`](http://darcs.haskell.org/ghc/rts/Printer.h)`::`\
`Genericprintingforheapobjectsandstacks(notusedmuch).`

[`Trace.c`](http://darcs.haskell.org/ghc/rts/Trace.c)`,`[`Trace.h`](http://darcs.haskell.org/ghc/rts/Trace.h)`::`\
`Genericsupportforvariouskindsoftraceanddebuggingmessages.`

### The Front Panel

The front panel is currently defunct. It offers a graphical view of the
running Haskell program in real time, and was pretty cool when it
worked.

[`FrontPanel.c`](http://darcs.haskell.org/ghc/rts/FrontPanel.c)`,`[`FrontPanel.h`](http://darcs.haskell.org/ghc/rts/FrontPanel.h)`::`\
[`VisCallbacks.c`](http://darcs.haskell.org/ghc/rts/VisCallbacks.c)`,`[`VisCallbacks.h`](http://darcs.haskell.org/ghc/rts/VisCallbacks.h)`::`\
[`VisSupport.c`](http://darcs.haskell.org/ghc/rts/VisSupport.c)`,`[`VisSupport.h`](http://darcs.haskell.org/ghc/rts/VisSupport.h)`::`\
[`VisWindow.c`](http://darcs.haskell.org/ghc/rts/VisWindow.c)`,`[`VisWindow.h`](http://darcs.haskell.org/ghc/rts/VisWindow.h)`::`

### Other

[`Main.c`](http://darcs.haskell.org/ghc/rts/Main.c)`::`\
`TheC``functionforastandaloneHaskellprogram;`\
`basicallythisisjustaclientof``.`

[`RtsFlags.c`](http://darcs.haskell.org/ghc/rts/RtsFlags.c)`::`\
`Understandsthe``flags.`

[`RtsMessages.c`](http://darcs.haskell.org/ghc/rts/RtsMessages.c)`::`\
`Supportforemittingmessagesfromtheruntime.`

[`RtsSignals.c`](http://darcs.haskell.org/ghc/rts/RtsSignals.c)`,`[`RtsSignals.h`](http://darcs.haskell.org/ghc/rts/RtsSignals.h)`::`\
`Signal-relatedstuff.`

Miscellaneous stuff:

[`RtsUtils.c`](http://darcs.haskell.org/ghc/rts/RtsUtils.c)`,`[`RtsUtils.h`](http://darcs.haskell.org/ghc/rts/RtsUtils.h)`::`\
[`GetTime.h`](http://darcs.haskell.org/ghc/rts/GetTime.h)`::`\
[`PosixSource.h`](http://darcs.haskell.org/ghc/rts/PosixSource.h)`::`\
[`Prelude.h`](http://darcs.haskell.org/ghc/rts/Prelude.h)`::`\
[`Typeable.c`](http://darcs.haskell.org/ghc/rts/Typeable.c)`::`\
[`RtsDllMain.c`](http://darcs.haskell.org/ghc/rts/RtsDllMain.c)`::`

### OLD stuff

`::`\
`CodeforGUM:parallelGHC.Thisisheavilybitrottedandcurrentlydoesn'twork(asofGHC6.6;itlastworkedaround`\
`5.02Ibelieve).`

`::`\
`BitrottedcodeforGHC.NET.`

Sanity Checking
===============

Source code: [GhcFile(rts/Sanity.c)](GhcFile(rts/Sanity.c) "wikilink"),
[GhcFile(rts/Sanity.h)](GhcFile(rts/Sanity.h) "wikilink").

The purpose of sanity checking is to catch bugs in the RTS as early as
possible; if the program is going to crash, we want it to crash as soon
as possible after the error occurred. The problem with debugging the RTS
is that heap corruption can go unnoticed through several GC cycles,
making it particularly difficult to trace back to the erroneous code.

Sanity checking is turned on by the \`+RTS -DS\` option. We treat it
like an expensive assertion: normal assertions are allowed to take a few
extra percent of run time, so we don't mind having them on all the time
in a \`DEBUG\` RTS, but sanity checking may double the run time of the
program or worse. So the rule of thumb is that expensive assertions go
into sanity checking, cheap assertions are on in \`DEBUG\`, or possibly
even on all the time.

Sanity checking does a complete traversal of the heap after each GC to
look for dangling pointers (see \`checkHeap\` in
[GhcFile(rts/Sanity.c)](GhcFile(rts/Sanity.c) "wikilink")). For this it
needs to ensure that there is no \[wiki:Commentary/Rts/Storage/Slop
slop\], which is why we can only do this in a \`DEBUG\` runtime: the
slop-avoiding machinery is only on with \`DEBUG\`.

Sanity checking also turns on some other expensive checks: for example
in the \[wiki:Commentary/Rts/HaskellExecution\#Genericapply generic
apply\] code we check that the arguments point to valid closures.

[PageOutline](PageOutline "wikilink")

The Scheduler
=============

The scheduler is the heart of the runtime: it is the single part of the
system through which all entry to the Haskell world goes, and it handles
requests from outside to invoke Haskell functions (foreign export).

In this part of the commentary we'll discuss the *threaded* version of
the runtime (see \[wiki:Commentary/Rts/Config\]), that is, the version
of the runtime that uses multiple OS threads, because it is by far the
most complex beast.

See also [Edward Yang's blog
post](http://blog.ezyang.com/2013/01/the-ghc-scheduler/) (2013); some of
the material there has been incorporated here.

We begin by discussing the basic abstractions used in the scheduler.

OS Threads
----------

Source files:
[GhcFile(includes/rts/OSThreads.h)](GhcFile(includes/rts/OSThreads.h) "wikilink"),
[GhcFile(rts/win32/OSThreads.c)](GhcFile(rts/win32/OSThreads.c) "wikilink"),
[GhcFile(rts/posix/OSThreads.c)](GhcFile(rts/posix/OSThreads.c) "wikilink")

We assume that the OS provides some kind of native threads, and for SMP
parallelism we assume that the OS will schedule multiple OS threads
across the available CPUs.

OS threads are only used by the runtime for two reasons:

`*Tosupportnon-blockingforeigncalls:aforeigncall`\
`shouldnotblocktheotherHaskellthreadsinthesystemfrom`\
`running,andusingOSthreadsistheonlywaytoensurethat.`

`*TosupportSMPparallelism.`

Haskell threads are much lighter-weight (at least 100x) than OS threads.

When running on an SMP, we begin by creating the number of OS threads
specified by the \`+RTS -N\` option, although during the course of
running the program more OS threads might be created in order to
continue running Haskell code while foreign calls execute. Spare OS
threads are kept in a pool attached to each \`Capability\` (see
\[\#Capabilities\]).

The RTS provides a platform-independent abstraction layer for OS threads
in
[GhcFile(includes/rts/OSThreads.h)](GhcFile(includes/rts/OSThreads.h) "wikilink").

Haskell threads
---------------

A Haskell thread is represented by a Thread State Object
(\[wiki:Commentary/Rts/Storage/HeapObjects\#ThreadStateObjects TSO\]).
These objects are *garbage-collected*, like other closures in Haskell.
The TSO, along with the stack allocated with it (STACK), constitute the
primary memory overhead of a thread. Default stack size, in particular,
is controlled by the GC flag , and is 1k by default (Actually, your
usable stack will be a little smaller than that because this size also
includes the size of the struct, so that a lot of allocated threads will
fit nicely into a single block.) There are two kinds of Haskell thread:

`*A`*`bound`*`threadiscreatedastheresultofa`*`call-in`*`from`\
`outsideHaskell;thatis,acallto``or`\
```.Aboundthreadistiedtothe`\
`OSthreadthatmadethecall;allfurtherforeigncallsmadeby`\
`thisHaskellthreadaremadeinthesameOSthread.(thisispart`\
`ofthedesignoftheFFI,describedinthepaper`\
``[`Extending` `the` `Haskell` `Foreign` `Function` `Inteface` `with`
`Concurrency`](http://www.haskell.org/~simonmar/papers/conc-ffi.pdf)`).`

`*An`*`unbound`*`threadiscreatedby`\
```.Foreigncallsmadebyanunbound`\
`threadaremadebyanarbitraryOSthread.`

Initialization of TSOs is handled in in
[GhcFile(rts/Threads.c)](GhcFile(rts/Threads.c) "wikilink"); this
function is in turn invoked by , and in
[GhcFile(rts/RtsAPI.c)](GhcFile(rts/RtsAPI.c) "wikilink"). These
functions setup the initial stack state, which controls what the thread
executes when it actually gets run. These functions are the ones invoked
by the and other primops (recall entry-points for primops are located in
[GhcFile(rts/PrimOps.cmm)](GhcFile(rts/PrimOps.cmm) "wikilink")).

Being garbage collected has two major implications for TSOs. First, TSOs
are not GC roots, so they will get GC'd if there is nothing holding on
to them (e.g. [in the case of
deadlock](http://blog.ezyang.com/2011/07/blockedindefinitelyonmvar)),
and their space is not automatically reclaimed when they finish
executing (so can cause memory leaks}}}. Usually, a TSO will be retained
by a Capability

Seq magic
=========

The innocent-looking \`seq\` operator causes all manner of mayhem in
GHC. This page summarises the issues. See also discussion in Trac
\#5129, \#5262

The baseline position
---------------------

Our initial story was that \`(seq e1 e2)\` meant precisely Indeed this
was \`seq\`'s inlining. This translation validates some important rules

But this approach has problems; see \`Note \[Deguaring seq\]\` in
\`DsUtils\`.

### Problem 1 (Trac \#1031)

Consider The \`\[CoreSyn let/app invariant\]\` (see \`CoreSyn\`) means
that, other things being equal, because the argument to the outer
\`seq\` has an unlifted type, we'll use call-by-value thus: But that is
bad for two reasons:

`` *wenowevaluate`y`before`x`,and ``\
`` *wecan'tbind`v`toanunboxedpair ``

Seq is very, very special! Treating it as a two-argument function,
strict in both arguments, doesn't work. We "fixed" this by treating
\`seq\` as a language construct, desugared by the desugarer, rather than
as a function that may (or may not) be inlined by the simplifier. So the
above term is desugared to:

### Problem 2 (Trac \#2273)

Consider Here the \`seq\` is designed to plug the space leak of
retaining \`(snd x)\` for too long.

If we rely on the ordinary inlining of \`seq\`, we'll get But since
\`chp\` is cheap, and the case is an alluring contet, we'll inline
\`chp\` into the case scrutinee. Now there is only one use of \`chp\`,
so we'll inline a second copy. Alas, we've now ruined the purpose of the
seq, by re-introducing the space leak: We can try to avoid doing this by
ensuring that the binder-swap in the case happens, so we get his at an
early stage: But this is fragile. The real culprit is the source
program. Perhaps we should have said explicitly But that's painful. So
the desugarer does a little hack to make \`seq\` more robust: a
saturated application of \`seq\` is turned **directly** into the case
expression, thus: So we desugar our example to: And now all is well.

Be careful not to desugar which stupidly tries to bind the datacon
'True'. This is easily avoided.

The whole thing is a hack though; if you define \`mySeq=seq\`, the hack
won't work on \`mySeq\`.

### Problem 3 (Trac \#5262)

Consider With the above desugaring we get and now ete expansion gives
Now suppose that we have Plainly \`(length xs)\` should be evaluated...
but it isn't because \`f\` has arity 2. (Without -O this doesn't
happen.)

### Problem 4: seq in the IO monad

See the extensive discussion in Trac \#5129.

### Problem 5: the need for special rules

Roman found situations where he had where he knew that \`f\` (which was
strict in \`n\`) would terminate if n did. Notice that the result of
\`(f n)\` is discarded. So it makes sense to transform to Rather than
attempt some general analysis to support this, I've added enough support
that you can do this using a rewrite rule: You write that rule. When GHC
sees a case expression that discards its result, it mentally transforms
it to a call to \`seq\` and looks for a RULE. (This is done in
\`Simplify.rebuildCase\`.) As usual, the correctness of the rule is up
to you.

To make this work, we need to be careful that \`seq\` is **not**
desguared into a case expression on the LHS of a rule.

To increase applicability of these user-defined rules, we also have the
following built-in rule for \`seq\` This eliminates unnecessary casts
and also allows other seq rules to match more often. Notably, and now a
user-defined rule for \`seq\` may fire.

A better way
============

Here's our new plan.

`` *Introduceanewprimop`seq#::a->State#s->(#a,State#s#)`(seebe5441799b7d94646dcd4bfea15407883537eaaa) ``\
`` *Implement`seq#`byturningitintotheobviousevalinthebackend.Infact,sincethereturnconventionfor`(#State#s,a#)`isexactlythesameasfor`a`,wecanimplement`seq#sa`by`a`(evenwhenitappearsasacasescrutinee). ``\
`` *Define`evaluate`thus ``

That fixes problem 4.

We could go on and desugar \`seq\` thus:

and if we consider \`seq\#\` to be expensive, then we won't eta-expand
around it, and that would fix problem 3.

However, there is a concern that this might lead to performance
regressions in examples like this:

so \`f\` turns into

and we won't get to eta-expand the \`\\s\` as we would normally do (this
is pretty important for getting good performance from IO and ST monad
code).

Arguably \`f\` should be rewritten with a bang pattern, and we should
treat bang patterns as the eta-expandable seq and translate them
directly into \`case\`, not \`seq\#\`. But this would be a subtle
difference between \`seq\` and bang patterns.

Furthermore, we already have \`pseq\`, which is supposed to be a
"strictly ordered seq", that is it preserves evaluation order. So
perhaps \`pseq\` should be the one that more accurately implements the
programmer's intentions, leaving \`seq\` as it currently is.

We are currently pondering what to do here.

The GHC Commentary: Signals
===========================

This section describes how the RTS interacts with the OS signal
facilities. Throughout we use the term "signal" to refer to both
POSIX-style signals and Windows *ConsoleEvents*.

Signal handling differs between the *threaded* version of the runtime
and the non-threaded version (see \[wiki:Commentary/Rts/Config\]). Here
we discuss only the threaded version, since we expect that to become the
standard version in due course.

Source files:

`*POSIXsignalhandling:`\
`*`[`GhcFile(rts/posix/Signals.h)`](GhcFile(rts/posix/Signals.h) "wikilink")`,`[`GhcFile(rts/posix/Signals.c)`](GhcFile(rts/posix/Signals.c) "wikilink")\
`*Windowsconsoleevents:`\
`*`[`GhcFile(rts/win32/ConsoleHandler.h)`](GhcFile(rts/win32/ConsoleHandler.h) "wikilink")`,`[`GhcFile(rts/win32/ConsoleHandler.c)`](GhcFile(rts/win32/ConsoleHandler.c) "wikilink")

Signal handling in the RTS
--------------------------

The RTS is interested in two signals: a timer signal, and an interrupt
signal.

### The timer signal

The timer signal is used for several things:

`*Tocausethe[wiki:Commentary/Rts/Schedulerscheduler]tocontextswitch`\
`*Samplingfor[wiki:Commentary/Profilingtimeprofiling]`\
`*Todetectdeadlock(see[wiki:Commentary/Rts/Scheduler])`

Source files:

`*Thetimerinterrupthandler,andstarting/stoppingthetimer:`\
`*`[`GhcFile(rts/Timer.h)`](GhcFile(rts/Timer.h) "wikilink")`,`[`GhcFile(rts/Timer.c)`](GhcFile(rts/Timer.c) "wikilink")\
`*Platform-independenttickerinterface,usedbythetimer:`\
`*`[`GhcFile(rts/Ticker.h)`](GhcFile(rts/Ticker.h) "wikilink")\
`*Posiximplementationofticker:`\
`*`[`GhcFile(rts/posix/Itimer.h)`](GhcFile(rts/posix/Itimer.h) "wikilink")`,`[`GhcFile(rts/posix/Itimer.h)`](GhcFile(rts/posix/Itimer.h) "wikilink")\
`*Windowsimplementationofticker:`\
`*`[`GhcFile(rts/win32/Ticker.c)`](GhcFile(rts/win32/Ticker.c) "wikilink")

On Posix, the timer signal is implemented by calling \`timer\_create()\`
to generate regular \`SIGVTALRM\` signals (this was changed from SIGALRM
in \#850).

On Windows, we spawn a new thread that repeatedly sleeps for the timer
interval and then executes the timer interrupt handler.

The interrupt signal
--------------------

The interrupt signal is \`SIGINT\` on POSIX systems or
\`CTRL\_C\_EVENT/CTRL\_BREAK\_EVENT\`on Windows, and is normally sent to
the process when the user hits Control-C. By default, interrupts are
handled by the runtime. They can be caught and handled by Haskell code
instead, using \`System.Posix.Signals\` on POSIX systems or
\`GHC.ConsoleHandler\` on Windows systems. For example,
\[wiki:Commentary/Compiler/Backends/GHCi GHCi\] hooks the interrupt
signal so that it can abort the current interpreted computation and
return to the prompt, rather than terminating the whole GHCi process.

When the interrupt signal is received, the default behaviour of the
runtime is to attempt to shut down the Haskell program gracefully. It
does this by calling \`interruptStgRts()\` in
[GhcFile(rts/Schedule.c)](GhcFile(rts/Schedule.c) "wikilink") (see
\[wiki:Commentary/Rts/Scheduler\#ShuttingDown\]). If a second interrupt
signal is received, then we terminate the process immediately; this is
just in case the normal shutdown procedure failed or hung for some
reason, the user is always able to stop the process with two control-C
keystrokes.

Signal handling in Haskell code
-------------------------------

Source files:

`*POSIX:`[`GhcFile(rts/posix/Signals.h)`](GhcFile(rts/posix/Signals.h) "wikilink")`,`[`GhcFile(rts/posix/Signals.c)`](GhcFile(rts/posix/Signals.c) "wikilink")\
`*Windows:`[`GhcFile(rts/win32/ConsoleHandler.h)`](GhcFile(rts/win32/ConsoleHandler.h) "wikilink")`,`[`GhcFile(rts/win32/ConsoleHandler.c)`](GhcFile(rts/win32/ConsoleHandler.c) "wikilink")

A Haskell program can ask to install signal handlers, via the
\`System.Posix.Signals\` API, or \`GHC.ConsoleHandler\` on Windows. When
a signal arrives that has a Haskell handler, it is the job of the
runtime to create a new Haskell thread to run the signal handler and
place the new thread on the run queue of a suitable
\[wiki:Commentary/Rts/Scheduler\#Capabilities Capability\].

When the runtime is idle, the OS threads will all be waiting inside
\`yieldCapability()\`, waiting for some work to arrive. We want a signal
to be able to create a new Haskell thread and wake up one of these OS
threads to run it, but unfortunately the range of operations that can be
performed inside a POSIX signal handler is extremely limited, and
doesn't include any inter-thread synchronisation (because the signal
handler might be running on the same stack as the OS thread it is
communicating with).

The solution we use, on both Windows and POSIX systems, is to pass all
signals that arrive to the \[wiki:Commentary/Rts/IOManager IO Manager\]
thread. On POSIX this works by sending the signal number down a pipe, on
Windows it works by storing the signal number in a buffer and signaling
the IO Manager's \`Event\` object to wake it up. The IO Manager thread
then wakes up and creates a new thread for the signal handler, before
going back to sleep again.

RTS Alarm Signals and Foreign Libraries
---------------------------------------

When using foreign libraries through the Haskell FFI, it is important to
ensure that the foreign code is capable of dealing with system call
interrupts due to alarm signals GHC is generating.

For example, in this \`strace\` output a \`select\` call is interrupted,
but the foreign C code interprets the interrupt as an application error
and closes a critical file descriptor:

Once the C code was modified to deal with the interrupt properly, it
proceeded correctly (note that foreign call is restarted 3 times before
it succeeds).

Slop
====

Slop is unused memory between objects in the heap.

|| Object1 || ... Slop ... || Object2 ||

Why do we want to avoid slop?
-----------------------------

Slop makes it difficult to traverse an area of memory linearly, visiting
all the objects, because we can't tell where \`Object2\` starts in the
above diagram. We need to do linear traversals for two reasons,
currently:

`*[wiki:Commentary/Profiling/HeapHeapprofiling]needstoperformacensusonthewholeheap.`\
`*[wiki:Commentary/Rts/SanitySanitychecking]needstoensurethatallthepointersintheheap`\
`pointtovalidobjects.`

Additionally, linear traversals are useful for the mark phase of the
\[wiki:Commentary/Rts/Storage compacting garbage collector\], and would
be useful if we were to allow objects to be pinned arbitrarily
(currently pinned objects cannot contain pointers, which means they
don't need to be scavenged by the GC).

How does slop arise?
--------------------

Slop can arise for two reasons:

`*Thecompiledcodeallocatestoomuchmemory,andonlyfillspartofitwithobjects.Forexample,`\
`whencompilingcodeforafunctionlikethis:`

`thecodegeneratortakesthemaximumoftheheaprequirementsofe1ande2andaggregatesitinto`\
`` theheapcheckatthebeginningofthefunction`f`(toavoiddoingtoomanyheapchecks). ``\
`` Unfortunatelythatmeanseither`e1`or`e2`hastoomuchheapallocatedtoit,leavingsomeslop. ``\
`Wesolvethisproblembymovingtheheappointer`*`backwards`*`beforemakingatail-callif`\
`thereisanyheapslop.`

`*Whenanobjectisoverwrittenwithasmallerobject.Thishappensintwoways:`\
`[wiki:Commentary/Rts/HaskellExecution/UpdatesUpdates]and[wiki:Commentary/Rts/Storage/HeapObjects#BlackholesBlackHoles].`

What do we do about it?
-----------------------

We avoid the problem for \[wiki:Commentary/Profiling/Heap heap
profiling\] by arranging that we only ever do a census on a newly
garbage-collected heap, which has no slop in it (the garbage collector
never leaves slop between objects in the heap).

Slop does arise due to updates and black holes during normal execution,
and GHC does not attempt to avoid it (because avoiding or filling slop
during an update is costly). However, if we're doing
\[wiki:Commentary/Rts/Sanity sanity checking\], then we need to arrange
that slop is clearly marked: so in a \`DEBUG\` version of the RTS (see
\[wiki:Commentary/Rts/Config RTS configurations\]) the update code and
the blackhole code both arrange to fill slop with zeros: see the
\`FILL\_SLOP\` macro in
[GhcFile(rts/Updates.h)](GhcFile(rts/Updates.h) "wikilink"). Hence
sanity checking only works with a \`DEBUG\` version of the RTS.

[PageOutline](PageOutline "wikilink")

Layout of important files and directories
=========================================

This page summarises the overall file and directory structure of GHC. We
include both source files and generated files; the latter are always
identified "build-tree only".

Everything starts with the main GHC repository (see
\[wiki:Building/GettingTheSources\]). The build system calls that
directory \`\$(TOP)\`. All the paths below are relative to \`\$(TOP)\`.

Files in \`\$(TOP)\`
--------------------

**`` `packages` ``**`::`\
`` Despitethename"package",thisfilecontainsthemasterlistofthe*repositories*thatmakeupGHC.Itisparsedby`./boot`. ``

**`` `tarballs` ``**`::`\
`Liststhevarioustarballs(binarypackages)thatghcreliesonandwheretounpackthemduringabuild.`

**`` `validate` ``**`` ::Run`validate`(ashellscript)beforecommitting(see[wiki:TestingPatches]).Thescriptisdocumentedinthefileitself. ``

**`Documentation`
`files`**`` ::`README`,`ANNOUNCE`,`HACKING`,`LICENSE`,`new_tc_notes` ``

**`GNU` `autoconf`
`machinery`**`` ::`aclocal.m4`,`config.guess`,`config.sub`,`configure.ac`,`install-sh`,`config.mk.in`,`settings.in` ``

**`` `Makefile` ``**`::Thetop-level``:see[wiki:Building/ArchitectureGHCBuildSystemArchitecture].GHCrequires`\
``[`GNU` `make`](http://www.gnu.org/software/make/)`.`

**`Make` `system` `files`**`` ::`ghc.mk`,`MAKEHELP`,`SUBMAKEHELP` ``

\`libraries/\`
--------------

The \`libraries/\` directory contains all the packages that GHC needs to
build. It has one sub-directory for each package repository (e.g.
\`base\`, \`haskell98\`, \`random\`). Usually each such repository
builds just one package, but there is more than one in \`dph\`.

GHC's libraries are described in more detail on the
\[wiki:Commentary/Libraries libraries page\].

\`compiler/\`, \`docs/\`, \`ghc/\`
----------------------------------

These directories contain the main GHC compiler and documentation. The
\`compiler/\` directory contains the ghc package, which is linked into
an executable in the \`ghc/\` directory.

There is \[wiki:ModuleDependencies documentation of the intended module
dependency structure\] of the \`compiler/\` directory.

`*`**`` `compiler/ghc.cabal.in` ``**`` :theCabalfileforGHCisgeneratedfromthis.IfyouaddamoduletoGHC'ssourcecode,youmustadditinthe`ghc.cabal.in`filetoo,elseyou'llgetlinkerrors. ``

The following directories appear only in the build tree:

`*`**`` `compiler/stage1` ``**`` :generatedfilesforthestage1buildofGHC.Thereareahandfuloffiles(`ghc_boot_platform.h`etc),andadirectory`compiler/stage1/build/`thatcontainsallthe`.o`and`.hi`filesforthecompiler. ``\
`*`**`` `compiler/stage2` ``**`:similarlystage2.`

You can't run a binary from here: look in the \`inplace/\` directory
below for that.

\`rts/\`
--------

Sources for the runtime system; see \[wiki:Commentary/SourceTree/Rts\].

\`includes/\`
-------------

Header files for the runtime system; see
\[wiki:Commentary/SourceTree/Includes\].

\`utils/\`, \`libffi/\`
-----------------------

The \`utils\` directory contains support utilities that GHC uses.

These utils may be built with the bootstrapping compiler, for use during
the build, or with the stage1 or stage2 compiler, for installing. Some
of them are built with both; we can't install the utils built with the
bootstrapping compiler as they may use different versions of C
libraries. The reason we use sometimes stage2 rather than stage1 is that
some utils, e.g. haddock, need the GHC API package.

`*`**`` `utils/ghc-cabal` ``**`` isalittleprogramweuseforbuildingthelibraries.It'ssimilartocabal-install,butwithoutthedependencieson`http`etc. ``\
`*`**`` `utils/count_lines` ``**`isaprogramthatcountsthenumberofsource-codelinesinGHC'scode-base.Itdistinguishescommentsfromnon-comments.`

\`driver/\`
-----------

This contains some simple wrapper programs and scripts, for example the
\`ghci\` wrapper that invokes the \`ghc\` binary with the
\`--interactive\` flag. These wrappers tend to be executable programs on
Windows and scripts on Unix systems.

\`ghc-tarballs/\` (Windows only)
--------------------------------

This contains some tarball files (binary packages) that GHC relies upon.
Used for easier development / deployment on windows.

\`testsuite/\`, \`nofib/\`
--------------------------

The \`testsuite/\` and \`nofib/\` directories contain apparatus for
testing GHC.

`*[wiki:Building/RunningTests]`\
`*[wiki:Building/RunningNoFib]`

\`mk/\`, \`rules/\`
-------------------

The \`mk/\` and \`rules.mk\` directories contains all the build system
Makefile boilerplate; see \[wiki:Building/Architecture GHC Build System
Architecture\]. Some particular files are interesting:

`*`**`` `mk/build.mk` ``**`` :containsMakefilesettingsthatcontrolyourbuild.Details[wiki:Building/Usinghere].Thefile`mk/build.mk.sample`containsastartingpointthatyoucancopyto`mk/build.mk`ifyouwant. ``\
`*`**`` `mk/are-validating.mk` ``**`` :thisfilerecordsthefactthatyouaredoing[wiki:TestingPatchesvalidation],bycontainingthesingleline`Validating=YES`.Thatinturnmeansthethebuildsystemgetsitssettingsfrom`mk/validate-settings.mk`insteadoffrom`mk/build.mk`.Removethefiletostopvalidating. ``\
`*`**`` `mk/validate.mk` ``**`` :justlike`build.mk`,butapplieswhenvalidating.Usethisfiletooverridethedefaultsettingsforvalidation,whicharein`mk/validate-settings.mk`. ``

\`distrib/\`
------------

Miscellaneous files for building distributions.

Stuff that appears only in a build tree
---------------------------------------

### \`inplace/\`

The \`inplace/\` directory is where we "install" stage1 and stage2
compilers, and other utility programs, when they are built, to be used
when building other things in the build tree. The layout is exactly the
same as that of an installed GHC on the host platform.

`*`**`` `inplace/bin/` ``**`:executables,including`\
`` *`ghc-stage1` ``\
`` *`ghc-stage2` ``\
`` *`ghc-pkg` ``\
`` *`hasktags` ``\
`` *`hsc2hs` ``\
`` *`haddock` ``\
`` *`count_lines` ``\
`` *`compareSizes` ``

`*`**`` `inplace/lib/` ``**`:suppportinglibrariesfortheexecutables.`

### \`.../dist\*/\`

In many directories, \`dist\*\` subdirectories appear. These are where
Cabal, and the build system makefiles, put all of the files generated
while building. Some particularly interesting files are:

`*`**`` `docs/users_guide/users_guide/index.html` ``**`:theHTMLfortheusermanual`\
`*`**`` `libraries/` ``*`lib`*`` `/dist-install/doc/html/` ``*`lib`***`:containstheHaddock'ddocumentationforlibrary`*`lib`*

[PageOutline](PageOutline "wikilink")

------------------------------------------------------------------------

Stack Layout
------------

The stack-layout phase decides where to spill variables. The important
goals are to avoid memory traffic and to minimize the size of the stack
frame. Both of these goals are accomplished by reusing stack slots.

### Representing Stack Slots

For each stack slot, we introduce a new name, then treat the name as the
addressing expression for the slot. At the end of the pipeline, we
choose a stack layout, then replace each stack slot with its offset from
the stack pointer. The benefit is that we break the phase-ordering
problem: any phase of the compiler can name a stack slot.

For example, for a variable \`x\`, the expression \`SS(x)\` is the
address of the stack slot where we can spill \`x\`. (I don't think we
output any C-- that uses SS anymore, but the new code generator marks
its stack slots prior to layout with \`young<k> + 4\`, etc. -- Edward)
The stack is assumed to grow down, and we assume that the address
\`SS(x)\` points to the old end of the slot. Therefore, to address the
low address of a 4-byte slot, we would use the expression \`SS(x + 4)\`.
And we would spill \`x\` using the following instruction:

where refers to an address in memory.

But what about parameter passing? We use a similar technique, but this
time we describe the slot for each location as an offset within the area
where the parameters are passed. For example, we lower a function call

into approximately the following C--:

We use the following types to represent stack slots and
parameter-passing areas:

An \`Area\` represents space on the stack; it may use either the
\`RegSlot\` constructor to represent a single stack slot for a register
or the \`CallArea\` constructor to represent parameters passed to/from a
function call/return. In a young \`CallArea\`, the \`BlockId\` is the
label of the function call's continuation, and it passes parameters to
the call.

**Area layout and addressing**

`` *Each`Area`growsdown,towardslowermachineaddresses. ``\
`*`*`Offsets`*`` arealways-positivebytedisplacementswithinan`Area`. ``\
`*Thelow-offsetendisalsocalledthe"oldend"ofthearea,thehigh-offsetendisalsocalledthe"youngend".`\
`*Noticethatthelow-offset(old)endhashighermachineaddresses.`\
`*Offset0(ifweallowedit)wouldaddressthebyteone`*`beyond`*`` thehigh-addressendofthe`Area`. ``\
`` *Largeroffsets(fromthebeginningofthe`Area`)correspondtolowermachineaddresses. ``\
`` *Hence,toaddressa4-byteobjectattheoldendof`Area`a,weusetheoffset+4,thus`(CmmStackSlota4)`. ``

The \`Old\` call area is the initial state of the stack on entry to the
function (the overflow parameters and the return address) as well as any
arguments that will be passed to a tail call. (SLPJ believes that:) On
entry to the function, register \`Sp\` contains the address of the
youngest (lowest-address, highest offset) byte in the \`Old\` area.

Note that \`RegSlot\` areas are very small (since they only need to
store a single register), while \`CallArea\` are contiguous chunks of
arguments.

To name a specific location on the stack, we represent its address with
a new kind of \`CmmExpr\`: the \`CmmStackSlot\`. A \`CmmStackSlot\` is
just an integer offset into an \`Area\`. [BR](BR "wikilink")

Notice that a \`CmmStackSlot\` is an *address*, so we can say to make
\`Sp\` point to a particular stack slot. Use a \`CmmLoad\` to load from
the stack slot.

The following figure shows the layout of a \`CallArea\` for both the
outgoing parameters (function call) and incoming results (continuation
after returning from the function call). Note that the incoming and
outgoing parameters may be different, and they may overlap.

[Image(CallArea.png)](Image(CallArea.png) "wikilink")

A \`RegSlot\` is laid out in the same fashion, with the offset 0
pointing off the high byte of the stack slot. To address an 8-byte
double-word, we would use the offset 8. To address only the high word of
the same stack slot, we would use the offset 4.

Currently, the intermediate code does not explicitly use a virtual frame
pointer, but when we talk about offsets into the stack, we implicitly
assume that there is a virtual frame pointer that points just off the
oldest byte of the return address on entry to the procedures. Therefore,
on entry to the procedure, the offset of the (4-byte) return address is
4.

### Laying out the stack

The business of the stack-layout pass is to construct a mapping (fixed
across a single procedure) which assigns a virtual stack slot (i.e.
offset in bytes, relative to the virtual frame pointer) to each
\`Area\`.

A naive approach to laying out the stack would be to give each variable
its own stack slot for spilling, and allocate only the ends of the stack
frame for parameter-passing areas. But this approach misses two
opportunities for optimization:

`*Stackslotscanbereusedbyvariablesthatareneveronthestackatthesametime`\
`*Ifafunctionreturnsavariableonthestack,wemightbeabletousethereturnlocationasthevariable'sstackslot.`

As it turns out, it is quite common in GHC that the first definition of
a variable comes when its value is returned from a function call. If the
value is returned on the stack, then an important optimization is to
avoid copying that value to some other location on the stack. How is
that achieved? By making sure the location where the value is returned
is also its spill slot.

### A greedy algorithm

We rewrite the stack slots in two passes:

`` 1.Walkoverthegraphandchooseanoffsetforeach`Area`. ``\
`1.Walkoverthegraph,keepingtrackofthestackpointer,andrewriteeachaddressofastackslotwithanoffsetfromthestackpointer.Also,insertadjustmentstothestackpointerbeforeandafterprocpoints.`

The details are in cmm/CmmProcPointZ.hs (they have not yet been
committed, but will be soon - Aug 4, 2008).

Layout of the stack
===================

Every \[wiki:Commentary/Rts/HeapObjects\#ThreadStateObjects TSO object\]
contains a stack. The stack of a TSO grows downwards, with the topmost
(most recently pushed) word pointed to by , and the bottom of the stack
given by .

The stack consists of a sequence of *stack frames* (also sometimes
called *activation records*) where each frame has the same layout as a
heap object:

|| Header || Payload... ||

There are several kinds of
\[wiki:Commentary/Rts/Storage/Stack\#KindsofStackFrame stack frames\],
but the most common types are those pushed when evaluating a expression:
The code for evaluating a pushes a new stack frame representing the
alternatives of the case, and continues by evaluating . When completes,
it returns to the stack frame pushed earlier, which inspects the value
and selects the appropriate branch of the case. The stack frame for a
includes the values of all the free variables in the case alternatives.

Info tables for stack frames
----------------------------

The info table for a stack frame has a couple of extra fields in
addition to the \[wiki:Commentary/Rts/HeapObjects\#InfoTables basic info
table layout\]. A stack-frame info table is defined by in
[GhcFile(includes/rts/storage/InfoTables.h)](GhcFile(includes/rts/storage/InfoTables.h) "wikilink").

[Image(ret-itbl-no-rv.png)](Image(ret-itbl-no-rv.png) "wikilink")

The *SRT* field points to the static reference table (SRT) for this
stack frame (see \[wiki:Commentary/Rts/Storage/GC/CAFs\] for details of
SRTs).

Layout of the payload
---------------------

Unlike heap objects which mainly have "pointers first" layout, in a
stack frame the pointers and non-pointers are intermingled. This is so
that we can support "stack stubbing" whereby a live variable stored on
the stack can be later marked as dead simply by pushing a new stack
frame that identifies that slot as containing a non-pointer, so the GC
will not follow it.

Stack frames therefore have
\[wiki:Commentary/Rts/HeapObjects\#Bitmaplayout bitmap layout\].

Kinds of Stack Frame
--------------------

The constants for the different types of stack frame are defined in
[GhcFile(includes/rts/storage/ClosureTypes.h)](GhcFile(includes/rts/storage/ClosureTypes.h) "wikilink").
More details about the layouts are available in
[GhcFile(includes/rts/storage/Closures.h)](GhcFile(includes/rts/storage/Closures.h) "wikilink")

`*`\
`*`\
`*`\
`*``-(Explainedabithere:`[`https://ghc.haskell.org/trac/ghc/wiki/Commentary/Compiler/CPS#Notes`](https://ghc.haskell.org/trac/ghc/wiki/Commentary/Compiler/CPS#Notes)`)`\
`*`\
`*`\
`*``-Thestackischunkednow.Connectedasalinkedlist.(SinceDec2010:f30d527344db528618f64a25250a3be557d9f287,`[`Blogpost`](https://ghc.haskell.org/trac/ghc/blog/stack-chunks)`)`\
`*`\
`*`\
`*`\
`*`

Video: [STG
language](http://www.youtube.com/watch?v=v0J1iZ7F7W8&list=PLBkRCigjPwyeCSD_DFxpd246YIF7_RDDI)
(17'21")

The STG syntax data types
=========================

Before code generation, GHC converts the Core-language program into .
The basic ideas are still pretty much exactly as described in the paper
[Implementing lazy functional languages on stock hardware: the Spineless
Tagless
G-machine](http://research.microsoft.com/en-us/um/people/simonpj/papers/spineless-tagless-gmachine.ps.gz).

The best way to think of STG is as special form of
\[wiki:Commentary/Compiler/CoreSynType Core\]. Specifically, the
differences are these (see
[GhcFile(compiler/stgSyn/StgSyn.hs)](GhcFile(compiler/stgSyn/StgSyn.hs) "wikilink")):

`*Functionargumentsareatoms(literalsorvariables),oftype``.`\
`*Therighthandsideofalet-binding,``,iseither`\
`` *`StgRhsCon`:aconstructorapplication,or ``\
`` *`StgRhsClosure`: ``**`lambda-form`**`(possiblywithzeroarguments,inwhichcaseit'sathunk).`\
`*Constructorapplicationsaresaturated.`\
`*Applicationsofprimitiveoperatorsaresaturated.`\
`*Lambdascanonlyappeartheright-handsideofalet-binding.(Thereisanexpressionform``,butitisonlyusedduringtheCore-to-STGtransformation,notinavalidSTGprogram.)`\
`*Typeshavelargelybeendiscarded,retainingonlyenoughtypeinformationasisneededtoguidecodegeneration.Thereisan``checker,whichmakessomeconsistencychecks,butthe!CoreLintguaranteethat"iftheprogrampassesLintitcannotcrash"hasbeenlost.`

In addition, the STG program is decorated with the results of some
analyses:

`` *Everylambda-form(`StgRhsClosure`)listsitsfreevariables.Thesearethevariablesthatareinthethunkoffunctionclosurethatisallocatedbythelet. ``

`*Everylambda-formgivesits[wiki:Commentary/Rts/CAFs`**`Static`
`Reference`
`Table`**`]or`**`SRT`**`.YoushouldthinkoftheSRTasthe`*`top-level`*`freevariablesofthebody.Theydonotneedtobedynamicallyallocatedintheheapobject,buttheydoneedtobeaccessiblefromtheobject'sinfo-table,sothatthegarbagecollectorcanfindtheCAFskeptalivebytheobject.`

`*A``expressionisdecoratedwithits`**`live`
`variables`**`;thatis,variablesreachablefromthecontinuationofthecase.Moreprecisely,twosetsoflivevariables,plustheSRTforthecontinuation.Todo:saymore.`

`*TheSTGprogramhasanewconstructcalled`**`let-no-escape`**`,thatencodesso-called`**`join`
`points`**`.Variablesboundbyalet-no-escapeareguaranteedtobetail-calls,notembeddedinsideadatastructure,inwhichcasewedon`

GHC Commentary: Software Transactional Memory (STM)
===================================================

This document gives an overview of the runtime system (RTS) support for
GHC's STM implementation. We will focus on the case where fine grain
locking is used ().

Some details about the implementation can be found in the papers
["Composable Memory
Transactions"](http://research.microsoft.com/en-us/um/people/simonpj/papers/stm/stm.pdf)
and ["Transactional memory with data
invariants"](http://research.microsoft.com/en-us/um/people/simonpj/papers/stm/stm-invariants.pdf).
Additional details can be found in the Harris et al book ["Transactional
memory"](http://www.morganclaypool.com/doi/abs/10.2200/s00272ed1v01y201006cac011).
Some analysis on performance can be found in the paper ["The Limits of
Software Transactional
Memory"](https://www.bscmsrc.eu/sites/default/files/cf-final.pdf) though
this work only looks at the coarse grain lock version. Many of the other
details here are gleaned from the comments in the source code.

Background
==========

This document assumes the reader is familiar with some general details
of GHC's execution and memory layout. A good starting point for this
information is can be found here:
\[wiki:Commentary/Compiler/GeneratedCode Generated Code\].

Definitions
-----------

### Useful RTS terms

`CorrespondstoaCPU.ThenumberofcapabilitiesshouldmatchthenumberofCPUs.See[wiki:Commentary/Rts/Scheduler#CapabilitiesCapabilities].`

TSO

`ThreadStateObject.ThestateofaHaskellthread.See[wiki:Commentary/Rts/Storage/HeapObjects#ThreadStateObjectsThreadStateObjects].`

Heap object

`Objectsontheheapalltaketheformofan``structurewithaheaderpointingandapayloadofdata.Theheaderpointstocodeandaninfotable.See[wiki:Commentary/Rts/Storage/HeapObjectsHeapObjects].`

### Transactional Memory terms

Read set

`Thesetof``sthatareread,butnotwrittentoduringatransaction.`

Write set

`Thesetof``sthatarewrittentoduringatransaction.Inthecodeeachwritten``iscalledan"updateentry"inthetransactionalrecord.`

Access set

`All``saccessedduringthetransaction.`

While GHC's STM does not have a separate read set and write set these
terms are useful for discussion.

Retry

`HerewewillusethetermretryexclusivelyfortheblockingprimitiveinGHC'sSTM.Thisshouldnotbeconfusedwiththestepstakenwhenatransactiondetectsthatithasseenaninconsistentviewofmemoryandmuststartagainfromthebeginning.`

Failure

`Afailedtransactionisonethathasseeninconsistentstate.Thisshouldnotbeconfusedwithasuccessfultransactionthatexecutesthe``primitive.`

------------------------------------------------------------------------

Overview of Features
====================

At the high level, transactions are computations that read and write to
s with changes only being committed atomically after seeing a consistent
view of memory. Transactions can also be composed together, building new
transactions out of existing transactions. In the RTS each transaction
keeps a record of its interaction with the s it touches in a . A pointer
to this record is stored in the TSO that is running the transaction.

Reading and Writing
-------------------

The semantics of a transaction require that when a is read in a
transaction, its value will stay the same for the duration of execution.
Similarly a write to a will keep the same value for the duration of the
transaction. The transaction itself, however, from the perspective of
other threads can apply all of its effects in one moment. That is, other
threads cannot see intermediate states of the transaction, so it is as
if all the effects happen in a single moment.

As a simple example we can consider a transaction that transfers value
between two accounts:

No other thread can observe the value in without also observing in .

Blocking
--------

Transactions can choose to block until changes are made to s that allow
it to try again. This is enabled with an explicit . Note that when
changes are made the transaction is restarted from the beginning.

Continuing the example, we can choose to block when there are
insufficient funds:

Choice
------

Any blocking transaction can be composed with to choose an alternative
transaction to run instead of blocking. The primitive operation creates
a nested transaction and if this first transaction executes , the
effects of the nested transaction are rolled back and the alternative
transaction is executed. This choice is biased towards the first
parameter. A validation failure in the first branch aborts the entire
transaction, not just the nested part. An explicit is the only mechanism
that gives partial rollback.

We now can choose the account that has enough funds for the transfer:

Data Invariants
---------------

Invariants support checking global data invariants beyond the atomicity
transactions demand. For instance, a transactional linked list (written
correctly) will never have an inconsistent structure due to the
atomicity of updates. It is no harder to maintain this property in a
concurrent setting then in a sequential one with STM. It may be desired,
however, to make statements about the consistency of the *data* in a
particular a sorted linked list is sorted, not because of the structure
(where the s point to) but instead because of the data in the structure
(the relation between the data in adjacent nodes). Global data invariant
checks can be introduced with the operation which demands that the
transaction it is given results in and that it continues to hold for
every transaction that is committed globally.

We can use data invariants to guard against negative balances:

Exceptions
----------

Exceptions inside transactions should only propagate outside if the
transaction has seen a consistent view of memory. Note that the
semantics of exceptions allow the exception itself to capture the view
of memory from inside the transaction, but this transaction is not
committed.

------------------------------------------------------------------------

Overview of the Implementation
==============================

We will start this section by considering building GHC's STM with only
the features of reading and writing. Then we will add then and finally
data invariants. Each of the subsequent features adds more complexity to
the implementation. Taken all at once it can be difficult to understand
the subtlety of some of the design choices.

------------------------------------------------------------------------

Transactions that Read and Write.
---------------------------------

With this simplified view we only support , , and as well as all the STM
type class instances except .

### Transactional Record

The overall scheme of GHC's STM is to perform all the effects of a
transaction locally in the transactional record or . Once the
transaction has finished its work locally, a value based consistency
check determines if the values read for the entire access set are
consistent. This only needs to consider the and the main memory view of
the access set as it is assumed that main memory is always consistent.
This check also obtains locks for the write set and with those locks we
can update main memory and unlock. Rolling back the effects of a
transaction is just forgetting the current and starting again.

The transactional record itself will have an entry for each
transactional variable that is accessed. Each entry has a pointer to the
heap object and a record of the value that the held when it was first
accessed.

### Starting

A transaction starts by initializing a new () assigning the TSO's
pointer to the new then executing the transaction's code.

(See [GhcFile(rts/PrimOps.cmm)](GhcFile(rts/PrimOps.cmm) "wikilink") and
[GhcFile(rts/STM.c)](GhcFile(rts/STM.c) "wikilink") ).

### Reading

When a read is attempted we first search the for an existing entry. If
it is found, we use that local view of the variable. On the first read
of the variable, a new entry is allocated and the value of the variable
is read and stored locally. The original does not need to be accessed
again for its value until a validation check is needed.

In the coarse grain version, the read is done without synchronization.
With the fine grain lock, the lock variable is the of the structure.
While reading an inconsistent value is an issue that can be resolved
later, reading a value that indicates a lock and handing that value to
code that expects a different type of heap object will almost certainly
lead to a runtime failure. To avoid this the fine grain lock version of
the code will spin if the value read is a lock, waiting to observe the
lock released with an appropriate pointer to a heap object.

(See [GhcFile(rts/STM.c)](GhcFile(rts/STM.c) "wikilink") )

### Writing

Writing to a requires that the variable first be in the . If it is not
currently in the , a read of the 's value is stored in a new entry (this
value will be used to validate and ensure that no updates were made
concurrently to this variable).

In both the fine grain and coarse grain lock versions of the code no
synchronization is needed to perform the write as the value is stored
locally in the until commit time.

(See [GhcFile(rts/STM.c)](GhcFile(rts/STM.c) "wikilink") )

### Validation

Before a transaction can make its effects visible to other threads it
must check that it has seen a consistent view of memory while it was
executing. Most of the work is done in by checking that s hold their
expected values.

For the coarse grain lock version the lock is held before entering
through the writing of values to s. With the fine grain lock, validation
acquires locks for the write set and reads a version number consistent
with the expected value for each in the read set. After all the locks
for writes have been acquired, The read set is checked again to see if
each value is still the expected value and the version number still
matches ().

(See [GhcFile(rts/STM.c)](GhcFile(rts/STM.c) "wikilink") and )

### Committing

Before committing, each invariant associated with each accessed needs to
be checked by running the invariant transaction with its own . The read
set for each invariant is merged into the transaction as those reads
must be included in the consistency check. The is then validated. If
validation fails, the transaction must start over from the beginning
after releasing all locks. In the case of the coarse grain lock
validation and commit are in a critical section protected by the global
STM lock. Updates to s proceeds while holding the global lock.

With the fine grain lock version when validation, including any
read-only phase, succeeds, two properties will hold simultaneously that
give the desired atomicity:

-   Validation has witnessed all s with their expected value.
-   Locks are held for all of the s in the write set.

Commit can proceed to increment each locked 's field and unlock by
writing the new value to the field. While these updates happen
one-by-one, any attempt to read from this set will spin while the lock
is held. Any reads made before the lock was acquired will fail to
validate as the number of updates will change.

(See [GhcFile(rts/PrimOps.cmm)](GhcFile(rts/PrimOps.cmm) "wikilink") and
[GhcFile(rts/STM.c)](GhcFile(rts/STM.c) "wikilink") )

### Aborting

Aborting is simply throwing away changes that are stored in the .

(See [GhcFile(rts/STM.c)](GhcFile(rts/STM.c) "wikilink") )

### Exceptions

An exception in a transaction will only propagate outside of the
transaction if the transaction can be validated. If validation fails,
the whole transaction will abort and start again from the beginning.
Nothing special needs to be done to support the semantics allowing the
view *inside* the aborted transaction.

(See [GhcFile(rts/Exception.cmm)](GhcFile(rts/Exception.cmm) "wikilink")
which calls from [GhcFile(rts/STM.c)](GhcFile(rts/STM.c) "wikilink")).

------------------------------------------------------------------------

Blocking with 
--------------

We will now introduce the blocking feature. To support this we will add
a watch queue to each where we can place a pointer to a blocked TSO.
When a transaction commits we will now wake up the TSOs on watch queues
for s that are written.

The mechanism for is similar to exception handling. In the simple case
of only supporting blocking and not supporting choice, an encountered
retry should validate, and if valid, add the TSO to the watch queue of
every accessed (see [GhcFile(rts/STM.c)](GhcFile(rts/STM.c) "wikilink")
and ). Locks are acquired for all s when validating to control access to
the watch queues and prevent missing an update to a before the thread is
sleeping. In particular if validation is successful the locks are held
after the return of , through the return to the scheduler, after the
thread is safely paused (see
[GhcFile(rts/HeapStackCheck.cmm)](GhcFile(rts/HeapStackCheck.cmm) "wikilink")
), and until is called. This ensures that no updates to the s are made
until the TSO is ready to be woken. If validation fails, the is
discarded and the transaction is started from the beginning. (See
[GhcFile(rts/PrimOps.cmm)](GhcFile(rts/PrimOps.cmm) "wikilink") )

When a transaction is committed, each write that it makes to a is
preceded by waking up each TSO in the watch queue. Eventually these TSOs
will be run, but before restarting the transaction its is validated
again if valid then nothing has changed that will allow the transaction
to proceed with a different result. If invalid, some other transaction
has committed and progress may be possible (note there is the additional
case that some other transaction is merely holding a lock temporarily,
causing validation to fail). The TSO is not removed from the watch
queues it is on until the transaction is aborted (at this point we no
longer need the ) and the abort happens after the failure to validate on
wakeup. (See [GhcFile(rts/STM.c)](GhcFile(rts/STM.c) "wikilink") and )

------------------------------------------------------------------------

Choice with 
------------

When executes it searches the stack for either a or the outer (the
boundary between normal execution and the transaction). The former is
placed on the stack by an (see
[GhcFile(rts/PrimOps.cmm)](GhcFile(rts/PrimOps.cmm) "wikilink") ) and if
executing the first branch we can partially abort and switch to the
second branch, otherwise we propagate the further. In the latter case
this represents a transaction that should block and the behavior is as
above with only .

How do we support a "partial abort"? This introduces the need for a
nested transaction. Our will now have a pointer to an outer (the field).
This allows us to isolate effects from the branch of the that we might
need to abort. Let's revisit the features that need to take this into
account.

`*`**`Reading`**`--Readsnowsearchthechainofnestedtransactionsinadditiontothelocal``.Whenanentryisfoundinaparentitiscopiedintothelocal``.Notethatthereisstillonlyasingleaccesstotheactual``throughthelifeofthetransaction(untilvalidation).`\
`*`**`Writing`**`--Writes,likereads,nowsearchtheparent``sandthewriteisstoredinthelocalcopy.`\
`*`**`Retry`**`--Asdescribedabove,wenowneedtosearchthestackfora``andiffound,abortingthenestedtransactionandattemptingthealternativeorpropagatingtheretryinsteadofimmediatelyworkingonblocking.`\
`*`**`Validation`**`--Ifwearevalidatinginthemiddleofarunningtransactionwewillneedtovalidatethewholenestoftransactions.`[`BR`](BR "wikilink")`(See`[`GhcFile(rts/STM.c)`](GhcFile(rts/STM.c) "wikilink")```anditsusesin`[`GhcFile(rts/Exception.cmm)`](GhcFile(rts/Exception.cmm) "wikilink")`and`[`GhcFile(rts/Schedule.c)`](GhcFile(rts/Schedule.c) "wikilink")`)`\
`*`**`Committing`**`--Justaswenowhaveapartialabort,weneedapartialcommitwhenwefinishabranchofan``.Thiscommitisdonewith``whichvalidatesjusttheinner``andmergesupdatesbackintoitsparent.Notethatanupdateisdistinguishedfromareadonlyentrybyvalue.Thismeansthatifanestedtransactionperformsawritethatrevertsavaluethisisachangeandmuststillpropagatetotheparent(seeticket#7493).`\
`*`**`Aborting`**`--Thereisanothersubtleissuewithhowchoiceandblockinginteract.Whenweblockweneedtowakeupifthereisachangeto`*`any`*`accessed``.Consideratransaction:`[`BR`](BR "wikilink")[`BRIf`](BR "wikilink")`both``and``execute``theneventhoughtheeffectsof``arethrownaway,itcouldbethatachangetoa``thatisonlyintheaccesssetof``willallowthewholetransactiontosucceedwhenitiswoken.`[`BRTo`](BR "wikilink")`solvethisproblem,whenabranchonanestedtransactionisabortedtheaccesssetofthenestedtransactionismergedasareadsetintotheparent``.Specificallyifthe``isin`*`any`*```upthechainofnestedtransactionsitmustbeignored,otherwiseitisenteredasanewentry(retainingjusttheread)intheparent``.`[`BR`](BR "wikilink")`(Seeagainticket#7493and`[`GhcFile(rts/STM.c)`](GhcFile(rts/STM.c) "wikilink")```)`\
`*`**`Exceptions`**`--Theonlychangeneededhereeach``onthestackrepresentsanestedtransaction.Asthestackissearchedforahandler,ateachencountered``thenestedtransactionisaborted.Whenthe``isencounteredwethenknowthatthereisnonestedtransaction.`[`BR`](BR "wikilink")`(See`[`GhcFile(rts/Exception.cmm)`](GhcFile(rts/Exception.cmm) "wikilink")```)`

(See [GhcFile(rts/PrimOps.cmm)](GhcFile(rts/PrimOps.cmm) "wikilink") and
)

------------------------------------------------------------------------

Invariants
----------

We will start this section with an overview of some of the details then
review with notes on the changes from the choice case.

### Details

As a transaction is executing it can collect dynamically checked data
invariants. These invariants are transactions that are never committed,
but if they raise an exception when executed successfully that exception
will propagate out of the atomic frame.

`Primitiveoperationthataddsaninvariant(transactiontorun)tothequeueofthecurrent``bycalling``.`

`Awrapperfor``(togiveitthe``type).`

`Thisisthe``fromthe"Transactionalmemorywithdatainvariants"paper.Theactionimmediatelyruns,wrappedinanestedtransactionsothatitwillnevercommitbutwillhaveanopportunitytoraiseanexception.Ifsuccessful,theoriginallypassedactionisaddedtotheinvariantqueue.`

`Takesan``actionthatresultsina``andaddsaninvariantthatthrowsanexceptionwhentheresultofthetransactionis``.`

The bookkeeping for invariants is in each s queue and the s field. Each
invariant is in a structure that includes the action, the where it was
last executed, and a lock. This is added to the current s queue when is
executed.

When a transaction completes, execution will reach the and the s will be
(a nested transaction would have a before the to handle cases of
non-empty ). The frame will then check the invariants by collecting the
invariants it needs to check with , dequeuing each, executing, and when
(or if) we get back to the frame, aborting the invariant action. If the
invariant failed to hold, we would not get here due to an exception and
if it succeeds we do not want its effects. Once all the invariants have
been checked, the frame will to commit.

Which invariants need to be checked for a given transaction? Clearly
invariants introduced in the transaction will be checked these are added
to the s queue directly when is executed. In addition, once the
transaction has finished executing, we can look at each entry in the
write set and search its watch queue for any invariants.

Note that there is a in the package in which matches the from the
[beauty](http://research.microsoft.com/pubs/74063/beautiful.pdf) chapter
of "Beautiful code":

It requires no additional runtime support. If it is a transaction that
produces the argument it will be committed (when ) and it is only a one
time check, not an invariant that will be checked at commits.

### Changes from Choice

With the addition of data invariants we have the following changes to
the implementation:

`*`**`Retrying`**`--Aretryinaninvariantindicatesthattheinvariantcouldnotproceedandthewholetransactionshouldblock.Thisspecialcaseisdetectedwhenan``isencounteredwithanestoftransactions(i.e.whenthe``fieldisnot``).Theinvariantissimplyabortedandexecutionproceedsto``(see`[`GhcFile(rts/PrimOps.cmm)`](GhcFile(rts/PrimOps.cmm) "wikilink")```).`\
`*`**`Commiting`**`--Commitnowneedsaphasewhereitrunsinvariantsafterthecodeofthetransactionhascompletedbutbeforecommit.Theimplementationrecyclesthestructurealreadyinplaceforthisphasesospecialcasesareneededinthe``thatcollectsinvariantsandworksthroughthemoneatatimethenmovesontocommitting(see`[`GhcFile(rts/PrimOps.cmm)`](GhcFile(rts/PrimOps.cmm) "wikilink")```).`[`BRTo`](BR "wikilink")`efficientlyhandleinvariantstheyneedtoonlybecheckedwhenarelevantdatadependencychanges.Thismeanswecanassociatethemwiththe``ofthelastcommitthatneededtochecktheinvariantatthecostofserializinginvarianthandlingcommits.Thisisenforcedbythelockoneachinvariant.Ifitcannotbeacquiredthewholetransactionmuststartover.`[`BRAt`](BR "wikilink")`committime,eachinvariantislockedandthereadsetforthelastcommitedtransactionofeachinvariantismergedintothe``.`[`BRValidation`](BR "wikilink")`acuqireslockforallentriesinthe``(notjustthewrites).Aftervalidation,eachinvariantisremovedfromthewatchqueueofeach``itpreviouslydependedon,thenthe``thatwasusedwhenexecutingtheinvariantcodeisupdatedtoreflectthevaluesfromthefinalexecutionofthemaintransactionandeach``,beingadatadepenencyoftheinvariant,hastheinvariantaddedtoitswatchqueue.`[`BR`](BR "wikilink")`(See`[`GhcFile(rts/STM.c)`](GhcFile(rts/STM.c) "wikilink")```,``and``)`\
`*`**`Exceptions`**`--Whenanexceptionpropagatestothe``therearenowtwostatesthatitcouldencounter.Ifthereisnoenclosing``wearenotdealingwithanexceptionfromaninvariantanditproceedsasabove.Seeinganestoftransactionsindicatesthatthetransactionwascheckinganinvariantwhenitencounteredtheexception.Theeffectofafailedinvariant`*`is`*`thisexceptionsonothingspecialneedstobedoneexcepttovalidateandabortboththeoutertransactionandthenestedtransaction(see`[`GhcFile(rts/Exception.cmm)`](GhcFile(rts/Exception.cmm) "wikilink")```).`

------------------------------------------------------------------------

Other Details
-------------

This section describes some details that can be discussed largely in
isolation from the rest of the system.

### Detecting Long Running Transactions

While the type system enforces STM actions to be constrained to STM side
effects, pure computations in Haskell can be non-terminating. It could
be that a transaction sees inconsistent data that leads to
non-termination that would never happen in a program that only saw
consistent data. To detect this problem, every time a thread yields it
is validated. A validation failure causes the transaction to be
condemned.

### Transaction State

Each has a field that holds the status of the transaction. It can be one
of the following:

`Thetransactionisactivelyrunning.`

`Thetransactionhasseenaninconsistency.`

`Thetransactionhascommittedandisintheprocessofupdating``values.`

`Thetransactionhasabortedandisworkingtoreleaselocks.`

`Thetransactionhashita``andiswaitingtobewoken.`

If a state is (some inconsistency was seen) validate does nothing. When
a top-level transaction is aborted in , if the state is it will remove
the watch queue entries for the . Similarly if a waiting is condemned
via an asynchronous exception when a validation failure is observed
after a thread yield, its watch queue entries are removed. Finally a in
the state is not condemned by a validation. In this case the is already
waiting for a wake up from a that changes and observing an inconsistency
merely indicates that this will happen soon.

In the work of Keir Fraser a transaction state is used for cooperative
efforts of transactions to give lock-free properties for STM systems.
The design of GHC's STM is clearly influenced by this work and seems
close to some of the algorithms in Fraser's work. It does not, however,
implement what would be required to be lock-free or live-lock free (in
the fine grain lock code). For instance, if two transactions and are
committing at the same time and has read and written while has read and
written , both the transactions can fail to commit. For example,
consider the interleaving:

||**\`T1\`** || **\`TVar\`** || **\`T2\`** ||**Action** || ||\`A 0 0\`
|| \`A 0\` || ||\`T1\` read A || || || \`B 0\` || \`B 0 0\` ||\`T2\`
read B || ||\`B 0 1\` || || ||\`T1\` write B 1 || || || || \`A 0 1\`
||\`T2\` write A 1 || ||\`A 0 0 0\` || \`A 0\` || ||\`T1\` Validation
Part 1 (read A) || || || \`A T2\` || ||\`T2\` Validation (Lock A) || ||
|| \`B 0\` || \`B 0 0 0\` ||\`T2\` Validation (Read B) || || || \`B T1\`
|| ||\`T1\` Validation Part 2 (Lock B) ||

Note: the first and third columns are the local state of the s and the
second column is the values of the structures. Each entry has the
expected value followed by the new value and a number of updates field
when it is read for validation.

At this point and both perform their and both could (at least one will)
discover that a in their read set is now locked. This leads to both
transactions aborting. The chances of this are narrow but not impossible
(see ticket \#7815). Fraser's work avoids this by using the transaction
status and the fact that locks point back to the holding the lock to
detect other transactions in a read only check (read phase) and
resolving conflicts so that at least one of the transactions can commit.

A simpler example can also cause both transactions to abort. Consider
two transactions with the same write set, but the writes entered the s
in a different order. Both transactions could encounter a lock from the
other before they have a chance to release locks and get out of the way.
Having an ordering on lock could avoid this problem but would add a
little more complexity.

### GC and ABA

GHC's STM does comparisons for validation by value. Since these are
always pure computations these values are represented by heap objects
and a simple pointer comparison is sufficient to know if the same value
is in place. This presents an ABA problem however if the location of
some value is recycled it could appear as though the value has not
changed when, in fact, it is a different value. This is avoided by
making the fields of the entries pointers into the heap followed by the
garbage collector. As long as a is still alive it will keep the original
value it read for a alive.

### Management of s

The structure is built as a list of chunks to give better locality and
amortize the cost of searching and allocating entries. Additionally s
are recycled to aid locality further when a transaction is aborted and
started again. Both of these details add a little complexity to the
implementation that is abated with some macros such as and .

### Tokens and Version Numbers.

When validating a transaction each entry in the is checked for
consistency. Any entry that is an update (in the write set) is locked.
This locking is a visible effect to the rest of the system and prevents
other committing transactions from progress. Reads, however, are not
going to be updated. Instead we check that a read to the value matches
our expected value, then we read a version number (the field) and check
again that the expected value holds. This gives us a read of that is
consistent with the holding the expected value. Once all the locks for
the write set are acquired we know that only our transaction can have an
effect on the write set. All that remains is to rule out some change to
the read set while we were still acquiring locks for the writes. This is
done in the read phase (with ) which checks first if the value matches
the expectation then checks if the version numbers match. If this holds
for each entry in the read set then there must have existed a moment,
while we held the locks for all the write set, where the read set held
all its values. Even if some other transaction committed a new value and
yet another transaction committed the expected value back the version
number will have been incremented.

All that remains is managing these version numbers. When a is updated
its version number is incremented before the value is updated with the
lock release. There is the unlikely case that the finite version numbers
wrap around to an expected value while the transaction is committing
(even with a 32-bit version number this is *highly* unlikely to happen).
This is, however, accounted for by allocating a batch of tokens to each
capability from a global variable. Each time a transaction is started it
decrements it's batch of tokens. By sampling at the beginning of commit
and after the read phase the possibility of an overflow can be detected
(when more then 32-bits worth of commits have been allocated out).

(See [GhcFile(rts/STM.c)](GhcFile(rts/STM.c) "wikilink") , , , , and )

### Implementation Invariants

Some of the invariants of the implementation:

`*Locksareonlyacquiredin`[`GhcFile(rts/STM.c)`](GhcFile(rts/STM.c) "wikilink")`andarealwaysreleasedbeforetheendofafunctioncall(withtheexceptionof``whichmustreleaselocksafterthethreadissafe).`\
`*Whenrunningatransactioneach``isreadexactlyonceandifitisawrite,isupdatedexactlyonce.`\
`*Mainmemory(``s)alwaysholdsconsistentvaluesorlocksofapartiallyupdatedcommit.Thatisasetofreadsatanymomentfrom``swillresultinconsistentdataifnoneofthevaluesarelocks.`\
`*Anestof``shasamatchingnestof``sendingwithan``onthestack.Oneexceptiontothisiswhencheckingdatainvariantstheinvariant's``isnestedunderthetoplevel``withouta``.`

### Fine Grain Locking

The locks in fine grain locking () are at the level and are implemented
by placing the locking thread's in the s current value using a compare
and swap (). The value observed when locking is returned by . To test if
a is locked the value is inspected to see if it is a (checking that the
closure's info table pointer is to ). If a is found will spin reading
the s current value until it is not a and then attempt again to obtain
the lock. Unlocking is simply a write of the current value of the .
There is also a conditional lock which will obtain the lock if the s
current value is the given expected value. If the is already locked this
will not be the case (the value would be a ) and if the has been updated
to a new (different) value then locking will fail because the value does
not match the expected value. A compare and swap is used for .

This arrangement is useful for allowing a transaction that encounters a
locked to know which particular transaction is locked (used in
algorithms in from Fraser). GHC's STM does not, however, use this
information.

Bibliography
------------

Fraser, Keir. *Practical lock-freedom*. Diss. PhD thesis, University of
Cambridge Computer Laboratory, 2004.

Jones, Simon Peyton. "Beautiful concurrency." *Beautiful Code: Leading
Programmers Explain How They Think* (2007): 385-406.

Harris, Tim, et al. "Composable memory transactions." *Proceedings of
the tenth ACM SIGPLAN symposium on Principles and practice of parallel
programming.* ACM, 2005.

Harris, Tim, James Larus, and Ravi Rajwar. "Transactional memory."
*Synthesis Lectures on Computer Architecture* 5.1 (2010): 1-263.

Harris, Tim, and Simon Peyton Jones. "Transactional memory with data
invariants." *First ACM SIGPLAN Workshop on Languages, Compilers, and
Hardware Support for Transactional Computing (TRANSACT'06), Ottowa.*
2006.

GHC Commentary: Storage
=======================

GHC's storage manager is designed to be quite flexible: there are a
large number of tunable parameters in the garbage collector, and partly
the reason for this was because we wanted to experiment with tweaking
these settings in the context of Haskell.

[Image(sm-top.png)](Image(sm-top.png) "wikilink")

`*[wiki:Commentary/Rts/Storage/HeapObjectsLayoutofHeapObjects]`\
`*[wiki:Commentary/Rts/Storage/StackLayoutoftheStack]`\
`*[wiki:Commentary/Rts/Storage/SlopSlop]`\
`*[wiki:Commentary/Rts/Storage/BlockAllocTheBlockAllocator]`\
`*[wiki:Commentary/Rts/Storage/GCTheGarbageCollector]`\
`*[wiki:Commentary/Rts/Storage/HeapAllocedTheHEAP_ALLOCED()macro]`

See also:

`*[wiki:Commentary/Rts/HaskellExecution/PointerTaggingPointertagging]`

General overview
================

GHC's approach to strictness analysis is that of "demand analysis", a
backwards analysis in which strictness analysis and absence analysis are
done in a single pass. In the future, analysis to perform unboxing, as
well as other analyses, may be implemented within this framework as
well.

IMPORTANT NOTE
==============

The rest of this commentary describes code that is not checked in to the
HEAD yet.

Update: as of 2014-02-12, newer documentation (apparently on the same
topic and apparently more up-to-date) is available at
[Commentary/Compiler/Demand](Commentary/Compiler/Demand "wikilink") (I
am not an expert on the GHC internals though). Also,
[GhcFile(compiler/basicTypes/NewDemand.lhs)](GhcFile(compiler/basicTypes/NewDemand.lhs) "wikilink")
is not any more in the sources, replaced by (or renamed to?)
[GhcFile(compiler/basicTypes/Demand.lhs)](GhcFile(compiler/basicTypes/Demand.lhs) "wikilink").

The demand analyzer
===================

Most of the demand analyzer lives in two files:

`*`[`GhcFile(compiler/basicTypes/NewDemand.lhs)`](GhcFile(compiler/basicTypes/NewDemand.lhs) "wikilink")`(definesthedatatypesusedbythedemandanalyzer,andsomefunctionsonthem)`\
`*`[`GhcFile(compiler/stranal/DmdAnal.lhs)`](GhcFile(compiler/stranal/DmdAnal.lhs) "wikilink")`(thedemandanalyzeritself)`

The demand analyzer does strictness analysis, absence analysis, and
box-demand analysis in a single pass. (!ToDo: explain what these are.)

In
[GhcFile(compiler/stranal/DmdAnal.lhs)](GhcFile(compiler/stranal/DmdAnal.lhs) "wikilink"),
is the function that performs demand analysis on an expression. It has
the following type: The first argument is an environment mapping
variables onto demand signatures. (!ToDo: explain more.) The second
argument is the demand that's being placed on the expression being
analyzed, which was determined from the context already. The third
argument is the expression being analyzed. returns a pair of a new
expression (possibly with demand information added to any
\[wiki:Commentary/Compiler/NameType Ids\] in it), and a .

Important datatypes
-------------------

 A demand consists of usage information, along with information about
usage of the subcomponents of the expression it's associated with.

 Usage information consists of a triple of three properties: strictness
(or evaluation demand), usage demand, and box demand.

 Something that is may or may not be evaluated. Something that is will
definitely be evaluated at least to its outermost constructor. Something
that is will be fully evaluated (e.g., in , can be said to have
strictness , because it doesn't matter how much we evaluate -- this
expression will diverge anyway.)

 In the context of function arguments, an argument that is is never used
by its caller (e.g., syntactically, it doesn't appear in the body of the
function at all). An argument that is will be used zero or one times,
but not more. Something that is may be used zero, one, or many times --
we don't know.

 Again in the context of function arguments, an argument that is is a
value constructed by a data constructor of a product type whose "box" is
going to be needed. For example, we say that } "uses the box", so in ,
has box-demand information . In }, doesn't "use the box" for its
argument, so in , has box-demand information . When in doubt, we assume
.

 For a compound data value, the type describes demands on its
components. means that we don't know anything about the expression's
type. says "this expression has a product type, and the demands on its
components consist of the demands in the following list". If the is
supplied, that means that this expression must be cast using the given
coercion before it is evaluated. (!ToDo: explain this more.)

(!ToDo: explain why all the above information is important)

Though any expression can have a associated with it, another datatype, ,
is associated with a function body.

 A consists of a (which provides demands for all explicitly mentioned
free variables in a functions body), a list of s on the function's
arguments, and a , which indicates whether this function returns an
explicitly constructed product:

The function takes a strictness environment, an
\[wiki:Commentary/Compiler/NameType Id\] corresponding to a function,
and a representing demand on the function -- in a particular context --
and returns a , representing the function's demand type in this context.
Demand analysis is implemented as a backwards analysis, so takes the
demand on a function's result (which was inferred based on how the
function's result is used) and uses that to compute the demand type of
this particular occurrence of the function itself.

 has four cases, depending on whether the function being analyzed is a
\[wiki:Commentary/Compiler/EntityTypes data constructor\] worker, an
imported (global) function, a local -bound function, or "anything else"
(e.g., a local lambda-bound function).

The data constructor case checks whether this particular constructor
call is saturated. If not, it returns , indicating that we know nothing
about the demand type. If so, it returns a with an empty environment
(since there are no free variables), a list of arg-demands based on the
that was passed in to (that is, the demand on the result of the data
constructor call), and a taken from the constructor Id's strictness
signature.

There are a couple of tricky things about the list of arg-demands:

`*Iftheresultdemand(i.e.,thepassed-indemand)hasitsboxdemanded,thenwewanttomakesuretheboxisdemandedineachofthedemandsfortheargs.(!ToDo:thismaynotbetrue)`\
`*Iftheresultdemandisnotstrict,wewanttouse`*`n`*`copiesof``asthelistofarg-demands,where`*`n`*`isthisdataconstructor'sarity.`

(!ToDo: explain the other cases of )

\[wiki:Commentary/Compiler/StrictnessAnalysis/KirstenNotes even more
sketchy notes\]

\[wiki:Commentary/Compiler/StrictnessAnalysis/Examples\]

Symbol Names
============

Since Haskell allows many symbols in constructor and variable names that
C compilers or assembly might not allow (e.g. :, %, \#) these have to be
encoded using z-encoding. The encoding is as follows. See
[GhcFile(compiler/utils/Encoding.hs)](GhcFile(compiler/utils/Encoding.hs) "wikilink").

Tuples
------

|| Decoded || Encoded || Comment || || \`()\` || Z0T || Unit / 0-tuple
|| || || || There is no Z1T || || \`(,)\` || Z2T || 2-tuple || ||
\`(,,)\` || Z3T || 3-tuple || || ... || || And so on ||

Unboxed Tuples
--------------

|| Decoded || Encoded || Comment || || || || There is no Z0H || || \`(\#
\#)\` || Z1H || unboxed 1-tuple (note the space) || || \`(\#,\#)\` ||
Z2H || unboxed 2-tuple || || \`(\#,,\#)\` || Z3H || unboxed 3-tuple ||
|| ... || || And so on ||

Alphanumeric Characters
-----------------------

|| Decoded || Encoded || Comment || || a-y, A-Y, 0-9 || a-y, A-Y, 0-9 ||
Regular letters don't need escape sequences || || z, Z || zz, ZZ || 'Z'
and 'z' must be escaped ||

Constructor Characters
----------------------

|| Decoded || Encoded || Comment || || \`(\` || ZL || Left || || \`)\`
|| ZR || Right || || \`\[\` || ZM || 'M' before 'N' in \[\] || || \`\]\`
|| ZN || || || \`:\` || ZC || Colon ||

Variable Characters
-------------------

|| Decoded || Encoded || Mnemonic || || \`&\` || za || Ampersand || ||
\`|\` || zb || Bar || || \`\^\` || zc || Caret || || \`\$\` || zd ||
Dollar || || \`=\` || ze || Equals || || \`&gt;\` || zg || Greater than
|| || \`\#\` || zh || Hash || || \`.\` || zi || The dot of the 'i' || ||
\`&lt;\` || zl || Less than || || \`-\` || zm || Minus || || \`!\` || zn
|| Not || || \`+\` || zp || Plus || || \`'\` || zq || Quote || || \`\\\`
|| zr || Reverse slash || || \`/\` || zs || Slash || || \`\*\` || zt ||
Times sign || || \`\_\` || zu || Underscore || || \`%\` || zv || (TODO:
I don't know what the mnemonic for this one is. Perhaps relatiVe or
diVide?) ||

Other
-----

Any other character is encoded as a 'z' followed by its hex code (lower
case, variable length) followed by 'U'. If the hex code starts with 'a',
'b, 'c', 'd', 'e' or 'f', then an extra '0' is placed before the hex
code to avoid conflicts with the other escape characters.

Examples
--------

|| Before || After || || \`Trak\` || \`Trak\` || || \`foo\_wib\` ||
\`foozuwib\` || || \`&gt;\` || \`zg\` || || \`&gt;1\` || \`zg1\` || ||
\`foo\#\` || \`foozh\` || || \`foo\#\#\` || \`foozhzh\` || ||
\`foo\#\#1\` || \`foozhzh1\` || || \`fooZ\` || \`fooZZ\` || || \`:+\` ||
\`ZCzp\` || || \`()\` || \`Z0T\` || || \`(,,,,)\` || \`Z5T\` || || \`(\#
\#)\` || \`Z1H\` || || \`(\#,,,,\#)\` || \`Z5H\` ||

\[ Up: \[wiki:Commentary/Compiler/TypeChecker\] \]

The monad for renaming, typechecking, desugaring
================================================

The renamer, typechecker, interface-file typechecker, and desugarer all
share a certain amount in common: they must report errors, handle
environments, do I/O, etc. Furthermore, because of Template Haskell we
have to interleave renaming and typechecking. So all four share a common
monad, called . This infrastructure is defined by the following modules:

`*`[`GhcFile(compiler/utils/IOEnv.lhs)`](GhcFile(compiler/utils/IOEnv.lhs) "wikilink")`:extendstheIOmonadwithanenvironment(justasimplereadermonad).`\
`*`[`GhcFile(compiler/typecheck/TcRnTypes)`](GhcFile(compiler/typecheck/TcRnTypes) "wikilink")`:buildsthe``monadontopof``:`\
`*`[`GhcFile(compiler/typecheck/TcRnMonad)`](GhcFile(compiler/typecheck/TcRnMonad) "wikilink")`:defineslotsofaccessfunctionsfortherenamer,typechecker,andinterfacetypechecker.`\
`*`[`GhcFile(compiler/typecheck/DsMonad)`](GhcFile(compiler/typecheck/DsMonad) "wikilink")`:specialisesthe``monadforthedesugarer.`

The typechecker and renamer use *exactly* the same monad, ; the
desugarer and interface-file checker use different instantiations of .
To give you the idea, here is how the monad looks: The details of the
global environment type and local environment type are also defined in
[GhcFile(compiler/typecheck/TcRnTypes.lhs)](GhcFile(compiler/typecheck/TcRnTypes.lhs) "wikilink").
Side effecting operations, such as updating the unique supply, are done
with TcRefs, which are simply a synonym for IORefs.

(NB out-of-date, but maybe historically useful; cf
\[wiki:Debugging/TickyTicky\])

Kirsten's sketchy notes on getting ticky to work
================================================

Macros for bumping ticky counters are now defined in
[GhcFile(includes/Cmm.h)](GhcFile(includes/Cmm.h) "wikilink").
Currently, code compiled with the flag fails to link because the macros
rely on counter variables (things with names like being declared, but
there are actually no declarations for them. I'll add those declarations
to
[GhcFile(includes/RtsExternal.h)](GhcFile(includes/RtsExternal.h) "wikilink")
so I can get something working. Really, there should be something that
automatically generates both the macros that are in
[GhcFile(includes/Cmm.h)](GhcFile(includes/Cmm.h) "wikilink") and the
declarations for the corresponding variables, so that they stay in sync.

Actually, maybe it would make more sense to add a new file, or
something, which contains only ticky counter declarations (the same
declarations that still exist in
[GhcFile(includes/StgTicky.h)](GhcFile(includes/StgTicky.h) "wikilink"),
which isn't used anymore), and that include that from
[GhcFile(includes/RtsExternal.h)](GhcFile(includes/RtsExternal.h) "wikilink").

No -- put actual declarations for counter variables in another file, or
something, and include that only from
[GhcFile(rts/Ticky.c)](GhcFile(rts/Ticky.c) "wikilink"); put *extern*
declarations for those counters in , still included from
[GhcFile(includes/RtsExternal.h)](GhcFile(includes/RtsExternal.h) "wikilink").
Then later we can automatically generate both and . The reason for this
is that the ticky **macros** are all over the place and they refer to
the ticky counters, so the ticky counters have to be **declared**
someplace that everyone includes, but of course the actual
initializations only need to happen in one place. (Maybe there's a
better way to do this...)

No, there don't need to be two files; I was confused. Just .

Huh - we define ticky macros now in but we can only include that in CMM
files and some C files, like , use ticky macros. This makes my brain
hurt a little. ''' Index by Title ''' | ''' \[RecentChanges Index by
Date\] '''

[TitleIndex(format=group,min=4)](TitleIndex(format=group,min=4) "wikilink")

The GHC Commentary: Checking Types
==================================

Probably the most important phase in the frontend is the type checker,
which is located at
[GhcFile(compiler/typecheck/)](GhcFile(compiler/typecheck/) "wikilink").
GHC type checks programs in their original Haskell form before the
desugarer converts them into Core code. This complicates the type
checker as it has to handle the much more verbose Haskell AST, but it
improves error messages, as those message are based on the same
structure that the user sees.

GHC defines the abstract syntax of Haskell programs in
[GhcModule(compiler/hsSyn/HsSyn.lhs)](GhcModule(compiler/hsSyn/HsSyn.lhs) "wikilink")
using a structure that abstracts over the concrete representation of
bound occurences of identifiers and patterns. The module
[GhcModule(compiler/typecheck/TcHsSyn.lhs)](GhcModule(compiler/typecheck/TcHsSyn.lhs) "wikilink")
defines a number of helper function required by the type checker. Note
that the type
[GhcModule(compiler/typecheck/TcRnTypes.lhs)](GhcModule(compiler/typecheck/TcRnTypes.lhs) "wikilink").\`TcId\`
used to represent identifiers in some signatures during type checking
is, in fact, nothing but a synonym for a
\[wiki:Commentary/Compiler/EntityTypes\#Typevariablesandtermvariables
plain Id\].

It is also noteworthy, that the representations of types changes during
type checking from \`HsType\` to \`TypeRep.Type\`. The latter is a
\[wiki:Commentary/Compiler/TypeType hybrid type\] representation that is
used to type Core, but still contains sufficient information to recover
source types. In particular, the type checker maintains and compares
types in their \`Type\` form.

The Overall Flow of Things
--------------------------

`` *`TcRnDriver`isthetoplevel.Itcalls ``\
`` *`TcTyClsDecls`:typeandclassdeclaration ``\
`` *`TcInstDcls`:instancedeclarations ``\
`` *`TcBinds`:valuebindings ``\
`` *`TcExpr`:expressions ``\
`` *`TcMatches`:lambda,case,listcomprehensions ``\
`` *`TcPat`:patterns ``\
`` *`TcForeign`:FFIdeclarations ``\
`` *`TcRules`:rewriterules ``\
`` *`TcHsTypes`:kind-checkingtypesignatures ``\
`` *`TcValidity`:asecondpassthatwalksoverthingsliketypesortypeconstructors,checkinganumberofextrasideconditions. ``

`*Theconstraintsolverconsistsof:`\
`` *`TcSimplify`:topleveloftheconstraintsolver ``\
`` *`TcCanonical`:canonicalisingconstraints ``\
`` *`TcInteract`:solvingconstraintswheretheyinteractwitheachother ``\
`` *`TcTypeNats`:solvingnatural-numberconstraints ``\
`` *`TcSMonad`:themonadoftheconstraintsolver(builtontopofthemaintypecheckermonad) ``\
`` *`TcEvidence`:thedatatypesusedforevidence(mostlypure) ``\
`` *`TcUnify`:solvesunificationconstraints"onthefly";ifitcan't,itgeneratesaconstraintfortheconstraintsolvertodealwithlater ``\
`` *`TcErrors`:generatesgooderrormessagesfromtheresidual,unsolvedconstraints. ``[`BR`](BR "wikilink")\
`Thebestplacereadingfortheconstraintsolveristhepaper`[`Modular`
`type` `inference` `with` `local`
`assumptions`](http://www.haskell.org/haskellwiki/Simonpj/Talk:OutsideIn)

`*Underlyinginfrastructure:`\
`` *`TcRnTypes`:abigcollectionofthetypesusedduringtypechecking ``\
`*[wiki:Commentary/Compiler/TcRnMonadTcRnMonad]:themaintypecheckermonad`\
`` *`TcType`:purefunctionsovertypes,usedbythetypechecker ``\
``

### Entry Points Into the Type Checker

The interface of the type checker (and
\[wiki:Commentary/Compiler/Renamer renamer\]) to the rest of the
compiler is provided by
[GhcModule(compiler/typecheck/TcRnDriver.lhs)](GhcModule(compiler/typecheck/TcRnDriver.lhs) "wikilink").
Entire modules are processed by calling \`tcRnModule\` and GHCi uses
\`tcRnStmt\`, \`tcRnExpr\`, and \`tcRnType\` to typecheck statements and
expressions, and to kind check types, respectively. Moreover,
\`tcTopSrcDecls\` is used by Template Haskell - more specifically by
\`TcSplice.tc\_bracket\` - to type check the contents of declaration
brackets.

### Renaming and Type Checking a Module

The functions \`tcRnModule\` and \`tcRnModuleTcRnM\` control the
complete static analysis of a Haskell module. They set up the combined
renamer and type checker monad, resolve all import statements, take care
of hi-boot files, initiate the actual renaming and type checking
process, and finally, wrap off by processing the export list.

The actual type checking and renaming process is initiated via
\`TcRnDriver.tcRnSrcDecls\`, which uses a helper called
\`tc\_rn\_src\_decls\` to implement the iterative renaming and type
checking process required by [Template
Haskell](http://darcs.haskell.org/ghc/docs/comm/exts/th.html) (TODO:
Point at new commentary equivalent). After it invokes
\`tc\_rn\_src\_decls\`, it simplifies type constraints and zonking (see
below regarding the later).

The function \`tc\_rn\_src\_decls\` partitions static analysis of a
whole module into multiple rounds, where the initial round is followed
by an additional one for each toplevel splice. It collects all
declarations up to the next splice into an \`HsDecl.HsGroup\`. To rename
and type check that declaration group it calls
\`TcRnDriver.rnTopSrcDecls\` and \`TcRnDriver.tcTopSrcDecls\`.
Afterwards, it executes the splice (if there are any left) and proceeds
to the next group, which includes the declarations produced by the
splice.

The renamer, apart from renaming, computes the global type checking
environment, of type \`TcRnTypes.TcGblEnv\`, which is stored in the
\[wiki:Commentary/Compiler/TcRnMonad type checking monad\] before type
checking commences.

Type Checking a Declaration Group
---------------------------------

The type checking of a declaration group, performed by \`tcTopSrcDecls\`
and its helper function \`tcTyClsInstDecls\`, starts by processing of
the type and class declarations of the current module, using the
function \`TcTyClsDecls.tcTyAndClassDecls\`. This is followed by a first
round over instance declarations using \`TcInstDcls.tcInstDecls1\`,
which in particular generates all additional bindings due to the
deriving process. Then come foreign import declarations
(\`TcForeign.tcForeignImports\`) and default declarations
(\`TcDefaults.tcDefaults\`).

Now, finally, toplevel value declarations (including derived ones) are
type checked using \`TcBinds.tcTopBinds\`. Afterwards,
\`TcInstDcls.tcInstDecls2\` traverses instances for the second time.
Type checking concludes with processing foreign exports
(\`TcForeign.tcForeignExports\`) and rewrite rules
(\`TcRules.tcRules\`). Finally, the global environment is extended with
the new bindings.

Type checking Type and Class Declarations
-----------------------------------------

Type and class declarations are type checked in a couple of phases that
contain recursive dependencies - aka *knots*. The first knot encompasses
almost the whole type checking of these declarations and forms the main
piece of \`TcTyClsDecls.tcTyAndClassDecls\`.

Inside this big knot, the first main operation is kind checking, which
again involves a knot. It is implemented by \`kcTyClDecls\`, which
performs kind checking of potentially recursively-dependent type and
class declarations using kind variables for initially unknown kinds.
During processing the individual declarations some of these variables
will be instantiated depending on the context; the rest gets by default
kind \* (during *zonking* of the kind signatures). Type synonyms are
treated specially in this process, because they can have an unboxed
type, but they cannot be recursive. Hence, their kinds are inferred in
dependency order. Moreover, in contrast to class declarations and other
type declarations, synonyms are not entered into the global environment
as a global \`TyThing\`. (\`TypeRep.TyThing\` is a sum type that
combines the various flavours of typish entities, such that they can be
stuck into type environments and similar.)

More Details
------------

### Types Variables and Zonking

During type checking type variables are represented by mutable variables
- cf. the [variable
story](http://darcs.haskell.org/ghc/docs/comm/the-beast/vars.html#TyVar)
(TODO: Point at new commentary equivalent). Consequently, unification
can instantiate type variables by updating those mutable variables. This
process of instantiation is (for reasons that elude me) called
[zonking](http://dictionary.reference.com/browse/zonk) in GHC's sources.
The zonking routines for the various forms of Haskell constructs are
responsible for most of the code in the module
[GhcModule(compiler/typecheck/TcHsSyn.lhs)](GhcModule(compiler/typecheck/TcHsSyn.lhs) "wikilink"),
whereas the routines that actually operate on mutable types are defined
in
[GhcModule(compiler/typecheck/TcMType.lhs)](GhcModule(compiler/typecheck/TcMType.lhs) "wikilink");
this includes the zonking of type variables and type terms, routines to
create mutable structures and update them as well as routines that check
constraints, such as that type variables in function signatures have not
been instantiated during type checking. The actual type unification
routine is \`uTys\` in the module
[GhcModule(compiler/typecheck/TcUnify.lhs)](GhcModule(compiler/typecheck/TcUnify.lhs) "wikilink").

All type variables that may be instantiated (those in signatures may
not), but haven't been instantiated during type checking, are zonked to
\`()\`, so that after type checking all mutable variables have been
eliminated.

### Type Representation

The representation of types is fixed in the module
[GhcModule(compiler/types/TypeRep.lhs)](GhcModule(compiler/types/TypeRep.lhs) "wikilink")
and exported as the data type \`Type\`. Read the comments in the
\`TypeRep\` module! A couple of points:

`` *Typesynonymapplicationsarerepresentedasa`TyConApp`witha`TyCon`thatcontainstheexpansion.Theexpansionisdoneon-demandby`Type.coreView`.Unexpandedtypesynonymsareusefulforgeneratingcomprehensibleerrormessages. ``

`` *The`PredTy`constructorwrapsatypeconstraintargument(dictionary,implicitparameter,orequality).Theyareexpandedon-demandby`coreView`. ``

As explained in
[GhcModule(compiler/typecheck/TcType.lhs)](GhcModule(compiler/typecheck/TcType.lhs) "wikilink"),
GHC supports rank-N types, but during type inference maintains the
restriction that type variables cannot be instantiated to quantified
types (i.e., the type system is predicative). However the type system of
Core is fully impredicative.

### Type Checking Environment

During type checking, GHC maintains a *type environment* whose type
definitions are fixed in the module
[GhcModule(compiler/typecheck/TcRnTypes.lhs)](GhcModule(compiler/typecheck/TcRnTypes.lhs) "wikilink")
with the operations defined in
[GhcModule(compiler/typecheck/TcEnv.lhs)](GhcModule(compiler/typecheck/TcEnv.lhs) "wikilink").
Among other things, the environment contains all imported and local
instances as well as a list of *global* entities (imported and local
types and classes together with imported identifiers) and *local*
entities (locally defined identifiers). This environment is threaded
through the \[wiki:Commentary/Compiler/TcRnMonad type checking monad\].

### Expressions

Expressions are type checked by
[GhcModule(compiler/typecheck/TcExpr)](GhcModule(compiler/typecheck/TcExpr) "wikilink").

Usage occurences of identifiers are processed by the function tcId whose
main purpose is to \[\#HandlingofDictionariesandMethodInstances
instantiate overloaded identifiers\]. It essentially calls
\`TcInst.instOverloadedFun\` once for each universally quantified set of
type constraints. It should be noted that overloaded identifiers are
replaced by new names that are first defined in the LIE (Local Instance
Environment?) and later promoted into top-level bindings.

### Handling of Dictionaries and Method Instances

GHC implements overloading using so-called *dictionaries*. A dictionary
is a tuple of functions -- one function for each method in the class of
which the dictionary implements an instance. During type checking, GHC
replaces each type constraint of a function with one additional
argument. At runtime, the extended function gets passed a matching class
dictionary by way of these additional arguments. Whenever the function
needs to call a method of such a class, it simply extracts it from the
dictionary.

This sounds simple enough; however, the actual implementation is a bit
more tricky as it wants to keep track of all the instances at which
overloaded functions are used in a module. This information is useful to
optimise the code. The implementation is the module
[GhcModule(compiler/typecheck/Inst.lhs)](GhcModule(compiler/typecheck/Inst.lhs) "wikilink").

The function \`instOverloadedFun\` is invoked for each overloaded usage
occurrence of an identifier, where overloaded means that the type of the
identifier contains a non-trivial type constraint. It proceeds in two
steps: (1) Allocation of a method instance (\`newMethodWithGivenTy\`)
and (2) instantiation of functional dependencies. The former implies
allocating a new unique identifier, which replaces the original
(overloaded) identifier at the currently type-checked usage occurrence.

The new identifier (after being threaded through the LIE) eventually
will be bound by a top-level binding whose rhs contains a partial
application of the original overloaded identifier. This papp applies the
overloaded function to the dictionaries needed for the current instance.
In GHC lingo, this is called a *method*. Before becoming a top-level
binding, the method is first represented as a value of type Inst.Inst,
which makes it easy to fold multiple instances of the same identifier at
the same types into one global definition. (And probably other things,
too, which I haven't investigated yet.)

**Note:** As of 13 January 2001 (wrt. to the code in the CVS HEAD), the
above mechanism interferes badly with RULES pragmas defined over
overloaded functions. During instantiation, a new name is created for an
overloaded function partially applied to the dictionaries needed in a
usage position of that function. As the rewrite rule, however, mentions
the original overloaded name, it won't fire anymore -- unless later
phases remove the intermediate definition again. The latest CVS version
of GHC has an option '-fno-method-sharing', which avoids sharing
instantiation stubs. This is usually/often/sometimes sufficient to make
the rules fire again.

Connection with GHC's Constraint Solver
---------------------------------------

The solver for the type nats is implemented as an extra stage in GHC's
constrraint solver (see \`TcInteract.thePipeline\`).

The following modules contain most of the code relevant for the solver:

`` *`TcTypeNats`:Themainsolvermachinery ``\
`` *`TcTypeNatsRules`:Therulesusedbythesolver ``\
`` *`TcTYpeNatsEval`:Functionsfordirectevaluationonconstants ``

Generating Evidence
-------------------

The solver produces evidence (i.e., proofs) when computing new "given"
constraints, or when solving existing "wanted" constraints. The evidence
is constructed by applications of a set of pre-defined rules. The rules
are values of type \`TypeRep.CoAxiomRule\`. Conceptually, rules have the
form: The rules have the usual logical meaning: the variables are
universally quantified, and the assumptions imply the concluson. As a
concrete example, consider the rule for left-cancellation of addtion:

The type \`CoAxiomRule\` also supports infinte literal-indexed families
of simple axioms using constructor \`CoAxiomTyLit\`. These have the
form: In this case \`conclusion\` is an equation that contains no type
variables but may depend on the literals in the name of the family. For
example, the basic definitional axiom for addition,
\`TcTypeNatsRules.axAddDef\`, uses this mechanism: At present, the
assumptions and conclusion of all rules are equations between types but
this restriction is not important and could be lifted in the future.

The rules used by the solver are in module \`TcTypeNatsRules\`.

The Solver
----------

The entry point to the solver is \`TcTypeNats.typeNatStage\`.

We start by examining the constraint to see if it is obviously
unsolvable (using function \`impossible\`), and if so we stash it in the
constraint-solver's state and stop. Note that there is no assumption
that \`impossible\` is complete, but it is important that it is sound,
so if \`impossible\` returns \`True\`, then the constraint is definitely
unsolvable, but if \`impossible\` returns \`False\`, then we don't know
if the constraint is solvable or not.

The rest of the stage proceeds depending on the type of constraint, as
follows.

### Given Constraints

Given constraints correspond to adding new assumptions that may be used
by the solver. We start by checking if the new constraint is trivial
(using function \`solve\`). A constraint is considered to be trivial if
it matches an already existing constraint or a rule that is known to the
solver. Such given constraints are ignored because they do not
contribute new information. If the new given is non-trivial, then it
will be recorded to the inert set as a new fact, and we proceed to
"interact" it with existing givens, in the hope of computing additional
useful facts (function \`computeNewGivenWork\`).

IMPORTANT: We assume that "given" constraints are processed before
"wanted" ones. A new given constraint may be used to solve any existing
wanted, so every time we added a new given to the inert set we should
move all potentially solvable "wanted" constraint from the inert set
back to the work queue. We DON'T do this, because it is quite
inefficient: there is no obvious way to compute which "wanted"s might be
affected, so we have to restart all of them!

The heart of the interaction is the function \`interactCt\`, which
performs one step of "forward" reasoning. The idea is to compute new
constraints whose proofs are made by an application of a rule to the new
given, and some existing givens. These new constraints are added as new
work, to be processed further on the next iteration of GHC's constraint
solver.

Aside: when we compute the new facts, we check to see if any are obvious
contradictions. This is not strictly necessary because they would be
detected on the next iteration of the solver. However, by doing the
check early we get slightly better error messages because we can report
the original constraint as being unsolvable (it leads to a
contradiction), which tends to be easier to relate to the original
program. Of course, this is not completely fool-proof---it is still
possible that a contradiction is detected at a later iteration. An
alternative idea---not yet implemented---would be to examine the proof
of a contradiction and extract the original constraints that lead to it
in the first place.

### Derived Constraints

\`\`Derived\`\` constraints are facts that are implied by the
constraints in the inert set. They do not have complete proofs because
they may depend on proofs of as yet unsolved wanted constraints. GHC
does not associate any proof terms with derived constraints (to keep
things simple?). In the constraint solver, they are mostly used as
"hints". For example, consider the wanted constraint , where is a free
unification variable. These are the steps we'll take to solve the
constraint:

The type-nat solver processes derived constraints in a similar fashion
to given constraints (\`computeNewDerivedWork\`): it checks to see if
they are trivially known and, if not, then it tries to generate some
additional derived constraints. The main difference is that derived
constraints can be interacted with all existing constraints to produce
new facts, while given constraints only interact with other givens.

### Wanted Constraints

The main purpose of the solver is to discharge \`\`wanted\`\`
constraints (the purpose of processing given and derived constraints is
to help solve existing wanted goals). When we encounter a new wanted
goals we proceed as follows:

`1.Trytosolvethegoal,usingafewdifferentstrategies:`\
`` 1.Trytoseeifitmatchestheconclusionofaniffrule(`solveIff`).Aassumptionsofrulebecomenewwantedwork. ``\
`` 2.Trytoseeifitmatchesanaxiomexactly(`solve`) ``\
`` 3.Trytheorderingsolverfor`<=`goals(`solveLeq`) ``\
`4.Trytousea(possiblysynthesized)assumption`

`2.Ifthatdidn'twork:`\
`1.Wantedisaddedtotheinertset`\
`` 2.Checktoseeifanyoftheexistingwantedsintheinertsetcanbesolvedintermsofthenewgoal(`reExamineWanteds`) ``\
`3.Generatenewderivedfacts.`

#### Using IFF Rules

These rules are used to replace a wanted constraint with a collection of
logically equivalent wanted constraints. If a wanted constraint matches
the head of one of these rules, than it is solved using the rules, and
the we generate new wanted constraints for the rule's assumptions.

The following are important properties of IFF rules:

`*Theyneedtobesound(ofcourse!)`\
`*Theassumptionsneedtobelogicallyequivalenttotheconclusion(i.e.,theyshouldnotresultinaharderproblemtosolvethantheoriginalgoal).`\
`*Theassumptionsneedtobe`*`simpler`*`fromthepointofviewoftheconstraintsolver(i.e.,weshouldn'tendupwiththeoriginalgoalaftersomesteps---thiswouldleadtonon-termination).`

At present, IFF rules are used to define certain operators in terms of
others. For example, this is the only rule for solving constraints about
subtraction:

#### Using Axioms

Basic operators are defined with an infinite family of axiom schemes. As
we can't have these written as a long list (searching might never
stop!), we have some custom code that checks to see if a constraint
might be solvable using one of the definitional axioms (see
\`solveWithAxiom\`, \`byAxiom\`).

#### Using the Order Model

Constraints about the ordering of type-level numbers are kept in a
datastructure (\`LeqFacts\`) which forms a \`\`model'' of the
information represented by the constraints (in a similar fashion to how
substitutions form a model for a set of equations).

The purpose of the model is to eliminate redundant constraints, and to
make it easy to find proofs for queries of the form \`x &lt;= y\`. In
practise, of particular interest are questions such as \`1 &lt;= x\`
because these appear as assumptions on a number of rules (e.g.,
cancellation of multiplication). In the future, this model could also be
used to implement an interval analysis, which would compute intervals
approximating the values of variables.

TODO: At present, this model is reconstructed every time it needs to be
used, which is a bit inefficient. Perhaps it'd be better to use this
directly as the representation of \`&lt;=\` constraints in the inert
set.

The model is a directed acyclic graph, as follows:

`` *vertices:constantsorvariables(ofkind`Nat`) ``\
`` *edges:theedgefrom`A`to`B`isaproofthat`A<=B`. ``

So, to find a proof of \`A &lt;= B\`, we insert \`A\` and \`B\` in the
model, and then look for a path from \`A\` to \`B\`. The proofs on the
path can be composed using the rule for transitivity of \`&lt;=\` to
form the final proof.

When manipulating the model, we maintain the following "minimality"
invariant: there should be no direct edge between two vertices \`A\` and
\`B\`, if there is a path that can already get us from \`A\` to \`B.
Here are some examples (with edges pointing upwards)

The purpose of the invariant is to eliminate redundant information.
Note, however, that it does not guarantee that there is a unique way to
prove a goal.

#### Using Extended Assumptions

Another way to prove a goal is to look it up in the assumptions. If the
goal matched an assumption exactly, then GHC would have already solved
it in one of its previous stages of the constraint solver. However, due
to the commutativity and associativity of some of the operators, it is
possible to have goal that could be solved by assumption, only if the
assumption was "massaged" a bit.

This "massaging" is implemented by the function \`widenAsmps\`, which
extends the set of assumption by performing a bit of forward reasoning
using a limited set of rules. Typically, these are commutativity an
associativity rules, and the \`widenAsmps\` function tries to complete
the set of assumptions with respect to these operations. For example:

Note that the extended assumptions are very similar to derived
constraints, except that we keep their proofs.

#### Re-examining Wanteds

If none of the strategies for solving a wanted constraint worked, then
the constraint is added to the inert set. Since we'd like to keep the
inert set minimal, we have to see if any of the existing wanted
constraints might be solvable in terms of the new wanted
(\`reExamineWanteds\`).

It is good to keep the inert set minimal for the following reasons:

`*Inferredtypesarenicer,`\
`*IthelpsGHCtosolveconstraintsby"inlining"(e.g.,ifwe`\
`` haveonlyasingleconstraint`x+y~z`,thenwecaneliminateit ``\
`` byreplacingalloccurrencesof`z`with`x+y`,howeverwecan't ``\
`` dothatifweendedupwithtwoconstraints`(x+y~z,y+x~z)). ``

We consider each (numeric) wanted constraint in the inert set and check
if we can solve it in terms of the new wanted and all other wanteds. If
so, then it is removed from the inert set, otherwise it stays there.

Note that we can't implement this by kicking out the existing wanted
constraints and putting them back on the work queue, because this would
lead to non-termination. Here is an example of how this might happen:

Perhaps there is a way around this but, for the moment, we just
re-examine the numeric wanteds locally, without going through the
constraint solver pipe-line.

[PageOutline](PageOutline "wikilink")

The data type  and its friends
=============================

GHC compiles a typed programming language, and GHC's intermediate
language is explicitly typed. So the data type that GHC uses to
represent types is of central importance.

The single data type is used to represent

`` *Types(possiblyofhigherkind);e.g.`[Int]`,`Maybe` ``\
`` *Kinds(whichclassifytypesandcoercions);e.g.`(*->*)`,`T:=:[Int]`.See[wiki:Commentary/Compiler/Kinds] ``\
`` *Sorts(whichclassifytypes);e.g.`TY`,`CO` ``

GHC's use of \[wiki:Commentary/Compiler/FC coercions and equality
constraints\] is important enough to deserve its own page.

The module exposes the representation because a few other modules (, , ,
etc) work directly on its representation. However, you should not
lightly pattern-match on ; it is meant to be an abstract type. Instead,
try to use functions defined by , etc.

Views of types
--------------

Even when considering only types (not kinds, sorts, coercions) you need
to know that GHC uses a *single* data type for types. You can look at
the same type in different ways:

`*The"typecheckerview"regardsthetypeasaHaskelltype,completewithimplicitparameters,classconstraints,andthelike.Forexample:`

`` Functionsin`TcType`takethisviewoftypes;e.g.`tcSplitSigmaTy`splitsupatypeintoitsforall'dtypevariables,itsconstraints,andtherest. ``

`*The"coreview"regardsthetypeasaCore-languagetype,whereclassandimplicitparameterconstraintsaretreatedasfunctionarguments:`

`` Functionsin`Type`takethisview. ``

The data type \`Type\` represents type synonym applications in
un-expanded form. E.g. Here \`f\`'s type doesn't look like a function
type, but it really is. The function \`Type.coreView :: Type -&gt; Maybe
Type\` takes a type and, if it's a type synonym application, it expands
the synonym and returns \`Just <expanded-type>\`. Otherwise it returns
\`Nothing\`.

Now, other functions use \`coreView\` to expand where necessary, thus:
Notice the first line, which uses the view, and recurses when the view
'fires'. Since \`coreView\` is non-recursive, GHC will inline it, and
the optimiser will ultimately produce something like:

The representation of 
----------------------

Here, then is the representation of types (see
[GhcFile(compiler/types/TypeRep.hs)](GhcFile(compiler/types/TypeRep.hs) "wikilink")
for more details):

Invariant: if the head of a type application is a , GHC *always* uses
the constructor, not . This invariant is maintained internally by 'smart
constructors'. A similar invariant applies to ; is never used with an
arrow type.

Type variables are represented by the \`TyVar\` constructor of the
\[wiki:Commentary/Compiler/EntityTypes data type Var\].

Overloaded types
----------------

In Haskell we write but in Core the \`=&gt;\` is represented by an
ordinary \`FunTy\`. So f's type looks like this: Nevertheless, we can
tell when a function argument is actually a predicate (and hence should
be displayed with \`=&gt;\`, etc), using The various forms of predicate
can be extracted thus: These functions are defined in module \`Type\`.

Classifying types
-----------------

GHC uses the following nomenclature for types:

**`Unboxed`**`::Atypeisunboxediffitsrepresentationisotherthanapointer.Unboxedtypesarealsounlifted.`

**`Lifted`**`::Atypeisliftediffithasbottomasanelement.Closuresalwayshaveliftedtypes:i.e.anylet-boundidentifierinCoremusthavealiftedtype.Operationally,aliftedobjectisonethatcanbeentered.Onlyliftedtypesmaybeunifiedwithatypevariable.`

**`Data`**`::Atypedeclaredwith`****`.Alsoboxedtuples.`

**`Algebraic`**`::Analgebraicdatatypeisadatatypewithoneormoreconstructors,whetherdeclaredwith``or``.Analgebraictypeisonethatcanbedeconstructedwithacaseexpression."Algebraic"is`**`NOT`**`thesameas"lifted",becauseunboxed(andthusunlifted)tuplescountas"algebraic".`

**`Primitive`**`::atypeisprimitiveiffitisabuilt-intypethatcan'tbeexpressedinHaskell.`\
``\
`Currently,allprimitivetypesareunlifted,butthat'snotnecessarilythecase.(E.g.Intcouldbeprimitive.)`

`` Someprimitivetypesareunboxed,suchasInt#,whereassomeareboxedbutunlifted(suchas`ByteArray#`).Theonlyprimitivetypesthatweclassifyasalgebraicaretheunboxedtuples. ``

Examples of type classifications:

|| || **Primitive** || **Boxed** || **Lifted** || **Algebraic** || ||
\`Int\#\` || Yes || No || No || No || || \`ByteArray\#\` || Yes || Yes
|| No || No || || \`(\# a, b \#)\` || Yes || No || No || Yes || || \`(
a, b )\` || No || Yes || Yes || Yes || || \`\[a\]\` || No || Yes || Yes
|| Yes ||

Unique
------

\`Unique\`s provide a fast comparison mechanism for more complex things.
Every \`RdrName\`, \`Name\`, \`Var\`, \`TyCon\`, \`TyVar\`, etc. has a
\`Unique\`. When these more complex structures are collected (in
\`UniqFM\`s or other types of collection), their \`Unique\` typically
provides the key by which the collection is indexed.

------------------------------------------------------------------------

Current design
--------------

A \`Unique\` consists of the *domain* of the thing it identifies and a
unique integer value 'within' that domain. The two are packed into a
single \`Int\#\`, with the *domain* being the top 8 bits.

The domain is never inspected (SLPJ believes). The sole reason for its
existence is to provide a number of different ranges of \`Unique\`
values that are guaranteed not to conflict.

=== Lifetime

The lifetime of a \`Unique\` is a single invocation of GHC, i.e. they
must not 'leak' to compiler output, the reason being that \`Unique\`s
may be generated/assigned non-deterministically. When compiler output is
non-deterministic, it becomes significantly harder to, for example,
\[wiki:Commentary/Compiler/RecompilationAvoidance avoid recompilation\].
Uniques do not get serialised into .hi files, for example.

Note, that "one compiler invocation" is not the same as the compilation
of a single \`Module\`. Invocations such as \`ghc --make\` or \`ghc
--interactive\` give rise to longer invocation life-times.

This is also the reasons why \`OccName\`s are *not* ordered based on the
\`Unique\`s of their underlying \`FastString\`s, but rather
*lexicographically* (see
[GhcFile(compiler/basicTypes/OccName.lhs)](GhcFile(compiler/basicTypes/OccName.lhs) "wikilink")
for details). &gt; &gt; **SLPJ:** I am far from sure that the Ord
instance for \`OccName\` is ever used, so this remark is probably
misleading. Try deleting it and see where it is used (if at all). &gt;
**PKFH:** At least \`Name\` and \`RdrName\` (partially) define their own
\`Ord\` instances in terms of the instance of \`OccName\`. Maybe these
\`Ord\` instances are also redundant, but for now it seems wise to keep
them in. When everything has \`Data\` instances (after this and many
other redesigns), I'm sure it will be easier to find such dependency
relations.

### Known-key things

A hundred or two library entities (types, classes, functions) are
so-called "known-key things". See \[wiki:Commentary/Compiler/WiredIn
this page\]. A known-key thing has a fixed \`Unique\` that is fixed when
the compiler is built, and thus lives across all invocations of that
compiler. These known-key \`Unique\`s *are* written into .hi files. But
that's ok because they are fully deterministic and never change.

&gt; **PKFH** That's fine then; we also know for sure these things fit
in the 30 bits used in the \`hi\`-files. I'll comment appropriately.

### Interface files

Entities in a interface file (.hi file) are, for the most part, stored
in a symbol table, and referred to (from elsewhere in the same interface
file) by an index into that table. Here are the details from
[GhcFile(compiler/iface/BinIface.lhs)](GhcFile(compiler/iface/BinIface.lhs) "wikilink"):

------------------------------------------------------------------------

Redesign (2014)
---------------

=== TL;DR The redesign is to accomplish the following:

`` *Allowderivationoftypeclassinstancesfor`Unique` ``\
`*Restoreinvariantsfromtheoriginaldesign;hiderepresentationdetails`\
`` *Eliminateviolationsofinvariantsanddesign-violationsinotherplacesofthecompiler(e.g.`Unique`sshouldn'tbewrittento`hi`-files,butare). ``

&gt; &gt; **SLPJ** I don't think this is a design violation; see above.
Do you have any other examples in mind? &gt; **PKFH** Not really of
design-violations (and no other compiler-output stuff) other than the
invariants mentioned above it, just yet. The key point, though, is that
there are a lot of comments in \`Unique\` about not exporting things so
that we know X, Y and Z, but then those things *are* exported, so we
don't know them to be true. Case in point is the export of \`mkUnique\`,
but also \`mkUniqueGrimily\`. The latter has a comment 'only for
\`UniqSupply\`' but is also used in other places (like Template
Haskell). One redesign is to put this restriction in the name, so there
still is the facility offered by \`mkUniqueGrimily\`, but now it's
called \`mkUniqueOnlyForUniqSupply\` (and
\`mkUniqueOnlyForTemplateHaskell\`), the ugliness of which should help,
over time, to get rid of them.

=== Longer

In an attempt to give more of GHC's innards well-behaved instances of
\`Typeable\`, \`Data\`, \`Foldable\`, \`Traversable\`, etc. the
implementation of \`Unique\`s was a bit of a sore spot. They were
implemented (20+ years earlier) using custom boxing, viz. making
automatic derivation of such type class instances hard. There was
already a comment asking why it wasn't simply a \`newtype\` around a
normal (boxed) \`Int\`. Independently, there was some discussion on the
mailinglists about the use of (signed) \`Int\`s in places where
\`Word\`s would be more appropriate. Further inspection of the
\`Unique\` implementation made clear that a lot of invariants mentioned
in comments had been violated by incremental edits. This is discussed in
more detail below, but these things together (the desire for automatic
derivation and the restoration of some important invariants) motivated a
moderate redesign.

=== Status Quo (pre redesign)

A \`Unique\` has a domain (\`TyCon\`, \`DataCon\`, \`PrelName\`,
\`Builtin\`, etc.) that was codified by a character. The remainder of
the \`Unique\` was an integer that should be unique for said domain.
This **was** once guaranteed through the export list of
[GhcFile(compiler/basicTypes/Unique.lhs)](GhcFile(compiler/basicTypes/Unique.lhs) "wikilink"),
where direct access to the domain-character was hidden, i.e. were not
exported. This should have guaranteed that every domain was assigned its
own unique character, because only in
[GhcFile(compiler/basicTypes/Unique.lhs)](GhcFile(compiler/basicTypes/Unique.lhs) "wikilink")
could those \`Char\`s be assigned. However, through this separation of
concerns leaked out to
[GhcFile(compiler/basicTypes/UniqSupply.lhs)](GhcFile(compiler/basicTypes/UniqSupply.lhs) "wikilink"),
because its \`Int\` argument is the *entire* \`Unique\` and not just the
integer part 'under' the domain character. &gt; &gt; **SLPJ** OK, but to
eliminate \`mkUniqueGrimily\` you need to examine the calls, decide how
to do it better, and document the new design. &gt; **PKFH** See above;
the solution for now is \`mkUniqueOnlyForUniqSupply\`. A separate patch
will deal with trying to refactor/redesign \`UniqSupply\` if this is
necessary.

The function \`mkSplitUniqSupply\` made the domain-character accessible
to all the other modules, by having a wholly separate implementation of
the functionality of \`mkUnique\`.

Where the intention was still to have a clean interface, the (would-be)
hidden \`mkUnique\` is only called by functions defined in the
\`Unique\` module with the corresponding character, e.g.

=== New plan

In the new design, the domains are explicitly encoded in a sum-type
\`UniqueDomain\`. At the very least, this should help make the code a
little more self-documenting *and* prevent accidental overlap in the
choice of bits to identify the domain. Since the purpose of \`Unique\`s
is to provide *fast* comparison for different types of things, the
redesign should remain performance concious. With this in mind, keeping
the \`UniqueDomain\` and the integer-part explicitly in the type seems
unwise, but by choosing we win the ability to automatically derive
things and should also be able to test how far optimisation has come in
the past 20+ years; does default boxing with \`newtype\`-style wrapping
have (nearly) the same performance as manual unboxing? This should
follow from the tests.

The encoding is kept the same, i.e. the \`Word\` is still built up with
the domain encoded in the most significant bits and the integer-part in
the remaining bits. However, instead encoding the domain as a \`Char\`
in the (internal *and* external interface), we now create an ADT
(sum-type) that encodes the domain. This has two advantages. First, it
prevents people from picking domain-tags ad hoc an possibly overlapping.
Second, encoding in the \`Word\` does not rely on the assumption that
the domain requires and/or fits in 8 bits. Since Haskell \`Char\`s are
unicode, the 8-bit assumption is wrong for the old design. In other
words, the above examples are changed to:

Ideal world scenario, the entire external interface would be: and the
instances for \`Eq\`, \`Ord\`, \`Data\`, etc. For now, though, it will
also have

&gt; &gt; **SLPJ** I agree that a \`newtype\` around a \`Word\` is
better than a \`data\` type around \`Int\#\`. That is a small, simple
change. But I think you plan to do more than this, and that "more" is
not documented here. E.g. what is the new API to \`Unique\`? &gt;
**PKFH** Added. See above.

Unpacking primitive fields
==========================

This page describes a proposal to automatically unpack (strict)
primitive fields. A primitive fields is a field that when unpacked has a
pointer-sized representation. Examples include \`Int\`, \`Word\`,
\`Float\`, and \`newtype\`s thereof.

Goals and non-goals
-------------------

This proposal is about changing the default behavior of GHC, not
changing expressiveness. Users can still use \`UNPACK\` and \`NOUNPACK\`
to explicitly control the memory representation of fields.

There are two goals:

`1.Reducetheamountofboilerplateexperiencedprogrammershavetowrite:AsofFeb18th2012,the`[`bytestring`](http://hackage.haskell.org/package/bytestring)`,`[`text`](http://hackage.haskell.org/package/text)`,and`[`containers`](http://hackage.haskell.org/package/containers)`` packageshad46fieldsthatmatchedthedefinitionofprimitivegivenabove.43ofthesehadanexplicit`UNPACK`pragma(andtheremaining3couldhavehadonewithoutchangingtheperformanceoftheprogram.) ``

`` 2.ToprovidebetterdefaultsforbeginnerandintermediatelevelHaskellers.Notunpackinge.g.`Int`fieldscanhavealarge,negativeeffectonperformanceandmanybeginnerandintermediatelevelHaskellersarebittenbythis. ``

Detailed design
---------------

Benchmarks
----------

Unused imports
==============

GHC has a series of bugs related to the "report unused imports" flags,
including \#1148, \#2267, \#1074, \#2436, \#10117.

This page describes a new design.

The current story
-----------------

Currently (GHC 6.10) we report three different things:

`*warnUnusedModules:importM,wherenothingisusedfromM`\
`*warnUnusedImports:importM(f),wherefisunused,andMdoesn'tfallunderwarnUnusedModules`\
`*warnDuplicateImports:importM+importM(f),evenwhenfisusedcomplainaboutduplicateimportoff`

Examples
--------

The hard bit is to specify what the warning should do. Consider these
examples, where \`Foo\` exports \`x\` and \`y\`, and \`FooPlus\`
re-exports all of \`Foo\`, plus \`z\`: Which import is redudant, in each
case?

Also: we might warn if you import the same module more than once, and
the imports can be combined (ie they have the same 'qualified' and 'as'
attributes) Here both are used, but we might want to suggest combining
them.

Specfication
------------

We can at least agree on this:

`*Ifthewarningsuggeststhatanimportcanbeomitted,andyouomitit,`\
`theprogramshouldstillcompile.`\
`*It'snotworthtryingtobetoosubtle.The90%caseisverysimple.`

Say that an *import-item* is either an entire import-all decl (eg
\`import Foo\`), or a particular item in an import list (eg \`import
Foo( ..., x, ...)\`). The general idea is that for each use of an
imported name, we will attribute that use to one (or possibly more)
import-items. Then, any import items with no uses attributed to them are
unused, and are warned about. More precisely:

`` 1.Forevery`RdrName`intheprogramtext,findalltheimport-itemsthatbroughtitintoscope.Thelookupmechanismon`RdrNames`alreadytakesaccountofwhetherthe`RdrName`wasqualified,andwhichimportshavetherightqualificationetc,sothisstepisveryeasy. ``

`2.Chooseoneofthese,the"chosenimport-item",andmarkit"used".`

`3.Nowbleataboutanyimport-itemsthatareunused.Foradecl`\
`` `importFoo(x,y)`,ifboththe`x`and`y`itemsareunused,it'dbebetter ``\
`tobleantabouttheentiredeclratherthantheindividualitems.`

The import-item choosing step 2 implies that there is a total order on
import-items. We say import-item A \`\`dominates\`\` import-item B if we
chooose A over B. Here is one possible dominance relationship:

`` *`importFoo`dominates`importFoo(x)`.(Youcouldalsoarguethatthe ``\
`reverseshouldhold.)`\
`*Otherwisechoosethetextuallyfirstone.`

Other notes:

`*Thealgorithmchoosesexactlyoneimport-iteminstep2.Itwould`\
`alsobesoundtochoosemorethanoneiftherewasatie,butthencompletely-duplicate`\
`importsmightnotbereported.`

`` *Notethatifwehaveanimportitem`importFoo(Bar(bar))`,then ``\
`` it'smarkedasusedifeither`Bar`or`bar`areused.Wecouldhaveyetfiner ``\
`resolutionandreportevenunusedsub-items.`

`` *Weshouldretainthespecialcaseofnotwarningabout`importFoo()`,whichimplies"instancedeclarationsonly". ``

------------------------------------------------------------------------

Implementation
--------------

We want to collect the set of all \`RdrNames\` that are mentioned in the
program. We must collect **\`RdrNames\`** not \`Names\`: Here both
imports are required, but you can only tell that by seeing the RdrNames,
not by knowing that the name 'x' is used.

I think that all lookups go through either, \`RnEnv.lookupGreRn\_maybe\`
or \`RnEnv.lookup\_sub\_bndr\`. So in \`RnEnv.lookupGreRn\_maybe\`, if
\`(gre\_prov gre)\` is \`(Imported \_)\`, and in
\`RnEnv.lookup\_sub\_bndr\`, put \`rdr\_name\` in a new in \`TcGblEnv\`.
All the \`tcg\_used\_rdrnames\` are in scope; if not, we report an error
and do not add it to \`tcg\_used\_rdrnames\`.

Other notes

`` *Anyparticular(in-scope)used`RdrName`isboughtintoscopeby ``\
`` oneormore`RdrName.ImportSpec`'s.Youcanfindthese`ImportSpecs` ``\
`intheGREreturnedbythelookup.`

`` *Theunitof"unusedimport"reportingisoneofthese`ImportSpecs`. ``

`` *Supposethat'rn'isaused,imported`RdrName`,and'iss'is ``\
`` the`[ImportSpecs]`thatbroughtitintoscope.Then,toafirst ``\
`approximationalltheissarecounted'used'.`

`` *Wecancompare`ImportSpecs`forequalitybytheir`SrcSpans` ``

`` *In`TcRnDriver.tcRnImports`,saveimport_declsinanew ``\
`` `tcg_rn_rdr_imports::Maybe[LImportDeclRdrName]` ``\
`` in`TcGblEnv` ``

------------------------------------------------------------------------

Algorithm
---------

The algorithm for deciding which imports have been used is based around
this datatype:

We convert import declarations into trees of \`ImportInfo\`s, e.g.
becomes (only the \`SDoc\` and \`\[RdrName\]\` fields are given, as
that's the interesting bit) If a node in the tree is marked as used,
then so are all nodes above it. For example, given the tree a use of
\`"D"\` marks both the first and third lines as used.

When we come to giving warnings, if a node is unused then we warn about
it, and do not descend into the rest of that subtree, as the node we
warn about subsumes its children. If the node is marked as used then we
descend, looking to see if any of its children are unused.

Here are how some example imports map to trees of \`ImportInfo\`,
assuming \`Foo\` exports \`a\`, \`b\`, \`D(c1, c2)\`.

These trees are built by \`RnNames.mkImportInfo\`. In
\`RnNames.warnUnusedImportDecls\` we make two lists of \`ImportInfo\`s;
one list contains all the explicit imports, e.g. and the other contains
the implicit imports, e.g.

Then \`RnNames.markUsages\` is called for each \`RdrName\` that was used
in the program. The current implementation marks all explicit import as
used unless there are no such imports, in which case it marks all
implicit imports as used. A small tweak to \`markUsages\` would allow it
to mark only the first import it finds as used.

As well as the \`RdrName\`s used in the source, we also need to mark as
used the names that are exported. We first call
\`RnNames.expandExports\` to expand \`D(..)\` into \`D(c1, c2)\`, and
then call \`RnNames.markExportUsages\`. Normally this just marks the
\`RdrName\`s as used in the same way that uses in the module body are
handled, but it is also possible for an entire module to be "used", if
\`module Foo\` is in the export list. In this case
\`RnNames.markModuleUsed\` does the hard work, marking every module
imported with that name as used.

Updates
=======

Source files:
[GhcFile(rts/Updates.h)](GhcFile(rts/Updates.h) "wikilink"),
[GhcFile(rts/Updates.cmm)](GhcFile(rts/Updates.cmm) "wikilink")

------------------------------------------------------------------------

CategoryStub

The user manual
===============

GHC's user manual contains documentation intended for users of GHC. They
are not interested in how GHC works; they just want to use it.

The user manual is held in
[GhcFile(docs/user\_guide)](GhcFile(docs/user_guide) "wikilink"), and is
written in !ReStructuredText format (\`.rst\` files). This allows us to
typeset it as HTML pages, or as LaTeX.

See also the \[wiki:Building/Docs notes on building the documentation\].

See the "Care and feeding of your GHC User's Guide" section for
conventions and a basic introduction to ReST.

GHC Boot Library Version History
================================

This table lists the versions of GHC against those of its boot
libraries, including most notably the \`base\` library. This may be
useful if you ever want to find out which version of the \`base\`
package was bundled with which version of GHC or vice versa.

See also: LanguagePragmaHistory, which lists the language extensions
added and/or removed in each GHC version.

|| ||= **HEAD** =||= **7.10.3** =||= **7.10.2** =||= **7.10.1** =||=
**7.8.4** =||= **7.8.3** =||= **7.8.2** =||= **7.8.1** =||= **7.6.3**
=||= **7.6.2** =||= **7.6.1** =||= **7.4.2** =||= **7.4.1** =||=
**7.2.2** =||= **7.2.1** =||= **7.0.4** =||= **7.0.3** =||= **7.0.2**
=||= **7.0.1** =|| ||=\`Cabal\` =|| 1.23.0.0 || 1.22.5.0 || 1.22.4.0 ||
1.22.2.0 || 1.18.1.5 |||||| 1.18.1.3 |||||| 1.16.0 |||| 1.14.0 ||||
1.12.0 || 1.10.2.0 |||| 1.10.1.0 || 1.10.0.0 || ||=\`Win32\` =||||||||
2.3.1.0 |||||||| 2.3.0.2 |||||| 2.3.0.0 |||| 2.2.2.0 |||| 2.2.1.0
|||||||| 2.2.0.2 || ||=\`array\` =|||||||| 0.5.1.0 |||||||| 0.5.0.0
|||||| 0.4.0.1 |||| 0.4.0.0 |||| 0.3.0.3 |||||||| 0.3.0.2 || ||=\`base\`
=|| 4.9.0.0 || 4.8.2.0 || 4.8.1.0 || 4.8.0.0 || 4.7.0.2 || 4.7.0.1 ||||
4.7.0.0 |||| 4.6.0.1 || 4.6.0.0 || 4.5.1.0 || 4.5.0.0 || 4.4.1.0 ||
4.4.0.0 |||||| 4.3.1.0 || 4.3.0.0 || ||=\`bin-package-db\` =|| *none*
|||||||||||||||||||||||||||||||||||| 0.0.0.0 || ||=\`binary\` =||
0.8.0.0 |||| 0.7.5.0 || 0.7.3.0 |||||||| 0.7.1.0 |||||| 0.5.1.1 ||||
0.5.1.0 |||| 0.5.0.2\* |||||||| *none* || ||=\`bytestring\` =|| 0.10.7.0
|||||| 0.10.6.0 |||||||| 0.10.4.0 |||| 0.10.0.2 || 0.10.0.0 |||| 0.9.2.1
|||| 0.9.2.0 |||||| 0.9.1.10 || 0.9.1.8 || ||=\`containers\` =|| 0.5.7.1
|||||| 0.5.6.2 |||||||| 0.5.5.1 |||||| 0.5.0.0 |||| 0.4.2.1 |||| 0.4.1.0
|||||||| 0.4.0.0 || ||=\`deepseq\` =|| 1.4.2.0 |||||| 1.4.1.1 ||||||||
1.3.0.2 |||||| 1.3.0.1 |||| 1.3.0.0 |||||||||||| *none* ||
||=\`directory\` =|| 1.2.5.0 |||||| 1.2.2.0 |||||||| 1.2.1.0 ||||
1.2.0.1 || 1.2.0.0 |||| 1.1.0.2 |||| 1.1.0.1 |||||||| 1.1.0.0 ||
||=\`extensible-exceptions\` =|||||||||||||||||||||| *none* |||| 0.1.1.4
|||| 0.1.1.3 |||||||| 0.1.1.2 || ||=\`ffi\` =||||||||||||||||||||||||||
*none* |||||||||||| 1.0 || ||=\`filepath\` =|| 1.4.1.0 |||||| 1.4.0.0
|||||||| 1.3.0.2 |||||| 1.3.0.1 |||| 1.3.0.0 |||| 1.2.0.1 ||||||||
1.2.0.0 || ||=\`ghc\` =|| 7.11.20151220\* || 7.10.3\* || 7.10.2\* ||
7.10.1\* || 7.8.4\* || 7.8.3\* || 7.8.2\* || 7.8.1\* || 7.6.3\* ||
7.6.2\* || 7.6.1\* || 7.4.2\* || 7.4.1\* || 7.2.2\* || 7.2.1\* ||
7.0.4\* || 7.0.3\* || 7.0.2\* || 7.0.1\* || ||=\`ghc-binary\`
=|||||||||||||||||||||||||||||| *none* |||||||| 0.5.0.2\* ||
||=\`ghc-boot\` =|| 0.0.0.0 |||||||||||||||||||||||||||||||||||| *none*
|| ||=\`ghc-prim\` =|| 0.5.0.0 |||||| 0.4.0.0 |||||||| 0.3.1.0 ||||||
0.3.0.0 |||||||||||||||| 0.2.0.0 || ||=\`ghci\` =|| 0
|||||||||||||||||||||||||||||||||||| *none* || ||=\`haskeline\` =||
0.7.2.2 |||||| 0.7.2.1 |||| 0.7.1.2 |||||||||||||||||||||||||| *none* ||
||=\`haskell2010\` =|||||||| *none* |||||||| 1.1.2.0\* |||||| 1.1.1.0\*
|||| 1.1.0.1\* |||| 1.1.0.0\* |||||||| 1.0.0.0\* || ||=\`haskell98\`
=|||||||| *none* |||||||| 2.0.0.3\* |||||| 2.0.0.2\* |||| 2.0.0.1\* ||||
2.0.0.0\* |||||| 1.1.0.1 || 1.1.0.0 || ||=\`hoopl\` =|| 3.10.2.0 ||||||
3.10.0.2 |||||||| 3.10.0.1 |||||| 3.9.0.0 |||| 3.8.7.3 |||| 3.8.7.1
|||||||| *none* || ||=\`hpc\` =|||||||| 0.6.0.2 |||||||| 0.6.0.1 ||||||
0.6.0.0 |||| 0.5.1.1 |||| 0.5.1.0 |||||||| 0.5.0.6 || ||=\`integer-gmp\`
=|||||||| 1.0.0.0 |||||||| 0.5.1.0 |||||| 0.5.0.0 |||| 0.4.0.0 ||||
0.3.0.0 |||||| 0.2.0.3 || 0.2.0.2 || ||=\`old-locale\` =|||||||| *none*
|||||||| 1.0.0.6 |||||| 1.0.0.5 |||| 1.0.0.4 |||| 1.0.0.3 ||||||||
1.0.0.2 || ||=\`old-time\` =|||||||| *none* |||||||| 1.1.0.2 ||||||
1.1.0.1 |||| 1.1.0.0 |||| 1.0.0.7 |||||||| 1.0.0.6 || ||=\`pretty\` =||
1.1.3.2 |||||| 1.1.2.0 |||||||| 1.1.1.1 |||||||||| 1.1.1.0 |||| 1.1.0.0
|||||||| 1.0.1.2 || ||=\`process\` =|| 1.4.1.0 |||||| 1.2.3.0 ||||||||
1.2.0.0 |||||| 1.1.0.2 |||| 1.1.0.1 |||| 1.1.0.0 |||||| 1.0.1.5 ||
1.0.1.4 || ||=\`random\` =|||||||||||||||||||||||||||||| *none* ||||||||
1.0.0.3 || ||=\`rts\` =|||||||||||||||||||||||||||||||||||||| 1.0 ||
||=\`template-haskell\` =|| 2.11.0.0 |||||| 2.10.0.0 |||||||| 2.9.0.0
|||||| 2.8.0.0 |||| 2.7.0.0 |||| 2.6.0.0 |||||||| 2.5.0.0 ||
||=\`terminfo\` =|| 0.4.0.2 |||||| 0.4.0.1 |||| 0.4.0.0
|||||||||||||||||||||||||| *none* || ||=\`time\` =|| 1.6 |||||| 1.5.0.1
|||||||| 1.4.2 |||||| 1.4.0.1 |||| 1.4 |||| 1.2.0.5 |||||||| 1.2.0.3 ||
||=\`transformers\` =|| 0.5.0.0 |||||| 0.4.2.0 |||||||| 0.3.0.0
|||||||||||||||||||||| *none* || ||=\`unix\` =|| 2.7.1.1 |||||| 2.7.1.0
|||||||| 2.7.0.1 |||| 2.6.0.1 || 2.6.0.0 || 2.5.1.1 || 2.5.1.0 ||||
2.5.0.0 |||||| 2.4.2.0 || 2.4.1.0 || ||=\`xhtml\` =|||||||||||| 3000.2.1
|||||||||||||||||||||||||| *none* || || ||= **HEAD** =||= **7.10.3**
=||= **7.10.2** =||= **7.10.1** =||= **7.8.4** =||= **7.8.3** =||=
**7.8.2** =||= **7.8.1** =||= **7.6.3** =||= **7.6.2** =||= **7.6.1**
=||= **7.4.2** =||= **7.4.1** =||= **7.2.2** =||= **7.2.1** =||=
**7.0.4** =||= **7.0.3** =||= **7.0.2** =||= **7.0.1** =||

Note: A \`\*\` after the version number denotes the package being hidden
by default.

A table covering some GHC 6.\* releases can be found at
<https://wiki.haskell.org/Libraries_released_with_GHC>

= Warnings and Deprecations

For now, see the relevant [GHC User's Guide
Section](http://downloads.haskell.org/~ghc/latest/docs/html/users_guide/pragmas.html#warning-deprecated-pragma)
describing the \`DEPRECATE\` and \`WARNING\` pragmas.

TODO

GHC Commentary: Weak Pointers and Finalizers
============================================

------------------------------------------------------------------------

CategoryStub [PageOutline](PageOutline "wikilink")

Work in Progress on the LLVM Backend
====================================

This page is meant to collect together information about people working
on (or interested in working on) LLVM in GHC, and the projects they are
looking at. See also the \[wiki:Commentary/Compiler/NewCodeGen state of
play of the whole back end\]. This is more a page of ideas for
improvements to the LLVM backend and less so an indication of actual
work going on.

### LLVM IR Representation

The LLVM IR is modeled in GHC using an algebraic data type to represent
the first order abstract syntax of the LLVM assembly code. The LLVM
representation lives in the 'Llvm' subdirectory and also contains code
for pretty printing. This is the same approach taken by EHC's LLVM
Back-end, and we adapted the module developed by them for this purpose.

The current design is overly complicated and could be faster. It uses
String + show operations for printing for example when it should be
using !FastString + Outputable. Before simplifying this design though it
would be good to investigate using the LLVM API instead of the assembly
language for interacting with LLVM. This would be done most likely by
using the pre-existing Haskell LLVM API bindings found
[here](http://hackage.haskell.org/package/llvm). This should hopefully
provide a speed up in compilation speeds which is greatly needed since
the LLVM back-end is \~2x slower at the moment.

### TABLES\_NEXT\_TO\_CODE

We now support
\[wiki:Commentary/Compiler/Backends/LLVM/Issues\#TABLES\_NEXT\_TO\_CODE
TNTC\] using an approach of gnu as subsections. This seems to work fine
but we would like still to move to a pure LLVM solution. Ideally we
would implement this in LLVM by allowing a global variable to be
associated with a function, so that LLVM is aware that the two will be
laid out next to each other and can better optimise (e.g using this
approach LLVM should be able to perform constant propagation on
info-tables).

**Update (30/06/2010):** The current TNTC solution doesn't work on Mac
OS X. So we need to implement an LLVM based solution. We currently
support OS X by post processing the assembly. Pure LLVM is a nicer way
forward.

### LLVM Alias Analysis Pass

**Update: This has been implemented, needs more work though**

LLVM doesn't seem to do a very good job of figuring out what can alias
what in the code generated by GHC. We should write our own alias
analysis pass to fix this.

### Optimise LLVM for the type of Code GHC produces

At the moment only a some fairly basic benchmarking has been done of the
LLVM back-end. Enough to give an indication of how it performs on the
whole (well as far as you trust benchmarks anyway) and of what it can
sometimes achieve. However this is by no means exauhstive or probably
even close to it and doesn't give us enough information about the areas
where LLVM performs badly. The LLVM optimisation pass also at the moment
just uses the standard '-O\[123\]' levels, which like GCC entail a whole
bunch of optimisation passes. These groups are designed for C programs
mostly.

So:

`*Morebenchmarking,particularlyfindingsomebadspotsfortheLLVMback-endandgeneratingagoodpictureofthecharacteristicsoftheback-end.`\
`*LookintotheLLVMoptimiser,e.gperhapssomemoreworkinthestyleof`[`Don's`
`work`](http://donsbot.wordpress.com/2010/03/01/evolving-faster-haskell-programs-now-with-llvm/)\
`*LookatanynewoptimisationpassesthatcouldbewrittenforLLVMwhichwouldhelptoimprovethecodeitgeneratesforGHC.`\
`*Lookatgeneralfixes/improvementtoLLVMtoimprovethecodeitgeneratesforLLVM.`\
`*SometimesthereisabenefitfromrunningtheLLVMoptimisertwiceofthecode(e.gopt-O3|opt-O3...).WeshouldaddacommandlineflagthatallowsyoutospecifythenumberofiterationsyouwanttheLLVMoptimisertodo.`

### Update the Back-end to use the new Cmm data types / New Code Generator

There is ongoing work to produce a new, nicer, more modular code
generator for GHC (the slightly confusingly name code generator in GHC
refers to the pipeline stage where the Core IR is compiled to the Cmm
IR). The LLVM back-end could be updated to make sure it works with the
new code generator and does so in an efficient manner.

### LLVM's Link Time Optimisations

One of LLVM's big marketing features is its support for link time
optimisation. This does things such as in-lining across module
boundaries, more aggressive dead code elimination... etc). The LLVM
back-end could be updated to make use of this. Roman apparently tried to
use the new 'gold' linker with GHC and it doesn't support all the needed
features.

`*`[`22`](http://llvm.org/releases/2.6/docs/LinkTimeOptimization.html)\
`*`[`23`](http://llvm.org/docs/GoldPlugin.html)

### LLVM Cross Compiler / Port

This is more of an experimental idea but the LLVM back-end looks like it
would make a great choice for Porting LLVM. That is, instead of porting
LLVM through the usual route of via-C and then fixing up the NCG, just
try to do it all through the LLVM back-end. As LLVM is quite portable
and supported on more platforms then GHC, it would be an interesting and
valuable experiment to try to port GHC to a new platform by simply
getting the LLVM back-end working on it. (The LLVM back-end works in
both unregistered and registered mode, another advantage for porting
compared to the C and NCG back-ends).

It would also be interesting to looking into improving GHC to support
cross compiling and doing this through the LLVM back-end as it should be
easier to fix up to support this feature than the C or NCG back-ends.

### Get rid of Proc Point Splitting

When Cmm code is first generated a single Haskell function will be
mostly compiled to one Cmm function. This Cmm function isn't passed to
the backends though as the CPS style used in it requires that the
backends be able to take the address of labels in a function since
they're used as return points. The C backend can't support this. While
there is a GNU C extension allowing the address of a label to be taken,
the address can only be used locally (in the same function). So what
proc point splitting does is cut a single Cmm function into multiple top
level Cmm functions so that instead of needing to take the address of a
label, we now take the address of a function.

It would be nice to get rid of proc point splitting. This is one of the
goals for the new code generator. This will give us much bigger Cmm
functions which should give more room for LLVM to optimise. There is an
issue though that LLVM doesn't support taking the address of a local
label either. So will need to add support to LLVM for taking label
addresses or convert CPS style into something more direct if thats
possible.

### Don't Pass Around Dead STG Registers

**Update: This has been implemented**

At the moment in the LLVM backend we always pass around the pinned STG
registers as arguments for every Cmm function. A huge amount of the time
though we aren't storing anything in the STG registers, they are dead
really. If we can treat the correctly as dead then LLVM will have more
free registers and the allocator should do a better job. We need to
change the STG -&gt; Cmm code generator to attach register liveness
information at function exit points (e.g calls, jumps, returns).

e.g This [bug (\#4308)](http://hackage.haskell.org/trac/ghc/ticket/4308)
is as a result of this problem.

[PageOutline](PageOutline "wikilink")

Wired-in and known-key things
=============================

There are three categories of entities that GHC "knows about"; that is,
information about them is baked into GHC's source code.

`*[wiki:Commentary/Compiler/WiredIn#WiredinthingsWired-inthings]---GHCknowseverythingaboutthese`\
`*[wiki:Commentary/Compiler/WiredIn#KnownkeythingsKnown-keythings]---GHCknowsthe`*`name`*`,includingthe``,butnotthedefinition`\
`*[wiki:Commentary/Compiler/WiredIn#OrigRdrNamethingsOrigRdrNamethings]---GHCknowswhichmoduleit'sdefinedin`

Wired-in things
---------------

A **Wired-in thing** is fully known to GHC. Most of these are \`TyCon\`s
such as \`Bool\`. It is very convenient to simply be able to refer to
\`boolTyCon :: TyCon\` without having to look it up in an environment.

All \[wiki:Commentary/Compiler/TypeType\#Classifyingtypes primitive
types\] are wired-in things, and have wired-in \`Name\`s. The primitive
types (and their \`Names\`) are all defined in
[GhcFile(compiler/prelude/TysPrim.hs)](GhcFile(compiler/prelude/TysPrim.hs) "wikilink").

The non-primitive wired-in type constructors are defined in
[GhcFile(compiler/prelude/TysWiredIn.hs)](GhcFile(compiler/prelude/TysWiredIn.hs) "wikilink").
There are a handful of wired-in \`Id\`s in
[GhcFile(compiler/basicTypes/MkId.hs)](GhcFile(compiler/basicTypes/MkId.hs) "wikilink").
There are no wired-in classes (they are too complicated).

All the non-primitive wired-in things are *also* defined in GHC's
libraries, because even though GHC knows about them we still need to
generate code for them. For example, \`Bool\` is a wired-in type
constructor, but it is still defined in \`GHC.Base\` because we need the
info table etc for the data constructors. Arbitrarily bad things will
happen if the wired-in definition in
[GhcFile(compiler/prelude/TysWiredIn.hs)](GhcFile(compiler/prelude/TysWiredIn.hs) "wikilink")
differs from that in the library module.

All wired-in things have a \`WiredIn\` \`Name\` (see
\[wiki:Commentary/Compiler/NameType Names\]), which in turn contains the
thing. See \[wiki:Commentary/Compiler/CaseStudies/Bool a case study of
Bool implementation\] for more details.

Known-key things
----------------

A **known-key thing** has a fixed, pre-allocated \`Unique\` or **key**.
They should really be called "known-Name" things, because the baked-in
knowledge is:

`` *Itsdefining`Module` ``\
`` *Its`OccName` ``\
`` *Its`Unique` ``

Almost all known-key names are defined in
[GhcFile(compiler/prelude/PrelNames.hs)](GhcFile(compiler/prelude/PrelNames.hs) "wikilink");
for example: .

The point about known-key things is that GHC knows its *name*, but not
its *definition*. The definition must still be read from an interface
file as usual. The known key just allows an efficient lookup in the
environment.

Initialisation
--------------

When reading an interface file, GHC might come across "GHC.Base.Eq",
which is the name of the \`Eq\` class. How does it match up this
occurrence in the interface file with \`eqClassName\` defined in
\`PrelNames\`? Because the global name cache maintained by the renamer
is initialised with all the known-key names. This is done by the
(hard-to-find) function \`HscMain.newHscEnv\`: Notice that the
initialisation embraces both the wired-in and ("basic") known-key names.

\`Orig\` \`RdrName\` things
---------------------------

An **Orig !RdrName thing** has a top-level definition of a \`RdrName\`,
using the \`Orig\` constructor. Here, the baked-in information is:

`` *Itsdefining`Module` ``\
`` *Its`OccName` ``

Again, almost all of these are in
[GhcFile(compiler/prelude/PrelNames.hs)](GhcFile(compiler/prelude/PrelNames.hs) "wikilink").
Example: .

GHC Commentary: The Word
========================

The most important type in the runtime is , defined in
[GhcFile(includes/stg/Types.h)](GhcFile(includes/stg/Types.h) "wikilink").
A word is defined to be the same size as a pointer on the current
platform. All these types are interconvertible without losing
information, and have the same size (as reported by ):

`::`\
`Anunsigedintegraltypeofwordsize`

`::`\
`Asignedintegraltypeofwordsize`

`::`\
`Pointerto`

The word is the basic unit of allocation in GHC: the heap and stack are
both allocated in units of a word. Throughout the runtime we often use
sizes that are in units of words, so as to abstract away from the real
word size of the underlying architecture.

The \`StgWord\` type is also useful for storing the *size* of a memory
object, since an \`StgWord\` is guaranteed to at least span the range of
addressable memory. It is rather like \`size\_t\` in this respect,
although we prefer to use \`StgWord\` in the RTS sources.

C-- only understands units of bytes, so we have various macros in
[GhcFile(includes/Cmm.h)](GhcFile(includes/Cmm.h) "wikilink") to make
manipulating things in units of words easier in files.
